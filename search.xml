<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>AWS Lambda</title>
      <link href="/AWS-Lambda/"/>
      <url>/AWS-Lambda/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><p>A compute service that lets you run code without provisioning or managing servers. It can executes your code thousands per second. You pay for the compute time you consume.</p><p>Lambda executes your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, code monitoring and logging.</p><p>You can run AWS lambda to run your code in response to events, such as changes to data in an Amazon S3 bucket or an Amazon DynamoDB table; to run your code in response to HTTP requests using Amazon API gateway; or invoke your code using API calls made using AWS SDKs. </p><h1 id="2-Getting-started"><a href="#2-Getting-started" class="headerlink" title="2. Getting started"></a>2. Getting started</h1><h2 id="2-1-Configuration-on-console-page"><a href="#2-1-Configuration-on-console-page" class="headerlink" title="2.1 Configuration on console page"></a>2.1 Configuration on console page</h2><ul><li>environment variables: for Lambda functions enable you to dynamically pass settings to your function code and libraries, without making changes to your code.</li><li>Tags: are key-value pairs that you attach to AWS resources to better organize them. </li><li>Execution role: which allows you to administer security on your function, using defined roles and policies or creating new ones.</li><li>Basic settings: allows you to dictate the memory allocation and timeout limit for your Lambda function.</li><li>Network: allow you to select a VPC your function will access</li><li>Debugging and error handling: allow you to select a AWS <a href="https://docs.aws.amazon.com/lambda/latest/dg/dlq.html" target="_blank" rel="noopener">Lambda Function Dead Letter Queues</a> resource to analyze failed function invocation retries. </li><li>Concurrency: allows you to allocate a specific limit of concurrent executions allowed for this function</li><li>Auditing and compliance: logs function invocations for operational and risk auditing, governance and compliance. </li></ul><h2 id="2-2-Lambda-concept"><a href="#2-2-Lambda-concept" class="headerlink" title="2.2 Lambda concept"></a>2.2 Lambda concept</h2><p>Lambda can automatically scales up the number of instances of your function to handle high number of events. </p><ul><li>Function: A script or program that runs in AWS lambda. Lambda passes invocation events to your function. The function processes an event and returns a response</li><li>Runtimes: Lambda runtimes allow functions in different languages to run in the same base execution environment, you configure your function to use a runtime that matches your programming language. The runtime sits in between the Lambda service and your function code, relaying invocation events, context information and responses between the two. </li><li>Layers: Lambda layers are a distribution mechanism for libraries, custom runtimes and other function dependencies. Layers let you manage your in-development function code independently from the unchanging code and resources that it uses. You can configure your function to use layers that you create, layers provided by AWS, or layers from other AWS customers.</li><li>Event source: An AWS service that triggers your function and executes its logic. </li><li>Downstream resources: An AWS service that your lambda function calls once it is triggered</li><li>Log streams: While Lambda automatically monitors your function invocations and reports metrics to CloudWatch, you can annotate your function code with custom logging statements that allow you to analyze the execution flow and performance of your Lambda function to ensure it’s working properly.</li></ul><h2 id="2-3-Programming-Model"><a href="#2-3-Programming-Model" class="headerlink" title="2.3 Programming Model"></a>2.3 Programming Model</h2><p>Regardless of the language you choose, there is a common pattern ti write code for a Lambda Function includes concepts as follow:</p><h3 id="2-3-1-Handler"><a href="#2-3-1-Handler" class="headerlink" title="2.3.1 Handler"></a>2.3.1 Handler</h3><p>The function AWS Lambda calls to start execution of your lambda function. When a Lambda function is invoked, Lambda starts executing your code by calling the handler function</p><h3 id="2-3-2-Context"><a href="#2-3-2-Context" class="headerlink" title="2.3.2 Context"></a>2.3.2 Context</h3><p>Lambda also passes a context object to the handler function, as the second parameter. Via this context object your code can interact with Lambda</p><h3 id="2-3-3-Logging"><a href="#2-3-3-Logging" class="headerlink" title="2.3.3 Logging"></a>2.3.3 Logging</h3><p>Function can contain logging statements, it writes these logs to cloudWatch logs. </p><h3 id="2-3-4-Exceptions"><a href="#2-3-4-Exceptions" class="headerlink" title="2.3.4 Exceptions"></a>2.3.4 Exceptions</h3><p>Function needs to communicate the result of the function execution to AWS Lambda. Many ways to end a request or to notify AWS Lambda an error occured during execution</p><h3 id="2-3-5-Concurrency"><a href="#2-3-5-Concurrency" class="headerlink" title="2.3.5 Concurrency"></a>2.3.5 Concurrency</h3><p>When your function is invoked more quickly than a single instance of your function can process events, Lambda scales by running additional instances. Each instance of your function handles only one request at a time, so you don’t need to worry about synchronizing threads or processes. You can, however, use asynchronous language features to process batches of events in parallel, and save data to the /tmp directory for use in future invocations on the same instance. </p><h1 id="3-Building-Lambda-Functions-with-Java"><a href="#3-Building-Lambda-Functions-with-Java" class="headerlink" title="3. Building Lambda Functions with Java"></a>3. Building Lambda Functions with Java</h1><h2 id="3-1-Handler-Input-Output-Types"><a href="#3-1-Handler-Input-Output-Types" class="headerlink" title="3.1 Handler Input/ Output Types"></a>3.1 Handler Input/ Output Types</h2><p>When AWS lambda executes the Lambda function, it invokes the handler. First parameter is the input to the handler which can be event data (published by an event source) or custom input you provide such as a string or any custom data object. </p><p>AWS Lambda supports the following input/ output types for a handler: </p><ul><li><p>Simple Java types (AWS Lambda supports the String, Integer, Boolean, Map and List types )</p></li><li><p>POJO</p></li><li><p>stream type </p><h3 id="3-1-1-Handler-input-output-String-Type"><a href="#3-1-1-Handler-input-output-String-Type" class="headerlink" title="3.1.1 Handler input/ output: String Type"></a>3.1.1 Handler input/ output: String Type</h3><p>  package example;</p><p>  import com.amazonaws.services.lambda.runtime.Context; </p><p>  public class Hello {</p><pre><code>public String myHandler(String name, Context context) {    return String.format(&quot;Hello %s.&quot;, name);}</code></pre><p>  }</p></li></ul><p>When you invoke a Lambda function asynchronously, any return value by your Lambda function will be ignored. Therefore you might want to <strong><em>set the return type to void</em></strong> to make this clear in your code </p><h3 id="3-1-2-Handler-Input-Output-POJO"><a href="#3-1-2-Handler-Input-Output-POJO" class="headerlink" title="3.1.2 Handler Input/ Output: POJO"></a>3.1.2 Handler Input/ Output: POJO</h3><pre><code>package example;import com.amazonaws.services.lambda.runtime.Context; public class HelloPojo {    // Define two classes/POJOs for use with Lambda function.    public static class RequestClass {      ...    }    public static class ResponseClass {      ...    }    public static ResponseClass myHandler(RequestClass request, Context context) {        String greetingString = String.format(&quot;Hello %s, %s.&quot;, request.getFirstName(), request.getLastName());        return new ResponseClass(greetingString);    }}</code></pre><p>Suppose your application events generate data that includes first name and last name as shown:</p><pre><code>{ &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Doe&quot; }  </code></pre><p>For this example, the handler receives this JSON and returns the string “Hello John Doe”.</p><pre><code>public static ResponseClass handleRequest(RequestClass request, Context context){        String greetingString = String.format(&quot;Hello %s, %s.&quot;, request.firstName, request.lastName);        return new ResponseClass(greetingString);}</code></pre><p>To create a Lambda function with this handler, you must provide implementation of the input and output types as shown in the following Java example. The HelloPojo class defines the handler method.</p><pre><code>package example;import com.amazonaws.services.lambda.runtime.Context; import com.amazonaws.services.lambda.runtime.RequestHandler;public class HelloPojo implements RequestHandler&lt;RequestClass, ResponseClass&gt;{       public ResponseClass handleRequest(RequestClass request, Context context){        String greetingString = String.format(&quot;Hello %s, %s.&quot;, request.firstName, request.lastName);        return new ResponseClass(greetingString);    }}</code></pre><p>In order to implement the input type, add the following code to a separate file and name it RequestClass.java. Place it next to the HelloPojo.java class in your directory structure:</p><pre><code>package example;     public class RequestClass {        String firstName;        String lastName;        public String getFirstName() {            return firstName;        }        public void setFirstName(String firstName) {            this.firstName = firstName;        }        public String getLastName() {            return lastName;        }        public void setLastName(String lastName) {            this.lastName = lastName;        }        public RequestClass(String firstName, String lastName) {            this.firstName = firstName;            this.lastName = lastName;        }        public RequestClass() {        }    }</code></pre><p>In order to implement the output type, add the following code to a separate file and name it ResponseClass.java. Place it next to the HelloPojo.java class in your directory structure:</p><pre><code>package example; public class ResponseClass {    String greetings;    public String getGreetings() {        return greetings;    }    public void setGreetings(String greetings) {        this.greetings = greetings;    }    public ResponseClass(String greetings) {        this.greetings = greetings;    }    public ResponseClass() {    }}</code></pre><h2 id="3-2-Context-Object-in-Java"><a href="#3-2-Context-Object-in-Java" class="headerlink" title="3.2 Context Object in Java"></a>3.2 Context Object in Java</h2><p>When Lambda runs your function, it passes a context object to the handler. The object provides methods and properties that provide information about the invocation, function and execution environment. </p><ul><li>getRemainingTimeInMillis()</li><li>getFunctionName()</li><li>getFunctionVersion()</li><li>getInvokedFunctionArn() - Returns the Amazon Resource Name(ARN) used to invoke the function. Indicates if the invoker specified a version number or alias. </li><li>getMemoryLimitInMB()</li><li>getAwsRequestId()</li><li>getLogGroupName()</li><li>getIdentity()</li><li>getClientContext()</li><li>getLogger() </li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> Lambda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS DynamoDBClient vs DynamoDBMapper</title>
      <link href="/AWS-DynamoDBClient-vs-DynamoDBMapper/"/>
      <url>/AWS-DynamoDBClient-vs-DynamoDBMapper/</url>
      
        <content type="html"><![CDATA[<p>Recently just did a project related with DynamoDB, use both DynamoDBClient and DynamoDBMapper in different circumstances. In this post, will compare those two, regarding with its convenience, latency, and their differences by nature. </p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>DynamoDBClient is Service client for accessing DynamoDB; while DynamoDBMapper use ORM (Object relational mapping) for converting data between incompatible type systems using object-oriented programming languages. </p><h2 id="1-1-How-to-use-DDBClient"><a href="#1-1-How-to-use-DDBClient" class="headerlink" title="1.1 How to use DDBClient"></a>1.1 How to use DDBClient</h2><pre><code>// Create POJO@Datapublic class Whitelist {    private String id;    private String content;    private String status;}// Create DDBClientAmazonDynamoDB ddbCLient = AmazonDynamoDBClientBuilder.standard()    .withCredentials(new AWSCredentialsProvider(KEY))    .withRegion(&quot;aaabbb&quot;)    .build();// Convert whitelist into MapMap&lt;String, AttributeValue&gt; whitelistMap = new HashMap&lt;&gt;();whitelistMap.put(&quot;id&quot;, &quot;123&quot;);whitelistMap.put(&quot;content&quot;, &quot;hello world&quot;);whitelistMap.put(&quot;status&quot;, &quot;onSale&quot;);// UpdateItem RequestUpdateItemRequest request = new UpdateItemRequest()    .withTableName(&quot;WhitelistTable&quot;)    .withItem(whitelistMap);// Use updateItem operationddbClient.updateItem(request);</code></pre><p>As you can see here, we need to build a String, AttributeValue map, which is a bit annoying. </p><h2 id="1-2-How-to-use-DynamoDBMapper"><a href="#1-2-How-to-use-DynamoDBMapper" class="headerlink" title="1.2 How to use DynamoDBMapper"></a>1.2 How to use DynamoDBMapper</h2><p>We can define the table schema when we define the POJO using DynamoDBMapper’s annotations. </p><pre><code>// Notice: lombok may not work well here, especially when you use GSI and LSI, it cannot retrive info correctly@DynamoDBTable(tableName = &quot;whitelistTable&quot;)public class Whitelist {    @DynamoDBHashKey    private String id;    @DynamoDBRangeKey    private String status;    @DynamoDBAttribute    private String content;}// Create DDBClientAmazonDynamoDB ddbCLient = AmazonDynamoDBClientBuilder.standard()    .withCredentials(new AWSCredentialsProvider(KEY))    .withRegion(&quot;aaabbb&quot;)    .build();//Create DDBMapperAmazonDynamoDBMapper ddbMapper = new AmazonDynamoDBMapper(ddbClient);// updateItemddbMapper.save(whitelistItem);</code></pre><p>Code looks much more succinct, isn’t it! As you can see above, you can think DDBMapper wraps DDBCLient in some way, and help you implement the String-AttributeValue map for you. </p><h1 id="2-Comparision"><a href="#2-Comparision" class="headerlink" title="2. Comparision"></a>2. Comparision</h1><ul><li>DynamoDBMapper <ul><li>Benefits<ul><li>Code look better</li><li>less dev work</li></ul></li><li>weakness<ul><li>it runs slower than using DDBClient<ul><li>E.G when using DDBClient::getItem, the average latency is about 2ms; whereas it comes to be around 8ms in DDBMapper.  </li></ul></li></ul></li></ul></li><li>DynamoDBClient<ul><li>Benefits<ul><li>As said above, run much faster</li></ul></li><li>weakness<ul><li>verbose code</li><li>easier for mistakes, typos </li></ul></li></ul></li></ul><p>So how to choose from those two – it depends. Suppose you have a daily job to put 100 millions items into one table and want to write faster, DDBClient should definitely be your choice in this situation. Otherwise, use DynamoDBMapper to save your life, lol. </p>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> DynamoDB </tag>
            
            <tag> Mapper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Distributed Tracing</title>
      <link href="/Distributed-Tracing/"/>
      <url>/Distributed-Tracing/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Why-we-need-it"><a href="#1-Why-we-need-it" class="headerlink" title="1. Why we need it?"></a>1. Why we need it?</h1><p>Short answer - due to disturbuted applications. </p><h2 id="1-1-Monolithis-software"><a href="#1-1-Monolithis-software" class="headerlink" title="1.1 Monolithis software"></a>1.1 Monolithis software</h2><p>Monolithis software is build upon a large and sprawling legacy code base that is often so tightly coupled that any changes in one small section often result in breaking one or several features that depend on it. In such app, high possibly it’ll break and we need to use tech - tracing to <strong>follw the course of a request or system event</strong> from its source to its ultimate destination. </p><p>In this way, each trace comes to be a narrative that tells the request’s story as it travels through system. </p><h2 id="2-2-Distributed-System"><a href="#2-2-Distributed-System" class="headerlink" title="2.2 Distributed System"></a>2.2 Distributed System</h2><p>Use distributed tracing to profile and monitor microservice-based apps/ architectures, locate failures, and improve performance. </p><h1 id="2-Key-Concepts"><a href="#2-Key-Concepts" class="headerlink" title="2. Key Concepts"></a>2. Key Concepts</h1><p>In general, distributed tracing start with a single request - the entity or event being traced. As the request makes its journey, it <strong>generates traces that record complete processing operations</strong> performed on it by entities within a distributed system/ network infrastructure. </p><p>Each trace is assigned with its own unique ID and passes through a segment that indicates a given activity that a host system performs on the request. Every segments represents a single step within the reqeust’s path and has a name, unique ID, and timestamp. A span(segment) can also carry additional metadata. </p><p>The idea is – specific request inflexion points mush be identified within a system and instrumented. All of the trace data mush be coordinated and collated to provide a meaningflow view of a request. </p><p>Challenge would be processing the volume of the data generated from increasingly large scale systems. </p><h1 id="3-Implementation"><a href="#3-Implementation" class="headerlink" title="3. Implementation"></a>3. Implementation</h1><p>Google created Dapper in the past as a middleware that supports using different language within the system. As said, the value of tracing is only realised through: </p><ul><li>ubiquitous deployment, and no parts of the system under observation are not instrumented </li><li>continuous monitoring <ul><li>system mush be monitoring constantly </li></ul></li></ul><h1 id="4-Why-we-need-distributed-tracing"><a href="#4-Why-we-need-distributed-tracing" class="headerlink" title="4. Why we need distributed tracing"></a>4. Why we need distributed tracing</h1><p>Greg Linden commented in 2006 that experiments ran by Amazon.com demonstrated a <a href="http://glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html" target="_blank" rel="noopener">significant drop in revenue</a> was experienced when 100ms delay to page load was added. Although understanding the flow of a web request through a system can be challenging, there can be significant commercial gains if performance bottlenecks are identified and eliminated.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://epsagon.com/blog/introduction-to-distributed-tracing/" target="_blank" rel="noopener">https://epsagon.com/blog/introduction-to-distributed-tracing/</a></li><li><a href="https://www.infoq.com/articles/distributed-tracing-microservices/" target="_blank" rel="noopener">https://www.infoq.com/articles/distributed-tracing-microservices/</a></li><li><a href="https://www.javacodegeeks.com/microservices-distributed-tracing.html" target="_blank" rel="noopener">https://www.javacodegeeks.com/microservices-distributed-tracing.html</a></li><li><a href="https://ai.google/research/pubs/pub36356" target="_blank" rel="noopener">https://ai.google/research/pubs/pub36356</a> </li></ol>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Distributed Tracing </tag>
            
            <tag> System Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DynamoDB-Advanced</title>
      <link href="/DynamoDB-Advanced/"/>
      <url>/DynamoDB-Advanced/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><h2 id="1-1-Features"><a href="#1-1-Features" class="headerlink" title="1.1 Features"></a>1.1 Features</h2><ul><li>Fully managed NoSQL database service </li><li>offer encryption at rest </li><li>on-demand backup capability <ul><li>allows you to create full backups of your tables for long-term retention and archival for regulatory compliance needs</li></ul></li><li>point-in-time recovery<ul><li>restore the table to any point in time during last 35 days  </li></ul></li><li>TTL <ul><li>delete expired items from tables automatically to reduce storage usage</li></ul></li></ul><h2 id="1-2-High-availability-and-durability"><a href="#1-2-High-availability-and-durability" class="headerlink" title="1.2 High availability and durability"></a>1.2 High availability and durability</h2><p>DynamoDB automatically spreads the data and traffic for your tables over a <strong>sufficient number of servers</strong>to handle your throughput and storage requirements, while maintaining consistent and fast performance. All of your data is stored on solid state disks (SSDs) and <strong>automatically replicated across multiple Availability Zones</strong> in an AWS region, providing built-in high availability and data durability. You can use <strong>global tables to keep DynamoDB tables in sync across AWS Regions</strong>. For more information, see <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html" target="_blank" rel="noopener">Global Tables</a>.</p><h1 id="2-How-it-works"><a href="#2-How-it-works" class="headerlink" title="2. How it works"></a>2. How it works</h1><h2 id="2-1-Core-Components"><a href="#2-1-Core-Components" class="headerlink" title="2.1 Core Components"></a>2.1 Core Components</h2><h3 id="2-1-1-Tables-Items-Attributes"><a href="#2-1-1-Tables-Items-Attributes" class="headerlink" title="2.1.1 Tables, Items, Attributes"></a>2.1.1 Tables, Items, Attributes</h3><blockquote><p>A table is a collection of items, each item is a collection of attributes.</p></blockquote><p>DynamoDB uses <strong>primary keys</strong> to uniquely identify each item in a table and <strong>secondary indexes</strong> to provide more querying flexibility. </p><p>Below is a diagram showing a table named People:</p><p><img src="https://i.loli.net/2020/01/29/jTZDz1AOoce26iq.png" alt="fig1.png"></p><p>Note about tables:</p><ul><li>Each item in the table has a unique identifier, or primary key, that distinguishes the item from all of the others in the table. In the People table, the primary key consists of one attribute (PersonID).</li><li>Other than the primary key, the People table is <strong>schemaless</strong>, which means that neither the attributes nor their data types need to be defined beforehand. Each item can have its own distinct attributes.</li><li>Some of the items have** a nested attribute **(Address). DynamoDB supports nested attributes up to 32 levels deep.</li></ul><h3 id="2-1-2-Primary-Key"><a href="#2-1-2-Primary-Key" class="headerlink" title="2.1.2 Primary Key"></a>2.1.2 Primary Key</h3><p>When creating tables, you must specify the primary key of the table. <strong><em>The primary key uniquely identifies each item in the table, so that no two items can have the same key</em></strong>. </p><p>DynamoDB supports two different kinds of primary keys: </p><ul><li>Partition Key </li></ul><p>DynamoDB uses the partition key’s value as input to an internal hash function. The output from the hash function determines the partition(Physical storage internal to DynamoDB) in which the item will be stored. </p><ul><li>Partition Key and sort key </li></ul><p>A composite primary key, composed of two attributes: partition key and sort key. </p><p>DynamoDB uses the partition key value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored. All items with the same partition key value are stored together, in sorted order by sort key value.</p><p>In a table that has a partition key and a sort key, it’s possible for two items to have the same partition key value. However, those two items must have different sort key values.</p><h3 id="2-1-3-Secondary-Indexes"><a href="#2-1-3-Secondary-Indexes" class="headerlink" title="2.1.3 Secondary Indexes"></a>2.1.3 Secondary Indexes</h3><p>A secondary index lets you query the data in the table using an alternate key, in addtion to queries against the primary key. </p><p>After you create a secondary index on a table, you can read data from the index in much the same way as you do from the table. </p><ul><li>Global Secondary Index</li></ul><p>An index with a partition key and sort key that can be different from those on the table </p><ul><li>Local Secondary Index</li></ul><p>An index that has the same partition key as the table, but a different sort key </p><h2 id="2-2-Limitations"><a href="#2-2-Limitations" class="headerlink" title="2.2 Limitations"></a>2.2 Limitations</h2><h3 id="2-2-1-Capacity-unit-sizes-For-Provisioned-tables"><a href="#2-2-1-Capacity-unit-sizes-For-Provisioned-tables" class="headerlink" title="2.2.1 Capacity unit sizes(For Provisioned tables)"></a>2.2.1 Capacity unit sizes(For Provisioned tables)</h3><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html" target="_blank" rel="noopener">Link is here: </a></p><p>One read capacity unit = one strongly consistent read per second, or two eventually consistent reads per second, for items up to 4 KB in size.</p><p>One write capacity unit = one write per second, for items up to 1 KB in size.</p><p>Transactional read requests require two read capacity units to perform one read per second for items up to 4 KB.</p><p>Transactional write requests require two write capacity units to perform one write per second for items up to 1 KB.</p><h3 id="2-2-2-Request-Unit-Sizes-For-On-Demand-tables"><a href="#2-2-2-Request-Unit-Sizes-For-On-Demand-tables" class="headerlink" title="2.2.2 Request Unit Sizes(For On-Demand tables)"></a>2.2.2 Request Unit Sizes(For On-Demand tables)</h3><p>One read request unit = one strongly consistent read, or two eventually consistent reads, for items up to 4 KB in size.</p><p>One write request unit = one write, for items up to 1 KB in size.</p><p>Transactional read requests require two read request units to perform one read for items up to 4 KB.</p><p>Transactional write requests require two write request units to perform one write for items up to 1 KB.</p><h3 id="2-2-3-Throughtput-Default-Limits"><a href="#2-2-3-Throughtput-Default-Limits" class="headerlink" title="2.2.3 Throughtput Default Limits"></a>2.2.3 Throughtput Default Limits</h3><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html" target="_blank" rel="noopener">See link here:</a></p><h3 id="2-2-4-Tables"><a href="#2-2-4-Tables" class="headerlink" title="2.2.4 Tables"></a>2.2.4 Tables</h3><p>Tables are unconstrained in terms of the number of items or the number of bytes. </p><p>For any AWS account, there is an initial limit of 256 tables per region.</p><p>You can define a maximum of 5 local secondary indexes.</p><p>There is an initial limit of 20 global secondary indexes per table.</p><h3 id="2-2-5-API-specific-Limits"><a href="#2-2-5-API-specific-Limits" class="headerlink" title="2.2.5 API specific Limits"></a>2.2.5 API specific Limits</h3><ul><li>CreateTable/UpdateTable/DeleteTable</li></ul><p>In general, you can have up to 50 CreateTable, UpdateTable, and DeleteTable requests running simultaneously (in any combination). In other words, the total number of tables in the CREATING, UPDATING or DELETING state cannot exceed 50.</p><ul><li>BatchGetItem</li></ul><p>A single BatchGetItem operation can retrieve a maximum of 100 items. The total size of all the items retrieved cannot exceed 16 MB.</p><ul><li>BatchWriteItem</li></ul><p>A single BatchWriteItem operation can contain up to 25 PutItem or DeleteItem requests. The total size of all the items written cannot exceed 16 MB.</p><ul><li>DescribeLimits</li></ul><p>DescribeLimits should only be called periodically. You can expect throttling errors if you call it more than once in a minute.</p><ul><li>Query</li></ul><p>The result set from a Query is limited to 1 MB per call. You can use the LastEvaluatedKey from the query response to retrieve more results.</p><ul><li>Scan</li></ul><p>The result set from a Scan is limited to 1 MB per call. You can use the LastEvaluatedKey from the scan response to retrieve more results.</p><h2 id="2-3-The-DynamoDB-API"><a href="#2-3-The-DynamoDB-API" class="headerlink" title="2.3 The DynamoDB API"></a>2.3 The DynamoDB API</h2><p>See Instructions <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.API.html" target="_blank" rel="noopener">here</a></p><h3 id="2-3-1-Control-Plane"><a href="#2-3-1-Control-Plane" class="headerlink" title="2.3.1 Control Plane"></a>2.3.1 Control Plane</h3><ul><li>CreateTable </li><li>DescribeTable </li><li>ListTables </li><li>UpdateTable </li><li>DeleteTable </li></ul><h3 id="2-3-2-Data-Plane"><a href="#2-3-2-Data-Plane" class="headerlink" title="2.3.2 Data Plane"></a>2.3.2 Data Plane</h3><ul><li>Creating Data <ul><li>PutItem </li><li>BatchWriteItem </li></ul></li><li>Reading Data <ul><li>GetItem </li><li>BatchGetItem </li><li>Query </li><li>Scan </li></ul></li><li>Updating Data <ul><li>UpdateItem </li></ul></li><li>Deleting Data <ul><li>DeleteItem </li><li>BatchWriteItem </li></ul></li><li>DynamoDB streams <ul><li>listStreams</li><li>DescribeStream </li><li>GetShardIterator: return a shard iterator, which is a data structure that your application uses to retrieve the records from the stream. </li><li>GetRecords</li></ul></li></ul><h2 id="2-4-Consistency-and-capacity"><a href="#2-4-Consistency-and-capacity" class="headerlink" title="2.4 Consistency and capacity"></a>2.4 Consistency and capacity</h2><p>Region - Availability Zones</p><p>When your application writes data to a DynamoDB table and receives an HTTP 200 response, the write has occured and is durable. The data is eventually consistent across all storage locations, usually within one second or less. </p><p>DDB supports <strong>eventually consistent and strongly consistent reads</strong>. </p><h3 id="2-4-1-Eventually-Consistent-Reads"><a href="#2-4-1-Eventually-Consistent-Reads" class="headerlink" title="2.4.1 Eventually Consistent Reads"></a>2.4.1 Eventually Consistent Reads</h3><p>When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data.</p><h3 id="2-4-2-Strongly-Consistent-Reads"><a href="#2-4-2-Strongly-Consistent-Reads" class="headerlink" title="2.4.2 Strongly Consistent Reads"></a>2.4.2 Strongly Consistent Reads</h3><p>When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful. A strongly consistent read might not be available if there is a network delay or outage. Consistent reads are not supported on global secondary indexes (GSI).</p><h3 id="2-4-3-Read-Write-Capacity-Mode-on-demand"><a href="#2-4-3-Read-Write-Capacity-Mode-on-demand" class="headerlink" title="2.4.3 Read/ Write Capacity Mode - on demand"></a>2.4.3 Read/ Write Capacity Mode - on demand</h3><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html" target="_blank" rel="noopener">See link here</a></p><p>When you choose on-demand mode, DynamoDB instantly accommodates your workloads as they ramp up or down to any previously reached traffic level. If a workload’s traffic level hits a new peak, DynamoDB adapts rapidly to accommodate the workload. Tables that use on-demand mode deliver the same single-digit millisecond latency, service-level agreement (SLA) commitment, and security that DynamoDB already offers. You can choose on-demand for both new and existing tables and you can continue using the existing DynamoDB APIs without changing code.</p><h3 id="2-4-4-Read-Write-Capacity-Mode-provisioned"><a href="#2-4-4-Read-Write-Capacity-Mode-provisioned" class="headerlink" title="2.4.4 Read/ Write Capacity Mode - provisioned"></a>2.4.4 Read/ Write Capacity Mode - provisioned</h3><p>If you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. You can use auto scaling to adjust your table’s provisioned capacity automatically in response to traffic changes. This helps you govern your DynamoDB use to stay at or below a defined request rate in order to obtain cost predictability.</p><h3 id="2-4-5-Read-Write-request-units"><a href="#2-4-5-Read-Write-request-units" class="headerlink" title="2.4.5 Read/ Write request units"></a>2.4.5 Read/ Write request units</h3><p>For on-demand mode tables, you don’t need to specify how much read and write throughput you expect your application to perform. DynamoDB charges you for the reads and writes that your application performs on your tables in terms of read request units and write request units.</p><p>One read request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size. Transactional read requests require 2 read request units to perform one read for items up to 4 KB. If you need to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units required depends on the item size, and whether you want an eventually consistent or strongly consistent read. For example, if your item size is 8 KB, you require 2 read request units to sustain one strongly consistent read, 1 read request unit if you choose eventually consistent reads, or 4 read request units for a transactional read request.</p><p>One write request unit represents one write for an item up to 1 KB in size. If you need to write an item that is larger than 1 KB, DynamoDB needs to consume additional write request units. Transactional write requests require 2 write request units to perform one write for items up to 1 KB. The total number of write request units required depends on the item size. For example, if your item size is 2 KB, you require 2 write request units to sustain one write request or 4 read request units for a transactional write request.</p><h3 id="2-4-6-DynamoDB-Auto-Scaling"><a href="#2-4-6-DynamoDB-Auto-Scaling" class="headerlink" title="2.4.6 DynamoDB Auto Scaling"></a>2.4.6 DynamoDB Auto Scaling</h3><p>Manage throughput capacity for tables and global secondary indexes. With auto scaling, you define a range for read and write capacity units. You also define a target utilization percentage within that range. DynamoDB auto scaling seeks to maintain your target utilization, even as your application workload increases or decreases.</p><h2 id="2-5-Partitions-and-Data-Distribution"><a href="#2-5-Partitions-and-Data-Distribution" class="headerlink" title="2.5 Partitions and Data Distribution"></a>2.5 Partitions and Data Distribution</h2><p>A partition is an allocation of storage for a table, backed by solid-state drives (SSDs) and automatically replicated across multiple Availability Zones within an AWS Region. Partition management is handled entirely by DynamoDB—you never have to manage partitions yourself.</p><p>DynamoDB allocates additional partitions to a table in the following situations:</p><ul><li>If you increase the table’s provisioned throughput settings beyond what the existing partitions can support.</li><li>If an existing partition fills to capacity and more storage space is required.</li></ul><p>Choose a partition key that can have a large number of distinct values relative to the number of items in the table. </p><h1 id="3-Java-Programming-with-DanamoDB"><a href="#3-Java-Programming-with-DanamoDB" class="headerlink" title="3. Java Programming with DanamoDB"></a>3. Java Programming with DanamoDB</h1><h2 id="3-1-Work-flow-using-Java"><a href="#3-1-Work-flow-using-Java" class="headerlink" title="3.1 Work flow using Java"></a>3.1 Work flow using Java</h2><h3 id="3-1-1-Create-a-table"><a href="#3-1-1-Create-a-table" class="headerlink" title="3.1.1 Create a table"></a>3.1.1 Create a table</h3><pre><code>public class MoviesCreateTable {    public static void main(String[] args) throws Exception {        AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()            .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(&quot;http://localhost:8000&quot;, &quot;us-west-2&quot;))            .build();        DynamoDB dynamoDB = new DynamoDB(client);        String tableName = &quot;Movies&quot;;        try {            System.out.println(&quot;Attempting to create table; please wait...&quot;);            Table table = dynamoDB.createTable(tableName,                Arrays.asList(new KeySchemaElement(&quot;year&quot;, KeyType.HASH), // Partition                                                                          // key                    new KeySchemaElement(&quot;title&quot;, KeyType.RANGE)), // Sort key                Arrays.asList(new AttributeDefinition(&quot;year&quot;, ScalarAttributeType.N),                    new AttributeDefinition(&quot;title&quot;, ScalarAttributeType.S)),                new ProvisionedThroughput(10L, 10L));            table.waitForActive();            System.out.println(&quot;Success.  Table status: &quot; + table.getDescription().getTableStatus());        }        catch (Exception e) {            System.err.println(&quot;Unable to create table: &quot;);            System.err.println(e.getMessage());        }    }}</code></pre><h3 id="3-1-2-Load-data"><a href="#3-1-2-Load-data" class="headerlink" title="3.1.2 Load data"></a>3.1.2 Load data</h3><pre><code>public class MoviesLoadData {    public static void main(String[] args) throws Exception {        AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()            .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(&quot;http://localhost:8000&quot;, &quot;us-west-2&quot;))            .build();        DynamoDB dynamoDB = new DynamoDB(client);        Table table = dynamoDB.getTable(&quot;Movies&quot;);        JsonParser parser = new JsonFactory().createParser(new File(&quot;moviedata.json&quot;));        JsonNode rootNode = new ObjectMapper().readTree(parser);        Iterator&lt;JsonNode&gt; iter = rootNode.iterator();        ObjectNode currentNode;        while (iter.hasNext()) {            currentNode = (ObjectNode) iter.next();            int year = currentNode.path(&quot;year&quot;).asInt();            String title = currentNode.path(&quot;title&quot;).asText();            try {                table.putItem(new Item().withPrimaryKey(&quot;year&quot;, year, &quot;title&quot;, title).withJSON(&quot;info&quot;,                    currentNode.path(&quot;info&quot;).toString()));                System.out.println(&quot;PutItem succeeded: &quot; + year + &quot; &quot; + title);            }            catch (Exception e) {                System.err.println(&quot;Unable to add movie: &quot; + year + &quot; &quot; + title);                System.err.println(e.getMessage());                break;            }        }        parser.close();    }}</code></pre><h3 id="3-1-3-Create-a-new-item"><a href="#3-1-3-Create-a-new-item" class="headerlink" title="3.1.3 Create a new item"></a>3.1.3 Create a new item</h3><pre><code>public class MoviesItemOps01 {    public static void main(String[] args) throws Exception {        AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()            .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(&quot;http://localhost:8000&quot;, &quot;us-west-2&quot;))            .build();        DynamoDB dynamoDB = new DynamoDB(client);        Table table = dynamoDB.getTable(&quot;Movies&quot;);        int year = 2015;        String title = &quot;The Big New Movie&quot;;        final Map&lt;String, Object&gt; infoMap = new HashMap&lt;String, Object&gt;();        infoMap.put(&quot;plot&quot;, &quot;Nothing happens at all.&quot;);        infoMap.put(&quot;rating&quot;, 0);        try {            System.out.println(&quot;Adding a new item...&quot;);            PutItemOutcome outcome = table                .putItem(new Item().withPrimaryKey(&quot;year&quot;, year, &quot;title&quot;, title).withMap(&quot;info&quot;, infoMap));            System.out.println(&quot;PutItem succeeded:\n&quot; + outcome.getPutItemResult());        }        catch (Exception e) {            System.err.println(&quot;Unable to add item: &quot; + year + &quot; &quot; + title);            System.err.println(e.getMessage());        }    }}</code></pre><h3 id="3-1-4-Read-an-Item"><a href="#3-1-4-Read-an-Item" class="headerlink" title="3.1.4 Read an Item"></a>3.1.4 Read an Item</h3><pre><code>public class MoviesItemOps02 {    public static void main(String[] args) throws Exception {        AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()            .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(&quot;http://localhost:8000&quot;, &quot;us-west-2&quot;))            .build();        DynamoDB dynamoDB = new DynamoDB(client);        Table table = dynamoDB.getTable(&quot;Movies&quot;);        int year = 2015;        String title = &quot;The Big New Movie&quot;;        GetItemSpec spec = new GetItemSpec().withPrimaryKey(&quot;year&quot;, year, &quot;title&quot;, title);        try {            System.out.println(&quot;Attempting to read the item...&quot;);            Item outcome = table.getItem(spec);            System.out.println(&quot;GetItem succeeded: &quot; + outcome);        }        catch (Exception e) {            System.err.println(&quot;Unable to read item: &quot; + year + &quot; &quot; + title);            System.err.println(e.getMessage());        }    }}</code></pre><h3 id="3-1-5-Update-an-Item"><a href="#3-1-5-Update-an-Item" class="headerlink" title="3.1.5 Update an Item"></a>3.1.5 Update an Item</h3><pre><code>public class MoviesItemOps03 {    public static void main(String[] args) throws Exception {        AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()            .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(&quot;http://localhost:8000&quot;, &quot;us-west-2&quot;))            .build();        DynamoDB dynamoDB = new DynamoDB(client);        Table table = dynamoDB.getTable(&quot;Movies&quot;);        int year = 2015;        String title = &quot;The Big New Movie&quot;;        UpdateItemSpec updateItemSpec = new UpdateItemSpec().withPrimaryKey(&quot;year&quot;, year, &quot;title&quot;, title)            .withUpdateExpression(&quot;set info.rating = :r, info.plot=:p, info.actors=:a&quot;)            .withValueMap(new ValueMap().withNumber(&quot;:r&quot;, 5.5).withString(&quot;:p&quot;, &quot;Everything happens all at once.&quot;)                .withList(&quot;:a&quot;, Arrays.asList(&quot;Larry&quot;, &quot;Moe&quot;, &quot;Curly&quot;)))            .withReturnValues(ReturnValue.UPDATED_NEW);        try {            System.out.println(&quot;Updating the item...&quot;);            UpdateItemOutcome outcome = table.updateItem(updateItemSpec);            System.out.println(&quot;UpdateItem succeeded:\n&quot; + outcome.getItem().toJSONPretty());        }        catch (Exception e) {            System.err.println(&quot;Unable to update item: &quot; + year + &quot; &quot; + title);            System.err.println(e.getMessage());        }    }}</code></pre><h3 id="3-1-6-Update-an-Item-conditionally"><a href="#3-1-6-Update-an-Item-conditionally" class="headerlink" title="3.1.6 Update an Item conditionally"></a>3.1.6 Update an Item conditionally</h3><pre><code>public class MoviesItemOps05 {    public static void main(String[] args) throws Exception {        AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()            .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(&quot;http://localhost:8000&quot;, &quot;us-west-2&quot;))            .build();        DynamoDB dynamoDB = new DynamoDB(client);        Table table = dynamoDB.getTable(&quot;Movies&quot;);        int year = 2015;        String title = &quot;The Big New Movie&quot;;        UpdateItemSpec updateItemSpec = new UpdateItemSpec()            .withPrimaryKey(new PrimaryKey(&quot;year&quot;, year, &quot;title&quot;, title)).withUpdateExpression(&quot;remove info.actors[0]&quot;)            .withConditionExpression(&quot;size(info.actors) &gt; :num&quot;).withValueMap(new ValueMap().withNumber(&quot;:num&quot;, 3))            .withReturnValues(ReturnValue.UPDATED_NEW);        // Conditional update (we expect this to fail)        try {            System.out.println(&quot;Attempting a conditional update...&quot;);            UpdateItemOutcome outcome = table.updateItem(updateItemSpec);            System.out.println(&quot;UpdateItem succeeded:\n&quot; + outcome.getItem().toJSONPretty());        }        catch (Exception e) {            System.err.println(&quot;Unable to update item: &quot; + year + &quot; &quot; + title);            System.err.println(e.getMessage());        }    }}</code></pre><h3 id="3-1-7-Delete-an-Item"><a href="#3-1-7-Delete-an-Item" class="headerlink" title="3.1.7 Delete an Item"></a>3.1.7 Delete an Item</h3><pre><code>public class MoviesItemOps06 {    public static void main(String[] args) throws Exception {        AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()            .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(&quot;http://localhost:8000&quot;, &quot;us-west-2&quot;))            .build();        DynamoDB dynamoDB = new DynamoDB(client);        Table table = dynamoDB.getTable(&quot;Movies&quot;);        int year = 2015;        String title = &quot;The Big New Movie&quot;;        DeleteItemSpec deleteItemSpec = new DeleteItemSpec()            .withPrimaryKey(new PrimaryKey(&quot;year&quot;, year, &quot;title&quot;, title)).withConditionExpression(&quot;info.rating &lt;= :val&quot;)            .withValueMap(new ValueMap().withNumber(&quot;:val&quot;, 5.0));        // Conditional delete (we expect this to fail)        try {            System.out.println(&quot;Attempting a conditional delete...&quot;);            table.deleteItem(deleteItemSpec);            System.out.println(&quot;DeleteItem succeeded&quot;);        }        catch (Exception e) {            System.err.println(&quot;Unable to delete item: &quot; + year + &quot; &quot; + title);            System.err.println(e.getMessage());        }    }}</code></pre><h3 id="3-1-8-Scan"><a href="#3-1-8-Scan" class="headerlink" title="3.1.8 Scan"></a>3.1.8 Scan</h3><p>The scan method reads every item in the entire table, and returns all the data in the table, you can provide an optional filter_expression so that only the items matching your criteria are returned. However, the filter is applied only after the entire table has been scanned. </p><pre><code>public class MoviesScan {    public static void main(String[] args) throws Exception {        AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()            .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(&quot;http://localhost:8000&quot;, &quot;us-west-2&quot;))            .build();        DynamoDB dynamoDB = new DynamoDB(client);        Table table = dynamoDB.getTable(&quot;Movies&quot;);        ScanSpec scanSpec = new ScanSpec().withProjectionExpression(&quot;#yr, title, info.rating&quot;)            .withFilterExpression(&quot;#yr between :start_yr and :end_yr&quot;).withNameMap(new NameMap().with(&quot;#yr&quot;, &quot;year&quot;))            .withValueMap(new ValueMap().withNumber(&quot;:start_yr&quot;, 1950).withNumber(&quot;:end_yr&quot;, 1959));        try {            ItemCollection&lt;ScanOutcome&gt; items = table.scan(scanSpec);            Iterator&lt;Item&gt; iter = items.iterator();            while (iter.hasNext()) {                Item item = iter.next();                System.out.println(item.toString());            }        }        catch (Exception e) {            System.err.println(&quot;Unable to scan the table:&quot;);            System.err.println(e.getMessage());        }    }}</code></pre><h2 id="3-2-AWS-SDK-support"><a href="#3-2-AWS-SDK-support" class="headerlink" title="3.2 AWS SDK support"></a>3.2 AWS SDK support</h2><h3 id="3-2-1-AWS-SDK-support-workflow"><a href="#3-2-1-AWS-SDK-support-workflow" class="headerlink" title="3.2.1 AWS SDK support workflow"></a>3.2.1 AWS SDK support workflow</h3><p><img src="https://i.loli.net/2020/01/29/vTVb5WzqQkFaLYm.png" alt="fig2.png"></p><ol><li><p>You write an application using an AWS SDK for your programming language.</p></li><li><p>Each AWS SDK provides one or more programmatic interfaces for working with DynamoDB. The specific interfaces available depend on which programming language and AWS SDK you use.</p></li><li><p>The AWS SDK constructs HTTP(S) requests for use with the low-level DynamoDB API.</p></li><li><p>The AWS SDK sends the request to the DynamoDB endpoint.</p></li><li><p>DynamoDB executes the request. If the request is successful, DynamoDB returns an HTTP 200 response code (OK). If the request is unsuccessful, DynamoDB returns an HTTP error code and an error message.</p></li><li><p>The AWS SDK processes the response and propagates it back to your application.</p></li></ol><h3 id="3-2-2-Services-AWS-SDK-provides"><a href="#3-2-2-Services-AWS-SDK-provides" class="headerlink" title="3.2.2 Services AWS SDK provides"></a>3.2.2 Services AWS SDK provides</h3><ul><li>formatting HTTP(s) requests and serializing request parameters </li><li>generating a cryptographic signature for each request </li><li>forwarding request to a DynamoDB endpoint and receiving responses from DynamoDB </li><li>extracting the results from those responses </li><li>implementing basic retry logic in case of errors </li></ul><h3 id="3-2-3-Programmatic-Interfaces"><a href="#3-2-3-Programmatic-Interfaces" class="headerlink" title="3.2.3 Programmatic Interfaces"></a>3.2.3 Programmatic Interfaces</h3><ol><li>Low level interfaces<br>Need data type descriptors </li></ol><pre><code>result.getItem().getN() // get number </code></pre><ol start="2"><li>Document Interfaces</li></ol><p>Documnet interface, allowing to perform data plane operations(creat, read, update, delete) on tables and indexes. No need to specify the data type descriptors. Data types are implied by the semantics of the data itself. Also provide methods to easily convert JSON documents to and from native DDB data types. </p><ol start="3"><li>Object Persistence Interface </li></ol><p>Provid an object persistence interface where you do not directly perform data plane operations. Instead, you create objects that represent items in DynamoDB tables and indexes, and interact only with those objects. <strong><em>This allow you to write object-centric code, rather than database-centric code</em></strong>. </p><pre><code>@DynamoDBTable(tableName=&quot;Music&quot;)public class MusicItem {    private String artist;    private String songTitle;    private String albumTitle;    private int year;    @DynamoDBHashKey(attributeName=&quot;Artist&quot;)    public String getArtist() { return artist;}    public void setArtist(String artist) {this.artist = artist;}    @DynamoDBRangeKey(attributeName=&quot;SongTitle&quot;)    public String getSongTitle() { return songTitle;}    public void setSongTitle(String songTitle) {this.songTitle = songTitle;}    @DynamoDBAttribute(attributeName = &quot;AlbumTitle&quot;)    public String getAlbumTitle() { return albumTitle;}    public void setAlbumTitle(String albumTitle) {this.albumTitle = albumTitle;}    @DynamoDBAttribute(attributeName = &quot;Year&quot;)    public int getYear() { return year; }    public void setYear(int year) { this.year = year; }}</code></pre><h3 id="3-2-4-Low-level-API"><a href="#3-2-4-Low-level-API" class="headerlink" title="3.2.4 Low level API"></a>3.2.4 Low level API</h3><p>The DynamoDB low-level API is the protocol level interface for Amazon DynamoDB. At this level, every HTTP(s) request must be correctly formatted and carry a valid digital signature. </p><p>The low-level DynamoDB API uses JavaScript Object Notation (JSON) as a wire protocol format. JSON presents data in a hierarchy, so that both data values and data structure are conveyed simultaneously. Name-value pairs are defined in the format name:value. The data hierarchy is defined by nested brackets of name-value pairs.</p><p>DynamoDB uses JSON only as a transport protocol, not as a storage format. The AWS SDKs use JSON to send data to DynamoDB, and DynamoDB responds with JSON, but DynamoDB does not store data persistently in JSON format.</p><h4 id="3-2-4-1-Request-Format"><a href="#3-2-4-1-Request-Format" class="headerlink" title="3.2.4.1 Request Format"></a>3.2.4.1 Request Format</h4><p>The DynamoDB low-level API accepts HTTP(S) POST requests as input. The AWS SDKs construct these requests for you.</p><p>Suppose that you have a table named Pets, with a key schema consisting of AnimalType (partition key) and Name (sort key). Both of these attributes are of type string. To retrieve an item from Pets, the AWS SDK constructs a request as shown following:</p><pre><code>POST / HTTP/1.1Host: dynamodb.&lt;region&gt;.&lt;domain&gt;;Accept-Encoding: identityContent-Length: &lt;PayloadSizeBytes&gt;     User-Agent: &lt;UserAgentString&gt;Content-Type: application/x-amz-json-1.0Authorization: AWS4-HMAC-SHA256 Credential=&lt;Credential&gt;, SignedHeaders=&lt;Headers&gt;, Signature=&lt;Signature&gt;X-Amz-Date: &lt;Date&gt; X-Amz-Target: DynamoDB_20120810.GetItem{    &quot;TableName&quot;: &quot;Pets&quot;,    &quot;Key&quot;: {        &quot;AnimalType&quot;: {&quot;S&quot;: &quot;Dog&quot;},        &quot;Name&quot;: {&quot;S&quot;: &quot;Fido&quot;}    }}</code></pre><ul><li>The Authorization header contains information required for DynamoDB to authenticate the request.</li><li>The X-Amz-Target header contains the name of a DynamoDB operation: GetItem. (This is also accompanied by the low-level API version, in this case 20120810.)</li><li>The payload (body) of the request contains the parameters for the operation, in JSON format. For the GetItem operation, the parameters are TableName and Key.</li></ul><h4 id="3-2-4-2-Response-Format"><a href="#3-2-4-2-Response-Format" class="headerlink" title="3.2.4.2 Response Format"></a>3.2.4.2 Response Format</h4><p>Upon receipt of the request, DynamoDB processes it and returns a response. For the request shown above, the HTTP(S) response payload contains the results from the operation. </p><pre><code>HTTP/1.1 200 OKx-amzn-RequestId: &lt;RequestId&gt; x-amz-crc32: &lt;Checksum&gt;Content-Type: application/x-amz-json-1.0Content-Length: &lt;PayloadSizeBytes&gt;Date: &lt;Date&gt; {    &quot;Item&quot;: {        &quot;Age&quot;: {&quot;N&quot;: &quot;8&quot;},        &quot;Colors&quot;: {            &quot;L&quot;: [                {&quot;S&quot;: &quot;White&quot;},                {&quot;S&quot;: &quot;Brown&quot;},                {&quot;S&quot;: &quot;Black&quot;}            ]        },        &quot;Name&quot;: {&quot;S&quot;: &quot;Fido&quot;},        &quot;Vaccinations&quot;: {            &quot;M&quot;: {                &quot;Rabies&quot;: {                    &quot;L&quot;: [                        {&quot;S&quot;: &quot;2009-03-17&quot;},                        {&quot;S&quot;: &quot;2011-09-21&quot;},                        {&quot;S&quot;: &quot;2014-07-08&quot;}                    ]                },                &quot;Distemper&quot;: {&quot;S&quot;: &quot;2015-10-13&quot;}            }        },        &quot;Breed&quot;: {&quot;S&quot;: &quot;Beagle&quot;},        &quot;AnimalType&quot;: {&quot;S&quot;: &quot;Dog&quot;}    }}</code></pre><h4 id="3-2-4-3-Data-Type-Descriptors"><a href="#3-2-4-3-Data-Type-Descriptors" class="headerlink" title="3.2.4.3 Data Type Descriptors"></a>3.2.4.3 Data Type Descriptors</h4><p>The low-level DynamoDB API protocol requires each attribute to be accompanied by a data type descriptor. Data type descriptors are tokens that tell DynamoDB how to interpret each attribute.</p><p>The examples in Request Format and Response Format show examples of how data type descriptors are used. The GetItem request specifies S for the Pets key schema attributes (AnimalType and Name), which are of type string. The GetItem response contains a Pets item with attributes of type string (S), number (N), map (M), and list (L).</p><h2 id="3-3-Error-Handling"><a href="#3-3-Error-Handling" class="headerlink" title="3.3 Error Handling"></a>3.3 Error Handling</h2><h3 id="3-3-1-Error-Components"><a href="#3-3-1-Error-Components" class="headerlink" title="3.3.1 Error Components"></a>3.3.1 Error Components</h3><p>Unsuccessful: returns an error, which contains: </p><ul><li>An HTTP status code</li><li>An exception name</li><li>An error message</li></ul><p>The AWS SDK tale care of propagating errors to your application, so that you can take appropriate action. </p><h3 id="3-3-2-Error-Messages-and-Codes"><a href="#3-3-2-Error-Messages-and-Codes" class="headerlink" title="3.3.2 Error Messages and Codes"></a>3.3.2 Error Messages and Codes</h3><ul><li>HTTP status code 400<ul><li>AccessDeniedException <ul><li>The client did not correctly sign the request.</li></ul></li><li>ConditionalCheckFailedException<ul><li>You specified a condition that evaluated to false. </li></ul></li><li>IncompleteSignatureException <ul><li>The request signature did not include all of the required components.</li></ul></li><li>ItemCollectionSizeLimitExceededException<ul><li>For a table with a local secondary index, a group of items with the same partition key value has exceeded the maximum size limit of 10 GB. </li></ul></li><li>LimitExceededException<ul><li>There are too many concurrent control plane operations. The cumulative number of tables and indexes in the CREATING, DELETING, or UPDATING state cannot exceed 50.</li></ul></li><li>MissingAuthenticationTokenException<ul><li>The request did not include the required authorization header, or it was malformed. </li></ul></li><li>ProvisionedThroughputExceededException<ul><li>Your request rate is too high. The AWS SDKs for DynamoDB automatically retry requests that receive this exception. Your request is eventually successful, unless your retry queue is too large to finish. </li></ul></li><li>RequestLimitExceeded<ul><li>Throughput exceeds the current throughput limit for your account.</li></ul></li><li>ResourceInUseException<ul><li>The resource which you are attempting to change is in use.</li></ul></li></ul></li></ul><pre><code>Table table = dynamoDB.getTable(&quot;Movies&quot;);try {    Item item = table.getItem(&quot;year&quot;, 1978, &quot;title&quot;, &quot;Superman&quot;);    if (item != null) {        System.out.println(&quot;Result: &quot; + item);    } else {         //No such item exists in the table        System.out.println(&quot;Item not found&quot;);    }} catch (AmazonServiceException ase) {    System.err.println(&quot;Could not complete operation&quot;);    System.err.println(&quot;Error Message:  &quot; + ase.getMessage());    System.err.println(&quot;HTTP Status:    &quot; + ase.getStatusCode());    System.err.println(&quot;AWS Error Code: &quot; + ase.getErrorCode());    System.err.println(&quot;Error Type:     &quot; + ase.getErrorType());    System.err.println(&quot;Request ID:     &quot; + ase.getRequestId());} catch (AmazonClientException ace) {    System.err.println(&quot;Internal error occured communicating with DynamoDB&quot;);    System.out.println(&quot;Error Message:  &quot; + ace.getMessage());</code></pre><h3 id="3-3-3-Error-Retries-and-Expotential-Backoff"><a href="#3-3-3-Error-Retries-and-Expotential-Backoff" class="headerlink" title="3.3.3 Error Retries and Expotential Backoff"></a>3.3.3 Error Retries and Expotential Backoff</h3><p>Numerous components on a network, such as DNS servers, switches, load balancers, and others can generate errors anywhere in the life of a given request. The usual technique for dealing with these error responses in a networked environment is to implement retries in the client application. This technique increases the reliability of the application.</p><p>In addition to simple retries, each AWS SDK implements exponential backoff algorithm for better flow control. The concept behind exponential backoff is to use progressively longer waits between retries for consecutive error responses. For example, up to 50 milliseconds before the first retry, up to 100 milliseconds before the second, up to 200 milliseconds before third, and so on.</p><h3 id="3-3-4-Batch-Operations-and-Error-Handling"><a href="#3-3-4-Batch-Operations-and-Error-Handling" class="headerlink" title="3.3.4 Batch Operations and Error Handling"></a>3.3.4 Batch Operations and Error Handling</h3><p>The DynamoDB low-level API supports batch operations for reads and writes. BatchGetItem reads items from one or more tables, and BatchWriteItem puts or deletes items in one or more tables. These batch operations are implemented as wrappers around other non-batch DynamoDB operations. In other words, BatchGetItem invokes GetItem once for each item in the batch. Similarly,BatchWriteItem invokes DeleteItem or PutItem, as appropriate, for each item in the batch.</p><p>A batch operation can tolerate the failure of individual requests in the batch. For example, consider a BatchGetItem request to read five items. Even if some of the underlying GetItem requests fail, this does not cause the entire BatchGetItem operation to fail. On the other hand, if all of the five reads operations fail, then the entire BatchGetItem will fail.</p><p>The batch operations return information about individual requests that fail, so that you can diagnose the problem and retry the operation. For BatchGetItem, the tables and primary keys in question are returned in the UnprocessedKeys parameter of the request. For BatchWriteItem, similar information is returned in UnprocessedItems.</p><h1 id="4-High-level-programming-interfaces-for-DynamoDB-DynamoDBMapper"><a href="#4-High-level-programming-interfaces-for-DynamoDB-DynamoDBMapper" class="headerlink" title="4 High level programming interfaces for DynamoDB - DynamoDBMapper"></a>4 High level programming interfaces for DynamoDB - DynamoDBMapper</h1><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HigherLevelInterfaces.html" target="_blank" rel="noopener">DynamoDBMapper</a></p><h2 id="4-1-Mapper-class-implementation"><a href="#4-1-Mapper-class-implementation" class="headerlink" title="4.1 Mapper class implementation"></a>4.1 Mapper class implementation</h2><p>With a low level database interface, developers must write methods for reading or writing object data to database tables and vice versa. The amount of extra code required for each combination of object type and database table can seem overwhelming. </p><p>The higher-level interfaces for DynamoDB let you define the relationships between objects in your program and the database tables that store those objects’ data. After you define this mapping, you call simple object methods such as save, load, or delete, and the underlying low-level DynamoDB operations are automatically invoked on your behalf. This allows you to write object-centric code, rather than database-centric code.</p><p>AWS SDK provides a DynamoDBMapper class, allowing you to map your client side classes to DynamoDB tables. To use DynamoDBMapper, you define the relationship between items in a DynamoDB table and their corresponding object instances in your code. </p><pre><code>@DynamoDBTable(tableName=&quot;ProductCatalog&quot;)public class CatalogItem {    private Integer id;    private String title;    private String ISBN;    private Set&lt;String&gt; bookAuthors;    private String someProp;    @DynamoDBHashKey(attributeName=&quot;Id&quot;)      public Integer getId() { return id; }    public void setId(Integer id) {this.id = id; }    @DynamoDBAttribute(attributeName=&quot;Title&quot;)      public String getTitle() {return title; }    public void setTitle(String title) { this.title = title; }    @DynamoDBAttribute(attributeName=&quot;ISBN&quot;)      public String getISBN() { return ISBN; }    public void setISBN(String ISBN) { this.ISBN = ISBN; }    @DynamoDBAttribute(attributeName=&quot;Authors&quot;)    public Set&lt;String&gt; getBookAuthors() { return bookAuthors; }    public void setBookAuthors(Set&lt;String&gt; bookAuthors) { this.bookAuthors = bookAuthors; }    @DynamoDBIgnore    public String getSomeProp() { return someProp; }    public void setSomeProp(String someProp) { this.someProp = someProp; }}</code></pre><h3 id="4-1-1-DynamoDBTable"><a href="#4-1-1-DynamoDBTable" class="headerlink" title="4.1.1 @DynamoDBTable"></a>4.1.1 @DynamoDBTable</h3><p>Maps the class CatalogItem to table ProductCatalog. You can store individual class instances as items in the table.  </p><h3 id="4-1-2-DynamoDBHashKey"><a href="#4-1-2-DynamoDBHashKey" class="headerlink" title="4.1.2 @DynamoDBHashKey"></a>4.1.2 @DynamoDBHashKey</h3><p>Maps the Id property to the primary key </p><h3 id="4-1-3-DynamoDBAttribute"><a href="#4-1-3-DynamoDBAttribute" class="headerlink" title="4.1.3 @DynamoDBAttribute"></a>4.1.3 @DynamoDBAttribute</h3><p>This annotation is optional when the name of the DynamoDB attribute matches the name of the property declared in the class. When they differ, use this annotation with the attributeName() parameter to specify which DynamoDB attribute this property corresponds to. </p><h3 id="4-1-4-DynamoDBIgnore"><a href="#4-1-4-DynamoDBIgnore" class="headerlink" title="4.1.4  @DynamoDBIgnore"></a>4.1.4  @DynamoDBIgnore</h3><p>Those properties will not be mapped to any attributes in the table</p><h2 id="4-2-Use-DynamoDBMapper-Method"><a href="#4-2-Use-DynamoDBMapper-Method" class="headerlink" title="4.2 Use DynamoDBMapper Method"></a>4.2 Use DynamoDBMapper Method</h2><p>Use Mapper method to write an instance of that class to a corresponding item in the Catalog table. </p><pre><code>AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard().build();DynamoDBMapper mapper = new DynamoDBMapper(client);CatalogItem item = new CatalogItem();item.setId(102);item.setTitle(&quot;Book 102 Title&quot;);item.setISBN(&quot;222-2222222222&quot;);item.setBookAuthors(new HashSet&lt;String&gt;(Arrays.asList(&quot;Author 1&quot;, &quot;Author 2&quot;)));item.setSomeProp(&quot;Test&quot;);mapper.save(item);          </code></pre><p>Following code shows how to retrieve the item and access some of its attributes: </p><pre><code>CatalogItem partitionKey = new CatalogItem();partitionKey.setId(102);DynamoDBQueryExpression&lt;CatalogItem&gt; queryExpression = new DynamoDBQueryExpression&lt;CatalogItem&gt;()    .withHashKeyValues(partitionKey);List&lt;CatalogItem&gt; itemList = mapper.query(CatalogItem.class, queryExpression);for (int i = 0; i &lt; itemList.size(); i++) {    System.out.println(itemList.get(i).getTitle());    System.out.println(itemList.get(i).getBookAuthors());}</code></pre><h2 id="4-3-Java-Annotations-for-DynamoDB"><a href="#4-3-Java-Annotations-for-DynamoDB" class="headerlink" title="4.3 Java Annotations for DynamoDB"></a>4.3 Java Annotations for DynamoDB</h2><h3 id="4-3-1-DynamoDBAttribute"><a href="#4-3-1-DynamoDBAttribute" class="headerlink" title="4.3.1 DynamoDBAttribute"></a>4.3.1 DynamoDBAttribute</h3><ul><li>Maps a property to a table attribute. </li></ul><pre><code>@DynamoDBAttribute(attributeName = &quot;Authors&quot;)public List&lt;String&gt; getBookAuthors() { return BookAuthors; }public void setBookAuthors(List&lt;String&gt; BookAuthors) { this.BookAuthors = BookAuthors; }</code></pre><h3 id="4-3-2-DynamoDBAutoGeneratedKey"><a href="#4-3-2-DynamoDBAutoGeneratedKey" class="headerlink" title="4.3.2 DynamoDBAutoGeneratedKey"></a>4.3.2 DynamoDBAutoGeneratedKey</h3><ul><li>Marks a partition key or sort key property as being auto-generated. DynamoDBMapper will generate a random UUID when saving these attributes. Only String properties can be marked as auto-generated keys. </li></ul><pre><code>@DynamoDBTable(tableName=&quot;AutoGeneratedKeysExample&quot;)public class AutoGeneratedKeys {     private String id;    private String payload;    @DynamoDBHashKey(attributeName = &quot;Id&quot;)    @DynamoDBAutoGeneratedKey    public String getId() { return id; }    public void setId(String id) { this.id = id; }     @DynamoDBAttribute(attributeName=&quot;payload&quot;)    public String getPayload() { return this.payload; }    public void setPayload(String payload) { this.payload = payload; }        public static void saveItem() {        AutoGeneratedKeys obj = new AutoGeneratedKeys();        obj.setPayload(&quot;abc123&quot;);        // id field is null at this point               DynamoDBMapper mapper = new DynamoDBMapper(dynamoDBClient);        mapper.save(obj);        System.out.println(&quot;Object was saved with id &quot; + obj.getId());    }}</code></pre><h3 id="4-3-3-DynamoDBDocument"><a href="#4-3-3-DynamoDBDocument" class="headerlink" title="4.3.3 DynamoDBDocument"></a>4.3.3 DynamoDBDocument</h3><ul><li>indicates that a class can be serialized as a DynamoDB document </li></ul><p>For example, suppose you wanted to map a JSON document to a DynamoDB attribute of type Map (M). The following code snippet defines an item containing a nested attribute (Pictures) of type Map.</p><pre><code>public class ProductCatalogItem {    private Integer id;  //partition key    private Pictures pictures;    /* ...other attributes omitted... */    @DynamoDBHashKey(attributeName=&quot;Id&quot;)      public Integer getId() { return id;}    public void setId(Integer id) {this.id = id;}    @DynamoDBAttribute(attributeName=&quot;Pictures&quot;)      public Pictures getPictures() { return pictures;}    public void setPictures(Pictures pictures) {this.pictures = pictures;}    // Additional properties go here.     @DynamoDBDocument    public static class Pictures {        private String frontView;        private String rearView;        private String sideView;        @DynamoDBAttribute(attributeName = &quot;FrontView&quot;)        public String getFrontView() { return frontView; }        public void setFrontView(String frontView) { this.frontView = frontView; }        @DynamoDBAttribute(attributeName = &quot;RearView&quot;)        public String getRearView() { return rearView; }        public void setRearView(String rearView) { this.rearView = rearView; }        @DynamoDBAttribute(attributeName = &quot;SideView&quot;)        public String getSideView() { return sideView; }        public void setSideView(String sideView) { this.sideView = sideView; }     }}</code></pre><p>You could then save a new ProductCatalog item, with pictures, as shown in the following snippet: </p><pre><code>ProductCatalogItem item = new ProductCatalogItem();Pictures pix = new Pictures();pix.setFrontView(&quot;http://example.com/products/123_front.jpg&quot;);pix.setRearView(&quot;http://example.com/products/123_rear.jpg&quot;);pix.setSideView(&quot;http://example.com/products/123_left_side.jpg&quot;);item.setPictures(pix);item.setId(123);mapper.save(item); </code></pre><p>The resulting ProductCalalog item would look like this: </p><pre><code>{  &quot;Id&quot; : 123  &quot;Pictures&quot; : {    &quot;SideView&quot; : &quot;http://example.com/products/123_left_side.jpg&quot;,    &quot;RearView&quot; : &quot;http://example.com/products/123_rear.jpg&quot;,    &quot;FrontView&quot; : &quot;http://example.com/products/123_front.jpg&quot;  }} </code></pre><h3 id="4-3-4-DynamoDBHashKey"><a href="#4-3-4-DynamoDBHashKey" class="headerlink" title="4.3.4 DynamoDBHashKey"></a>4.3.4 DynamoDBHashKey</h3><ul><li>Maps a class property to the partition key of the table. The property must be one of the scalar string, number or binary types; it cannot be a collection type. </li></ul><pre><code>@DynamoDBTable(tableName=&quot;ProductCatalog&quot;) public class CatalogItem {     private Integer Id;      @DynamoDBHashKey(attributeName=&quot;Id&quot;)   public Integer getId() {        return Id;   }   public void setId(Integer Id) {        this.Id = Id;   }   // Additional properties go here. }</code></pre><h3 id="4-3-5-DynamoDBIgnore"><a href="#4-3-5-DynamoDBIgnore" class="headerlink" title="4.3.5 DynamoDBIgnore"></a>4.3.5 DynamoDBIgnore</h3><ul><li>Indicates to the DynamoDBMapper instance that the associated property should be ignored. When saving data to the table, the DynamoDBMapper does not save property to the table. </li></ul><h3 id="4-3-6-DynamoDBIndexHashKey"><a href="#4-3-6-DynamoDBIndexHashKey" class="headerlink" title="4.3.6 DynamoDBIndexHashKey"></a>4.3.6 DynamoDBIndexHashKey</h3><ul><li>Maps a class property to the partition key of a global secondary index. The property must be one of the scalar string, number or binary types; it cannot be a collection type.</li><li>Use this annotation if you need to Query a global secondary index. You must specify the index name (globalSecondaryIndexName). If the name of the class property is different from the index partition key, you must also specify the name of that index attribute (attributeName).</li><li>Global Secondary Indexes <ul><li>aims to speed up queries on non-key values </li></ul></li></ul><h3 id="4-3-7-DynamoDBRangeKey"><a href="#4-3-7-DynamoDBRangeKey" class="headerlink" title="4.3.7 DynamoDBRangeKey"></a>4.3.7 DynamoDBRangeKey</h3><ul><li>Maps a class property to the sort key of the table. The property mush be one of the scalar string, number or binary types; it cannot be a collection type. </li><li>If the primary key is composite (partition key and sort key), you can use this tag to map your class field to the sort key.</li></ul><pre><code>@DynamoDBTable(tableName=&quot;Reply&quot;)public class Reply {     private Integer id;    private String replyDateTime;    @DynamoDBHashKey(attributeName=&quot;Id&quot;)    public Integer getId() { return id; }    public void setId(Integer id) { this.id = id; }     @DynamoDBRangeKey(attributeName=&quot;ReplyDateTime&quot;)    public String getReplyDateTime() { return replyDateTime; }    public void setReplyDateTime(String replyDateTime) { this.replyDateTime = replyDateTime; }    // Additional properties go here. }</code></pre><h3 id="4-3-8-DynamoDBTable"><a href="#4-3-8-DynamoDBTable" class="headerlink" title="4.3.8 DynamoDBTable"></a>4.3.8 DynamoDBTable</h3><ul><li>Identifies the target table in DynamoDB. </li><li>The @DynamoDBTable annotation can be inherited. Any new class that inherits from the Developer class also maps to the People table. For example, assume that you create a Lead class that inherits from the Developer class. Because you mapped the Developer class to the People table, the Lead class objects are also stored in the same table.</li></ul><pre><code>@DynamoDBTable(tableName=&quot;People&quot;) public class Developer { ...} </code></pre><h3 id="4-3-9-DynamoDBTypeConverted"><a href="#4-3-9-DynamoDBTypeConverted" class="headerlink" title="4.3.9 DynamoDBTypeConverted"></a>4.3.9 DynamoDBTypeConverted</h3><pre><code>+ Annotation to mark a property as using a custom type-converter. + This interface lets you map your own arbitrary data types to a data type that is natively supported by DynamoDB</code></pre><h3 id="4-3-10-DynamoDBTyped"><a href="#4-3-10-DynamoDBTyped" class="headerlink" title="4.3.10 DynamoDBTyped"></a>4.3.10 DynamoDBTyped</h3><pre><code>+ Annotation to override the standard attribute type binding. Standard types do not require the annotation if applying the default attribute binding for that type. </code></pre><h3 id="4-3-11-DynamoDBVersionAttribute"><a href="#4-3-11-DynamoDBVersionAttribute" class="headerlink" title="4.3.11 DynamoDBVersionAttribute"></a>4.3.11 DynamoDBVersionAttribute</h3><ul><li>Identifies a class property for storing an optimistic locking version number. DynamoDBMapper assigns a version number to this property when it saves a new item, and increments it each time you update the item.  </li></ul><h2 id="4-4-The-DynamoDBMapper-Class"><a href="#4-4-The-DynamoDBMapper-Class" class="headerlink" title="4.4 The DynamoDBMapper Class"></a>4.4 The DynamoDBMapper Class</h2><p>The DynamoDBMapper class is the entry point to DynamoDB. It provides access to a DynamoDB endpoint and enables you to access data in various tables, perform various CRUD operations on items, and execute queries and scans against tables. </p><h3 id="4-4-1-save"><a href="#4-4-1-save" class="headerlink" title="4.4.1 save"></a>4.4.1 save</h3><p>Saves the specified object to the table. The object that you wish to save is the only required parameter for this method. You can provide optional configuration parameters using the <strong><em>DynamoDBMapperConfig</em></strong> object. </p><p>If an item that has the same primary key does not exist, this method creates a new item in the table. If an item that has the same primary key exists, it updates the existing item. If the partition key and sort key are of type String, and annotated with @DynamoDBAutoGeneratedKey, then they are given a random universally unique identifier (UUID) if left uninitialized. Version fields annotated with @DynamoDBVersionAttribute will be incremented by one. Additionally, if a version field is updated or a key generated, the object passed in is updated as a result of the operation.</p><p>By default, only attributes corresponding to mapped class properties are updated; any additional existing attributes on an item are unaffected. However, if you specify SaveBehavior.CLOBBER, you can force the item to be completely overwritten.</p><pre><code>mapper.save(obj, new DynamoDBMapperConfig(DynamoDBMapperConfig.SaveBehavior.CLOBBER));</code></pre><h3 id="4-4-2-load"><a href="#4-4-2-load" class="headerlink" title="4.4.2 load"></a>4.4.2 load</h3><p>Retrieves an item from a table. You must provide the primary key of the item that you wish to retrieve. You can provide optional configuration parameters using the DynamoDBMapperConfig object. </p><pre><code>CatalogItem item = mapper.load(CatalogItem.class, item.getId(),                 new DynamoDBMapperConfig(DynamoDBMapperConfig.ConsistentReads.CONSISTENT)); </code></pre><h3 id="4-4-3-delete"><a href="#4-4-3-delete" class="headerlink" title="4.4.3 delete"></a>4.4.3 delete</h3><p>Deletes an item from the table, must pass in an object instance of the mapped class. </p><h3 id="4-4-4-query"><a href="#4-4-4-query" class="headerlink" title="4.4.4 query"></a>4.4.4 query</h3><p>Queries a table or a secondary index. You can query a table or an index only if it has a composite primary key (partition key and sort key). This method requires you to provide a partition key value and a query filter that is applied on the sort key. A filter expression includes a condition and a value.</p><pre><code>String forumName = &quot;DynamoDB&quot;;String forumSubject = &quot;DynamoDB Thread 1&quot;;String partitionKey = forumName + &quot;#&quot; + forumSubject;long twoWeeksAgoMilli = (new Date()).getTime() - (14L*24L*60L*60L*1000L);Date twoWeeksAgo = new Date();twoWeeksAgo.setTime(twoWeeksAgoMilli);SimpleDateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSS&apos;Z&apos;&quot;);String twoWeeksAgoStr = df.format(twoWeeksAgo);Map&lt;String, AttributeValue&gt; eav = new HashMap&lt;String, AttributeValue&gt;();eav.put(&quot;:v1&quot;, new AttributeValue().withS(partitionKey));eav.put(&quot;:v2&quot;,new AttributeValue().withS(twoWeeksAgoStr.toString()));DynamoDBQueryExpression&lt;Reply&gt; queryExpression = new DynamoDBQueryExpression&lt;Reply&gt;()     .withKeyConditionExpression(&quot;Id = :v1 and ReplyDateTime &gt; :v2&quot;)    .withExpressionAttributeValues(eav);List&lt;Reply&gt; latestReplies = mapper.query(Reply.class, queryExpression);</code></pre><p>By default, the query method returns a “<strong>lazy-loaded</strong>“ collection. It initially returns only one page of results, and then makes a service call for the next page if needed. To obtain all the matching items, you only need to iterate over the latestReplies collection.</p><p>To query an index, you must first <strong><em>model the index as a mapper class</em></strong>, Suppose that the Reply table has a global secondary index named PostedBy-Message-Index. The partition key for this index is PostedBy, and the sort key is Message. The class definition for an item in the index would look like this:</p><pre><code>@DynamoDBTable(tableName=&quot;Reply&quot;)public class PostedByMessage {     private String postedBy;    private String message;    @DynamoDBIndexHashKey(globalSecondaryIndexName = &quot;PostedBy-Message-Index&quot;, attributeName = &quot;PostedBy&quot;)    public String getPostedBy() { return postedBy; }    public void setPostedBy(String postedBy) { this.postedBy = postedBy; }     @DynamoDBIndexRangeKey(globalSecondaryIndexName = &quot;PostedBy-Message-Index&quot;, attributeName = &quot;Message&quot;)    public String getMessage() { return message; }    public void setMessage(String message) { this.message = message; }    // Additional properties go here. }</code></pre><p>The @DynamoDBTable annotation indicates that this index is associated with the Reply table. The @DynamoDBIndexHashKey annotation denotes the partition key (PostedBy) of the index, and @DynamoDBIndexRangeKey denotes the sort key (Message) of the index.</p><p>Now you can use DynamoDBMapper to query the index, retrieving a subset of messages that were posted by a particular user. You must specify withIndexName so that DynamoDB knows which index to query. In the following code snippet, we are querying a global secondary index. Because global secondary indexes support eventually consistent reads, but not strongly consistent reads, we must specify withConsistentRead(false).</p><pre><code>HashMap&lt;String, AttributeValue&gt; eav = new HashMap&lt;String, AttributeValue&gt;();eav.put(&quot;:v1&quot;,  new AttributeValue().withS(&quot;User A&quot;));eav.put(&quot;:v2&quot;,  new AttributeValue().withS(&quot;DynamoDB&quot;));DynamoDBQueryExpression&lt;PostedByMessage&gt; queryExpression = new DynamoDBQueryExpression&lt;PostedByMessage&gt;()    .withIndexName(&quot;PostedBy-Message-Index&quot;)    .withConsistentRead(false)    .withKeyConditionExpression(&quot;PostedBy = :v1 and begins_with(Message, :v2)&quot;)    .withExpressionAttributeValues(eav);List&lt;PostedByMessage&gt; iList =  mapper.query(PostedByMessage.class, queryExpression);</code></pre><h2 id="4-5-Configuration-settings-for-DynamoDBMapper"><a href="#4-5-Configuration-settings-for-DynamoDBMapper" class="headerlink" title="4.5 Configuration settings for DynamoDBMapper"></a>4.5 Configuration settings for DynamoDBMapper</h2><p>When you create an instance of DynamoDBMapper, it has certain default behaviors; you can override these defaults by using the DynamoDBmapperConfig class. </p><pre><code>AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard().build();DynamoDBMapperConfig mapperConfig = new DynamoDBMapperConfig(    DynamoDBMapperConfig.SaveBehavior.CLOBBER,    DynamoDBMapperConfig.ConsistentReads.CONSISTENT,    null, //TableNameOverride - leaving this at default setting    DynamoDBMapperConfig.PaginationLoadingStrategy.EAGER_LOADING    );DynamoDBMapper mapper = new DynamoDBMapper(client, mapperConfig, cp);</code></pre><p><a href="https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMapperConfig.html" target="_blank" rel="noopener">API doc for DynamoDBMapperConfig</a></p><ul><li>DynamoDBMapperConfig.ConsistentReads<ul><li>EVENTUAL—the mapper instance uses an eventually consistent read request ++<strong>(default)</strong>++</li><li>CONSISTENT—the mapper instance uses a strongly consistent read request. You can use this optional setting with load, query, or scan operations. Strongly consistent reads have implications for performance and billing; see the DynamoDB product detail page for more information</li></ul></li><li>DynamoDBMapperConfig.PaginationLoadingStrategy - controls how the mapper instance processes a paginated list of data, such as results from a query or scan <ul><li>LAZY_LOADING—the mapper instance loads data when possible, and keeps all loaded results in memory ++<strong>(default)</strong>++</li><li>EAGER_LOADING—the mapper instance loads the data as soon as the list is initialized</li><li>ITERATION_ONLY—you can only use an Iterator to read from the list. During the iteration, the list will clear all the previous results before loading the next page, so that the list will keep at most one page of the loaded results in memory. This also means the list can only be iterated once. This strategy is recommended when handling large items, in order to reduce memory overhead</li></ul></li><li>DynamoDBMapperConfig.SaveBehavior enumeration value - Specifies how the mapper instance should deal with attributes during save operations<ul><li>UPDATE—during a save operation, all modeled attributes are updated, and unmodeled attributes are unaffected. Primitive number types (byte, int, long) are set to 0. Object types are set to null. ++<strong>(default)</strong>++</li><li>CLOBBER—clears and replaces all attributes, included unmodeled ones, during a save operation. This is done by deleting the item and re-creating it. Versioned field constraints are also disregarded.</li></ul></li><li>DynamoDBMapperConfig.TableNameOverride object—Instructs the mapper instance to ignore the table name specified by a class’s DynamoDBTable annotation, and instead use a different table name that you supply. This is useful when partitioning your data into multiple tables at run time.</li></ul><h2 id="4-6-Example-CRUD-Operations"><a href="#4-6-Example-CRUD-Operations" class="headerlink" title="4.6 Example: CRUD Operations"></a>4.6 Example: CRUD Operations</h2><p>The following Java code example declares a CatalogItem class that has Id, Title, ISBN and Authors properties. It uses the annotations to map these properties to the ProductCatalog table in DynamoDB. The code example then uses the DynamoDBMapper to save a book object, retrieve it, update it and delete the book item.</p><pre><code>package com.amazonaws.codesamples.datamodeling;import java.io.IOException;import java.util.Arrays;import java.util.HashSet;import java.util.Set;import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;import com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBAttribute;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBHashKey;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBMapper;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBMapperConfig;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBTable;public class DynamoDBMapperCRUDExample {    static AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard().build();    public static void main(String[] args) throws IOException {        testCRUDOperations();        System.out.println(&quot;Example complete!&quot;);    }    @DynamoDBTable(tableName = &quot;ProductCatalog&quot;)    public static class CatalogItem {        private Integer id;        private String title;        private String ISBN;        private Set&lt;String&gt; bookAuthors;        // Partition key        @DynamoDBHashKey(attributeName = &quot;Id&quot;)        public Integer getId() {            return id;        }        public void setId(Integer id) {            this.id = id;        }        @DynamoDBAttribute(attributeName = &quot;Title&quot;)        public String getTitle() {            return title;        }        public void setTitle(String title) {            this.title = title;        }        @DynamoDBAttribute(attributeName = &quot;ISBN&quot;)        public String getISBN() {            return ISBN;        }        public void setISBN(String ISBN) {            this.ISBN = ISBN;        }        @DynamoDBAttribute(attributeName = &quot;Authors&quot;)        public Set&lt;String&gt; getBookAuthors() {            return bookAuthors;        }        public void setBookAuthors(Set&lt;String&gt; bookAuthors) {            this.bookAuthors = bookAuthors;        }        @Override        public String toString() {            return &quot;Book [ISBN=&quot; + ISBN + &quot;, bookAuthors=&quot; + bookAuthors + &quot;, id=&quot; + id + &quot;, title=&quot; + title + &quot;]&quot;;        }    }    private static void testCRUDOperations() {        CatalogItem item = new CatalogItem();        item.setId(601);        item.setTitle(&quot;Book 601&quot;);        item.setISBN(&quot;611-1111111111&quot;);        item.setBookAuthors(new HashSet&lt;String&gt;(Arrays.asList(&quot;Author1&quot;, &quot;Author2&quot;)));        // Save the item (book).        DynamoDBMapper mapper = new DynamoDBMapper(client);        mapper.save(item);        // Retrieve the item.        CatalogItem itemRetrieved = mapper.load(CatalogItem.class, 601);        System.out.println(&quot;Item retrieved:&quot;);        System.out.println(itemRetrieved);        // Update the item.        itemRetrieved.setISBN(&quot;622-2222222222&quot;);        itemRetrieved.setBookAuthors(new HashSet&lt;String&gt;(Arrays.asList(&quot;Author1&quot;, &quot;Author3&quot;)));        mapper.save(itemRetrieved);        System.out.println(&quot;Item updated:&quot;);        System.out.println(itemRetrieved);        // Retrieve the updated item.        DynamoDBMapperConfig config = new DynamoDBMapperConfig(DynamoDBMapperConfig.ConsistentReads.CONSISTENT);        CatalogItem updatedItem = mapper.load(CatalogItem.class, 601, config);        System.out.println(&quot;Retrieved the previously updated item:&quot;);        System.out.println(updatedItem);        // Delete the item.        mapper.delete(updatedItem);        // Try to retrieve deleted item.        CatalogItem deletedItem = mapper.load(CatalogItem.class, updatedItem.getId(), config);        if (deletedItem == null) {            System.out.println(&quot;Done - Sample item is deleted.&quot;);        }    }}</code></pre><h2 id="4-7-Examples-Batch-Write-Query-Scan-Optimistic-Locking-with-Version-Number"><a href="#4-7-Examples-Batch-Write-Query-Scan-Optimistic-Locking-with-Version-Number" class="headerlink" title="4.7 Examples: Batch Write + Query + Scan + Optimistic Locking with Version Number"></a>4.7 Examples: Batch Write + Query + Scan + Optimistic Locking with Version Number</h2><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.BatchWriteExample.html" target="_blank" rel="noopener">BatchWrite</a></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.QueryScanExample.html" target="_blank" rel="noopener">Query and Scan</a></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html" target="_blank" rel="noopener">Optimistic Locking With Version Number</a></p><h2 id="4-8-Example-Mapping-Arbitrary-Data"><a href="#4-8-Example-Mapping-Arbitrary-Data" class="headerlink" title="4.8 Example: Mapping Arbitrary Data"></a>4.8 Example: Mapping Arbitrary Data</h2><p>In addtion to the supported java types, you can use types in your application for which there is no direct mapping to the DynamoDB types. </p><blockquote><p> To map these types, you must provide an implementation that converts your complex type to a DynamoDB supported type and vice-versa, and annotate the complex type accessor method using the @DynamoDBTypeConverted annotation. </p></blockquote><p>The converter code transforms data when objects are saved or loaded. It is also used for all operations that consume complex types. Note that when comparing data during query and scan operations, the comparisons are made against the data stored in DynamoDB.</p><p>For example, consider the following CatalogItem class that defines a property, Dimension, that is of DimensionType. This property stores the item dimensions, as height, width, and thickness. Assume that you decide to store these item dimensions as a string (such as 8.5x11x.05) in DynamoDB. The following example provides converter code that converts the DimensionType object to a string and a string to the DimensionType.</p><pre><code>package com.amazonaws.codesamples.datamodeling;import java.io.IOException;import java.util.Arrays;import java.util.HashSet;import java.util.Set;import com.amazonaws.regions.Regions;import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;import com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBAttribute;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBHashKey;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBMapper;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBTypeConverted;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBTypeConverter;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBTable;public class DynamoDBMapperExample {    static AmazonDynamoDB client;    public static void main(String[] args) throws IOException {        // Set the AWS region you want to access.        Regions usWest2 = Regions.US_WEST_2;        client = AmazonDynamoDBClientBuilder.standard().withRegion(usWest2).build();        DimensionType dimType = new DimensionType();        dimType.setHeight(&quot;8.00&quot;);        dimType.setLength(&quot;11.0&quot;);        dimType.setThickness(&quot;1.0&quot;);        Book book = new Book();        book.setId(502);        book.setTitle(&quot;Book 502&quot;);        book.setISBN(&quot;555-5555555555&quot;);        book.setBookAuthors(new HashSet&lt;String&gt;(Arrays.asList(&quot;Author1&quot;, &quot;Author2&quot;)));        book.setDimensions(dimType);        DynamoDBMapper mapper = new DynamoDBMapper(client);        mapper.save(book);        Book bookRetrieved = mapper.load(Book.class, 502);        System.out.println(&quot;Book info: &quot; + &quot;\n&quot; + bookRetrieved);        bookRetrieved.getDimensions().setHeight(&quot;9.0&quot;);        bookRetrieved.getDimensions().setLength(&quot;12.0&quot;);        bookRetrieved.getDimensions().setThickness(&quot;2.0&quot;);        mapper.save(bookRetrieved);        bookRetrieved = mapper.load(Book.class, 502);        System.out.println(&quot;Updated book info: &quot; + &quot;\n&quot; + bookRetrieved);    }    @DynamoDBTable(tableName = &quot;ProductCatalog&quot;)    public static class Book {        private int id;        private String title;        private String ISBN;        private Set&lt;String&gt; bookAuthors;        private DimensionType dimensionType;        // Partition key        @DynamoDBHashKey(attributeName = &quot;Id&quot;)        public int getId() {            return id;        }        public void setId(int id) {            this.id = id;        }        @DynamoDBAttribute(attributeName = &quot;Title&quot;)        public String getTitle() {            return title;        }        public void setTitle(String title) {            this.title = title;        }        @DynamoDBAttribute(attributeName = &quot;ISBN&quot;)        public String getISBN() {            return ISBN;        }        public void setISBN(String ISBN) {            this.ISBN = ISBN;        }        @DynamoDBAttribute(attributeName = &quot;Authors&quot;)        public Set&lt;String&gt; getBookAuthors() {            return bookAuthors;        }        public void setBookAuthors(Set&lt;String&gt; bookAuthors) {            this.bookAuthors = bookAuthors;        }        @DynamoDBTypeConverted(converter = DimensionTypeConverter.class)        @DynamoDBAttribute(attributeName = &quot;Dimensions&quot;)        public DimensionType getDimensions() {            return dimensionType;        }        @DynamoDBAttribute(attributeName = &quot;Dimensions&quot;)        public void setDimensions(DimensionType dimensionType) {            this.dimensionType = dimensionType;        }        @Override        public String toString() {            return &quot;Book [ISBN=&quot; + ISBN + &quot;, bookAuthors=&quot; + bookAuthors + &quot;, dimensionType= &quot;                + dimensionType.getHeight() + &quot; X &quot; + dimensionType.getLength() + &quot; X &quot; + dimensionType.getThickness()                + &quot;, Id=&quot; + id + &quot;, Title=&quot; + title + &quot;]&quot;;        }    }    static public class DimensionType {        private String length;        private String height;        private String thickness;        public String getLength() {            return length;        }        public void setLength(String length) {            this.length = length;        }        public String getHeight() {            return height;        }        public void setHeight(String height) {            this.height = height;        }        public String getThickness() {            return thickness;        }        public void setThickness(String thickness) {            this.thickness = thickness;        }    }    // Converts the complex type DimensionType to a string and vice-versa.    static public class DimensionTypeConverter implements DynamoDBTypeConverter&lt;String, DimensionType&gt; {        @Override        public String convert(DimensionType object) {            DimensionType itemDimensions = (DimensionType) object;            String dimension = null;            try {                if (itemDimensions != null) {                    dimension = String.format(&quot;%s x %s x %s&quot;, itemDimensions.getLength(), itemDimensions.getHeight(),                        itemDimensions.getThickness());                }            }            catch (Exception e) {                e.printStackTrace();            }            return dimension;        }        @Override        public DimensionType unconvert(String s) {            DimensionType itemDimension = new DimensionType();            try {                if (s != null &amp;&amp; s.length() != 0) {                    String[] data = s.split(&quot;x&quot;);                    itemDimension.setLength(data[0].trim());                    itemDimension.setHeight(data[1].trim());                    itemDimension.setThickness(data[2].trim());                }            }            catch (Exception e) {                e.printStackTrace();            }            return itemDimension;        }    }}</code></pre><h1 id="5-Best-Practices"><a href="#5-Best-Practices" class="headerlink" title="5. Best Practices"></a>5. Best Practices</h1><h2 id="5-1-Partition-Key-Design"><a href="#5-1-Partition-Key-Design" class="headerlink" title="5.1 Partition Key Design"></a>5.1 Partition Key Design</h2><p>Generally speaking, you should design your application for <strong>uniform activity across all logical partition keys</strong> in the Table and its secondary indexes. You can determine the access patterns that your application requires, and estimate the total RCUs and WCUs that each table and secondary Index requires.</p><p>As traffic starts to flow, DynamoDB automatically supports your access patterns using the throughput you have provisioned, as long as the traffic against a given partition key does not exceed 3000 RCUs or 1000 WCUs.</p><h3 id="5-1-1-Using-Burst-Capacity-Effectively"><a href="#5-1-1-Using-Burst-Capacity-Effectively" class="headerlink" title="5.1.1 Using Burst Capacity Effectively"></a>5.1.1 Using Burst Capacity Effectively</h3><p>DynamoDB provides some flexibility in your per-partition throughput provisioning by providing burst capacity, as follows. Whenever you are not fully using a partition’s throughput, DynamoDB reserves a portion of that unused capacity for later bursts of throughput to handle usage spikes.</p><p>DynamoDB currently retains up to <strong>five minutes (300 seconds)</strong> of unused read and write capacity. During an occasional burst of read or write activity, these extra capacity units can be consumed quickly—even faster than the per-second provisioned throughput capacity that you’ve defined for your table.</p><h3 id="5-1-2-Adaptive-Capacity"><a href="#5-1-2-Adaptive-Capacity" class="headerlink" title="5.1.2 Adaptive Capacity"></a>5.1.2 Adaptive Capacity</h3><p>When data access is imbalanced, a “hot” partition can receive such a higher volume of read and write traffic compared to other partitions. In extreme cases, throttling can occur if a single partition receives more than 3,000 RCUs or 1,000 WCUs. </p><p>To better accommodate uneven access patterns, DynamoDB adaptive capacity enables your application to continue reading and writing to hot partitions without being throttled, provided that traffic does not exceed your table’s total provisioned capacity or the partition maximum capacity. Adaptive capacity works by automatically increasing throughput capacity for partitions that receive more traffic.</p><h1 id="6-Working-with-Stream"><a href="#6-Working-with-Stream" class="headerlink" title="6. Working with Stream"></a>6. Working with Stream</h1><p>DynamoDB streams captures a time-ordered sequence of item-level modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time.</p><blockquote><p>A DynamoDB stream is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. </p></blockquote><p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the “before” and “after” images of modified items.</p><p>DynamoDB Streams guarantees the following: </p><ul><li>Each stream record appears exactly once in the stream </li><li><strong>For each item that is modified in a DynamoDB table, the stream records appear in the same sequence as the actual modifications to the item.</strong></li></ul><p>DynamoDB Streams writes stream records in near real time, so that you can build applications that consume these streams and take action based on the contents.</p><p>Use one endpoint for accessing DynamoDB, and another endpoint within same region for accessing DynamoDB streams. </p><h2 id="6-1-Reading-and-Processing-a-Stream"><a href="#6-1-Reading-and-Processing-a-Stream" class="headerlink" title="6.1 Reading and Processing a Stream"></a>6.1 Reading and Processing a Stream</h2><p>To read and process a stream, your application will need to connect to a DynamoDB Streams endpoint and issue API requests. </p><p>A stream consists of stream records. Each stream record represents a single data modification in the DynamoDB table to which the stream belongs. Each stream record is assigned a sequence number, reflecting the order in which the record was published to the stream.</p><p>Stream records are organized into groups, or shards. Each shard acts as a container for multiple stream records, and contains information required for accessing and iterating through these records. The stream records within a shard are removed automatically after 24 hours.</p><p>Shards are ephemeral: They are created and deleted automatically, as needed. Any shard can also split into multiple new shards; this also occurs automatically. (Note that it is also possible for a parent shard to have just one child shard.) A shard might split in response to high levels of write activity on its parent table, so that applications can process records from multiple shards in parallel.</p><p>Because shards have a lineage (parent and children), an application must always process a parent shard before it processes a child shard. This will ensure that the stream records are also processed in the correct order. (If you use the DynamoDB Streams Kinesis Adapter, this is handled for you: Your application will process the shards and stream records in the correct order, and automatically handle new or expired shards, as well as shards that split while the application is running.</p><p>To access a stream and process the stream records within, you must do the following: </p><ul><li>Determine the unique Amazon Resource Name (ARN) of the stream that you want to access.</li><li>Determine which shard(s) in the stream contain the stream records that you are interested in.</li><li>Access the shard(s) and retrieve the stream records that you want</li></ul><h2 id="6-2-DynamoDB-Streams-API"><a href="#6-2-DynamoDB-Streams-API" class="headerlink" title="6.2 DynamoDB Streams API"></a>6.2 DynamoDB Streams API</h2><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Operations_Amazon_DynamoDB_Streams.html" target="_blank" rel="noopener">API Instructions</a></p><h3 id="6-2-1-ListStreams"><a href="#6-2-1-ListStreams" class="headerlink" title="6.2.1 ListStreams"></a>6.2.1 ListStreams</h3><p>returns a list of stream descriptors for the current account and endpoint. You can optionally request just the stream descriptors for a particular table name.</p><h3 id="6-2-2-DescribeStream"><a href="#6-2-2-DescribeStream" class="headerlink" title="6.2.2 DescribeStream"></a>6.2.2 DescribeStream</h3><p>returns detailed information about a given stream. The output includes a list of shards associated with the stream, including the shard IDs.</p><h3 id="6-2-3-GetShardIterator"><a href="#6-2-3-GetShardIterator" class="headerlink" title="6.2.3 GetShardIterator"></a>6.2.3 GetShardIterator</h3><p>returns a shard iterator, which describes a location within a shard. You can request that the iterator provide access to the oldest point, the newest point, or a particular point in the stream.</p><h3 id="6-2-4-GetRecords"><a href="#6-2-4-GetRecords" class="headerlink" title="6.2.4 GetRecords"></a>6.2.4 GetRecords</h3><p>returns the stream records from within a given shard. </p><h2 id="6-3-Using-the-DynamoDB-Streams-Kinesis-Adapter-to-Process-Stream-Records"><a href="#6-3-Using-the-DynamoDB-Streams-Kinesis-Adapter-to-Process-Stream-Records" class="headerlink" title="6.3 Using the DynamoDB Streams Kinesis Adapter to Process Stream Records"></a>6.3 Using the DynamoDB Streams Kinesis Adapter to Process Stream Records</h2><p><a href="https://aws.amazon.com/blogs/database/how-to-perform-ordered-data-replication-between-applications-by-using-amazon-dynamodb-streams/" target="_blank" rel="noopener">How to perform ordered data replication between applications by using Amazon DynamoDB Streams</a></p>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> DynamoDB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.11 Deploying Applications</title>
      <link href="/Developing-on-AWS-Note-11-Deploying-Applications/"/>
      <url>/Developing-on-AWS-Note-11-Deploying-Applications/</url>
      
        <content type="html"><![CDATA[<h1 id="1-DevOps"><a href="#1-DevOps" class="headerlink" title="1. DevOps"></a>1. DevOps</h1><ul><li>Application is the application plus all of the associated infrastructure</li><li>includes<ul><li>VPCs </li><li>load balancers</li><li>auto scaling groups </li><li>Amazon RDS databases</li><li>Amazon S3 bucket </li><li>elastiCache servers</li></ul></li><li>deploying in the cloud helps break down these traditional silos. Bugs due to different environment. </li><li>practices<ul><li>microservices</li><li>CI/ CD</li><li>Infrastructure as code </li></ul></li><li>Tools <ul><li>AWS Code Services </li></ul></li><li>Under a DevOps model, development and operations are no longer siloed. </li><li>Sometimes, those two functions are merged into a single team where engineers work across the entire application lifecycle, from development and test to deployment to operations, and develop a range of skills not limited to a single function. </li><li>Quality assurance and security teams could become more tightly integrated with development and operations throughout the application lifecycle. </li></ul><h1 id="2-Release-Processes-Major-Phases"><a href="#2-Release-Processes-Major-Phases" class="headerlink" title="2. Release Processes Major Phases"></a>2. Release Processes Major Phases</h1><p>Each steps can be automated without the entire release process being automated. </p><ul><li>Source<ul><li>Check in source code such as .java files </li><li>Code review</li></ul></li><li>Build<ul><li>Compile code </li><li>style checkers</li><li>code metrics</li><li>create container images </li></ul></li><li>Test <ul><li>Integration tests with other systems</li><li>load testing</li><li>UI tests</li><li>penetration testing </li></ul></li><li>Deploy<ul><li>Deployment to production environments </li></ul></li><li>Monitor<ul><li>Monitor in production to quickly detect unusual activity or errors</li></ul></li></ul><h1 id="3-Understanding-CI-amp-CD"><a href="#3-Understanding-CI-amp-CD" class="headerlink" title="3. Understanding CI &amp; CD"></a>3. Understanding CI &amp; CD</h1><ul><li><p>Continuous Integration </p><ul><li>The practice of checking code and verifying each change with an automated build and test process</li><li>Require teams to write automated tests which can improve the quality of the software being released and reduce the time it takes to validate that the new version of software is good. </li><li>Architecture for cloud Continuous Integration<ul><li>Developer commits changes to the central repo. Pre-commit hooks should run and verify that the code meets specified requirements</li><li>A CI server pulls the changes from the central repo and builds the code</li><li>The CI server runs all required tests against the new branch or mainline change. </li><li>The CI server returns a report to developer and stops the build job if a failure occurs. </li><li>If the changes pass the required tests, the CI server builds the artifacts</li><li>The CI server pushes artifacts to the package builder</li><li>The <strong>package builder</strong> gets configuration information from the version control system </li><li>The pacakge builder uses the configuration information and the artifacts to build the specified packages </li><li>the packages are stored in a repository </li><li>the repo uses a post-receive hook to deploy specific packages to staging. </li><li>Do not fear rollbacks. Errors will happen, mistakes will be made, and the benefit of employing version control systems is that when appropriate, you can always revert to a previously working state and save yourself the time and effort of trying to debug </li></ul></li></ul></li><li><p>Continuous Delivery</p><ul><li>It extends continuous Integration to include testing out to production-like stages and running verification testing against those deployments. </li><li>CD may extend all the way to a production deployment, but they have some form of manual intervention between a code check-in and when that code is available for customers to use</li></ul></li><li><p>Continuous Deployment </p><ul><li>extends continuous delivery and is the automated release of software to customers from check in through to production without human intervention.  </li></ul></li></ul><h1 id="4-How-do-you-deploy-all-infrastructure-along-with-your-application"><a href="#4-How-do-you-deploy-all-infrastructure-along-with-your-application" class="headerlink" title="4. How do you deploy all infrastructure along with your application?"></a>4. How do you deploy all infrastructure along with your application?</h1><ul><li>Infrastructure as code <ul><li>Define your AWS environment so that it can be created in a repeatable automated fashion </li><li>stand up identical dev/ test environments on demand </li><li>use the same code to create your production environment that you used to create your other environments</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CI/ CD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.10 Docker, ECS, Security</title>
      <link href="/Developing-on-AWS-Note-10-Docker-ECS-Security/"/>
      <url>/Developing-on-AWS-Note-10-Docker-ECS-Security/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Why-Containers"><a href="#1-Why-Containers" class="headerlink" title="1. Why Containers?"></a>1. Why Containers?</h1><h2 id="1-1-Benefits"><a href="#1-1-Benefits" class="headerlink" title="1.1 Benefits"></a>1.1 Benefits</h2><ul><li>software development lifecycle<ul><li>source code</li><li>code repository </li><li>build env</li><li>artifact repository </li><li>test environment</li><li>deployment env</li></ul></li><li>Containers<ul><li>a method of <strong>operating system virtualization</strong> that allow you to run an application and its dependencies in resource-isolated processes. Containers allow you to easily package an application’s code, configurations, and dependencies into easy to use building blocks that deliver <strong>environmental consistency, operational efficiency, developer productivity, and version control</strong> </li><li>help resolve problems<ul><li>different application stacks</li><li>different hardware deployment environments</li><li>run applications across different envs</li><li>migration to different envs</li></ul></li><li>benefits<ul><li>allow you to easily package an application’s code, configurations, and dependencies into easy to use building blocks that deliver environmental consistency, operational efficiency, developer productivity, and version control</li></ul></li></ul></li><li>docker containers<ul><li>decouple applications from operating systems </li></ul></li><li>Containers vs VM <ul><li>Containers can run on any Linux system with appropriate kernel feature support and the Docker daemon present. This makes them extremely portable. Your laptop, your VM, your EC2 instance, and your bare metal server are all potential hosts.</li><li>The lack of a hypervisor requirement also results in almost no noticeable performance overhead. The processes are talking directly to the kernel and are largely unaware of their container silo. Most containers boot in just a couple of seconds.</li><li>Where VMs are isolated at the operating system level, containers are isolated at the kernel level. This means that several applications can run on a single host operating system, and yet still have their own file system, storage, RAM, libraries, essentially, their own “view” of the system. </li></ul></li></ul><h2 id="1-2-When-to-use-Docker-Containers"><a href="#1-2-When-to-use-Docker-Containers" class="headerlink" title="1.2 When to use Docker Containers"></a>1.2 When to use Docker Containers</h2><ul><li>Distributed apps and microservices<ul><li>breaking up monoliths </li><li>service oriented architecture</li></ul></li><li>Batch Jobs<ul><li>short lived jobs</li><li>variety and flexibility </li><li>elastic</li></ul></li><li>CI/ CD pipelins<ul><li>packaging your code in a Docker images</li><li>test</li><li>deploy in production </li></ul></li></ul><h2 id="1-3-Microservices-architecture-with-Containers"><a href="#1-3-Microservices-architecture-with-Containers" class="headerlink" title="1.3 Microservices architecture with Containers"></a>1.3 Microservices architecture with Containers</h2><p>Using Docker, all of the different tiers of your Web application architecture could be <strong>constructed as independent Docker containers</strong>. If you are deploying a microservices architecture, you could <strong>implement each service as its own separate Docker container</strong>. Such an approach has several advantages: </p><ul><li>You can distribute running services across instances on an Amazon EC2 fleet.</li><li>You can <strong>run multiple services on a single Amazon EC2 instance within your fleet</strong>. This allows maximizing the CPU and memory usage of your existing instances.</li><li>You can <strong>run multiple different versions of a service simultaneously</strong> - even on the same machine, assuming that they bind to different ports. This allows you to release breaking changes in services while retaining backward compatibility with applications that may not have yet been modified to work against the new version.</li></ul><h2 id="1-4-Docker-Image-Registry"><a href="#1-4-Docker-Image-Registry" class="headerlink" title="1.4 Docker Image Registry"></a>1.4 Docker Image Registry</h2><ul><li>Amazon Elastic Container Registry (Amazon ECR) is a fully-managed Docker container registry that makes it easy for you to store, manage, and deploy Docker container images. </li><li>Amazon ECR can be used standalone and also has deep integration with Amazon ECS, simplifying your development to production workflow. </li><li>Amazon ECR eliminates the need to operate your own container repositories or worry about scaling the underlying infrastructure. </li><li>Amazon ECR hosts your images in a highly available and scalable architecture, allowing you to reliably deploy containers for your applications.</li></ul><h1 id="2-Amazon-Container-Services"><a href="#2-Amazon-Container-Services" class="headerlink" title="2. Amazon Container Services"></a>2. Amazon Container Services</h1><ul><li>You need a way to intelligently place your containers on the hosts that have the resources and that means you need to know the state of everything in your system.</li><li>Amazon Container Services<ul><li>management <ul><li>deploy, schedule, scale,</li><li>Elastic Container Service (ECS)</li><li>Elastic Container Service for Kubernetes (EKS)</li></ul></li><li>hosting <ul><li>Amazon EC2</li><li>AWS Fargate</li></ul></li><li>Image Registry <ul><li>Amazon Elastic Container Registry (ECR)</li></ul></li></ul></li></ul><h1 id="3-Developing-Secure-Applications"><a href="#3-Developing-Secure-Applications" class="headerlink" title="3. Developing Secure Applications"></a>3. Developing Secure Applications</h1><h2 id="3-1-AWS-Certificates-Manager"><a href="#3-1-AWS-Certificates-Manager" class="headerlink" title="3.1 AWS Certificates Manager"></a>3.1 AWS Certificates Manager</h2><ul><li>AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/ Transport Layer Security (SSL/ TLS) certificates for use with AWS services and your internal connected resources. </li><li>SSL/ TLS are used to secure network communications and establish the identity of websites over the internet as well as resources on private networks. </li><li>With ACM, you can quickly request a certificate, deploy it on ACM-integrated AWS resources, such as Elastic Load Balancing, Amazon CloudFront distributions, and APIs on API Gateway, and let ACM handle certificate renewals. </li><li>It also enables you to create private certificates for your internal resources and manage the certificate lifecycle centrally. Public and private certificates provisioned through ACM for use with ACM-integrated services are free.</li></ul><h2 id="3-2-AWS-Secrets-Manager"><a href="#3-2-AWS-Secrets-Manager" class="headerlink" title="3.2 AWS Secrets Manager"></a>3.2 AWS Secrets Manager</h2><ul><li>Rotate, manage, and retrieve databse credentials, API keys, and other secrets throughout their lifecycle. </li><li>IT administrators<ul><li>store and manage access to secrets securely and at scale</li></ul></li><li>Security administrators<ul><li>audit and monitor the use of secrets, and rotate secrets without a risk of breaking applications </li><li>offer secret rotation with build-in integration for Amazon RDS for MySQL, PostgreSQL and Amazon Aurora. </li></ul></li><li>Developers<ul><li>avoid dealing with secrets in the applications </li></ul></li></ul><h2 id="3-3-AWS-Security-Token-Service"><a href="#3-3-AWS-Security-Token-Service" class="headerlink" title="3.3 AWS Security Token Service"></a>3.3 AWS Security Token Service</h2><ul><li>provides trusted users with temporary security credentials </li><li>configurable credential lifetime</li><li>once expired, cannot be reused</li><li>use IAM policies to control the privileges </li><li>no limit on the number of temporary credentials issued </li><li>Important points<ul><li>all calls go to the global endpoint, by default</li><li>global endpoint maps to the US East region </li><li>Regional endpoints are activated by default</li><li>use AWS cloudTrail to log SRS calls</li></ul></li></ul><h2 id="3-4-Identity-Providers"><a href="#3-4-Identity-Providers" class="headerlink" title="3.4 Identity Providers"></a>3.4 Identity Providers</h2><ul><li>an alternative to create IAM users in AWS account</li><li>can manage user identities outside of AWS, and you can give these external user identities permissions to use AWS resources in account </li><li>To use an identity provider, create an IAM identity provider entity to establish trust between your AWS account and the external identity provider.</li></ul><h2 id="3-5-Security-Assetion-Markup-Language-SAML"><a href="#3-5-Security-Assetion-Markup-Language-SAML" class="headerlink" title="3.5 Security Assetion Markup Language (SAML)"></a>3.5 Security Assetion Markup Language (SAML)</h2><ul><li>Use single sign-on to sign in to all of your SAML-enbabled applications by using a single set of credentials </li><li>Manage access to your applications centrally </li></ul><h2 id="3-6-Amazon-Cognito"><a href="#3-6-Amazon-Cognito" class="headerlink" title="3.6 Amazon Cognito"></a>3.6 Amazon Cognito</h2><ul><li>Allow saving and synchronizing user data on different devices</li><li>Allow access to AWS cloud services using <ul><li>public login providers</li><li>own user identity system </li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> Docker </tag>
            
            <tag> ECS </tag>
            
            <tag> Security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.9 Step Functions, ElasticCache</title>
      <link href="/Developing-on-AWS-Note-9-Step-Functions-ElasticCache/"/>
      <url>/Developing-on-AWS-Note-9-Step-Functions-ElasticCache/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Understanding-the-need-for-step-functions"><a href="#1-Understanding-the-need-for-step-functions" class="headerlink" title="1. Understanding the need for step functions"></a>1. Understanding the need for step functions</h1><ul><li><p>Step functions make it easy to coordinate components of distributed applications and microservices by using visual workflows. </p></li><li><p>Microservices are processes that communicate with each other over a network to complete a larger goal. </p></li><li><p>Applications built as a collections of microservices are more resilient and easier to scale. </p></li><li><p>Have cases that we begin to have more functions and they continue to grow</p></li><li><p>Need a mechanism to scale out, easily handle errors and timeouts, easily build and operate</p></li></ul><h1 id="2-Intro-to-AWS-Step-Functions"><a href="#2-Intro-to-AWS-Step-Functions" class="headerlink" title="2. Intro to AWS Step Functions"></a>2. Intro to AWS Step Functions</h1><ul><li>define a workflow called a state machine made up of states</li><li>each order is an execution through this state machine </li><li>each execution starts with an input and the states transform it</li><li>step functions keep track of the state of each execution </li><li>actually, it’s a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows.</li><li>provides a reliable way to coordinate components and step through the functions of your application. </li><li>automatically triggers and tracks each step, and retries when there are errors</li><li>lifecycle<ul><li>define workflow as a series of steps and transitions between each step, also known as a state machine</li><li>step functions ingests your JSON template and turns it into a real-time graphical view , help you make sense of your state machine’s current state</li></ul></li><li>benefits <ul><li>productivity<ul><li>build applications quickly  </li></ul></li><li>agility<ul><li>scale and recover reliably </li></ul></li><li>resilience <ul><li>evolve applications easily </li></ul></li></ul></li><li>Terminology<ul><li>state machine - workflow template<ul><li>an object that has a set number of operating conditions that depend on its previous condition to determine output </li><li>AWS step functions allows you to create and automate state machines within the AWS env<ul><li>with the use of a JSON-based Amazon State Language</li><li>a collection states, that can to work (task states), determine which states to transition to next (Choicestates), stop an execution with an error</li></ul></li><li>state common features<ul><li>each state must have a type field indicating what type of state it is</li><li>each state can have an optinal comment field to hold a human readable comment about, or description of, the state</li><li>Each state (except a Succeed or Fail state) requires a Next field or, alternatively, can become a terminal state by specifying an End field.</li></ul></li><li>state types<ul><li>task </li><li>choise - adds branching logic</li><li>parallel </li><li>wait</li><li>fail</li><li>succeed </li><li>pass</li></ul></li></ul></li><li>execution - specific workflow based on template</li><li>task - lambda function or activity <ul><li>An activity consists of program code or a task that waits for <strong>an operator to perform</strong> an action or to provide input. You can host activities on Amazon EC2, on Amazon ECS, or even on mobile devices. Activities poll Step Functions using the <strong>GetActivityTask</strong> and <strong>SendTaskSuccess</strong>, <strong>SendTaskFailure</strong>, and <strong>SendTaskHeartbeat</strong> API actions. Activities represent workers (processes or threads), implemented and hosted by you, that perform a specific task.</li><li>A Lambda function is a cloud-native task that runs on AWS Lambda. You can write Lambda functions in a variety of programming languages, using the AWS Management Console or by uploading code to Lambda. Lambda functions execute a function using AWS Lambda. To specify a Lambda function, use the ARN of the Lambda function in the Resource field </li></ul></li><li>activity - handle for external compute</li><li>task token - ID for instance of activity </li><li>heartbeat - ping from task indicating that it is still running </li><li>failure </li><li>success </li></ul></li></ul><h1 id="3-Caching-for-scalibility"><a href="#3-Caching-for-scalibility" class="headerlink" title="3. Caching for scalibility"></a>3. Caching for scalibility</h1><h2 id="3-1-Caching-Overview"><a href="#3-1-Caching-Overview" class="headerlink" title="3.1 Caching Overview"></a>3.1 Caching Overview</h2><ul><li>Benefits<ul><li>Provides high throughput, low latency access to commonly accessed application data, <strong>by storing the data in memory</strong></li><li>imrpove the speed </li><li>reduce the response latency </li><li>the following types of information or applications can benefit from caching:<ul><li>results of database queries</li><li>results of intensive calculations</li><li>results of remote API calls </li></ul></li></ul></li></ul><ul><li>when to consider caching your data<ul><li>data that requires a slow and expensive query to acquire </li><li>relatively static and frequently accessed data </li><li>information that can afford to be stale for some time</li><li>data should be relatively static and frequently accessed</li><li>Cache data should always be considered and treated as stale</li></ul></li></ul><h2 id="3-2-Caching-Strategy"><a href="#3-2-Caching-Strategy" class="headerlink" title="3.2 Caching Strategy"></a>3.2 Caching Strategy</h2><h1 id="4-Amazon-ElastiCache"><a href="#4-Amazon-ElastiCache" class="headerlink" title="4. Amazon ElastiCache"></a>4. Amazon ElastiCache</h1><ul><li>a webservice that makes it easy to deploy, operate and scale an in-memory cache in the cloud</li><li>ElastiCache improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases </li><li>ElastiCache supports<ul><li>Memcached</li><li>Redis</li></ul></li></ul><h2 id="4-1-Memcached-vs-Redis"><a href="#4-1-Memcached-vs-Redis" class="headerlink" title="4.1 Memcached vs Redis"></a>4.1 Memcached vs Redis</h2><ul><li>Memcached<ul><li>multithreading </li><li>low maintenance </li><li>easy horizontal scalability with auto discovery </li><li>single AZ </li><li>lack persistence<ul><li>if you terminate node or scale it down, you lose the data stored in that cache memory  </li></ul></li></ul></li><li>Redis<ul><li>single thread </li><li>support for data structures <ul><li>strings</li><li>hashes</li><li>lists</li><li>sets</li><li>sorted sets with range queries</li><li>bitmaps</li><li>hypolog</li><li>geospatial indexes with radius queries</li></ul></li><li>persistence </li><li>atomic operations </li><li>pub/ sub messaging</li><li>read replicas/ failover</li><li>cluster mode/ sharded clusters</li><li>multiple AZ </li></ul></li></ul><h2 id="4-2-Terminology"><a href="#4-2-Terminology" class="headerlink" title="4.2 Terminology"></a>4.2 Terminology</h2><ul><li>node<ul><li>smallest building block of an ElastiCache deployment </li></ul></li><li>cluster<ul><li>a logical grouping of one or more nodes</li></ul></li><li>replication group<ul><li>a collection of Redis clusters</li><li>with one primary read-write cluster and up to five secondary, read only clusters, which are called read replicas. </li><li>each read replica maintains a copy of the data from the primary cluster</li><li>asynchronous replication mechanisms are used to keep the read-replicas synchronized with the primary cluster </li><li>applications can <strong>read from any cluster in the replication group</strong> </li><li>applications can write only to the primary cluster</li><li>read replicas enhance scalability and guard against data loss </li></ul></li></ul><h2 id="4-3-Cache-hit-amp-Cache-Miss-Scenarios"><a href="#4-3-Cache-hit-amp-Cache-Miss-Scenarios" class="headerlink" title="4.3 Cache hit &amp; Cache Miss Scenarios"></a>4.3 Cache hit &amp; Cache Miss Scenarios</h2><ul><li>cache hit occurs when the cache contains the information required</li><li>cache miss occurs when the cache does not contain the information requested</li><li>ElastiCache caches data as key-value pairs.</li><li>An application can retrieve a value corresponding to a specific key. </li><li>An application can store an item in cache by specifying a key, value, and an expiration time(TTL). </li></ul><h2 id="4-4-Cache-strategies"><a href="#4-4-Cache-strategies" class="headerlink" title="4.4 Cache strategies"></a>4.4 Cache strategies</h2><ul><li>Lazy loading<ul><li>Whenever your application requests data, it first makes the request to the ElastiCache cache. </li><li>If the data exists in the cache and is current, ElastiCache returns the data to your application. </li><li>If the data does not exist in the cache, or the data in the cache has expired, your application requests the data from your data store which returns the data to your application. </li><li>Your application then writes the data received from the store to the cache so it can be more quickly retrieved next time it is requested.</li><li>Lazy Loading is a caching strategy that loads data into the cache only when necessary. </li><li>Avoid filling up the cache with unnecessary data</li><li>advantages<ul><li>Only requested data is cached. Since most data is never requested, lazy loading avoids filling up the cache with data that isn’t requested.</li><li>Node failures are not fatal. </li><li>When a node fails and is replaced by a new, empty node the application continues to function, though with increased latency. As requests are made to the new node each cache miss results in a query of the database and adding the data copy to the cache so that subsequent requests are retrieved from the cache.</li></ul></li><li>disadvantages<ul><li>a cache miss penalty, each cache miss results in 3 trips<ul><li>initial request for data from the cache</li><li>query of the database for the data</li><li>write the data to the cache </li></ul></li><li>may receive stale data because another application may have updated the data in the database behind the scenes. </li></ul></li></ul></li><li>write through <ul><li>this strategy adds data or updates data in the cache whenever data is written to the database </li><li>advantages<ul><li>data in the cache is never stale, always current</li></ul></li><li>disadvantages<ul><li>write penalty, involve two trips<ul><li>a write to cache, and a write to the database</li><li>missing data: When a new node is created to scale up or to replace a failed node, the node does not contain all data. Data continues to be missing until it is added or updated in the database. In this scenario, you might choose to use a lazy caching approach to repopulate the cache</li><li>Unused data: Since most data is never read, there can be a lot of data in the cluster that is never read.</li><li>Cache churn: The cache may be updated often if certain records are updated repeatedly.</li></ul></li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> Step Functions </tag>
            
            <tag> ElasticCache </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.8 SQS, SNS</title>
      <link href="/Developing-on-AWS-Note-8-SQS-SNS/"/>
      <url>/Developing-on-AWS-Note-8-SQS-SNS/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Why-use-a-queuing-service"><a href="#1-Why-use-a-queuing-service" class="headerlink" title="1. Why use a queuing service?"></a>1. Why use a queuing service?</h1><p>Consider a scenario where an application produces messages that must be processed by a consumer downstream. The producer needs to know how to connect to the consumer. If the consumer fails for some reason, then messages may be lost. If new consumer instances are launched to recover from failure or to keep up with an increased workload, the producer needs to be explicitly made aware of the new consumer instances. In this scenario, the producer is tightly coupled with the consumers and the coupling is prone to brittleness.</p><p>In this way, there will be a strong interdependency between teh consumer and the producer, which is a <strong>tightly coupled system</strong>. which is not fault tolerant, if any one component in our system fails, the entire system will fail. </p><ul><li>Having a queue service decouples the producer from the consumer.<ul><li>queue is a temprary repositiory for messages that are awaiting processing. </li><li>acts as a buffer between the component producing data and the component receiving the data for processing. </li><li>A queue supports multiple producers and consumers interacting with the same queue. </li><li>A single queue can be used <strong>simultaneously</strong> by many distributed application components, with no need for those components to coordinate with each other to share the queue. A queue delivers each message at least once.</li><li>In this way, a producer can put messages on the queue regardless if they are being read by the consumer or not. </li></ul></li></ul><h1 id="2-Developing-with-Amazon-Simple-Queue-Service-Amazon-SQS"><a href="#2-Developing-with-Amazon-Simple-Queue-Service-Amazon-SQS" class="headerlink" title="2. Developing with Amazon Simple Queue Service (Amazon SQS)"></a>2. Developing with Amazon Simple Queue Service (Amazon SQS)</h1><h2 id="2-1-Types"><a href="#2-1-Types" class="headerlink" title="2.1 Types"></a>2.1 Types</h2><ul><li>Standard queues<ul><li>message ordering is not guranteed</li><li>message may be duplicated </li><li>maximum throughput </li></ul></li><li>FIFO queue<ul><li>message ordering is preserved</li><li>message only receive once</li><li>limited throughput (300 transactions per second)</li></ul></li></ul><h2 id="2-2-Used-to-solve-tightly-linked-systems"><a href="#2-2-Used-to-solve-tightly-linked-systems" class="headerlink" title="2.2 Used to solve tightly linked systems"></a>2.2 Used to solve tightly linked systems</h2><ul><li>problem to be solved<ul><li>An example of image processing: the sequential operations of uploading, storing, and encoding the image, creating a thumbnail, and copyrighting are tightly linked to each other. This tight linkage complicates the recovery operations when there has been a failure.</li></ul></li><li>queuing chain pattern<ul><li>Achieve loose coupling of systems by using queues between systems and exchanging messages that transfer jobs</li><li>This enables asynchronous linking of systems.</li><li>lets you increase the number of virtual servers that receive and process the messages in parallel. </li><li>If there is no image to process, you can configure auto scaling to terminate the servers that are in excess.</li></ul></li></ul><h2 id="2-3-Operations"><a href="#2-3-Operations" class="headerlink" title="2.3 Operations"></a>2.3 Operations</h2><h3 id="2-3-1-Client"><a href="#2-3-1-Client" class="headerlink" title="2.3.1 Client"></a>2.3.1 Client</h3><ul><li><p>sendMessage </p><ul><li>send message to a specific queue</li><li>max size: 256 KB </li><li>parameters of a sendMessage operation<ul><li>QueueUrl: specify the url of the queue that the message should be sent to</li><li>MessageBody: specify the message to send </li><li>DelaySeconds: specify the number of seconds to delay a specific message. Messages will become available for processing after the delay time is finished. </li><li>MessageAttributes <ul><li>specify structured metadata about the message<ul><li>timestamp</li><li>signature</li><li>geospatial data</li></ul></li></ul></li></ul></li></ul></li><li><p>receiveMessage</p><ul><li>specify short polling or long polling </li><li>When requesting to get a message from the queue, <strong>you cannot specify which message to get</strong>. You simply specify the maximum number of messages you want to get (up to 10), and Amazon SQS returns up to that maximum number.</li><li>parameters <ul><li>WaitTimeSeconds</li><li>MaxNumberOfMessages</li><li>VisibilityTimeout<ul><li>period of time that a message is invisible to the rest of your application after an application component gets it from the queue.</li><li>prevents multiple components from processing the same message </li><li>during the visibility time, the component that received the message usually processes it and then delete it from the queue. </li><li>this prevents multiple components from processing the same message </li></ul></li></ul></li><li>polling types<ul><li>short polling<ul><li>Amazon SQS samples a subset of the servers (based on a weighted random distribution) and returns messages from only the sampled servers</li><li>if you keep retrieving from your queues, SQS samples all the servers, and you will eventually receive all of your messages. </li><li>occurs when the WaitTimeSeconds parameter of a ReceiveMessage call is set to 0 or the queue attribute ReceiveMessageWaitTimeSeconds is 0</li></ul></li><li>long polling<ul><li>better and preferred way to retrieve messages </li><li>if your application has a single thread polling multiple queues, switching from short polling to long polling will likely not work, because the single thread will wait for the long poll timeout on any empty queues, delaying the processing of any queues which may contain messages. </li><li>Amazon SQS long polling doesn’t return a response until a message arrives in the queue or the long poll times out </li><li>inexpensive </li><li>unless the connection time out, the response to the ReceiveMessage request will contain at least one of the available messages. </li><li>Reduce the cost of using Amazon SQS by reducing the number of empty responsed and false empty responses. </li></ul></li></ul></li></ul></li></ul><ul><li>deleteMessage<ul><li>When you receive the message, you <strong>must delete it from the queue</strong> to acknowledge that you processed the message and no longer need it. </li><li>You specify which message to delete by providing the <strong>receipt handle</strong> that Amazon SQS returned when you received the message.</li></ul></li><li>deleteMessageBatch</li><li>PuregeQueue<ul><li>delete all the messages in an AmazonSQS queue without deleting the queue itself.  </li></ul></li></ul><h3 id="2-3-2-Basic-Queue-Operations"><a href="#2-3-2-Basic-Queue-Operations" class="headerlink" title="2.3.2 Basic Queue Operations"></a>2.3.2 Basic Queue Operations</h3><ul><li>CreateQueue <ul><li>attributes<ul><li>delaySeconds<ul><li>the delivery of all messages in the queue will be delayed </li><li>default 0, maximum 15 min</li></ul></li><li>maximumMessageSize<ul><li>the limit of how many bytes a message can contain before Amazon SQS rejects it </li><li>max 256 KB</li></ul></li><li>messageRetentionPeriod<ul><li>seconds SQS retains a message </li></ul></li><li>ReceiveMessageWaitTimeSeconds<ul><li>time for which a ReceiveMessage call will wait for a message to arrive </li><li>max configurable wait time is 20 seconds </li><li>default 0</li></ul></li><li>VisibilityTimeout<ul><li>period of time that a message is invisbile to the rest of your application  </li></ul></li></ul></li></ul></li><li>SetQueueAttributes</li><li>GetQueueAttributes</li><li>GetQueueUrl</li><li>ListQueues</li><li>DeleteQueue</li></ul><h2 id="2-4-Message-Lifecycle"><a href="#2-4-Message-Lifecycle" class="headerlink" title="2.4 Message Lifecycle"></a>2.4 Message Lifecycle</h2><ul><li>Immediately after a message is received, it <strong>remains in the queue</strong>. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. </li><li>The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.</li><li>Or consumer could send a separate request which acknowledges that you no longer need the message because you have successfully received and processed it </li><li>Maximum message retention period<ul><li>SQS automatically deletes messages that have been in a queue for more than maximum message retension period</li><li>default is 4 days </li><li>can be set from 60 seconds to 14 days </li></ul></li></ul><h2 id="2-5-Queue-and-Message-Identifiers"><a href="#2-5-Queue-and-Message-Identifiers" class="headerlink" title="2.5 Queue and Message Identifiers"></a>2.5 Queue and Message Identifiers</h2><ul><li>Queue URL <ul><li>when creating a new queue, must provide a queue name that is unique within the scope of all your queues. </li><li>AWS assgin each queue an identifier called a <strong>queue URL</strong>, which includes the queue name and other components that Amazon SQS determines. </li></ul></li><li>Message ID<ul><li>For each message, Amazon SQS returns a system-assigned message ID in the SendMessage response. </li></ul></li><li>Receipt Handle<ul><li>Each time you receive a message from a queue, you receive a receipt handle for that message. </li><li>The handle is assoiciated with the act of receiving the message, not with the message itself. </li><li>To delete a message, you need the message’s receipt handle instead of the message ID. </li></ul></li></ul><h2 id="2-6-Dead-letter-queues"><a href="#2-6-Dead-letter-queues" class="headerlink" title="2.6 Dead letter queues"></a>2.6 Dead letter queues</h2><ul><li>A queue of messages that were not able to be processed </li><li>Use dead-letter queues with standard queues.</li><li>Dead letter queues help you troubleshoot incorrect message transmission operations </li></ul><h2 id="2-7-Sharing-a-Queue"><a href="#2-7-Sharing-a-Queue" class="headerlink" title="2.7 Sharing a Queue"></a>2.7 Sharing a Queue</h2><ul><li>Shared queues<ul><li>Queue can be shared with other AWS accounts</li><li>Queue can be shared anonymously </li><li>A permission gives access to another person to use your queue in some particular way</li><li>A policy is the actual document that contains the permissions you granted </li></ul></li></ul><h2 id="2-8-Use-cases"><a href="#2-8-Use-cases" class="headerlink" title="2.8 Use cases"></a>2.8 Use cases</h2><ul><li>Work queues<ul><li>decouple components of a distributed application that may not all process the same amount of work simultaneourly </li></ul></li><li>Buffer and batch operations<ul><li>add scalability and reliability to your architecture and smooth out temporary volume spikes without losing messages or increasing latency </li></ul></li><li>request offloading<ul><li>move slow operations off of interactive request paths by enqueuing the request</li></ul></li><li>Auto scaling<ul><li>Use queue to help determine the load on an application, and when combined with auto sclaing, you can sclae the numebr of Amazon Ec3 intances out or in, depending on the volumne of traffic </li></ul></li><li>fan out<ul><li>combine SQS with SNS to send identical copies of a message to multiple queues in parallel for simultaneous processing  </li></ul></li></ul><h1 id="3-Amazon-Simple-Notification-Service"><a href="#3-Amazon-Simple-Notification-Service" class="headerlink" title="3. Amazon Simple Notification Service"></a>3. Amazon Simple Notification Service</h1><h2 id="3-1-Introduction"><a href="#3-1-Introduction" class="headerlink" title="3.1 Introduction"></a>3.1 Introduction</h2><ul><li>A web service that makes it easy to set up, operate and send notifications from the cloud.</li><li>Follow the publish-subscribe messaging paradigm, with notifications being delivered to clients using a push mechanism that eliminates the need to periodically check or poll for new information and updates </li><li>When using Amazon SNS, you (as the owner) create a <strong>topic</strong> and <strong>control access to it by defining policies</strong> that determine which publishers and subscribers can communicate with the topic. </li><li>A publisher sends messages to topics they have created or to topics they have permission to publish to. </li><li>Instead of including a specific destination address in each message, a publisher sends a message to the topic. </li><li>Amazon SNS matches the topic to a list of subscribers who have subscribed to that topic and delivers the message to each of those subscribers. </li><li>Each topic has a <strong>unique name</strong> that identifies the Amazon SNS endpoint for <strong>publishers to post messages and subscribers to register for notifications</strong>. Subscribers receive all messages published to the topics that they subscribe to, and all subscribers to a topic receive the same messages.</li><li>Subscriber<ul><li>Web servers </li><li>email addresses</li><li>amazon sqs queues</li><li>aws lambda</li></ul></li><li>topic <ul><li>an access point for allowing recipients to dynamically subscribe for identical copies of the same notification </li></ul></li></ul><h2 id="3-2-Use-case-Fan-out"><a href="#3-2-Use-case-Fan-out" class="headerlink" title="3.2 Use case: Fan out"></a>3.2 Use case: Fan out</h2><ul><li>An Amazon SNS message is sent to a topic and then replicated and pushed to multiple Amazon SQS queues, HTTP endpoints, or email addresses. </li><li>Allow for parallel asynchronous processing</li><li>All subscribers get identical information </li></ul><h2 id="3-3-Operations"><a href="#3-3-Operations" class="headerlink" title="3.3 Operations"></a>3.3 Operations</h2><ul><li>CreateTopic<ul><li>Input: Topic name </li><li>Output: ARN of topic </li><li>creates a topic to which notifications can be published </li><li>action is idempotent, so if the requester already owns a topic with the specified name, that topic’s ARN is returned without creating a new topic </li></ul></li><li>Subscribe<ul><li>Input<ul><li>subscriber’s endpoint</li><li>protocol </li><li>ARN of topic </li></ul></li><li>prepare to subscribe an endpoint by sending the endpoint a confirmation message</li><li>to actually create a subscription, the endpoint owner must call the confirmSubscription action with the token from the confirmation message. </li><li>The ConfirmSubscription request verify an endpoint owner’s intent to receive messages by validating the token sent to the endpoint by an earlier Subscribe action. </li><li>If the token is valid, the action creates a new subscription and returns its ARN </li></ul></li><li>DeleteTopic <ul><li>Input<ul><li>ARN of topic  </li></ul></li><li>Deleting a topic might prevent some messages previously sent to the topic being delivered to subscribers</li><li>Action is idempotent, will not result in an error if the topic doesn’t exist</li></ul></li><li>Publish <ul><li>Input <ul><li>Message</li><li>Messsage attributes</li><li>Message structure : json</li><li>subject </li><li>ARN of topic </li></ul></li><li>output <ul><li>message ID </li></ul></li><li>Sends a message to all of a topic’s subscribed endpoints.</li><li>When a messageId is returned, the message has been saved and Amazon SNS will attempt to deliver it to the topic’s subscribers shortly. </li><li>The format of the outgoing message to each subscribed endpoint depends on the notification protocol selected.</li></ul></li></ul><h2 id="3-4-Best-practices"><a href="#3-4-Best-practices" class="headerlink" title="3.4 Best practices"></a>3.4 Best practices</h2><h3 id="3-4-1-Characteristics-of-Amazon-SNS"><a href="#3-4-1-Characteristics-of-Amazon-SNS" class="headerlink" title="3.4.1 Characteristics of Amazon SNS"></a>3.4.1 Characteristics of Amazon SNS</h3><ul><li>Each notification message contains a single published message</li><li>Message order is not guaranteed</li><li>A message cannot be deleted after it has been published </li><li>Amazon SNS delivery policy can be used to control retries in case of message delivery failure</li><li>Message can contain up to 256 kb of text data</li></ul><h3 id="3-4-2-Manage-Access-to-Amazon-SNS"><a href="#3-4-2-Manage-Access-to-Amazon-SNS" class="headerlink" title="3.4.2 Manage Access to Amazon SNS"></a>3.4.2 Manage Access to Amazon SNS</h3><ul><li>which endpoints</li><li>who can publish notifications</li><li>who can subscribe to notifications</li></ul><h3 id="3-4-3-SQS-vs-SNS"><a href="#3-4-3-SQS-vs-SNS" class="headerlink" title="3.4.3 SQS vs SNS"></a>3.4.3 SQS vs SNS</h3><ul><li>both messaging services within AWS</li><li>SNS<ul><li>allow applications to send time-critical messages to multiple subscribers through a push mechanism</li><li>eliminate the need to periodically check or poll for updates </li></ul></li><li>SQS <ul><li>message queue service used by distributed applications to exchange messages through a polling model, and it can be used to decouple sending and receiving components. </li><li>provides flexibility for distributed components of applications to send and receive messages without requiring each component to be concurrently available</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> SQS </tag>
            
            <tag> SNS </tag>
            
            <tag> Notification Service </tag>
            
            <tag> Queue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.7 API Gateway</title>
      <link href="/Developing-on-AWS-Note-7-API-Gateway/"/>
      <url>/Developing-on-AWS-Note-7-API-Gateway/</url>
      
        <content type="html"><![CDATA[<h1 id="1-What-is-Amazon-API-Gateway"><a href="#1-What-is-Amazon-API-Gateway" class="headerlink" title="1. What is Amazon API Gateway?"></a>1. What is Amazon API Gateway?</h1><h2 id="1-1-Functionalities"><a href="#1-1-Functionalities" class="headerlink" title="1.1 Functionalities"></a>1.1 Functionalities</h2><ul><li>enables developers to create, publish, maintain, monitor and secure APIs</li><li>allow you to connect your applications to AWS services and other public or private websites</li><li>provides consistent RESTFUL APIs for mobile and web applications to access AWS services and other resources hosted outside of AWS</li><li>Handles all the tasks involved in <strong>accepting and processing</strong> up to hundreds of thousands of concurrent API calls, including <strong>traffic management, authorization and access control, monitoring and API version management</strong></li></ul><h2 id="1-2-Use-cases"><a href="#1-2-Use-cases" class="headerlink" title="1.2 Use cases"></a>1.2 Use cases</h2><ul><li><p>Create a unified API frontend for multiple microservices</p></li><li><p>DDoS protection and throttling for backend </p></li><li><p>Authenticate and authorize requests to a backend</p></li><li><p>Throttle, meter, and monetize API usage by third party developers </p></li><li><p>message transformation and validation</p><ul><li><strong>models</strong> can be created to define a schema for reqeust/ response messages</li><li>A <strong>Mapping Template</strong> can then be used to transform data from one model to another</li><li>request/ response payload and header can be validated against the model</li><li>message transformation and mapping can be done using API Gateway</li><li>customers will often map request messages to a canonical format for downstream applications using API Gateway.  –&gt; <strong>transform a response body from the backend data format to the frontend data format</strong></li></ul></li><li><p>Expose backend resources</p><ul><li>allow you to create an API that acts as a front door for applications to access data, business logic or functionality from your backend service</li><li>expose<ul><li>HTTP endpoints</li><li>AWS services</li><li>AWS Lambda functions</li></ul></li></ul></li><li><p>Increase API performance : Cache</p><ul><li>eploy APIs to Regional or Edge-optimized endpoints to bring them closer to their clients. Cache API responses to the API Gateway response cache.</li><li>You can also enable API caching in Amazon API Gateway to cache your endpoint’s response. </li><li>With caching, you can reduce the number of calls made to your endpoint and also improve the latency of the requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. </li><li>API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint</li></ul></li><li><p>Control Access to APIs</p><ul><li>method level throttling </li><li>client usage throttling and quota limits sepcified in a usage plan </li><li>help prevent one customer from consuming all of your backend system’s capacity</li></ul></li><li><p>Secure API method invocations</p><ul><li>creating a resource policy <ul><li>a JSON policy document that you attach to an API to control whether a specified principal(IAM user or role) can invoke the API </li><li>You can use a resource policy to enable users from a different AWS account to securely access your API or to allow the API to be invoked only from specified source IP address ranges or Classless Inter-Domain Routing (CIDR) blocks.</li></ul></li><li>Creating IAM permission policy, can protect: <ul><li>the creation, deployment, and management of an API</li><li>the invocation of the methods in the API and refresh of its cache</li></ul></li><li>Creating a Private API endpoint that can only be accessed by a VPC client</li><li>Integrating with Amazon Cognito or Lambda authorizers to authenticate and authorize clients before accessing backend resources</li><li>Resource policy and IAM permission capabilities offer flexible and robust access controls that can be applied to an entire API set or individual methods. <h1 id="2-Best-practices"><a href="#2-Best-practices" class="headerlink" title="2. Best practices"></a>2. Best practices</h1></li></ul></li></ul><h2 id="2-1-Developing-an-API"><a href="#2-1-Developing-an-API" class="headerlink" title="2.1 Developing an API"></a>2.1 Developing an API</h2><ul><li>WHen API client requests come from the same region where the API is deployed, choose a regional API endpoint type</li><li>Test invking the API before deploying it</li><li>Use HTTP 500 error code for error handling </li><li>Cache only GET methods </li></ul><h1 id="3-Serverless-Application-Model-SAM"><a href="#3-Serverless-Application-Model-SAM" class="headerlink" title="3. Serverless Application Model (SAM)"></a>3. Serverless Application Model (SAM)</h1><p>Template driven development model for defining serverless apps</p><ul><li>supports <ul><li>Lambda</li><li>API Gateway</li><li>DynamoDB table</li><li>Any resource that AWS CloudFormation supports </li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> Gateway </tag>
            
            <tag> RESTFul </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.6 AWS Serverless platform, Lambda</title>
      <link href="/Developing-on-AWS-Note-6-AWS-Serverless-platform-Lambda/"/>
      <url>/Developing-on-AWS-Note-6-AWS-Serverless-platform-Lambda/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Serverless-Computing"><a href="#1-Serverless-Computing" class="headerlink" title="1. Serverless Computing"></a>1. Serverless Computing</h1><h2 id="1-1-Benefits"><a href="#1-1-Benefits" class="headerlink" title="1.1 Benefits"></a>1.1 Benefits</h2><ul><li>with serverless deployment and operation, only need to<ul><li>build and deploy apps</li><li>monitor and maintain apps</li></ul></li><li>no need to provision, scale and manage any servers </li></ul><h2 id="1-2-Use-cases"><a href="#1-2-Use-cases" class="headerlink" title="1.2 Use cases"></a>1.2 Use cases</h2><ul><li>web applications <ul><li>automatically scale up and down </li><li>run in a highly available configuration across multiple data centers </li></ul></li><li>backends<ul><li>build serverless backends using AWS lambda to handle web, mobile, internet of Things(IoT), and 3rd party APR requests </li></ul></li><li>mobile backends</li><li>data processing <ul><li>execute code in response to triggers <ul><li>changes in data </li><li>shifts in system state</li><li>actions by users</li></ul></li></ul></li></ul><h1 id="2-AWS-serverless-Platform"><a href="#2-AWS-serverless-Platform" class="headerlink" title="2. AWS serverless Platform"></a>2. AWS serverless Platform</h1><ul><li>Compute<ul><li>AWS lambda</li><li>AWS Fargate</li></ul></li><li>API Proxy<ul><li>Amazon API Gateway</li><li>AWS AppSync</li></ul></li><li>Storage<ul><li>Amazon S3</li></ul></li><li>Database<ul><li>Amazon DynamoDB</li><li>Amazon Aurora</li></ul></li><li>Interprocess Messaging<ul><li>Amazon SNS</li><li>Amazon SQS</li></ul></li><li>Orchetration<ul><li>AWS Step Functions </li></ul></li><li>Analytics<ul><li>Amazon Kinesis</li><li>Amazon Athena</li></ul></li><li>Developer Tools<ul><li>Frameworks</li><li>SDKs</li><li>Libraries</li></ul></li></ul><p>Serverless applications don’t require provisioning, maintaining, and administering servers for backend components such as compute, databases, storage, stream processing, message queueing, and more. You also no longer need to worry about ensuring application fault tolerance and availability.</p><h1 id="3-AWS-Lambda"><a href="#3-AWS-Lambda" class="headerlink" title="3. AWS Lambda"></a>3. AWS Lambda</h1><h2 id="3-1-What-is-AWS-Lambda"><a href="#3-1-What-is-AWS-Lambda" class="headerlink" title="3.1 What is AWS Lambda?"></a>3.1 What is AWS Lambda?</h2><ul><li>Compute service that enables you to run code without provisioning or managing servers. </li><li>Pay only for the compute time you consume </li><li>Run code for virtually any type of application or backend service, all with zero administration. </li><li>can set up code to automatically trigger from other AWS services or call it directly from any web or mobile app</li></ul><h2 id="3-2-Concepts"><a href="#3-2-Concepts" class="headerlink" title="3.2 Concepts"></a>3.2 Concepts</h2><ul><li>Event source - what triggers the call<ul><li>Used to pass in event data to the handler </li><li>java/C# supports simple data types and stream input/ output</li><li>includes all of the data and metadata Lambda needs</li></ul></li><li>Context object<ul><li>provides handler runtime information </li><li>interact with Lambda execution environment </li><li>contain<ul><li>AWS requestId - Used to track specific invocations of a Lambda function</li><li>Remaining time - The amount of time in milliseconds that remain before your function timeout occurs</li><li>logging - Each language runtime provides the ability to stream log statements to Amazon CloudWatch Logs.</li></ul></li></ul></li><li>Language choice</li><li>Execution environment - permissions and resources</li><li>Runtime <ul><li>a program that runs a lambda function’s handler method when the function is invoked</li><li>can include a runtime in your function’s deployment package in the form of an executable file named bootstrap</li><li>responsible for running the function’s setup code</li><li>read the handler name</li><li>read invocation events from the runtime API</li><li>runtime passes the event data to the function handler, and posts response from the handler back to Lambda</li></ul></li><li>handler function<ul><li>When a Lambda function is invoked, code execution begins at what is called the handler. The handler is a specific code method (Java, C#) or function (Node.js, Python) that you’ve created and included in your package. </li></ul></li></ul><h2 id="3-3-Using-Lambda"><a href="#3-3-Using-Lambda" class="headerlink" title="3.3 Using Lambda"></a>3.3 Using Lambda</h2><ul><li>Bring own code <ul><li>bring own libraries</li><li>custom runtimes </li></ul></li><li>Simple resource model <ul><li>CPU and network allocated proportionately</li></ul></li><li>Flexible Use<ul><li>Synchronous/ Asynchronous</li><li>Integrated with other AWS services</li></ul></li><li>Flexible Authorization<ul><li>securely grant access to resources and VPCs </li><li>Fine grained control for invoking your functions </li></ul></li></ul><h2 id="3-4-How-it-works"><a href="#3-4-How-it-works" class="headerlink" title="3.4 How it works"></a>3.4 How it works</h2><p>Function can be invoked by</p><ul><li>push model<ul><li>event based invocation </li><li>event sources invoke your Lambda function </li><li>e.g<ul><li>S3, SNS, Cognito, Echo </li></ul></li></ul></li><li>request-response invocation <ul><li>causes Lambda to execute the function <strong>synchronously</strong> and returns the response immediately to the calling application. This invocation type is available for custom applications</li></ul></li><li>pull event model<ul><li>Lambda polls the event source and invokes function when it detects an event</li><li>E.G<ul><li>DynamoDB, SQS, Kinesis </li></ul></li></ul></li></ul><h2 id="3-5-Develop-and-deploy-workflow"><a href="#3-5-Develop-and-deploy-workflow" class="headerlink" title="3.5 Develop and deploy workflow"></a>3.5 Develop and deploy workflow</h2><ul><li>create a lambda handler class in code</li><li>create lambda function </li><li>allow Lambda to assume an IAM role</li><li>upload the code</li><li>invoke the AWS Lambda Function </li><li>Monitor function </li></ul><h2 id="3-6-Lambda-Layers"><a href="#3-6-Lambda-Layers" class="headerlink" title="3.6 Lambda Layers"></a>3.6 Lambda Layers</h2><ul><li><p>Centrally manage code and data that is shared across multiple functions</p><ul><li>reduce size of deployments</li><li>speed up deployment </li><li>Limits<ul><li>5 layers</li><li>250 MB</li></ul></li></ul></li><li><p>Layer</p><ul><li>ZIP archive that contain libraries, a custom runtime, or other dependencies</li><li>with layers, you can use libraries in your function without needing to include them in deployment package</li><li>extracted to the /opt directory in the function execution env </li><li>use AWS Serverless Application Model (AWS SAM) to manage layers and your function’s layer configuration</li></ul></li></ul><h2 id="3-7-Best-practices"><a href="#3-7-Best-practices" class="headerlink" title="3.7 Best practices"></a>3.7 Best practices</h2><ul><li>Function Code<ul><li>Separate the Lambda handler (entry point) from your core logic <ul><li>can make a more unit-testable function </li></ul></li><li>take advantage of Execution Context reuse<ul><li>make sure any externalized configuration or dependencies that your code retrieves are stored and referenced locally after initial execution</li><li>Limit the re-initialization of variables/objects on every invocation. Instead use static initialization/constructor, global/static variables and singletons. Keep alive and reuse connections (HTTP, database, etc.) that were established during a previous invocation.</li></ul></li><li>use environment variables</li><li>control the dependencies in your function’s deployment package</li><li>minimize the complexity of your dependencies <ul><li>Prefer simpler Java dependency injection frameworks like Dagger or Guice, over more complex ones like Spring Framework</li></ul></li><li>avoid using recursive code </li><li>share common dependencies with layers</li></ul></li><li>Function Configuration<ul><li>performance testing your Lambda function for memory<ul><li>crucial part in ensuring you pick the optimum memory size configuration</li><li>Any increase in memory size triggers an equivalent increase in CPU available to your function. </li><li>The memory usage for your function is determined per-invoke and can be viewed in AWS CloudWatch logs.</li></ul></li><li>Load test Lambda Function<ul><li>Determine an optimum timeout value</li><li>Important to analyze how long your function runs so that you can better determine any problems with a dependency service that may increase the concurrency of the function beyond what you expect</li><li>This is especially important when your Lambda function makes network calls to resources that may not handle Lambda’s scaling.</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> Lambda </tag>
            
            <tag> Serverless </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.5 DynamoDB</title>
      <link href="/Developing-on-AWS-Note-5-DynamoDB/"/>
      <url>/Developing-on-AWS-Note-5-DynamoDB/</url>
      
        <content type="html"><![CDATA[<h1 id="1-AWS-Database-Options"><a href="#1-AWS-Database-Options" class="headerlink" title="1. AWS Database Options"></a>1. AWS Database Options</h1><h2 id="1-1-SQL-vs-NoSQL-database"><a href="#1-1-SQL-vs-NoSQL-database" class="headerlink" title="1.1 SQL vs NoSQL database"></a>1.1 SQL vs NoSQL database</h2><table><thead><tr><th>Attr</th><th>SQL</th><th>NoSQL</th></tr></thead><tbody><tr><td>Data Storage</td><td>rows and columns</td><td>key-value, document, wide-column, graph</td></tr><tr><td>Schemas</td><td>fixed</td><td>dynamic</td></tr><tr><td>Querying</td><td>Using SQL</td><td>Focused on collection of documents</td></tr><tr><td>Scalability</td><td>Vertical</td><td>Horizontal</td></tr><tr><td>Transactions</td><td>Supported</td><td>Support varies</td></tr><tr><td>Consistency</td><td>Strong</td><td>Eventual and strong</td></tr></tbody></table><ul><li>Relational Database supports vertical scaling which means that a single server must be made more powerful </li><li>Relational Database support ACID transactions<ul><li>atomicity</li><li>consistency</li><li>isolation </li><li>durability</li></ul></li><li>Relational databases automatically support strong data consistency due to ACID properties of transactions.</li></ul><h2 id="1-2-AWS-database-Options"><a href="#1-2-AWS-database-Options" class="headerlink" title="1.2 AWS database Options"></a>1.2 AWS database Options</h2><table><thead><tr><th>Type</th><th>SQL</th><th>NoSQL</th></tr></thead><tbody><tr><td>Transactional Databses</td><td>Amazon RDS</td><td>Amazon DynamoDB</td></tr><tr><td>Data Analytics/ Relationshiups</td><td>Amazon Redshift</td><td>Amazon Neptune</td></tr><tr><td>In-memory Data Store and Cache</td><td></td><td>Amazon ElastiCache</td></tr></tbody></table><ul><li>Amazon Relational Database Service(RDS): provides relational database services in the cloud with support for the following db engines: <ul><li>AMazon Aurora</li><li>PostgreSQL</li><li>MySQL</li><li>MariaDB</li><li>Oracle</li><li>Microsoft SQL server </li></ul></li><li>Amazon Redshift: fast, fully managed data warehouse<ul><li>includes Redshift Spectrum, allowing you to directly run SQL queries against exabytes of unstructured data in Amazon S3. </li></ul></li><li>Amazon DynamoDB: NoSQL db that supports both document and key-value store models </li><li>Amazon Neptune: fully managed graph databse service<ul><li>fully manged graoh databse service </li><li>purpose-built, high-performance <strong>graph database engine</strong> optimized for storing billions of <strong>relationships</strong> and querying the graph with milliseconds latency.</li><li>A graph database is ideal when you need to create relationships between data and quickly query these relationships. </li><li>This type of requirement is challenging to satisfy using a relational database because you would need multiple tables with multiple foreign keys. </li><li>In addition, SQL queries to navigate this data would require nested queries and complex joins that could quickly become complex and inefficient as your data size grows over time. </li><li>Neptune uses graph structures such as nodes (data entities), edges (relationships), and properties to represent and store data. </li><li>The relationships are stored as first order citizens of the data model. </li><li>This allows data in nodes to be directly linked, dramatically improving the performance of queries that navigate relationships in the data.</li></ul></li><li>Amazon ElastiCache: <strong>in-memory data cache</strong> that supports a fully managed Redis or Memcached engine<ul><li>easier to deploy, operate, and scale an in-memory data store or cache in the cloud. </li><li>improve the performance of web applications by allowing you to retrieve information from fast, managed, in memory caches</li><li>provides <ul><li>redis</li><li>memcached</li></ul></li></ul></li></ul><h1 id="2-DynamoDB"><a href="#2-DynamoDB" class="headerlink" title="2. DynamoDB"></a>2. DynamoDB</h1><h2 id="2-1-Intro"><a href="#2-1-Intro" class="headerlink" title="2.1 Intro"></a>2.1 Intro</h2><p>Amazon DynamoDB is a fast and flexible non-relational database service for all applications that need consistent, <strong>single-digit millisecond latency</strong> at any scale. It is a fully managed cloud database and supports both document and key-value store models.</p><h2 id="2-2-Components"><a href="#2-2-Components" class="headerlink" title="2.2 Components"></a>2.2 Components</h2><ul><li>Table <ul><li>data is stored in tables </li><li>contain<ul><li>item with attributes </li></ul></li></ul></li><li>Partition<ul><li>ddb can divide a table’s items into multiple partitions based on the primary key value. </li><li>an allocation of storage for a table</li><li>backed by SSDs and automatically replicated across multiple Availability Zones within an AWS region</li><li>partition key (hashkey), ddb use this to do partition </li></ul></li><li>sort key (range key) A sort key can be defined to store all of the items with the same partition key value <strong>physically close together and order them by sort key value in the partition</strong>. It represents a one-to-many relationship based on the partition key and enables querying on the sort key attribute.</li><li>Primary key - uniquely identify an Item<ul><li>types<ul><li>partition primary key</li><li>partition and sort primary key</li></ul></li></ul></li><li>item (400 KB at most)<ul><li>collection of attributes </li><li>not cosntrained by a predefined schema</li><li>items in a table can have different types of attributes </li></ul></li><li>attribute<ul><li>name</li><li>data type<ul><li>scalar<ul><li>number, string, binary, boolean, null</li></ul></li><li>multi-valued types <ul><li>string set</li><li>number set</li><li>binary set</li></ul></li><li>Document types<ul><li>List</li><li>Map</li></ul></li></ul></li><li>value</li></ul></li><li>Read/ Write Consistency<ul><li>Read<ul><li>eventually consistent</li><li>strongly consistent: return most up-to-date data</li><li>transactional: provides ACID consistency</li></ul></li><li>write<ul><li>standard</li><li>transactional </li></ul></li></ul></li><li>Read/ write Throughput<ul><li>RCU: number of strongly consistent reads per second of items up to <strong>4KB</strong> in size </li><li>WCU: number of <strong>1KB</strong> writes per second</li></ul></li><li>Secondary Indexes<ul><li>allow you to query data based on non-primary key attributes</li><li>contain<ul><li>alternate key attributes</li><li>primary key attributes</li><li>optinal subset of other attributes from the base table </li></ul></li><li>type<ul><li>GSI<ul><li>queries on this index can span all the data in a table, across all partitions</li><li>can have different partition key and sort key from original table</li><li>key values do not to be unique</li><li>can be deleted </li><li>supports eventually consistent only </li><li>its own provisioned WCU and RCU</li><li>*<em>queries only return attributes that are projected into the index *</em></li></ul></li><li>LSI<ul><li>index is located on the same table partition</li><li>sort key can be any scalar attribute </li><li>cannot be deleted </li><li>support eventually consitent and strong consistent </li><li>use table’s read and write capacity units</li></ul></li></ul></li></ul></li><li>Streams<ul><li>Ordered flow of information about changes to a table </li><li>contains changes to items in a single table </li><li>When you make an update to a table, DynamoDB first <strong>persists the data durably</strong> to the table. </li><li>It then asynchronously updates the corresponding stream with information about the changes made. </li><li>The asynchronous update is made to the stream with <strong>sub-second latency</strong>. </li><li>The update to the stream does not affect the write throughput of the table</li><li><strong>stricly in the order</strong> </li><li>each change contains exactly one stream record, available for 24 hours</li><li>streams scale by splitting data across shards </li><li>shards in detail<ul><li>A shard is created per partition in your DynamoDB table. If a partition split is required due to too many items in the same partition, the shard gets split into children as well.</li><li>DynamoDB Streams captures a time-ordered sequence of item-level modifications in your DynamoDB table. This time-ordered sequence is preserved at a per shard level. In other words, the order within a shard is established based on the order in which items were created, updated or deleted. </li></ul></li><li>configuration<ul><li>StreamEnabled: specify whether a stream is enabled or disabled </li><li>StreamViewType: specify the information that will be written to the stream whenever data in the table is modified<ul><li>KEYS_ONLY: only the key attributes </li><li>NEW_IMAGE: entire item, as it appears after modified</li><li>OLD_IMAGE: entire item, as it appears before modified</li><li>NEW_AND_OLD_IMAGES: both the new and old images of the item</li></ul></li></ul></li><li>when a stream is created, DDB assigns an ARN(Amazon Resource Name) that can be used to retrieve information about a stream. </li></ul></li><li>Global table<ul><li>A collection of one or more DynamoDB tables, all ownd by a single AWS account, identified as replica tables </li><li>A replica table (or replica, for short) is a single DynamoDB table that functions as a part of a global table. Each replica stores the same set of data items.</li><li>data replication<ul><li>Any changes made to any item in any replica table will be replicated to all of the other replicas within the same global table. </li><li>propagate within seconds </li></ul></li><li>concurrent updates <ul><li>all replicas agree on the latest update, and converge toward a state in which they all have identical data</li></ul></li><li>Read Consistency <ul><li>An application can read and write data to any replica table. </li><li>If your application only uses eventually consistent reads, and only issues reads against one AWS region, then it will work without any modification. </li><li>However, if your application requires strongly consistent reads, then it must perform all of its strongly consistent reads and writes in the same region. DynamoDB does not support strongly consistent reads across AWS regions; </li><li>therefore, if you write to one region and read from another region, the read response might include stale data that doesn’t reflect the results of recently-completed writes in the other region. </li></ul></li></ul></li><li>Backup and Restore<ul><li>on-demand backup and restore capabilities </li><li>all backups in DDB work without consuming any provisioned throughput on the table </li><li>point-in-time recovery can restore the table to any point in time during last 35 days</li></ul></li></ul><h2 id="2-3-APIs-and-operations"><a href="#2-3-APIs-and-operations" class="headerlink" title="2.3 APIs and operations"></a>2.3 APIs and operations</h2><h3 id="2-3-1-Control-operations"><a href="#2-3-1-Control-operations" class="headerlink" title="2.3.1 Control operations"></a>2.3.1 Control operations</h3><p>Create and manage DynamoDB tables. Let you work with indexes, streams, and other objects that are dependent on tables. </p><h3 id="2-3-2-Data-operations"><a href="#2-3-2-Data-operations" class="headerlink" title="2.3.2 Data operations"></a>2.3.2 Data operations</h3><p>Perform CRUD actions on data in a table. Also let you read data from a secondary index. </p><ul><li>PutItem<ul><li>create a new item or replace an existing item </li></ul></li><li>GetItem<ul><li>reads an item from a table</li></ul></li><li>UpdateItem<ul><li>edit an existing item’s attributes, or adds a new item to the table</li><li>can perform a conditional update on an existing item </li><li>can only bring some attributes instead of all comparing with putItem</li></ul></li><li>deleteItem<ul><li>can delete an item in a table using its primary key  </li></ul></li></ul><h3 id="2-3-3-Stream-operations"><a href="#2-3-3-Stream-operations" class="headerlink" title="2.3.3 Stream operations"></a>2.3.3 Stream operations</h3><p>Enable or disable a stream on a table, and allow access to the data modification records contained in a stream. </p><h3 id="2-3-4-Object-persistence-Model"><a href="#2-3-4-Object-persistence-Model" class="headerlink" title="2.3.4 Object persistence Model"></a>2.3.4 Object persistence Model</h3><ul><li>Allow you to persist client-side objects in DynamoDB<ul><li>supports the mapping of objects to tables</li></ul></li><li>Provides higher-level programming interfaces to:<ul><li>connect to DynamoDB</li><li>perform CRUD operations</li><li>execute queries</li></ul></li></ul><h3 id="2-3-5-Batch-Operations"><a href="#2-3-5-Batch-Operations" class="headerlink" title="2.3.5 Batch Operations"></a>2.3.5 Batch Operations</h3><ul><li>BatchGetItem<ul><li>Read up to 16MB of data consisting of up to 100 items from multiple tables</li></ul></li><li>BatchWriteItem<ul><li>write up to 16MB of data consisting of up to 25 put or delete requests in multiple tables </li></ul></li><li>retry <ul><li>if one request in a batch fails, the entire operation does not fail</li><li>retry with failed keys and data returned </li></ul></li></ul><h3 id="2-3-6-Transactional-Operations"><a href="#2-3-6-Transactional-Operations" class="headerlink" title="2.3.6 Transactional Operations"></a>2.3.6 Transactional Operations</h3><ul><li>TransactWriteItems<ul><li>contains a write set </li><li>includes one or more PutItem, updateItem, and DeleteItem operations across</li></ul></li><li>TransactGetItems<ul><li>contains a read set </li><li>includes one or more getItem operations across multiple tables</li></ul></li></ul><h2 id="2-4-On-demand-mode"><a href="#2-4-On-demand-mode" class="headerlink" title="2.4 On-demand mode"></a>2.4 On-demand mode</h2><p>Amazon DynamoDB on-demand is a flexible billing option capable of serving thousands of requests per second without capacity planning. DynamoDB on-demand offers pay-per-request pricing for read and write requests so that you pay only for what you use. </p><h2 id="2-5-Query-and-Scan"><a href="#2-5-Query-and-Scan" class="headerlink" title="2.5 Query and Scan"></a>2.5 Query and Scan</h2><ul><li>Query <ul><li>reads from a table or secondary index only the items that match the primary key specified in the key condition expression.  </li><li>parameters<ul><li>tableName</li><li>KeyContditionExpression<ul><li>must specify partition key name and value</li></ul></li><li>ProjectExpression </li><li>ConsistentRead</li><li>FilterExpression<ul><li>a string that contains conditions that DDB applies after the query operation, but before the data is returned to you </li><li>all other records are discarded </li></ul></li></ul></li></ul></li><li>Scan <ul><li>reads all items from the table or index </li><li>parameters <ul><li>tableName</li><li>ProjectionExpression <ul><li>a string that identify one or more attributes to retrieve from the table </li></ul></li><li>consistentRead</li><li>filterExpression </li></ul></li></ul></li></ul><h2 id="2-6-Best-Practices"><a href="#2-6-Best-Practices" class="headerlink" title="2.6 Best Practices"></a>2.6 Best Practices</h2><ul><li>Uniform workloads</li><li>One-To-Many tables<ul><li>If your table has items that store a large number of values in an attribute of set type, such as string set or number set, consider removing the set attribute from the table and splitting it as separate items in another table.</li><li>If you frequently access large items in a table but do not use the large attribute values, consider storing frequently accessed smaller attributes in a separate table</li></ul></li><li>Optimistic Locking with Version Number <ul><li>Use optimistic locking with a version number to make sure that an item has not changed since the last time you read it. </li><li>Maintain a version number to check that the item has not been updated between the last read and update </li></ul></li></ul><h2 id="2-7-DynamoDB-Accelerator-DAX"><a href="#2-7-DynamoDB-Accelerator-DAX" class="headerlink" title="2.7 DynamoDB Accelerator (DAX)"></a>2.7 DynamoDB Accelerator (DAX)</h2><ul><li>deliver fast response times for accessing eventually consistent data </li><li>DAX is a DynamoDB compatible caching service that enables you to benefit from fast in memory performance for demanding applications. It addresses three core scenarios: <ul><li>reduce the response times of eventually consistent read workloads by an order of magnitude, from single-digit milliseconds to microsends</li><li>reduce operational and application complexity by providing a managed service that is API-compatible with Amazon DynamoDB</li><li>DAX provides increased throughput and potential operational cost savings by reducing the need to over-provision read capacity units</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> NoSQL </tag>
            
            <tag> DynamoDB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.4 Storage Options, S3 in detail</title>
      <link href="/Developing-on-AWS-Note-4-Storage-Options-S3-in-detail/"/>
      <url>/Developing-on-AWS-Note-4-Storage-Options-S3-in-detail/</url>
      
        <content type="html"><![CDATA[<h1 id="1-AWS-Storage-Options"><a href="#1-AWS-Storage-Options" class="headerlink" title="1. AWS Storage Options"></a>1. AWS Storage Options</h1><ul><li>Amazon S3<ul><li>Scalable, highly durable object storage in the cloud</li></ul></li><li>Amazon Glacier<ul><li>Low-cost, highly durable <strong>archive</strong> storage in the cloud</li></ul></li><li>Amazon EFS<ul><li>Scalable network file storage for Amazon EC2 instances</li><li>network file system that can grow to petabytes </li><li>allows massively parallel access from EC2 instances to your data within a region</li><li>designed to <strong>meet the performance needs of big data and analytics</strong></li></ul></li><li>Amazon EBS<ul><li>Network attached volumes that provide durable block-level storage for Amazon EC2 instances </li></ul></li><li>AWS storage gateway<ul><li>change to hybrid later </li><li>connects an on-premises software appliance with cloud-based storage to provide seamless and secure storage integration between an organization’s on-premises IT environment and the AWS storage infrastructure like Amazon S3, Amazon Glacier and EBS.</li></ul></li></ul><h1 id="2-Amazon-S3"><a href="#2-Amazon-S3" class="headerlink" title="2. Amazon S3"></a>2. Amazon S3</h1><p>Amazon Simple Storage Service provides develipers with <strong>high secure, durable, and scalable object storage</strong>. </p><h2 id="2-1-Use-cases"><a href="#2-1-Use-cases" class="headerlink" title="2.1 Use cases"></a>2.1 Use cases</h2><ul><li>storage solution <ul><li>content storage and distribution</li></ul></li><li>backup</li><li>archiving </li><li>big data analytics</li><li>static web site hosting</li><li>disaster recovery <ul><li>cross region replication(CRR) automatically replicates every S3 object to a destination bucket located in a different AWS Region. </li></ul></li></ul><h2 id="2-2-Components"><a href="#2-2-Components" class="headerlink" title="2.2 Components"></a>2.2 Components</h2><ul><li>bucket<ul><li>global unique</li><li>use only lower case letters, numbers and hyphens</li><li>associated with a region<ul><li>choose region by considering<ul><li>latency</li><li>cost</li><li>regulatory requirements </li></ul></li></ul></li></ul></li><li>Object<ul><li>S3 refers to files as objects </li><li>you can store any number of objects inside bucket</li><li>each object is <strong>identified by a unique key</strong></li><li>object metadata</li><li>version<ul><li>each object has a version ID if you enable this feature</li><li>Object locking supported on versioned buckets<ul><li>use object lock to prevent data from being changed, overwritten, or deleted </li></ul></li></ul></li><li>URLs for S3 Objects<ul><li>Path style URL<ul><li><code>http://&lt;region-specific endpoint&gt;/&lt;bucket name&gt;/&lt;object name&gt;</code></li></ul></li><li>virtual hosted-style URL<ul><li><code>http://&lt;bucket name&gt;.s3.amazonaws.com/&lt;object key&gt;</code> </li></ul></li></ul></li></ul></li><li>Key<ul><li>unique identifier for each object in an S3 bucket </li></ul></li><li>Object Url<ul><li>specify region, bucket name, object name(key)</li></ul></li></ul><h2 id="2-3-Operations"><a href="#2-3-Operations" class="headerlink" title="2.3 Operations"></a>2.3 Operations</h2><h3 id="2-3-1-put"><a href="#2-3-1-put" class="headerlink" title="2.3.1 put"></a>2.3.1 put</h3><ul><li>upload object</li><li>copy object <ul><li>create copies of an object </li><li>rename obejcts by creating a copy and deleting the original object </li><li>move objects across S3 locations </li><li>update object metadata </li></ul></li><li>limits<ul><li>5 GB at most in a single PUT operation </li><li>recommened: use multipart upload if size &gt; 100MB<ul><li>Multipart upload allows you to upload a single object as a set of parts. </li><li>You can upload each part separately. </li><li>If one of the parts fails to upload, you can retransmit that particular part without retransmitting the remaining parts. After all the parts of your object are uploaded to the server, you must send a complete multipart upload request that indicates that multipart upload has been completed. </li><li>Amazon S3 then assembles these parts and creates the complete object. </li><li>Amazon S3 retains all parts on the server until you complete or abort the upload.</li><li>You can upload parts in parallel to improve throughput, recover quickly from network issues, pause and resume object uploads </li></ul></li></ul></li></ul><h3 id="2-3-2-Get"><a href="#2-3-2-Get" class="headerlink" title="2.3.2 Get"></a>2.3.2 Get</h3><ul><li>retrieve a complete object in a single GET request </li><li>You can also retrieve an object in parts by specifying the range of bytes needed. This is useful in scenarios where network connectivity is poor or your application can or must process only subsets of object data.</li></ul><h3 id="2-3-3-Select"><a href="#2-3-3-Select" class="headerlink" title="2.3.3 Select"></a>2.3.3 Select</h3><ul><li>Select content from Object instead of retrieving Object </li><li>filter of content handled at S3 service level <ul><li>works by providing the ability to retrieve a subset of data from an object in Amazon S3 using simple SQL expressions </li><li>simply change API from get to select </li></ul></li></ul><h3 id="2-3-4-Delete"><a href="#2-3-4-Delete" class="headerlink" title="2.3.4 Delete"></a>2.3.4 Delete</h3><ul><li>can delete a single object or delete multiple objects in a single delete request <ul><li>versioning disabled <ul><li>can permanently delete an object by specifying the key that you want to delete</li></ul></li><li>versioning enabled <ul><li>can permanently delete an object by invoking a delete request with a key and version ID</li><li>must delete each individual version to completely remove an object </li></ul></li></ul></li></ul><h3 id="2-3-5-Listing-Keys"><a href="#2-3-5-Listing-Keys" class="headerlink" title="2.3.5 Listing Keys"></a>2.3.5 Listing Keys</h3><ul><li>There is no hierarchy of objects in S3 buckets. You can use prefixes in key names to group similar items. </li><li>You can use delimiters (any string such as / or _) in key names to organize your keys and create a logical hierarchy</li></ul><h2 id="2-4-Features"><a href="#2-4-Features" class="headerlink" title="2.4 Features"></a>2.4 Features</h2><h3 id="2-4-1-Pre-Signed-URLs"><a href="#2-4-1-Pre-Signed-URLs" class="headerlink" title="2.4.1 Pre-Signed URLs"></a>2.4.1 Pre-Signed URLs</h3><ul><li>Provide access to PUT/ GET objects without opening permissions to do anything else </li><li>Use permissions of the user who creates the URL</li><li>Provide security credentials, a bucket name, an object key, HTTP method and expiration date and time </li><li>onlu valid until expiration time </li></ul><h3 id="2-4-2-Date-Encryption"><a href="#2-4-2-Date-Encryption" class="headerlink" title="2.4.2 Date Encryption"></a>2.4.2 Date Encryption</h3><ul><li>Securing data in transit <ul><li>SSL-encrypted endpoints with HTTPS</li><li>client-side encryption - via SDKs</li><li>server-side encryption<ul><li>S3 encrypts your data at the object level  </li></ul></li></ul></li><li>Securing data at rest on server<ul><li>Amazon S3 managed keys (SSE-S3)</li><li>AWS KMS-managed keys (SSE-KMS)</li><li>Customer-provided keys (SSE-C)</li></ul></li></ul><h3 id="2-4-3-Corss-Origin-Resource-Sharing-CORS"><a href="#2-4-3-Corss-Origin-Resource-Sharing-CORS" class="headerlink" title="2.4.3 Corss Origin Resource Sharing (CORS)"></a>2.4.3 Corss Origin Resource Sharing (CORS)</h3><ul><li>defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.</li></ul><h2 id="2-5-Best-practices"><a href="#2-5-Best-practices" class="headerlink" title="2.5 Best practices"></a>2.5 Best practices</h2><ul><li>Avoid unnecessary requests <ul><li>handle noSuchBucket errors instead of checking for existence of fixed buckets </li><li>set the object metadata before uploading an object </li><li>avoid using the copy operation to update metadata</li><li>cache bucket and key names if your application design allows it </li></ul></li><li>Network latency<ul><li>choose the bucket region closest to latency-sensitive customers </li><li>consider compressing data stored in Amazon S3 to reduce the size of data transferred and storage used</li><li>use a CDN to distribute content </li></ul></li><li>Data integrity<ul><li>ensure the data has not been corrupted in transit</li><li>check MD5 checksum of the object retrieved from the GET and PUT operation <ul><li>AWS SDK automatically specifies MD5 checksum in a PUT operation. Amazon S3 recalculates MD5 checksum and compares it with the specified value. </li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> S3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.3  - CloudWatch, CloudTrail, server-design for fail</title>
      <link href="/Developing-on-AWS-Note-3-CloudWatch-CloudTrail-server-design-for-fail/"/>
      <url>/Developing-on-AWS-Note-3-CloudWatch-CloudTrail-server-design-for-fail/</url>
      
        <content type="html"><![CDATA[<h1 id="1-CloudWatch"><a href="#1-CloudWatch" class="headerlink" title="1. CloudWatch"></a>1. CloudWatch</h1><h2 id="Why-use-Amazon-CloudWatch"><a href="#Why-use-Amazon-CloudWatch" class="headerlink" title="Why use Amazon CloudWatch?"></a>Why use Amazon CloudWatch?</h2><p>Need it to: </p><ul><li>monitor CPU, memory, disk I/O, network  -&gt; metrics</li><li>react to application log events and availability -&gt; logs/ event</li><li>automatically scale ec2 instance fleet -&gt; logs/ event</li><li>view operational status and identify issues -&gt; alarms, dashboard </li></ul><p>Actually, we could use Amazon CloudWatch to gain <strong>system-wide visibility</strong> into <strong>resource utilization</strong>, <strong>application performance</strong>, and <strong>operational health</strong>. You can use these insights to react and keep your application running smoothly. Amazon CloudWatch monitors your AWS Cloud resources and your cloud-powered applications. It tracks the metrics so that you can visualize and review them. You can also set alarms that will fire when a metric goes beyond a limit that you specified. CloudWatch gives you visibility into resource utilization, application performance, and operational health.</p><h1 id="2-CloudTrail"><a href="#2-CloudTrail" class="headerlink" title="2. CloudTrail"></a>2. CloudTrail</h1><p>CloudTrail is integrated with several AWS services. </p><ul><li>EC2</li><li>VPC</li><li>S3</li><li>EBS</li><li>DDB</li><li>RDS</li><li>Redshift</li><li>CloudFormation </li><li>IAM</li><li>…etc. </li></ul><p>AWS CloudTrail is an AWS service that generates logs of calls to the AWS API. AWS CloudTrail can <strong>record all activity</strong> against the services it monitors. Here are questions that you can answer using CloudTrail logs: <strong>who, when, what, which, where</strong>? While the coverage is extensive, not all services are covered in CloudTrail logs. You can use the AWS API <strong>call history produced by CloudTrail to track changes to AWS resources</strong>, including creation, modification, and deletion of AWS resources such as Amazon EC2 instances, Amazon VPC security groups, and Amazon EBS volumes.</p><p>You can use the CloudTrail console to view the last 90 days of recorded API activity and events in an AWS region. You can also download a file with that info, or a subset of info based on the filter and time range you choose</p><h1 id="3-Best-practices-of-developing-cloud-apps"><a href="#3-Best-practices-of-developing-cloud-apps" class="headerlink" title="3. Best practices of developing cloud apps"></a>3. Best practices of developing cloud apps</h1><ul><li>consider designing applications that are <strong>loosely coupled</strong>. <ul><li>think of your application as a consumer and provider of services </li><li>design and develop app as <strong>granular components</strong> that can be delivered and scaled independently. </li></ul></li><li>Architect for resilience; <ul><li>Set up your servers to scale automatically based on the number of users concurrently visiting your application.</li><li>Autoscaling would enable your application to handle a surge in volumn during a sale or propmotion and go back to normal </li><li>set up a <strong>cluster of nodes</strong> such that when one node fails, another node automatically picks up all the traffic. </li><li>Consider setting up <strong>read replicas for your database</strong>. </li></ul></li><li>design for failure<ul><li>In case of service failure, your application may log the failure and retry at a later time.</li><li>If the service is slow to respond, your application could retry by using an exponential backoff algorithm: retry after increasing amounts of time between attempts.<ul><li>This approach attempts to <strong>reach the service without overwhelming it</strong> with repeated requests and potentially aggravating the latency issue. </li></ul></li></ul></li><li>log metrics and monitor performance </li><li>implement a strong DevOps model<ul><li>Operationalize the development and deployment process for your application</li><li>Develop a <strong>modular, automated, and continuous build</strong> process. </li><li>Ensure <strong>consistency</strong> in the development, staging, and production environments. Set up scripts or use robust tools to consistently configure your environments.</li></ul></li><li>implement security in every layer <ul><li>infrastructure</li><li>application</li><li>data at transit and at rest </li><li>user authentication and authorization</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CloudWatch </tag>
            
            <tag> CloudTrail </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.2  - IAM</title>
      <link href="/Developing-on-AWS-Note-2-IAM/"/>
      <url>/Developing-on-AWS-Note-2-IAM/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Why-need-IAM"><a href="#1-Why-need-IAM" class="headerlink" title="1. Why need IAM?"></a>1. Why need IAM?</h1><p>IAM: AWS Identity and Access Management</p><ul><li><p>web service that helps you securely control access to AWS resources for your users. </p></li><li><p>You use IAM to control <strong>who</strong> can use your AWS resources (<strong>authentication</strong>) and <strong>what resources</strong> they can use and in what ways (<strong>authorization</strong>).</p></li><li><p>To set up your dev env to wrok with the AWS SDK. </p><ul><li>Need an AWS acount and AWS credentials </li><li>Use an IAM user to provide access credentials to <strong><em>increase the security of your AWS account</em></strong>. </li></ul></li><li><p>grant least privilege - <strong>grant only permissions required to perform a task</strong></p></li></ul><h1 id="2-Concepts"><a href="#2-Concepts" class="headerlink" title="2. Concepts"></a>2. Concepts</h1><p><img src="https://i.loli.net/2020/01/29/TbyX6r1jtKR2VuJ.png" alt="fig1.png"></p><ul><li>User<ul><li>we can set up a user account for every developer in organization </li><li>each user has credentials that they <strong>must</strong> use to access AWS services. </li></ul></li><li>groups </li><li>roles <ul><li>trusted entities </li><li>A role has policies granting access to specific services and operations </li><li>create role like developer, and associate it with each developer user account</li><li>developer role can be configured with policies that control which services and operations that role has access to </li><li>a role does not have standard long-term credentials(pwd or access keys) associated with it</li></ul></li><li>Policy<ul><li>contain permissions which specify which actions an entity can perform and on which resources </li><li>a JSON document that defines effect, actions, resources, and optional conditions for what API calls an entity can invoke </li><li>Type<ul><li>Managed policy<ul><li>standablon policies that you can attach to multiplke users, groups and roles </li><li>reusability </li><li>central change management </li><li>version</li><li>rollback </li></ul></li><li>Inline policy<ul><li>embedded in a principal entity like a user, group, or role. </li><li>you can use the same policy across multiple entities, but those entities are not sharing the policy </li></ul></li></ul></li></ul></li><li>Resources <ul><li>The user, role, group, and policy objects that are stored in IAM</li><li>you can add, edit, and remove resources from IAM</li></ul></li><li>Identities <ul><li>The IAM resource objects that are used to identify and group. These include users, groups, and roles.</li></ul></li><li>Entities<ul><li>The IAM resource objects that AWS uses for authentication</li><li>includes users and roles </li></ul></li><li>Principles<ul><li>a person or application that uses an entity to sign in and make requests </li></ul></li><li>Authentication<ul><li>As a principal, you must be authenticated (signed in to AWS) using an IAM entity to send a request to AWS.</li><li>Must provide your access key and secret key when accessing by CLI</li></ul></li><li>Authorization <ul><li>Must be authorized to complete request </li><li>AWS uses values from the request context to check for policies that apply to the request. </li></ul></li></ul><h1 id="3-Features"><a href="#3-Features" class="headerlink" title="3. Features"></a>3. Features</h1><ul><li>management<ul><li>user, role, federated users </li></ul></li><li>Shared access to your AWS account </li><li>Granular permissions <ul><li>grant different permissions to different people for different resources</li></ul></li><li>secire access tp AWS respirces for applications that run on Amazon EC2 </li><li>multi factor authentication </li><li>eventually consistent </li><li>identity based permissions<ul><li>attached to the IAM user and indicate what the user is permitted to do.</li><li>attached to a resource and indicate what a specified user (or group of users) is permitted to do with it. <strong>Amazon S3, Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS), and AWS OpsWorks are the only services that support resource-based permissions</strong>.</li></ul></li><li>resource based permissions</li><li>IAM Evaluation logic (In order)<ul><li>By default, all requests are denied. (In general, requests made using the account/root credentials for resources in the account are always allowed.)</li><li>An explicit allow overrides this default.</li><li>An explicit deny overrides any allows</li></ul></li></ul><h1 id="4-IAM-best-practice"><a href="#4-IAM-best-practice" class="headerlink" title="4. IAM best practice"></a>4. IAM best practice</h1><ul><li>IAM policies are specified with JSON-formatted text.</li><li>Policies are used to control access permissions for AWS APIs and other AWS resources. </li><li>They are not used for operating system permissions or application permissions. For those, use LDAP or Active Directory/Active Directory Federation Services (AD FS).</li><li>When you create IAM policies, follow the standard security advice of granting least privilege; <ul><li>i.e., grant only the permissions required to perform a task. </li><li>Determine what users need to do, and then craft policies for them that let the users perform only those tasks. </li><li>Similarly, create policies for individual resources that identify precisely who is allowed to access the resource, and allow only the minimal permissions for those users. </li></ul></li></ul><h1 id="5-Amazon-Shared-Responsibility-Model"><a href="#5-Amazon-Shared-Responsibility-Model" class="headerlink" title="5. Amazon Shared Responsibility Model"></a>5. Amazon Shared Responsibility Model</h1><p>Customer and AWS table the responsibility together: </p><ul><li>customer<ul><li>responsible for what you implement using AWS and for the applications you connect to AWS</li></ul></li><li>AWS<ul><li>goes from the ground up to the hypervisor. </li><li>secure the hardware, software, facilities, and networks that run all of our products and services. Customers are responsible for securely configuring the services they sign up for and anything they put on those services. </li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> Identity and Access Management </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Developing on AWS Note.1  - AWS Models, EC2, ELB, autoScaling</title>
      <link href="/Developing-on-AWS-Note-1-AWS-Models-EC2-ELB-autoScaling/"/>
      <url>/Developing-on-AWS-Note-1-AWS-Models-EC2-ELB-autoScaling/</url>
      
        <content type="html"><![CDATA[<h1 id="0-Overview"><a href="#0-Overview" class="headerlink" title="0. Overview"></a>0. Overview</h1><p>We use SDKs to interract with Application Programing Interface(API), and then connect to all AWS services. </p><h1 id="1-Cloud-computing-definition"><a href="#1-Cloud-computing-definition" class="headerlink" title="1.  Cloud computing definition"></a>1.  Cloud computing definition</h1><ul><li>enable you to stop thinking of your infrastructure as hardware, and instead think of it and use it as software. </li></ul><h1 id="2-Models-of-Cloud-Computing"><a href="#2-Models-of-Cloud-Computing" class="headerlink" title="2. Models of Cloud Computing"></a>2. Models of Cloud Computing</h1><ul><li><p>IaaS (Infrastructure as a Service)</p><ul><li>basic buiding blocks for Cloud IT <ul><li>Networking features </li><li>Computers </li><li>Data storage space </li></ul></li><li>PaaS (Platform as a Service)<ul><li>enables you to run applications without the need to manage underlying infrastructure(hardware and operating systems)</li></ul></li><li>SaaS (Software as a Service)<ul><li>A complete product that is run and managed by the service provider<h1 id="3-AWS-Service-Stack"><a href="#3-AWS-Service-Stack" class="headerlink" title="3. AWS Service Stack"></a>3. AWS Service Stack</h1></li></ul></li></ul></li><li><p>Infrastructure </p><ul><li>Regions </li><li>Availability Zones </li><li>Edge Locations </li></ul></li><li><p>Foundation Services </p><ul><li>Compute<ul><li>virtual instances </li><li>auto scaling </li><li>load balancing </li></ul></li><li>networking </li><li>storage <ul><li>object </li><li>block </li><li>archive </li></ul></li></ul></li><li><p>Platform Services </p><ul><li>Compute<ul><li>AWS Lambda</li><li>AWS Elastic Beanstalk </li><li>Amazon ECS</li><li>Amazon EKS </li></ul></li><li>database<ul><li>relational </li><li>No SQL </li><li>Caching </li><li>Products <ul><li>DynamoDB</li><li>RDS - relational database service </li><li>Elastic Cache </li><li>Redshift - data warehouse, for analysis and migration </li></ul></li></ul></li><li>Analytics <ul><li>Cluster computing </li><li>real time </li><li>data warehouse </li><li>data workflows </li><li>Products <ul><li>EMR - managed hadoop framework </li><li>Kinesis </li><li>CloudSearch </li><li>ElasticSearch </li></ul></li></ul></li><li>App services <ul><li>Queuing </li><li>Orchestration </li><li>App streaming </li><li>Transcoding </li><li>Email </li><li>Search </li><li>Products <ul><li>SQS </li><li>SNS </li><li>SES </li><li>Amazon Step Functions </li></ul></li></ul></li><li>Deployment and management <ul><li>containers </li><li>Dev/ ops tools </li><li>resource templates </li><li>usage tracking </li><li>monitoring and logs </li><li>products <ul><li>CodeCommit </li><li>CodeDeploy </li><li>CodePipeline </li><li>CodeBuild </li><li>X-Ray </li></ul></li></ul></li><li>Mobile Services <ul><li>identity </li><li>sync </li><li>mobile analytics </li><li>notifications </li><li>products <ul><li>Cognito </li><li>Pinpoint </li><li>API gateway </li></ul></li></ul></li></ul></li><li><p>Applications </p><ul><li>Virtual Desktops </li><li>Collaboration and Sharing </li></ul></li></ul><h1 id="4-Compute-services"><a href="#4-Compute-services" class="headerlink" title="4. Compute services"></a>4. Compute services</h1><h2 id="4-1-EC2"><a href="#4-1-EC2" class="headerlink" title="4.1 EC2"></a>4.1 EC2</h2><ul><li>Computers in the cloud. </li><li>Can create images of your servers at any time with a few clicks or simple API call. </li><li>different instance type for different use cases:<ul><li>low traffic websites </li><li>small database </li><li>high performance web services </li><li>high performance databases </li><li>distributed memory caches </li><li>data warehousing </li><li>log or data-processing applications </li><li>3D visualizations </li><li>Machine learning </li></ul></li><li>Pricing <ul><li>on demand </li><li>reserved instances </li><li>spot instances </li></ul></li></ul><h2 id="4-2-ELB-Elastic-Load-Balancing"><a href="#4-2-ELB-Elastic-Load-Balancing" class="headerlink" title="4.2 ELB - Elastic Load Balancing"></a>4.2 ELB - Elastic Load Balancing</h2><ul><li><p>distribute traffic across multiple EC2 instances, in multiple Availability Zones </p></li><li><p>Support health checks to detect unhealthy Amazon EC2 instances </p><ul><li>To discover the availability of instances, a ELB periodically sends pings, attempts connections or sends requests to test the EC2 instances.  </li></ul></li><li><p>Supports the routing and load balancing of traffic to Amazon EC2 instances. </p></li><li><p>when the LB determins that an instance is unhealthy, it stops routing requests to that instance. </p></li><li><p>sticky sessions</p><ul><li>enables the load balancer to bind a user’s session to a <strong>specific server instance</strong>. </li></ul></li><li><p>we should get rid of sticky sessions since: </p><ul><li>limit application’s scalability </li><li>lead to unequal load across servers </li><li>affect end-user response time since a single user’s load isn’t even spread across servers. </li></ul></li><li><p>Instead of using sticky sessions: cache </p><ul><li>manage user sessions by<ul><li>store locally to the node responding to the HTTP request </li><li>designate a layer which can store those sessions in a scalable and robust manner. </li></ul></li><li>Duration based session stickiness<ul><li>LB uses a special LB generated cookie to rack the application instance for each request. <strong>When the load balancer reveives a request, it first checks to see whether this cookie is present in the request</strong>.  If so, the reqeust is sent to the application instance specified in the cookie. If not, the LB chooses an application instance based on the existing load balancing algo. <strong>A cookie is inserted into the response for binding subsequent requests from the same user to that application instance</strong>. The stickiness policy configuration defines a cookie expiration, which establishes the duration of validity for each cookie. Cookie will be automatically updated after its duration expires. </li></ul></li><li>Application base session stickiness <ul><li>LB uses a <strong>special cookie</strong> to associate the session with the original server that handled the reqeust. But follows the lifetime of the application-generated cookie corresponding to the cookie name specified in the policy configuration. </li><li>The LB only inserts a new stickiness cookie if the application response includes a new application cookie. </li><li>The load balancer stickiness cookie does not update with each request. If the application cookie is explicitly removed or expires, the session stops being sticky until a new application cookie is issued.</li><li>Application often store session data in memory, but this approach does not scale well</li></ul></li></ul></li><li><p>Methods available to manage session data without sticky sessions include: </p><ul><li>using ElasticCache to store session data </li><li>using Amazon DynamoDB to store session data </li></ul></li></ul><h2 id="4-3-Auto-Scaling"><a href="#4-3-Auto-Scaling" class="headerlink" title="4.3 Auto Scaling"></a>4.3 Auto Scaling</h2><p>Auto Scaling helps you ensure that you have the correct number of EC2 instances available to handle the load for your application. Auto Scaling is particularly well-suited for applications that experience hourly, daily, or weekly variability in usage.</p><h1 id="5-Exceptions-and-Errors-handle"><a href="#5-Exceptions-and-Errors-handle" class="headerlink" title="5. Exceptions and Errors handle"></a>5. Exceptions and Errors handle</h1><ul><li>400 series: handle error in application </li><li>500 series: retry operations </li></ul><p>Java SDK throes the following unchecked(runtime) exceptions when error occur:</p><ul><li>AmazonServiceException<ul><li>indicates that the reqeust was correctly transmitted to the service, but for some reason, the service was not able to process it, and returned an error response instead. </li></ul></li><li>AmazonClientException <ul><li>indicates that a problem occured inside the hava client code <ul><li>try to send a request to AWS </li><li>try to parse a response from AWS </li></ul></li></ul></li><li>IllegalArgumentException <ul><li>throw if you pass an illegal argument when performing an operation on a service  </li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> EC2 </tag>
            
            <tag> ELB </tag>
            
            <tag> AutoScaling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么要合并HTTP请求?</title>
      <link href="/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%90%88%E5%B9%B6HTTP%E8%AF%B7%E6%B1%82/"/>
      <url>/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%90%88%E5%B9%B6HTTP%E8%AF%B7%E6%B1%82/</url>
      
        <content type="html"><![CDATA[<p>思考路径：<br>为什么要实现batch call? -&gt; 减少网络中的传输损耗 -&gt; 如何减少的? -&gt; 通过合并HTTP请求 -&gt; 合并HTTP请求是如何减少网络损耗的？ </p><p>本文将解决这个问题。一起看看单个请求携载大量信息和多个请求携载小量信息对于整个时间的影响。</p><h1 id="1-Client发出请求"><a href="#1-Client发出请求" class="headerlink" title="1. Client发出请求"></a>1. Client发出请求</h1><h2 id="1-1-HTTP-1-1"><a href="#1-1-HTTP-1-1" class="headerlink" title="1.1 HTTP 1.1"></a>1.1 HTTP 1.1</h2><p>可以保持长连接，但是每个不同的请求之间，client要向server发一个请求头</p><p>请求无法并行执行的，在一个连接里面</p><p>假设如果不合并的话需要建立N个连接，那么合并就可以省去(N-1)*RTT的时间，RTT指网络延迟（在传输介质中传输所用的时间，即从报文开始进入网络到它开始离开网络之间的时间）。</p><h2 id="1-2-TCP丢包问题"><a href="#1-2-TCP丢包问题" class="headerlink" title="1.2 TCP丢包问题"></a>1.2 TCP丢包问题</h2><p>慢启动，拥塞控制窗口</p><p>TCP报文乱序到达，合并后的文件可以允许队首丢包以后在队中补上来，但是分开资源的时候，前一个资源未加载完成后面的资源是不能加载的，会有更严重的队首阻塞问题，丢包率会严重影响Keep alive情况下多个文件的传输速率。</p><h2 id="1-3-浏览器线程数限制"><a href="#1-3-浏览器线程数限制" class="headerlink" title="1.3 浏览器线程数限制"></a>1.3 浏览器线程数限制</h2><p>多为2-6个线程，会在每个连接上串行发送若干个请求。TCP连接太多，会给服务器造成很大的压力的。</p><h2 id="1-4-DNS缓存问题"><a href="#1-4-DNS缓存问题" class="headerlink" title="1.4 DNS缓存问题"></a>1.4 DNS缓存问题</h2><p> 每次请求都需要找DNS缓存，多个请求就需要查找多次，而且缓存有可能被无故清空</p><h1 id="2-服务器处理请求"><a href="#2-服务器处理请求" class="headerlink" title="2. 服务器处理请求"></a>2. 服务器处理请求</h1><p>每个请求需要使用一个连接，建立一个线程，分配一部分CPU, 对于CPU而言，是种负担，尤其是一般来说建立了连接以后，哪怕发回了请求，这个连接还会保持一段时间才会timeout。这种时候，维持连接是对服务器资源的一种巨大的浪费。</p><h1 id="3-HTTP-2-0"><a href="#3-HTTP-2-0" class="headerlink" title="3. HTTP 2.0"></a>3. HTTP 2.0</h1><p>上面描述的所有都是基于HTTP/1.1的一些特性，或者说弊端，有长连接但是无法并行处理请求，TCP的慢启动和拥塞控制，队首阻塞问题都给整个性能带来很多弊端，因此我们有了HTTP2.0来做针对性的改进。很有意思的东西，直接看图： </p><ul><li><p>HTTP/1.1 network的请求图<br><img src="https://i.loli.net/2020/01/29/JPaxGAR2lrnKh6b.png" alt="http1-waterfall.png"></p></li><li><p>HTTP/2 network的请求图<br><img src="https://i.loli.net/2020/01/29/C64pmQAVzZrtyus.png" alt="http2-waterfall.png"></p></li></ul><p>就是这么酷炫，HTTP/2多了很多特性来解决HTTP/1.1的很多问题</p><h2 id="3-1-Fully-multiplexed"><a href="#3-1-Fully-multiplexed" class="headerlink" title="3.1 Fully multiplexed"></a>3.1 Fully multiplexed</h2><p>解决了队首阻塞的问题。对于同一个TCP连接，现在可以发送多个请求，接收多个回应了！在HTTP/1.1里面，如果在一个连接里上一个请求发生了丢包，那么后面的所有请求都必须等第一个请求补上包，收到回应以后才能继续执行。而在HTTP/2里面，可以直接并行处理。</p><h2 id="3-2-Header-Compression"><a href="#3-2-Header-Compression" class="headerlink" title="3.2 Header Compression"></a>3.2 Header Compression</h2><p>所有的HTTP request和response都有header，但是header里很可能包含缓存信息，导致他的大小会迅速增大的。但是在一个连接里大部分请求的请求头其实携带的信息都很类似，所以HTTP/2使用了索引表，存储了第一次出现的请求的请求头，然后后面的类似的请求只需要携带这个索引的数字就好了。头部压缩平均减少了30%的头部大小，加快了整体的网络中传输的速度。</p><p>这两点是和本文关系最大的，有了这两点，实质上合并HTTP请求的好处在HTTP/2的协议下，已经基本上消失了。合并不合并请求，更多的是看业务上的需求，后端的一些配置。</p><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><p>It’s a trade-off. 其实最重要的是看你传输什么东西，因为合并HTTP请求实质上是减少了网络延时，但是如果你在服务器上处理的时间远远大于网络延时的时间的时候，那么合并HTTP请求并不会给你带来很多性能上的提升。而且大数据量的传输一定会降低浏览器的cache hit rate,对于缓存的利用率会降低很多。但是对于HTTP请求携带的数据量比较少的情况，合并请求带来的性能提升会是显而易见的。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.zhihu.com/question/34689035" target="_blank" rel="noopener">1. 网络延迟</a></p><p><a href="https://www.zhihu.com/question/34401250" target="_blank" rel="noopener">2.知乎:合并HTTP请求是否真的有意义？</a></p><p><a href="https://deliciousbrains.com/performance-best-practices-http2/" target="_blank" rel="noopener">3. HTTP/2 Intro</a></p><p><a href="https://www.tutorialdocs.com/article/merge-parallel-http-request.html" target="_blank" rel="noopener">4. Merge parallel htto requests</a></p>]]></content>
      
      
      <categories>
          
          <category> Network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FrontEnd </tag>
            
            <tag> Network </tag>
            
            <tag> HTTP </tag>
            
            <tag> Batch Processing </tag>
            
            <tag> Web Development </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java-Synchronized方法</title>
      <link href="/Java-Synchronized%E6%96%B9%E6%B3%95/"/>
      <url>/Java-Synchronized%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="1-synchronized-type"><a href="#1-synchronized-type" class="headerlink" title="1. synchronized type"></a>1. synchronized type</h1><ol><li>Synchronized methods</li><li>Synchronized statements </li></ol><h1 id="2-Synchronized-Methods"><a href="#2-Synchronized-Methods" class="headerlink" title="2. Synchronized Methods"></a>2. Synchronized Methods</h1><p>修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁。修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁。 </p><h2 id="2-1-不可插入"><a href="#2-1-不可插入" class="headerlink" title="2.1 不可插入"></a>2.1 不可插入</h2><p>It is not possible for two invocations of synchronized methods on the same object to interleave. When one thread is executing a synchronized method for an object, all other threads that invoke synchronized methods for the same object block (suspend execution) until the first thread is done with the object.</p><p>当一个方法是同步的时候，当前线程在执行时，其他线程都会停止运行，直到线程完成工作，下一个线程继续执行。</p><h2 id="2-2-自动传递状态"><a href="#2-2-自动传递状态" class="headerlink" title="2.2 自动传递状态"></a>2.2 自动传递状态</h2><p>when a synchronized method exits, it automatically establishes a happens-before relationship with any subsequent invocation of a synchronized method for the same object. This guarantees that changes to the state of the object are visible to all threads.</p><p>保证状态可见，上个线程对对象的操作结果会作为输入给下一个线程来使用。 </p><h1 id="3-Synchronized-Statements"><a href="#3-Synchronized-Statements" class="headerlink" title="3. Synchronized Statements"></a>3. Synchronized Statements</h1><p>修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。</p><p>Synchronized statements must specify the object that provides the intrinsic lock. </p><pre><code>public void addName(String name) {    synchronized(this) {        lastName = name;        nameCount++;    }    nameList.add(name);}</code></pre><p>这里lastname 和namecount都要改变，是同步的。但要注意在声明里不可以调用其他对象的方法。</p><h1 id="4-底层实现原理"><a href="#4-底层实现原理" class="headerlink" title="4. 底层实现原理"></a>4. 底层实现原理</h1><p>可以锁代码块，也可以锁方法。如果锁的是类的实例对象，那么就是锁这个。如果锁的是类对象，那么尽管new多个实例对象，他们仍然属于同一个类，依然会被锁住，即线程之间保证同步关系。</p><p><code>synchronized</code> 同步语句块的实现使用的是 <code>monitorenter</code> 和 <code>monitorexit</code> 指令，其中 <code>monitorenter</code> 指令指向同步代码块的开始位置，<code>monitorexit</code> 指令则指明同步代码块的结束位置。</p><p>当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权.当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。</p><p>最开始的Synchronized是调用OS的mutex lock，要完成context switch ，映射到原生操作系统里，从用户态转到内核态。现在从JVM层面做了大量的优化，减少了锁开销。</p><h1 id="5-synchronized-优化"><a href="#5-synchronized-优化" class="headerlink" title="5. synchronized 优化"></a>5. synchronized 优化</h1><p>synchronized是互斥的，我们需要找方法加快中间过程，比如传统的零售交钱排队，找零到扫码付费的转变。这里介绍轻量级锁，偏向锁。</p><h2 id="5-1-CAS操作"><a href="#5-1-CAS操作" class="headerlink" title="5.1 CAS操作"></a>5.1 CAS操作</h2><p>使用锁的时候，线程获取锁是一种悲观锁，即认为每一次执行临界区的代码都会产生冲突，所以当前线程获取锁的时候同时会堵塞其他线程获取锁。而CAS是一种乐观锁策略，<strong>假设所有线程访问共享资源的时候不会出现冲突</strong>。出现了冲突以后采取CAS(compare and swap) 策略，用来比较交换，看线程之间是否出现了冲突。</p><h3 id="5-1-1-操作过程"><a href="#5-1-1-操作过程" class="headerlink" title="5.1.1 操作过程"></a>5.1.1 操作过程</h3><p>CAS比较交换的过程可以通俗的理解为CAS(V,O,N)，包含三个值分别为：V 内存地址存放的实际值；O 预期的值（旧值）；N 更新的新值。当V和O相同时，也就是说旧值和内存中实际的值相同表明该值没有被其他线程更改过，即该旧值O就是目前来说最新的值了，自然而然可以将新值N赋值给V。反之，V和O不相同，表明该值已经被其他线程改过了则该旧值O不是最新版本的值了，所以不能将新值N赋给V，返回V即可。当多个线程使用CAS操作一个变量是，只有一个线程会成功，并成功更新，其余会失败。失败的线程会重新尝试，当然也可以选择挂起线程。</p><p>It compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value. This is done as a single atomic operation. The atomicity guarantees that the new value is calculated based on up-to-date information; if the value had been updated by another thread in the meantime, the write would fail.</p><p>非阻塞同步。</p><h3 id="5-1-2-存在的问题"><a href="#5-1-2-存在的问题" class="headerlink" title="5.1.2 存在的问题"></a>5.1.2 存在的问题</h3><ol><li>ABA问题</li></ol><p>发生了变化，但又变了回去。（加上序号来解决）</p><ol start="2"><li>自旋时间过长</li><li>只能保证一个共享变量的原子操作</li></ol><h2 id="5-2-对象头"><a href="#5-2-对象头" class="headerlink" title="5.2 对象头"></a>5.2 对象头</h2><p>对象的锁 -&gt;  对象的标记，存在java对象的对象头里面。存放有</p><ol><li>锁状态<ul><li>无锁状态</li><li>偏向锁状态</li><li>轻量级锁状态</li><li>重量级锁状态</li></ul></li><li>对象的hashcode</li><li>对象分代年龄</li><li>是否是偏向锁</li><li>锁标志位</li></ol><blockquote><p>Tips: 级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率。</p></blockquote><h2 id="5-3-偏向锁"><a href="#5-3-偏向锁" class="headerlink" title="5.3 偏向锁"></a>5.3 偏向锁</h2><p>大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。</p><p>当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程</p><p>偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。</p><h2 id="5-4-轻量级锁"><a href="#5-4-轻量级锁" class="headerlink" title="5.4 轻量级锁"></a>5.4 轻量级锁</h2><p>线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。</p><p>轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。</p><h2 id="5-5-比较"><a href="#5-5-比较" class="headerlink" title="5.5 比较"></a>5.5 比较</h2><p><img src="https://i.loli.net/2020/01/29/meDvzPd6g24ITU9.png" alt="锁比较.png"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://en.wikipedia.org/wiki/Compare-and-swap" target="_blank" rel="noopener">1. Wiki: Compare and swap</a></p>]]></content>
      
      
      <categories>
          
          <category> BackEnd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Concurrency </tag>
            
            <tag> Synchronized </tag>
            
            <tag> Lock </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Volatile关键字</title>
      <link href="/Volatile%E5%85%B3%E9%94%AE%E5%AD%97/"/>
      <url>/Volatile%E5%85%B3%E9%94%AE%E5%AD%97/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Synchronized-vs-volatile"><a href="#1-Synchronized-vs-volatile" class="headerlink" title="1. Synchronized vs volatile"></a>1. Synchronized vs volatile</h1><p>synchronized是阻塞式同步，在线程竞争激烈的情况下会升级为重量级锁。而volatile是java虚拟机提供的最轻量级的同步机制。而针对volatile修饰的变量给java虚拟机特殊的约定，线程对volatile变量的修改会立刻被其他线程所感知，即不会出现数据脏读的现象，从而保证数据的“可见性”。</p><p><strong>被volatile修饰的变量能够保证每个线程能够获取该变量的最新值，从而避免出现数据脏读的现象。</strong></p><h1 id="2-实现原理"><a href="#2-实现原理" class="headerlink" title="2. 实现原理"></a>2. 实现原理</h1><p>生成汇编代码时会在Volatile修饰的共享变量进行写操作的时候多出lock前缀的指令：<strong>该指令会将当前处理器缓存行的数据写会系统内存；这个写回内存的操作会使得其他CPU里缓存了该内存地址的数据无效</strong></p><p>为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到<strong>内部缓存（L1，L2或其他</strong>）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现<strong>缓存一致性协</strong>议，<strong>每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。</strong>因此，经过分析我们可以得出如下结论：</p><ol><li>lock前缀的指令会引起处理器缓存写回内存</li><li>一个处理器的缓存回写到内存会导致其他处理器的缓存失效</li><li>当处理器发现本地缓存失效后，就会从内存中重读该变量数据，即可以获得当前最新值</li></ol><h1 id="3-volatile的happen-before关系"><a href="#3-volatile的happen-before关系" class="headerlink" title="3. volatile的happen before关系"></a>3. volatile的happen before关系</h1><p>写后读，线程A改本地内存的变量，同步到主内存，线程B的本地内存废弃，到主内存中拿到更新的数据。</p><h1 id="4-volatile的内存语义实现"><a href="#4-volatile的内存语义实现" class="headerlink" title="4. volatile的内存语义实现"></a>4. volatile的内存语义实现</h1><p>为了性能优化，JMM在不改变正确语义的前提下，会允许编译器和处理器对指令序列进行重排序，那如果想阻止重排序要怎么办了？答案是可以添加内存屏障。</p><p>内存屏障类型： </p><p><img src="https://i.loli.net/2020/01/29/kIQaATpeCf56nxs.png" alt="内存屏障.png"></p><p><img src="https://i.loli.net/2020/01/29/RKDH1XxFAJeQVjY.png" alt="重排序.png"></p><p>“NO”表示禁止重排序。为了实现volatile内存语义时，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎是不可能的，为此，JMM采取了保守策略：</p><ol><li>在每个volatile写操作的前面插入一个StoreStore屏障；</li><li>在每个volatile写操作的后面插入一个StoreLoad屏障；</li><li>在每个volatile读操作的后面插入一个LoadLoad屏障；</li><li>在每个volatile读操作的后面插入一个LoadStore屏障。</li></ol><p><img src="https://i.loli.net/2020/01/29/8JCkKYZg2NfMzP5.png" alt="volatile内存屏障.png"><br><img src="https://i.loli.net/2020/01/29/uY9qUvorMLK5a1V.png" alt="volatile读插入内存屏障示意图.png"></p>]]></content>
      
      
      <categories>
          
          <category> BackEnd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BackEnd </tag>
            
            <tag> Java </tag>
            
            <tag> Concurrency </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java并发图谱</title>
      <link href="/Java%E5%B9%B6%E5%8F%91%E5%9B%BE%E8%B0%B1/"/>
      <url>/Java%E5%B9%B6%E5%8F%91%E5%9B%BE%E8%B0%B1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在网上看到的描述Java并发的非常棒的知识图谱，分享/标注一波。</p></blockquote><p>包含： </p><ol><li>并发理论</li><li>并发关键字</li><li>Lock体系</li><li>并发容器</li></ol><p><img src="https://i.loli.net/2020/01/29/neDuY6XEaA2FsV9.png" alt="java-concurrency.png"></p>]]></content>
      
      
      <categories>
          
          <category> BackEnd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BackEnd </tag>
            
            <tag> Java </tag>
            
            <tag> Concurrency </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浏览器输入url以后都发生了什么</title>
      <link href="/%E6%B5%8F%E8%A7%88%E5%99%A8%E8%BE%93%E5%85%A5url%E4%BB%A5%E5%90%8E%E9%83%BD%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"/>
      <url>/%E6%B5%8F%E8%A7%88%E5%99%A8%E8%BE%93%E5%85%A5url%E4%BB%A5%E5%90%8E%E9%83%BD%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/</url>
      
        <content type="html"><![CDATA[<p>从输入一个网址开始，都调用了哪些服务，经历了哪些步骤，深度解析。以输入<a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a> 为例。</p><h1 id="1-Client端"><a href="#1-Client端" class="headerlink" title="1. Client端"></a>1. Client端</h1><p>一般来说，这里的Client指用户，即browser浏览器。这里我们以输入google.com为例。</p><h2 id="1-1-输入提示"><a href="#1-1-输入提示" class="headerlink" title="1.1 输入提示"></a>1.1 输入提示</h2><p>浏览器会根据历史访问，书签等信息给出输入建议。</p><p>还会根据默认搜索引擎的搜索记录，去匹配最近的搜索记录。</p><h2 id="1-2-url解析"><a href="#1-2-url解析" class="headerlink" title="1.2 url解析"></a>1.2 url解析</h2><p>如果是不合法的地址，会转给默认的搜索引擎,例如如果你正在使用chrome，可以在url输入框输入你想要搜索的内容，然后搜索引擎会根据关键字进行搜索。</p><p>HSTS列表 安全策略机制，强行使用https</p><h2 id="1-3-DNS解析"><a href="#1-3-DNS解析" class="headerlink" title="1.3 DNS解析"></a>1.3 DNS解析</h2><p>域名通过DNS转化为ip地址，这个转化主要是为了人机交互的友好型。没有人喜欢记一堆数字来访问一个网站。DNS做的事情就是把你输入的<a href="http://www.google.com翻译成计算机可以理解的IP地址，类似于192.188.1.1这种样子。" target="_blank" rel="noopener">www.google.com翻译成计算机可以理解的IP地址，类似于192.188.1.1这种样子。</a></p><h3 id="1-3-1查询过程"><a href="#1-3-1查询过程" class="headerlink" title="1.3.1查询过程"></a>1.3.1查询过程</h3><p>在解析的过程中，浏览器会由近及远寻找是否有缓存信息，即存没存从域名到地址的映射，整个查询过程分为如下几步，值得注意的是一旦查询到，就会立刻返回，不会再继续执行下去了。</p><ol><li>查看浏览器内部缓存</li></ol><p>浏览器内会会存有在一段时间内你曾经访问过的网站的域名地址的映射。</p><ol start="2"><li>系统缓存</li></ol><p>操作系统的缓存。浏览器会发出system call， 去询问操作系统是否存有相应的映射。</p><ol start="3"><li>路由器缓存， ISP缓存</li></ol><p>查询路由器的缓存。如果在路由器缓存中没有找到映射，就会去ISP(Internet Service Provider)处去寻找</p><ol start="4"><li><p>本地DNS服务器</p></li><li><p>域名服务器  根域服务器  -&gt; 顶级域名服务器</p></li></ol><p>寻找方式类似于一个树状结构，从最底层的子叶开始向上遍历，不停向更高级的域名服务器发出请求。这个过程会不停发送携带有请求和IP地址的数据包，会经过在client和server之间的多个网路设备直到其到达正确的DNS服务器。</p><h1 id="2-网络"><a href="#2-网络" class="headerlink" title="2 网络"></a>2 网络</h1><p>找到了正确的IP地址以后就要开始建立连接了，建立连接的过程一般会使用TCP协议，通过三次握手建立连接。</p><h2 id="2-1-TCP连接"><a href="#2-1-TCP连接" class="headerlink" title="2.1 TCP连接"></a>2.1 TCP连接</h2><p>会用TCP，建立连接。并在Client和Server之间传递数据包。</p><h3 id="2-1-1-IP封装-socket"><a href="#2-1-1-IP封装-socket" class="headerlink" title="2.1.1 IP封装  socket"></a>2.1.1 IP封装  socket</h3><h3 id="2-1-2-TCP-三次握手"><a href="#2-1-2-TCP-三次握手" class="headerlink" title="2.1.2 TCP 三次握手"></a>2.1.2 TCP 三次握手</h3><ol><li>Client 发出建立连接的请求。数据包携带有<code>SYN</code>。</li><li>如果Server有开放的端口，可以接受并建立连接，那么server会返回<code>SYN</code> + <code>ACK</code>, 告诉Client我可以接受你的请求。</li><li>Client收到Server的回应，发送<code>ACK</code>给Server。 连接建立。</li></ol><p>给一个知乎连接，<a href="https://www.zhihu.com/question/24853633" target="_blank" rel="noopener">为什么是三次握手，不是两次或者四次？</a>  非常有意思的例子。</p><h3 id="2-1-3-TCP-四次挥手"><a href="#2-1-3-TCP-四次挥手" class="headerlink" title="2.1.3 TCP 四次挥手"></a>2.1.3 TCP 四次挥手</h3><ol><li>Client发起中断请求，发送<code>FIN</code>到server</li><li>Server收到请求，可能数据还没有发完。这个时候不会关闭socket，而是回复<code>ACK</code>，告诉Client知道了</li><li>Client进入<code>Fin_Wait</code>状态，继续等待Server端的<code>FIN</code>报文。Server端发送完毕后，会向Client发送<code>FIN</code></li><li>Client收到后就回复<code>ACK</code>，并关闭连接</li></ol><h1 id="3-Server"><a href="#3-Server" class="headerlink" title="3 Server"></a>3 Server</h1><p>这里主要描述TCP连接建立和断开之间发生的一些事情。</p><p>TCP/IP是个协议组，是网络层和传输层的协议。Client首先建立一条与服务器的TCP连接（上文中的三次握手）。而后Client发送HTTP请求，这里为了获得页面，会发送一个GET请求给服务器。请求会包含浏览器ID，用户数据头，连接头（包含额外信息，比如是否需要保持TCP连接等），从cookie获取的数据等。</p><p>Server收到Client的Request，会将请求传递给Request Handler，去处理请求（从数据库查找数据，处理数据，构建Response）。构建完毕后会返回一个Response。值得注意的是这个Response里会含有状态信息： </p><ul><li>1xx informational message only  —— 包含信息</li><li>2xx success of some kind  ——成功信息</li><li>3xx redirects the client to another URL  ——将Client转到其他URL</li><li>4xx indicates an error on the client’s part  ——Client端错误 </li><li>5xx indicates an error on the server’s part  ——Server端错误</li></ul><h1 id="4-页面渲染"><a href="#4-页面渲染" class="headerlink" title="4 页面渲染"></a>4 页面渲染</h1><p>浏览器根据Resonse返回数据，渲染出DOM树，将返回的数据呈现在页面上。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://github.com/sunyongjian/blog/issues/34" target="_blank" rel="noopener">https://github.com/sunyongjian/blog/issues/34</a></p>]]></content>
      
      
      <categories>
          
          <category> FrontEnd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FrontEnd </tag>
            
            <tag> Browser </tag>
            
            <tag> CDN </tag>
            
            <tag> network </tag>
            
            <tag> TCP/ IP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>React初探</title>
      <link href="/React%E5%88%9D%E6%8E%A2/"/>
      <url>/React%E5%88%9D%E6%8E%A2/</url>
      
        <content type="html"><![CDATA[<p>初探React,很喜欢Component这种方式，很大程度提高了复用性，如果抛除C/S的区别，感觉有点像mason，毕竟刚刚弃掉mason的坑，很有意思的React。</p><h1 id="1-Hello-World"><a href="#1-Hello-World" class="headerlink" title="1. Hello World"></a>1. Hello World</h1><p>React.Component   A component takes in parameters, called props and returns a hierarchy of views to display via the render method. </p><p>To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. <strong>The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component.</strong> </p><pre><code>ReactDOM.render(    &lt;h1&gt;Hello, world!&lt;/h1&gt;,    document.getElementById(&apos;root&apos;));</code></pre><h1 id="2-JSX"><a href="#2-JSX" class="headerlink" title="2. JSX"></a>2. JSX</h1><p>jsx, 一种JavaScript的语法扩展。用来声明React当中的元素。可以任意在<strong>大括号{}</strong>里面使用<strong>JS表达式</strong>.</p><h2 id="2-1-JS表达式"><a href="#2-1-JS表达式" class="headerlink" title="2.1 JS表达式"></a>2.1 JS表达式</h2><blockquote><p>Any valid unit of code that resolves to a value. </p></blockquote><h3 id="2-1-1-分类"><a href="#2-1-1-分类" class="headerlink" title="2.1.1 分类"></a>2.1.1 分类</h3><ul><li><p>Arithmetic</p></li><li><p>String </p></li><li><p>Logical</p></li><li><p>Primary Expressions</p></li></ul><p>Basic keywords and general expressions in JS.</p><ol><li><p>this: refer to the current object.</p></li><li><p>grouping operator() : controls the precedence of evaluation in expressions. </p></li><li><p>new: to create an instance of a user-defined object type </p></li><li><p>super: call functions on an object’s parent.</p></li><li><p>spread operator: allow an expression to be expanded in places where multiple arguments or multiple elements are expected. </p><pre><code> function f(x, y, z) { }var args = [0, 1, 2];f(...args);</code></pre></li></ol><ul><li>Left hand side expressions</li></ul><h2 id="2-2-JSX-属性"><a href="#2-2-JSX-属性" class="headerlink" title="2.2 JSX 属性"></a>2.2 JSX 属性</h2><p>编译之后，会被转化为普通的JS对象。这意味着可以在if 或者for语句里使用JSX，将其赋值给变量，当做参数传入或者作为返回值都可以。</p><pre><code>// 使用引号定义以字符串为值得属性const element = &lt;div tabIndex=&quot;0&quot;&gt;&lt;/div&gt;;// 使用大括号来定义以js表达式为值得属性const element = &lt;img src={user.avatarUrl}&gt;&lt;/img&gt;;</code></pre><p>JSX代表Objects, Babel转译器会把JSX转换成一个名为React.createEliment()的方法来调用</p><h2 id="2-3-嵌套与防注入攻击"><a href="#2-3-嵌套与防注入攻击" class="headerlink" title="2.3 嵌套与防注入攻击"></a>2.3 嵌套与防注入攻击</h2><pre><code>const element = (  &lt;div&gt;    &lt;h1&gt;Hello!&lt;/h1&gt;    &lt;h2&gt;Good to see you here.&lt;/h2&gt;  &lt;/div&gt;);React DOM在渲染之前会过滤所有传入的值，可以确保应用不会被注入攻击，因为所有内容渲染之前都已经被转化为了字符串，有效防止XSS。 </code></pre><h1 id="3-元素渲染"><a href="#3-元素渲染" class="headerlink" title="3. 元素渲染"></a>3. 元素渲染</h1><p>React中的元素实际上是普通的对象，React DOM可以确保浏览器DOM的数据内容与React元素保持一致。<br>寻找React 根节点，渲染在根节点上</p><pre><code>const element = &lt;h1&gt;Hello, world&lt;/h1&gt;;ReactDOM.render(element, document.getElementById(&apos;root&apos;));</code></pre><h2 id="3-1-更新元素渲染"><a href="#3-1-更新元素渲染" class="headerlink" title="3.1 更新元素渲染"></a>3.1 更新元素渲染</h2><p>React 元素都是immutable的，更新界面的方式就是创建一个新的元素，然后将其传入<code>ReactDOM.render()</code> </p><p>React DOM 会比较元素的内容的先后的不同，而在渲染过程中只会更新改变了的部分。</p><h1 id="4-组件-amp-props"><a href="#4-组件-amp-props" class="headerlink" title="4. 组件 &amp; props"></a>4. 组件 &amp; props</h1><p>组件将UI切分成一些独立的，可复用的部件，这样就可以专注于构建每一个单独的部件。概念上像<strong>函数</strong>一样，可以接受任意的输入值(props)，并返回一个在页面上展示的React元素。</p><h2 id="4-1-函数定义组件"><a href="#4-1-函数定义组件" class="headerlink" title="4.1 函数定义组件"></a>4.1 函数定义组件</h2><pre><code>function Welcome(props) {    return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;;}</code></pre><h2 id="4-2-ES6-class-定义组件"><a href="#4-2-ES6-class-定义组件" class="headerlink" title="4.2 ES6 class 定义组件"></a>4.2 ES6 class 定义组件</h2><pre><code>class Welcome extends React.Component {    render() {        return &lt;h1&gt;Hello, {this.props.name}&lt;/h1&gt;    }}</code></pre><h2 id="4-3-组件渲染"><a href="#4-3-组件渲染" class="headerlink" title="4.3 组件渲染"></a>4.3 组件渲染</h2><pre><code>// React 元素可以使用户自定义的组件const element = &lt;Welcome name=&quot;Sara&quot; /&gt;;</code></pre><p><strong>当React遇到的元素是用户自定义的组件，它会将JSX属性作为单个对象传递给该组件，这个对象被称为”props”</strong></p><blockquote><p>组件名称必须大写</p></blockquote><h2 id="4-4-组合组件"><a href="#4-4-组合组件" class="headerlink" title="4.4 组合组件"></a>4.4 组合组件</h2><p>组件可以在它的输出中引用其他组件，这样我们就可以用同一组件来抽象出任意层次的细节。</p><blockquote><p>一个新的React应用程序的顶部是一个App组件。但是，如果要将React集成到现有应用程序中，则可以从下而上使用像Button这样的小组件作为开始，并逐渐运用到视图层的顶部。</p></blockquote><blockquote><p>组件的返回值只能有一个根元素。这也是我们要用一个<div>来包裹所有<Welcome />元素的原因。</p></blockquote><h2 id="4-5-提取组件"><a href="#4-5-提取组件" class="headerlink" title="4.5 提取组件"></a>4.5 提取组件</h2><p>分割组件，</p><h2 id="4-6-Props的只读性"><a href="#4-6-Props的只读性" class="headerlink" title="4.6 Props的只读性"></a>4.6 Props的只读性</h2><p>所有的React组件必须像纯函数那样使用它们的props</p><h1 id="5-State-amp-生命周期"><a href="#5-State-amp-生命周期" class="headerlink" title="5. State &amp; 生命周期"></a>5. State &amp; 生命周期</h1><p>更新UI的方法： <code>ReactDOM.render()</code></p><p>还可以通过更新状态来更新UI，<strong>状态是私有的，完全受控于当前组件</strong></p><h2 id="5-1-将函数转换为类"><a href="#5-1-将函数转换为类" class="headerlink" title="5.1 将函数转换为类"></a>5.1 将函数转换为类</h2><p>定义为类的组件有状态这个特性，还有生命周期钩子。</p><p>函数转换为类的步骤： </p><ol><li>创建一个名称扩展为<code>React.Component</code>的类</li><li>创建一个<code>render()</code>空方法</li><li>将函数体移动到render()方法中</li><li>在render()方法中，使用this.props替换props</li><li>删除剩余的空函数声明</li></ol><h2 id="5-2-为类添加局部状态"><a href="#5-2-为类添加局部状态" class="headerlink" title="5.2 为类添加局部状态"></a>5.2 为类添加局部状态</h2><pre><code>Class Clock extends React.Component {    constructor(props) {        super(props);        this.state = {date: new Date()};    }    render() {        return (            &lt;div&gt;                &lt;h1&gt;Hello, world!&lt;/h1&gt;                &lt;h2&gt;It is {this.state.date.toLocaleTimeString()}.&lt;/h2&gt;            &lt;/div&gt;        );    }}ReactDOM.render(    &lt;Clock/&gt;    document.getElementById(&apos;root&apos;));</code></pre><h2 id="5-3-添加生命周期方法到类中"><a href="#5-3-添加生命周期方法到类中" class="headerlink" title="5.3 添加生命周期方法到类中"></a>5.3 添加生命周期方法到类中</h2><p>当组件第一次加载到DOM中时，生成定时器，挂载</p><pre><code>componentDidMount() {}</code></pre><p>当Clock生成的这个DOM被移除时，清除定时器，卸载</p><pre><code>componentWillUnmount() {}</code></pre><p>一个完整的Clock的例子： </p><pre><code>class Clock extends React.Component {    constructor(props) {        super(props);        this.state = {date: new Date()};    }    // 3. Called when Clock&apos;s output is injected into DOM     componentDidMount() {        this.timerID = setInterval(            () =&gt; this.tick(),                1000        );    }    componentWillUnmount() {        clearInterval(this.timerID);    }    // 4. when setState() is being called, render() is called     tick() {        this.setState({            date: new Date()        });    }    // 2. Call render(), react know what need to be shown on screen. Update DOM     render() {        return (          &lt;div&gt;            &lt;h1&gt;Hello, world!&lt;/h1&gt;            &lt;h2&gt;It is {this.state.date.toLocaleTimeString()}.&lt;/h2&gt;          &lt;/div&gt;        );    }}ReactDOM.render(// 1. call Clock&apos;s constructor  &lt;Clock /&gt;,  document.getElementById(&apos;root&apos;));</code></pre><h2 id="5-4-如何使用状态"><a href="#5-4-如何使用状态" class="headerlink" title="5.4 如何使用状态"></a>5.4 如何使用状态</h2><ol><li><p>不要直接更新状态</p><p> use: this.setState({comment: ‘hi’});</p></li></ol><blockquote><p>构造函数是唯一能够初始化this.state的地方</p></blockquote><ol start="2"><li>状态更新可能是异步的</li></ol><p>React可以将多个<code>setState()</code>调用合并成一个来提高性能</p><pre><code>// Wrongthis.setState({  counter: this.state.counter + this.props.increment,});// Correctthis.setState((prevState, props) =&gt; ({  counter: prevState.counter + props.increment}));// Correctthis.setState(function(prevState, props) {  return {    counter: prevState.counter + props.increment  };});</code></pre><ol start="3"><li>当调用<code>setState()</code>的时候，React会将你提供的对象合并到当前状态。可以只提供state的一部分。</li></ol><h2 id="5-5-数据流动方向：-自顶向下"><a href="#5-5-数据流动方向：-自顶向下" class="headerlink" title="5.5 数据流动方向： 自顶向下"></a>5.5 数据流动方向： 自顶向下</h2><p>父组件或子组件都不知道某个组件是否有状态，组件可以选择将其状态作为属性传递给其子组件。</p><h1 id="6-事件处理"><a href="#6-事件处理" class="headerlink" title="6. 事件处理"></a>6. 事件处理</h1><p>React事件绑定属性的命名采用驼峰式写法</p><p>采用jsx的语法你需要传入一个函数作为事件处理函数，而不是一个字符串。</p><pre><code>&lt;button onClick={activateLasers}&gt;    Activate Lasers&lt;/button&gt;</code></pre><h2 id="6-1-Toggle"><a href="#6-1-Toggle" class="headerlink" title="6.1 Toggle"></a>6.1 Toggle</h2><pre><code>class Toggle extends React.Component {    constructor(props) {    super(props);    this.state = {isToggleOn: true};    // This binding is necessary to make `this` work in the callback    this.handleClick = this.handleClick.bind(this);    }    handleClick() {        this.setState(prevState =&gt; ({            isToggleOn: !prevState.isToggleOn        }));    }    render() {        return (        // &lt;button onClick={(e) =&gt; this.handleClick(e)}&gt;         // 问题L每次渲染的时候都会创建一个不同的回调函数            &lt;button onClick={this.handleClick}&gt;                {this.state.isToggleOn ? &apos;ON&apos; : &apos;OFF&apos;}            &lt;/button&gt;        );    }}ReactDOM.render(    &lt;Toggle /&gt;,    document.getElementById(&apos;root&apos;));</code></pre><p>必须谨慎对待JSX回调函数中的this，类的方法默认不会绑定this的。如果你忘记绑定 <code>this.handleClick</code> 并把它传入 <code>onClick</code>, 当你调用这个函数的时候 <code>this</code> 的值会是 <code>undefined</code>。</p><h2 id="6-2-Todolist"><a href="#6-2-Todolist" class="headerlink" title="6.2 Todolist"></a>6.2 Todolist</h2><pre><code>class TodoApp extends React.Component {    constructor(props) {        super(props);        this.state = { items: [], text: &apos;&apos; };        this.handleChange = this.handleChange.bind(this);        this.handleSubmit = this.handleSubmit.bind(this);    }    render() {        return (            &lt;div&gt;                &lt;h3&gt;TODO&lt;/h3&gt;                &lt;TodoList items={this.state.items} /&gt;                &lt;form onSubmit={this.handleSubmit}&gt;                  &lt;input                    onChange={this.handleChange}                    value={this.state.text}                  /&gt;                  &lt;button&gt;                    Add #{this.state.items.length + 1}                  &lt;/button&gt;                &lt;/form&gt;            &lt;/div&gt;        );    }    handleChange(e) {        this.setState({ text: e.target.value });    }    handleSubmit(e) {        e.preventDefault();        if (!this.state.text.length) {            return;        }        const newItem = {            text: this.state.text,            id: Date.now()        };        this.setState(prevState =&gt; ({            items: prevState.items.concat(newItem),            text: &apos;&apos;        }));    }}class TodoList extends React.Component {    render() {        return (            &lt;ul&gt;                {this.props.items.map(item =&gt; (                  &lt;li key={item.id}&gt;{item.text}&lt;/li&gt;                ))}            &lt;/ul&gt;        );    }}ReactDOM.render(&lt;TodoApp /&gt;, mountNode);</code></pre><h2 id="6-3-向事件处理程序传递参数"><a href="#6-3-向事件处理程序传递参数" class="headerlink" title="6.3 向事件处理程序传递参数"></a>6.3 向事件处理程序传递参数</h2><pre><code>&lt;button onClick={(e) =&gt; this.deleteRow(id, e)}&gt;Delete Row&lt;/button&gt;&lt;button onClick={this.deleteRow.bind(this, id)}&gt;Delete Row&lt;/button&gt;</code></pre><p>参数 e 作为 React 事件对象将会被作为第二个参数进行传递。通过箭头函数的方式，事件对象必须显式的进行传递，但是通过 bind 的方式，事件对象以及更多的参数将会被隐式的进行传递。</p><h2 id="6-4-bind-向监听函数传参，-事件对象e需要放在最后"><a href="#6-4-bind-向监听函数传参，-事件对象e需要放在最后" class="headerlink" title="6.4 bind 向监听函数传参， 事件对象e需要放在最后"></a>6.4 bind 向监听函数传参， 事件对象e需要放在最后</h2><pre><code>class Popper extends React.Component{    constructor(){        super();        this.state = {name:&apos;Hello world!&apos;};    }    preventPop(name, e){    //事件对象e要放在最后        e.preventDefault();        alert(name);    }    render(){        return (            &lt;div&gt;                &lt;p&gt;hello&lt;/p&gt;                {/* Pass params via bind() method. */}                &lt;a href=&quot;https://reactjs.org&quot; onClick={this.preventPop.bind(this,this.state.name)}&gt;Click&lt;/a&gt;            &lt;/div&gt;        );    }}</code></pre><h1 id="7-条件渲染"><a href="#7-条件渲染" class="headerlink" title="7. 条件渲染"></a>7. 条件渲染</h1><p>可以创建不同的组件来封装各种你需要的行为。然后根据应用的状态变化只渲染其中的一部分。(if)</p><pre><code>function Greeting(props) {    const isLoggedIn = props.isLoggedIn;    if (isLoggedIn) {        return &lt;UserGreeting /&gt;;    }    return &lt;GuestGreeting /&gt;;}ReactDOM.render(  // Try changing to isLoggedIn={true}:  &lt;Greeting isLoggedIn={false} /&gt;,  document.getElementById(&apos;root&apos;));</code></pre><h2 id="7-1-与运算符-amp-amp"><a href="#7-1-与运算符-amp-amp" class="headerlink" title="7.1 与运算符 &amp;&amp;"></a>7.1 与运算符 &amp;&amp;</h2><pre><code>function Mailbox(props) {    const unreadMessages = props.unreadMessages;    return (        &lt;div&gt;          &lt;h1&gt;Hello!&lt;/h1&gt;          {unreadMessages.length &gt; 0 &amp;&amp;            &lt;h2&gt;              You have {unreadMessages.length} unread messages.            &lt;/h2&gt;          }        &lt;/div&gt;    );}const messages = [&apos;React&apos;, &apos;Re: React&apos;, &apos;Re:Re: React&apos;];ReactDOM.render(  &lt;Mailbox unreadMessages={messages} /&gt;,  document.getElementById(&apos;root&apos;));</code></pre><p><strong>在 JavaScript 中，true &amp;&amp; expression 总是返回 expression，而 false &amp;&amp; expression 总是返回 false。</strong></p><h2 id="7-2-阻止组件渲染"><a href="#7-2-阻止组件渲染" class="headerlink" title="7.2 阻止组件渲染"></a>7.2 阻止组件渲染</h2><pre><code>function WarningBanner(props) {    if (!props.warn) {        return null;    }    return (        &lt;div className=&quot;warning&quot;&gt;            Warning!        &lt;/div&gt;    );}class Page extends React.Component {    constructor(props) {        super(props);        this.state = {showWarning: true}        this.handleToggleClick = this.handleToggleClick.bind(this);    }    handleToggleClick() {        this.setState(prevState =&gt; ({            showWarning: !prevState.showWarning        }));    }    render() {        return (            &lt;div&gt;                &lt;WarningBanner warn={this.state.showWarning} /&gt;                &lt;button onClick={this.handleToggleClick}&gt;                {this.state.showWarning ? &apos;Hide&apos; : &apos;Show&apos;}                &lt;/button&gt;            &lt;/div&gt;        );    }}ReactDOM.render(  &lt;Page /&gt;,  document.getElementById(&apos;root&apos;));</code></pre><h1 id="8-列表-amp-Keys"><a href="#8-列表-amp-Keys" class="headerlink" title="8. 列表 &amp; Keys"></a>8. 列表 &amp; Keys</h1><h2 id="8-1-渲染多个组件"><a href="#8-1-渲染多个组件" class="headerlink" title="8.1 渲染多个组件"></a>8.1 渲染多个组件</h2><pre><code>const numbers = [1, 2, 3, 4, 5];const listItems = numbers.map(    (number) =&gt; &lt;li&gt;{number}&lt;/li&gt;);ReactDOM.render(    &lt;ul&gt;{listItems}&lt;/ul&gt;    documnet.getElementById(&apos;root&apos;));</code></pre><h2 id="8-2-基础列表组件"><a href="#8-2-基础列表组件" class="headerlink" title="8.2 基础列表组件"></a>8.2 基础列表组件</h2><pre><code>function NumberList(props) {    const numbers = props.numbers;    const listItems = numbers.map((number) =&gt;        &lt;li key={number.toString()}&gt;            {number}        &lt;/li&gt;    );    return (        &lt;ul&gt;{listItems}&lt;/ul&gt;    );}const numbers = [1, 2, 3, 4, 5];ReactDOM.render(  &lt;NumberList numbers={numbers} /&gt;,  document.getElementById(&apos;root&apos;));</code></pre><h2 id="8-3-Keys"><a href="#8-3-Keys" class="headerlink" title="8.3 Keys"></a>8.3 Keys</h2><p>Keys可以在DOM中的某些元素被增加或删除的时候帮助React识别哪些元素发生了变化。最好是该元素在列表中拥有的独一无二的字符串。使用来自数据的id作为元素的key</p><p>元素的key只有在它和它的兄弟节点对比时才有意义。</p><h1 id="9-表单"><a href="#9-表单" class="headerlink" title="9.表单"></a>9.表单</h1><p>HTML 表单元素与React中其他DOM元素有所不同，因为表单元素本来就保留一些内部状态了。会构造一个处理提交表单并可访问用户输入表单数据的函数。标准方法是使用受控组件。</p><h2 id="9-1-受控组件"><a href="#9-1-受控组件" class="headerlink" title="9.1 受控组件"></a>9.1 受控组件</h2><p>在HTML当中，像<code>&lt;input&gt;,&lt;textarea&gt;, 和 &lt;select&gt;</code>这类表单元素会维持自身状态，并根据用户输入进行更新。但在React中，可变的状态通常保存在组件的状态属性中，并且只能用 setState() 方法进行更新。</p><pre><code>class NameForm extends React.Component {    constructor(props) {        super(props);        this.state = {value: &apos;&apos;};        this.handleChange = this.handleChange.bind(this);        this.handleSubmit = this.handleSubmit.bind(this);    }    handleChange(event) {        this.setState({value: event.target.value});    }    handleSubmit(event) {        alert(&apos;A name was submitted: &apos; + this.state.value);        event.preventDefault();    }    render() {        return (            &lt;form onSubmit={this.handleSubmit}&gt;                &lt;label&gt;                  Name:                  &lt;input type=&quot;text&quot; value={this.state.value} onChange={this.handleChange} /&gt;                &lt;/label&gt;                &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;            &lt;/form&gt;        );    }}</code></pre><h2 id="9-2-textarea标签"><a href="#9-2-textarea标签" class="headerlink" title="9.2 textarea标签"></a>9.2 textarea标签</h2><pre><code>class EssayForm extends React.Component {    constructor(props) {        super(props);        this.state = {          value: &apos;Please write an essay about your favorite DOM element.&apos;        };        this.handleChange = this.handleChange.bind(this);        this.handleSubmit = this.handleSubmit.bind(this);    }    handleChange(event) {        this.setState({value: event.target.value});    }    handleSubmit(event) {        alert(&apos;An essay was submitted: &apos; + this.state.value);        event.preventDefault();    }    render() {        return (            &lt;form onSubmit={this.handleSubmit}&gt;                &lt;label&gt;                Name:                &lt;textarea value={this.state.value} onChange={this.handleChange} /&gt;                &lt;/label&gt;                &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;            &lt;/form&gt;        );    }}</code></pre><h2 id="9-3-select标签"><a href="#9-3-select标签" class="headerlink" title="9.3 select标签"></a>9.3 select标签</h2><p>React中，不适用selected属性表明选中项，而是在根select标签上用value属性来表示选中项。这在受控组件中更方便，因为只需要在一个地方更新组件。</p><pre><code>class FlavorForm extends React.Component {    constructor(props) {    super(props);    this.state = {value: &apos;coconut&apos;};    this.handleChange = this.handleChange.bind(this);    this.handleSubmit = this.handleSubmit.bind(this);}handleChange(event) {    this.setState({value: event.target.value});}handleSubmit(event) {    alert(&apos;Your favorite flavor is: &apos; + this.state.value);    event.preventDefault();}    render() {        return (            &lt;form onSubmit={this.handleSubmit}&gt;                &lt;label&gt;                    Pick your favorite La Croix flavor:                    &lt;select value={this.state.value} onChange={this.handleChange}&gt;                    &lt;option value=&quot;grapefruit&quot;&gt;Grapefruit&lt;/option&gt;                    &lt;option value=&quot;lime&quot;&gt;Lime&lt;/option&gt;                    &lt;option value=&quot;coconut&quot;&gt;Coconut&lt;/option&gt;                    &lt;option value=&quot;mango&quot;&gt;Mango&lt;/option&gt;                    &lt;/select&gt;                &lt;/label&gt;                &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;            &lt;/form&gt;        );    }}</code></pre><h2 id="9-4-多个输入的解决方法"><a href="#9-4-多个输入的解决方法" class="headerlink" title="9.4 多个输入的解决方法"></a>9.4 多个输入的解决方法</h2><p>通过给每个元素添加一个name属性，来让处理函数根据event.target.name的值来选择做什么</p><pre><code>class Reservation extends React.Component {    constructor(props) {        super(props);        this.state = {            isGoing: true,            numberOfGuests: 2        };        this.handleInputChange = this.handleInputChange.bind(this);    }    handleInputChange(event) {        const target = event.target;        const value = target.type === &apos;checkbox&apos; ? target.checked : target.value;        const name = target.name;        this.setState({            [name]: value        });    }    render() {        return (            &lt;form&gt;                &lt;label&gt;                    Is going:                    &lt;input                        name=&quot;isGoing&quot;                        type=&quot;checkbox&quot;                        checked={this.state.isGoing}                        onChange={this.handleInputChange} /&gt;                &lt;/label&gt;                &lt;br /&gt;                &lt;label&gt;                    Number of guests:                    &lt;input                        name=&quot;numberOfGuests&quot;                        type=&quot;number&quot;                        value={this.state.numberOfGuests}                        onChange={this.handleInputChange} /&gt;                &lt;/label&gt;            &lt;/form&gt;        );    }}</code></pre><h1 id="10-状态提升"><a href="#10-状态提升" class="headerlink" title="10. 状态提升"></a>10. 状态提升</h1><h2 id="10-1-摄氏度华氏度的例子"><a href="#10-1-摄氏度华氏度的例子" class="headerlink" title="10.1 摄氏度华氏度的例子"></a>10.1 摄氏度华氏度的例子</h2><p>状态分享是通过将state数据提升至离需要这些数据的组件最近的父组件来完成的</p><pre><code>class Calculator extends React.Component {    constructor(props) {        super(props);        this.handleCelsiusChange = this.handleCelsiusChange.bind(this);        this.handleFahrenheitChange = this.handleFahrenheitChange.bind(this);        this.state = {temperature: &apos;&apos;, scale: &apos;c&apos;};    }    handleCelsiusChange(temperature) {        this.setState({scale: &apos;c&apos;, temperature});    }    handleFahrenheitChange(temperature) {        this.setState({scale: &apos;f&apos;, temperature});    }    render() {        const scale = this.state.scale;        const temperature = this.state.temperature;        const celsius = scale === &apos;f&apos; ? tryConvert(temperature, toCelsius) : temperature;        const fahrenheit = scale === &apos;c&apos; ? tryConvert(temperature, toFahrenheit) : temperature;        return (          &lt;div&gt;            &lt;TemperatureInput              scale=&quot;c&quot;              temperature={celsius}              onTemperatureChange={this.handleCelsiusChange} /&gt;            &lt;TemperatureInput              scale=&quot;f&quot;              temperature={fahrenheit}              onTemperatureChange={this.handleFahrenheitChange} /&gt;            &lt;BoilingVerdict              celsius={parseFloat(celsius)} /&gt;          &lt;/div&gt;        );    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> FrontEnd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FrontEnd </tag>
            
            <tag> React </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二战时间线</title>
      <link href="/%E4%BA%8C%E6%88%98%E6%97%B6%E9%97%B4%E7%BA%BF/"/>
      <url>/%E4%BA%8C%E6%88%98%E6%97%B6%E9%97%B4%E7%BA%BF/</url>
      
        <content type="html"><![CDATA[<!--toc--><blockquote><p>对二战很感兴趣，依旧在不断了解中。未了解前完全没想到在不到80年前，我们这个世界因为战争在短短十多年间死去了7000万人。中间有太多的泯灭人性，亦有很多人性的光辉。不知道列宁格勒，如今的圣彼得堡的居民，在两年半的被围城中是如何活下来的，每个人，都真的是挺直了腰背，这一刻，尊严比命重要。有太多的细节可以探寻，也有很多微妙的节点，好多次盟军的胜利是因为一些不期而遇的变化，小人物的选择，天气的突然放晴。每每读到这种时候，心里总会长舒一口气，庆幸啊。（PS: 有部美剧，就特地开了脑洞，讲二战轴心国胜利以后的世界的样子… 一个真的敢拍，一个真的敢播，hhh ）Anyway，铭记历史~ </p></blockquote><h1 id="时间线（欧洲战场-and-太平洋战场）"><a href="#时间线（欧洲战场-and-太平洋战场）" class="headerlink" title="时间线（欧洲战场 and 太平洋战场）"></a>时间线（欧洲战场 and 太平洋战场）</h1><p>二战整个过程，德国东征波兰，北伐北欧、西吞法国、南并巴尔干，在欧洲大陆上大杀四方，无人可挡。但到了第二阶段，苏联以强大的力量阻遏了德国的闪电攻势，而年底日本对美国的偷袭更将可怕的敌人拉入了战争。在第三阶段，同盟国在太平洋战场、北非战场、东线战场相继赢得转折点性质的胜利。在第四阶段，扫清了北非的盟军开始进攻西欧大陆，苏联在东面的战场大举反攻。到第五阶段，轴心国崩溃，德国、日本相继投降（意大利早在上一阶段就已投降），战争结束。</p><h2 id="1939-1940"><a href="#1939-1940" class="headerlink" title="1939 - 1940"></a>1939 - 1940</h2><ul><li>1939.9 </li></ul><p>德国进攻波兰，闪电战，波兰迅速崩溃。波兰盟友英法向德国宣战，却没有采取大规模行动，形成了西线无战事的奇怪的战争。与德国签订有《德苏互不侵犯条约》的苏联更是趁机从东面入侵了波兰，一个月，战争结束，波兰被德苏两国瓜分。</p><ul><li>1930.11</li></ul><p>苏联入侵芬兰，在芬兰人的抵抗下，苏联损失惨重。两方面原因，芬兰人英勇抵抗，苏联在大清洗中洗掉了大量的优秀军官。最终苏联惨胜，签订《莫斯科和平协定》。</p><ul><li>1940.5 </li></ul><p>德国德国兵锋西指，几天内就攻陷了荷兰和比利时，驻守在<strong><em>马其诺防线</em></strong>北侧的英军、法军北上迎击，这落入了曼施坦因的圈套，德军出其不意地从<strong>阿登山区</strong>突入，“闪击战之父”古德里安率领坦克部队果断前进，切入到盟军侧背，形成围歼之势。盟军被迫在敦刻尔克乘英国的大小船只撤退到英国，这就是代号为“发电机行动”的敦刻尔克大撤退。无力抵抗的法国在德军的继续进攻下被迫投降，一部分国土被德国和意大利占领，另一部分国土则由贝当的“维希法国”管理。戴高乐在伦敦发表演说，不承认维希法国的合法性，组织自由法国继续抗争。几乎与此同时，苏联吞并了位于波罗的海的三个国家——爱沙尼亚、拉脱维亚和立陶宛。</p><ul><li>1940.8</li></ul><p>攻占法国后，希特勒开始着眼于英国，制定了“海狮计划”准备登陆英国。为了争夺登陆作战的制空权，德国空军从8月份开始对英国的空中攻势，英国军民在丘吉尔的领导下奋勇抵抗 —— 不列颠空战。1941年不列颠空战中，英国取得了最终胜利。</p><ul><li>1940.9</li></ul><p>利比亚的意大利军队入侵埃及，而后被英军击败并在12月反推至利比亚。10月，意大利入侵希腊，却迅速失败反而被希腊军队反推到阿尔巴尼亚。德国被迫卷入战争，入侵南斯拉夫和希腊。经过一系列战役后，巴尔干半岛上的战争最终以轴心国的胜利结束，整个巴尔干半岛都落入了轴心国的掌控之内。希特勒在次年又将隆美尔派往北非营救意大利。隆美尔取得了一系列胜利，赢得了“沙漠之狐”的美誉。轴心国在地中海、北非战场取得了一定进展，但德军因此推迟了入侵苏联的时间，希特勒后来为之懊恼不已。</p><h2 id="1941"><a href="#1941" class="headerlink" title="1941"></a>1941</h2><ul><li>1941.6</li></ul><p>德国巴巴罗萨行动，三路大军入侵苏联。北方集团军群攻占波罗的海三国，保卫列宁格勒，列宁格勒保卫战开始。中央集团军攻克斯摩棱斯克，直指莫斯科。南方军团在基辅大胜苏军，完成了历史上最大规模的歼灭战。</p><ul><li>1941.7</li></ul><p>英美冻结日本的资产，美国对日本实施了石油禁运政策，釜底抽薪之策啊。</p><ul><li>1941.12</li></ul><p>日本在太平洋上对英美发起了进攻。山本五十六偷袭珍珠港（12.7），美国从孤立主义转向参战。美国总统罗斯福对日本宣战，半年内日军依旧在太平洋战场占据优势。德军潜艇部队在邓尼茨的指挥下，运用狼群战术，对盟军航运船只造成了巨大的伤害。 </p><h2 id="1942"><a href="#1942" class="headerlink" title="1942"></a>1942</h2><ul><li>1942.5 </li></ul><p>太平洋珊瑚海，日军对抗盟军航母，日军航母祥凤号被击沉，翔鹤号受到重创，瑞鹤号飞机损耗严重。美军航母列星顿号沉没，约克城号受伤。这是日本在太平洋战场上的扩张势头第一次受到阻遏，盟军保住了美国到澳大利亚间的交通线。</p><ul><li>1942.6 </li></ul><p>中途岛海战爆发。酷炫的圈套和反圈套作战，最终日军四艘航母全沉没了。此战后，日军在太平洋战场上的优势不复存在。</p><ul><li>1942.5-6</li></ul><p>北非战场，“沙漠之狐”隆美尔率领轴心国军队在加查拉战役中战胜了奥金莱克指挥的盟军，盟军向东退守阿拉曼防线。7月，隆美尔进攻阿拉曼防线，奥金莱克率军抵抗，两军打成了消耗战。8月，奥金莱克的指挥职务被蒙哥马利取代。10月，第二次阿拉曼战役打响，在拥有制空权和后勤方面的优势条件下，蒙哥马利击败了隆美尔，一路追到了突尼斯。</p><ul><li>1942.7</li></ul><p>斯大林格勒战役打响，德军攻入了斯大林格勒，但是与苏军展开巷战，遭到英勇抵抗。11月，天王星计划，完成了反包围。德军在整个战争过程中第一个大规模失败，被俘90，000余人。转折点，盟军进入战略反攻阶段。</p><h2 id="1943"><a href="#1943" class="headerlink" title="1943"></a>1943</h2><ul><li>1942.11 </li></ul><p>北非火炬行动，在阿尔及利亚和摩洛哥登陆后，向突尼斯的轴心国军队进攻。到1943年5月，在实力雄厚的盟军的两面夹击之下，轴心国的部队完全失败，除了一部分逃走外，全部向盟军投降。至此盟军取得了在北非战场上的全面胜利，他们可以把目光投向地中海对面的意大利了。</p><ul><li>1943.7 </li></ul><p>盟军进攻西西里岛，取得胜利。</p><p>东线，德军元帅曼施坦因对阵苏军元帅朱可夫。曼施坦因想用钳形攻势攻击突出的库尔斯克地区，却被朱可夫抵挡住。战争发展成了历史上规模最大的坦克大会战，苏联人的损失比德国更大，但他们能承受这些。最后，由于盟军在西西里岛的入侵，希特勒急需从东线抽调兵力，曼施坦因被迫撤退。从此苏联人开始大规模收复失地，德国人节节败退。</p><ul><li>1943.9</li></ul><p>盟军入侵意大利本土，墨索里尼下台，意大利政府投降。</p><ul><li>1943.11</li></ul><p>开罗会议 - 英美中</p><p>德黑兰会议 - 苏美英  商讨进攻轴心国的战略和战后的安排</p><h1 id="1944-1945"><a href="#1944-1945" class="headerlink" title="1944 - 1945"></a>1944 - 1945</h1><ul><li>1944.1 </li></ul><p>长达两年四个月的列宁格勒围城战终于结束了。苏联法功十次斯大林突进。到年底，收回了全部领土，更控制了东欧大部分国家。</p><ul><li>1944.6</li></ul><p>盟军霸王行动，诺曼底登陆，三百万士兵横渡英吉利海峡。巴顿将军率部横扫法兰西。8月，法国解放。</p><ul><li>1944.12</li></ul><p>12月，德国进行最后的挣扎，在阿登地区向盟军发动攻势，莫德尔成功地在布莱德利的防线上打出了一个“突出部”，所以这场战役被称为突出部战役。德军将小股盟军包围在巴斯托尼。在守军即将崩溃的时候，天气放晴，盟军的空军优势得以发挥，他们对德军进行了猛烈的轰炸，并将物资空投到巴斯托尼。巴顿的援军迅速北上，德军大势已去。希特勒终于同意了莫德尔的撤军请求，他的孤注一掷失败了。</p><ul><li>1945.2 </li></ul><p>雅尔塔会议 —— 苏美英</p><ul><li>1945.4</li></ul><p>朱可夫率领苏军攻占柏林，占领国会大厦，希特勒自杀。</p><p>太平洋战场，盟军在麦克阿瑟的率领下，用蛙跳式跃岛战术。冲绳岛作战。</p><ul><li>1945.7 </li></ul><p>波茨坦会议 —— 苏美英，商讨战后欧洲问题及对日本作战的问题</p><ul><li>1945.8 </li></ul><p>广岛长崎原子弹</p><p>苏联红军攻进中国东北，击溃关东军。</p><p>日本宣布接收《波茨坦公告》公告，无条件投降。</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> history </tag>
            
            <tag> world war </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读懂财报</title>
      <link href="/%E8%AF%BB%E6%87%82%E8%B4%A2%E6%8A%A5/"/>
      <url>/%E8%AF%BB%E6%87%82%E8%B4%A2%E6%8A%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="1-资产负债表"><a href="#1-资产负债表" class="headerlink" title="1 资产负债表"></a>1 资产负债表</h1><h2 id="1-1-资产负债表-分类"><a href="#1-1-资产负债表-分类" class="headerlink" title="1.1 资产负债表 分类"></a>1.1 资产负债表 分类</h2><p>企业需要做三张报表，分别是</p><ul><li>资产负债表</li><li>利润表</li><li>现金流</li></ul><h2 id="1-2-财务报表是用来做什么的？"><a href="#1-2-财务报表是用来做什么的？" class="headerlink" title="1.2 财务报表是用来做什么的？"></a>1.2 财务报表是用来做什么的？</h2><p>企业从事的经营活动的种类有：</p><ul><li>经营活动</li></ul><p>比如一个企业需要生产产品、销售产品、回收货款等等</p><ul><li>投资活动</li></ul><p>如果一个企业想要到一个新的地区去开展业务，想进入一个新的业务领域，或者想设计生产一个新的产品</p><ul><li>融资活动</li></ul><p>在经营和投资的过程中，当缺钱了的时候，需要去银行借钱，或者找别人来投资自己</p><p>可以这样子说，企业一辈子只做这三件事，经营，投资以及融资。在这整个过程中，无论其处在什么发展阶段，其日常经济活动都可以抽象成这样一个过程，从现金开始，转了一圈再搞到现金的过程。</p><h2 id="1-3-资产负债表详解"><a href="#1-3-资产负债表详解" class="headerlink" title="1.3 资产负债表详解"></a>1.3 资产负债表详解</h2><p>分为如下几个部分： </p><ul><li>资产 - 流动资产</li><li>资产 - 非流动资产</li><li>负债 - 流动负债</li><li>负债 - 非流动负债</li><li>股东权益</li></ul><h3 id="1-3-1-流动资产"><a href="#1-3-1-流动资产" class="headerlink" title="1.3.1 流动资产"></a>1.3.1 流动资产</h3><ul><li>货币资金</li></ul><p>放在银行里面或者放在公司里面的钱。包括库存现金，银行贷款，和其他货币资金三个项目的期末余额。</p><ul><li>应收账款</li></ul><p>销售产品的时候，发生的卖掉产品但是收不到钱的情况。<br>即需要核算的企业因为销售商品、提供劳务等经营活动应收取的款项。</p><ul><li>其他应收款</li></ul><p>企业除了存出保证金（租房子时交付的未来将退回的保证金、押金等）、买入返售的金融资产、应收票据、应收账款、预付账款、应收股利、应收利息、应收代位追偿款、应收分保账款、应收分包合同准备金、长期应收款等以外的其他各种应收及暂付款项。</p><ul><li>预付账款</li></ul><p>基本发生在货品很紧缺，带了一种向卖方收款的权利。预付账款也是一种资产。</p><ul><li>存货</li></ul><p>生产产品所需的原材料，生产出来的产成品，以及尚且处在生产过程中的没有完成的在产品</p><ul><li>待摊费用</li></ul><p>资产 vs 费用。 如果这笔钱可以换来对将来有用的东西，就是资产。如果画完就完了，就是费用。</p><p>各项流动资产在资产负债表中是按照<strong>各自转换为现金的速度</strong>来排序的。</p><h3 id="1-3-2-非流动资产"><a href="#1-3-2-非流动资产" class="headerlink" title="1.3.2 非流动资产"></a>1.3.2 非流动资产</h3><p>返回现金的时间长度，无法在一个循环内完成的。</p><ul><li>长期投资</li></ul><p>指不满足短期投资条件的投资，即不准备在一年或长于一年的经营周期之内转变为现金的投资。可以分为长期股票投资，长期债券投资，其他长期投资</p><ul><li>固定资产</li></ul><p>指同时具有以下特征的有形资产： （1）为生产商品、提供劳务、出租或经营管理而持有的； （2）使用寿命超过了一个会计年度</p><ul><li>无形资产</li></ul><p>专利权，版权等，还有土地使用权</p><p>指企业拥有或者控制的没有实物形态的可辨认的非货币性的资产。包括专利权、非专利技术、商标权、著作权、土地使用权等。</p><h3 id="1-3-3-资产-gt-企业"><a href="#1-3-3-资产-gt-企业" class="headerlink" title="1.3.3 资产 -&gt; 企业"></a>1.3.3 资产 -&gt; 企业</h3><p> 固定资产多，应收账款多，可能是有经营压力的传统企业</p><p> 生物资产，指有生命的动物和植物，生物资产分为消耗性生物资产，生产性生物资产和公益性生物资产。</p><p> 资产的结构会告诉你这家公司是什么样子的。 </p><h3 id="1-3-4-资产如何计价？"><a href="#1-3-4-资产如何计价？" class="headerlink" title="1.3.4 资产如何计价？"></a>1.3.4 资产如何计价？</h3><p> 会计们会用原来购买的资产价格当做这个资产的价值。</p><blockquote><p>历史成本</p></blockquote><blockquote><p>资产在其取得时为其所支付的现金或现金等价物的金额。负债在正常经营活动中为交换而收到或为偿付将要支付的现金或现金等价物的金额。</p></blockquote><blockquote><p>Bug: 历史成本无法体现出资产的变化。故解决方案为： 如果资产价值减小了，就把减值记下来。因为资产计价体系是一个历史成本的体系，一定要在历史成本的基础上扣除这个资产的减值。 </p></blockquote><ul><li>历史成本的含义</li></ul><ol><li>只有花了的钱才能记在账上。</li><li>在历史成本的计价体系下，增加资产价值的唯一途径是发生一个新的交易。</li></ol><h3 id="1-3-5-负债"><a href="#1-3-5-负债" class="headerlink" title="1.3.5 负债"></a>1.3.5 负债</h3><blockquote><p>负债：由于过去的交易或事务所引起的公司企业的现有债务，这种债务需要企业在将来以转移资产或提供劳务加以清偿，从而引起未来经济利益的流出。<br>其他：是为了简化，欠员工的工资，因为是月底发钱； 欠税务局的钱。这些都是流动负债。非流动负债： 应付债券。</p></blockquote><blockquote><p>应付债券：企业为了筹集资金而对外发行的期限在一年以上的长期借款性质的书面证明，约定在一定期限内还本付息的一种书面承诺。</p></blockquote><h3 id="1-3-6-股东权益"><a href="#1-3-6-股东权益" class="headerlink" title="1.3.6 股东权益"></a>1.3.6 股东权益</h3><blockquote><p>股东权益： 公司总资产中扣除负债剩余的部分，也成为净资产，反映了公司的自有资本。</p></blockquote><blockquote><p>股本： 股本金额相当于公司的注册资本。股本的总额体现了这个公司对外承担法律责任的上限。股本的组成则确定了多个股东之间的权利义务关系。</p></blockquote><blockquote><p>资本公积： 企业收到的投资者的超出其在企业注册资本所占份额，以及直接计入所有者权益的利得和损失等。</p></blockquote><blockquote><p>股东权益 + 负债 = 资产</p></blockquote><p>注意资产负债表是一个时间点的概念，是状态，不是过程。</p><h1 id="2-利润表"><a href="#2-利润表" class="headerlink" title="2. 利润表"></a>2. 利润表</h1><p>资产负债表： 可以看到投入的本金是否得到保障。利润表，则能得知投入的本金有没有赚钱。</p><p>毛利 = 营业收入 - 营业成本</p><h2 id="2-1-税种"><a href="#2-1-税种" class="headerlink" title="2.1 税种"></a>2.1 税种</h2><ul><li>营业税</li></ul><p>国家对工商营利事业按照营业额征收的税</p><ul><li>营业税金及附加</li></ul><p>企业经营活动应负担的相关税费，包括营业税、消费税、城市维护建设税、资源税、教育费附加等。不是所得税，是流转税，只要是有业务的企业就得缴纳流转税。</p><ul><li>常见的流转税： 营业税（价内税） + 增值税（价外税）</li><li>价内税： 税金包含在商品价值或价格之内</li><li>价外税： 税款不包括在价格内</li><li>增值税： 一种销售税，是消费者承担的税费，属于累退税，是基于商品或服务的增值而增税的一种间接税</li></ul><h2 id="2-2-其他项目"><a href="#2-2-其他项目" class="headerlink" title="2.2 其他项目"></a>2.2 其他项目</h2><ul><li>补贴收入<br>中国特色： 政府为一些企业提供的补贴。</li></ul><h2 id="2-3-利润表的分析"><a href="#2-3-利润表的分析" class="headerlink" title="2.3 利润表的分析"></a>2.3 利润表的分析</h2><p>告诉了是否赚钱，在哪些方面赚钱的基本信息。同时因为将可持续的和不可持续的营业收入分开，就可以帮助企业推断出自己未来一段时间以内的收益。</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> reading </tag>
            
            <tag> finance </tag>
            
            <tag> stock </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
