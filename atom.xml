<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leilei&#39;s Blog | 磊磊的博客</title>
  
  <subtitle>Because it&#39;s there</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.llchen60.com/"/>
  <updated>2021-09-30T02:22:44.858Z</updated>
  <id>https://www.llchen60.com/</id>
  
  <author>
    <name>Leilei Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>碳达峰与碳中和</title>
    <link href="https://www.llchen60.com/%E7%A2%B3%E8%BE%BE%E5%B3%B0%E4%B8%8E%E7%A2%B3%E4%B8%AD%E5%92%8C/"/>
    <id>https://www.llchen60.com/%E7%A2%B3%E8%BE%BE%E5%B3%B0%E4%B8%8E%E7%A2%B3%E4%B8%AD%E5%92%8C/</id>
    <published>2021-09-30T02:21:14.000Z</published>
    <updated>2021-09-30T02:22:44.858Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/09/30/XGS6Qey4qHki2FC.png" alt="碳达峰与碳中和"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/09/30/XGS6Qey4qHki2FC.png&quot; alt=&quot;碳达峰与碳中和&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Cassandra</title>
    <link href="https://www.llchen60.com/Cassandra/"/>
    <id>https://www.llchen60.com/Cassandra/</id>
    <published>2021-09-17T13:05:55.000Z</published>
    <updated>2021-09-17T13:41:43.051Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/09/17/IGdzKBJwD9TbZ6f.png" alt="Cassandra MindMap.png"></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><h2 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h2><ul><li>We wanna have a distributed and scalable system that can store a <strong>huge amount of structured data</strong>, which is indexed by a row key where each row can have an <strong>unbounded</strong> number of columns.</li><li>Cassandra was originally developed at Facebook in 2007 for index search feature. It’s designed to provide scalability, availability, and reliability to store large amounts of data.</li><li>It combines nature of Dynamo which is a <strong>key value store</strong> and the data model of Bigtable which is a <strong>column based</strong> data store</li><li>Cassandra is in favor of availability and partition tolerance, it could be tuned with <strong>replication factor</strong> and <strong>consistency levels</strong> to meet <strong>strong consistency</strong> requirements, and of course with a performance cost.</li><li>It uses peer to peer architecture, with each node connected to all other nodes</li><li>Each Cassandra node performs all database operations and can serve client requests without the need for any leader node.</li></ul><h2 id="1-2-Use-cases"><a href="#1-2-Use-cases" class="headerlink" title="1.2 Use cases"></a>1.2 Use cases</h2><ul><li>Store key value data with high availability</li><li>Time series data model<ul><li>Due to its data model and log structured storage engine, cassandra benefits from high performing write operations, This also make it well suited for storing and analyzing sequentially captured metrics</li></ul></li><li>Write Heavy Applications<ul><li>Suited for write intensive applications such as time series streaming services, sensor logs, and IoT applications</li></ul></li></ul><h1 id="2-High-Level-Architecture"><a href="#2-High-Level-Architecture" class="headerlink" title="2. High Level Architecture"></a>2. High Level Architecture</h1><h2 id="2-1-Common-Terms"><a href="#2-1-Common-Terms" class="headerlink" title="2.1 Common Terms"></a>2.1 Common Terms</h2><p><img src="https://i.loli.net/2021/09/17/IrfBD5HFqAX76NJ.png" alt="Primary and Clustering Keys"></p><ul><li>Column<ul><li>A key value pair and is the most basic unit of data structure</li><li>Column Key: Uniquely identifies a column in a row</li><li>Column Value: Store a value or a collection of values</li></ul></li><li>Row<ul><li>A container for columns referenced by primary key. Cassandra does not store a column that has a null value, this saves a lot of space</li></ul></li><li>Table<ul><li>A container of rows</li></ul></li><li>Keyspace<ul><li>A container for tables that span over one or more Cassandra nodes</li></ul></li><li>Cluster<ul><li>Container of Keyspace</li></ul></li><li>Node<ul><li>A computer system running an instance of Cassandra,</li><li>Can be a physical host, a machine instance in the cloud or even a docker container</li></ul></li></ul><h2 id="2-2-Data-Partitioning"><a href="#2-2-Data-Partitioning" class="headerlink" title="2.2 Data Partitioning"></a>2.2 Data Partitioning</h2><ul><li>Cassandra use consistent hashing as DynamoDB does</li></ul><h2 id="2-3-Primary-Key"><a href="#2-3-Primary-Key" class="headerlink" title="2.3 Primary Key"></a>2.3 Primary Key</h2><ul><li>The primary key consists of two parts:  E.G Primary Key as (city_id, employee_id)<ul><li>Partition Key<ul><li>Decides how data is distributed across nodes</li><li>city_id is the primary key, means the data will be partitioned by the city_id field, all rows with the same city_id will reside on the same node</li></ul></li><li>Clustering Key<ul><li>Decides how data is stored within a node</li><li>We could have multiple clustering keys, clustering columns specify the order that the data is arranged on a node.</li><li>employee_id is the clustering key. Within each node, the data is stored in sorted order according to the employee_id column.</li></ul></li></ul></li></ul><h2 id="2-4-Partitioner"><a href="#2-4-Partitioner" class="headerlink" title="2.4 Partitioner"></a>2.4 Partitioner</h2><p><img src="https://i.loli.net/2021/09/17/3NdkOaXUpbgnWq9.png" alt="Partitioner Flow"></p><ul><li>Responsible for determining how data is distributed on the consistent hash ring.</li><li>Cassandra use <strong>Murmur3 hashing function</strong> — which will always produce the same hash for a given partition key</li><li>All Cassandra nodes learn about the <strong>token assignments of other nodes</strong> through gossip. This means any node can handle a request for any other node’s range. The node receiving the request is called the <strong>coordinator</strong>, and any node can act in this role. If a key does not belong to the coordinator’s range, it <strong>forwards the request</strong> to the replicas responsible for that range.</li></ul><h2 id="2-5-Coordinator-Node"><a href="#2-5-Coordinator-Node" class="headerlink" title="2.5 Coordinator Node"></a>2.5 Coordinator Node</h2><ul><li>A client may connect to any node in the cluster to initiate a read or write query. This node is known as the coordinator node, the coordinator identifies the nodes responsible for the data that is being written or read    and forwards the queries to them</li></ul><h1 id="3-Low-Level-Architecture"><a href="#3-Low-Level-Architecture" class="headerlink" title="3. Low Level Architecture"></a>3. Low Level Architecture</h1><h2 id="3-1-Replication-Strategy"><a href="#3-1-Replication-Strategy" class="headerlink" title="3.1 Replication Strategy"></a>3.1 Replication Strategy</h2><ul><li><p>Each node in Cassandra serves as a replica for a different range of data.</p></li><li><p>It stores <strong>multiple copies of data</strong> and <strong>spreads them across various replicas</strong>.</p></li><li><p>The replication behavior is controlled by two factors</p><ul><li><p>Replication Factor</p><ul><li>Decides how many replicas the system will have</li><li>This represents the <strong>number of nodes that will receive the copy of the same data</strong></li><li>Each keyspace in cassandra can have a different replication factor</li></ul></li><li><p>Replication Strategy</p><ul><li><p>Decides which nodes will be responsible for the replicas</p></li><li><p>The node that owns the range in which the hash of the partition key falls will be the first replica</p></li><li><p>All the additional replicas are placed on the <strong>consecutive nodes</strong></p></li><li><p>Cassandra places the subsequent replicas on the next nodes in a clockwise manner</p></li><li><p>Two kinds of replication strategies</p><ul><li><p>Simple Replication Strategy</p><p>  <img src="https://i.loli.net/2021/09/17/cnz12lGFWEPw4fS.png" alt="Simple Replication Strategy"></p><ul><li>Used for a <strong>single data center cluster</strong></li><li>Cassandra places the first replica on a node determined by the partitioner and the subsequent replicas on the next node in a clockwise manner</li></ul></li><li><p>Network Topology Strategy</p><p>  <img src="https://i.loli.net/2021/09/17/TSAZbXKCYf9IsoN.png" alt="Network Topology Strategy"></p><ul><li>Used for multiple data centers</li><li>We can specify different replication factors for different data centers. We could then specify how many replicas will be placed in each data center</li><li>Additional replicas, in the same data center, are placed by <strong>walking the ring clockwise until reaching the 1st node in another rack</strong>. This is done to guard against a complete rack failure, as nodes in the same rack(or similar physical grouping) tend to fail together due to power, cooling or network issues.</li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="3-2-Consistency-Levels"><a href="#3-2-Consistency-Levels" class="headerlink" title="3.2 Consistency Levels"></a>3.2 Consistency Levels</h2><ul><li>Definition<ul><li><strong>Minimum number of nodes</strong> that must fulfill a read or write operation before the operation can be considered successful</li><li>It allows use to <strong>specify different consistency levels</strong> for read and write</li><li>It also has <strong>tunable consistency level</strong></li><li>Tradeoff between consistency and response time<ul><li>As a higher consistency level means more nodes need to respond to a read or write query, giving user more assurance that the values present on each replica are the same</li></ul></li></ul></li></ul><h3 id="3-2-1-Write-Consistency-Levels"><a href="#3-2-1-Write-Consistency-Levels" class="headerlink" title="3.2.1 Write Consistency Levels"></a>3.2.1 Write Consistency Levels</h3><ul><li>Consistency Levels specify how many replica nodes must respond for the write to be reported as successful to the client</li><li>Level is specified <strong>per query by the client</strong></li><li>Cassandra is eventually consistent, updates to other replica nodes may continue in the background</li><li>How does Cassandra perform a write operation?<ul><li>Coordinator node contacts all replicas, as determined by the <strong>replication factor</strong> , and consider the write successful when a number of replicas equal to the consistency level acknowledge the write</li></ul></li><li>Write Consistency Levels List:<ul><li>One/ Two/ Three<ul><li>The data must be written to at least the specified number of replica nodes before a write is considered successful</li></ul></li><li>Quorum<ul><li>Data must be written to at least a quorum of replica nodes</li><li>Quorum is defined as <code>floor(RF/2 + 1)</code>  RF represents replication factor</li></ul></li><li>All<ul><li>ensures the data is written to all replica nodes</li><li>provides the highest consistency but lowest availability as writes will fail if any replica is down</li></ul></li><li>Local Quorum<ul><li>Ensure that data is written to a quorum of nodes in the same datacenter as the coordinator</li><li>Does not wait for the response from the other data centers</li></ul></li><li>Each Quorum<ul><li>Ensures that the data is written to a quorum of nodes in each datacenter</li></ul></li><li>Any<ul><li>The data must be written to at least one node</li><li>In the extreme case, when all replica nodes for the given partition key are down, the write can still succeed after a hinted handoff (see 3.2.4 section) has been written.<ul><li>In this case, an any write could succeed with hinted handoff, but it will not be readable until the replica nodes for that partition has recovered and the latest data is written on them</li></ul></li></ul></li></ul></li></ul><h3 id="3-2-2-Read-Consistency-Levels"><a href="#3-2-2-Read-Consistency-Levels" class="headerlink" title="3.2.2 Read Consistency Levels"></a>3.2.2 Read Consistency Levels</h3><ul><li><p>Read Query Consistency Level specify how many replica nodes must respond to a read request before returning the data</p></li><li><p>It has the same consistency levels for read operations as that of write operations exception Each_Quorum cause it’s too expensive</p></li><li><p>To achieve strong consistency, we need to do <code>R + W &gt; RF</code> R represents read replica count, W represents write replication count, RF represents replication factor</p><ul><li>All client reads will see the most recent write in this scenario, and we will have strong consistency</li></ul></li><li><p>How does Cassandra perform a read operation?</p><ul><li><p>Coordinator always sends the read request to the fastest node</p><ul><li>E.G  for quorum=2, the coordinator sends the requests to the fastest node and the <strong>digest of the data</strong> from the second fastest node<ul><li>digest is the checksum of the data, we use this to save network bandwidth</li></ul></li></ul></li><li><p>if the digest doesn’t match, means some replica do not have the latest version of data</p><ul><li><p>Coordinator then <strong>reads the data from all the replicas</strong> to determine the latest data</p></li><li><p>Then coordinator <strong>returns the latest data to the client and initiates a read repair request</strong></p></li><li><p>The read repair request will help push the newer version of data to nodes with the older version</p><p>  <img src="https://i.loli.net/2021/09/17/sPM6HnKthBpT945.png" alt="Read Operation with Snitch"></p></li></ul></li><li><p>latest write timestamp is used as a mark for the correct version of data, read repair operation is performed only in a portion of the total reads to avoid performance degradation</p></li></ul></li></ul><h3 id="3-2-3-Snitch"><a href="#3-2-3-Snitch" class="headerlink" title="3.2.3 Snitch"></a>3.2.3 Snitch</h3><ul><li><p>Functions</p><ul><li>Application that determines the proximity of nodes within the ring, also tells which nodes are faster — monitor the read latencies</li><li>It keeps track of the network topology of Cassandra nodes, determines which <strong>data centers and racks</strong> nodes belong to</li><li>Replication strategy use this information provided by the Snitch to spread the replicas across the cluster intelligently. It could do its best by not having more than one replica on the same rack</li></ul></li><li><p>Cassandra nodes use this info to route read/ write requests efficiently</p><p>  <img src="https://i.loli.net/2021/09/17/ZnaOJqIvgdcMmAW.png" alt="Request when set consistency to one"></p></li></ul><h3 id="3-2-4-Hinted-Handoff"><a href="#3-2-4-Hinted-Handoff" class="headerlink" title="3.2.4 Hinted Handoff"></a>3.2.4 Hinted Handoff</h3><p><img src="https://i.loli.net/2021/09/17/QvSmb5wntE2AHJd.png" alt="Hinted Handoff"></p><ul><li>To let Cassandra still serve write requests even when nodes are down</li><li>When a node is down, the coordinator nodes <strong>writes a hint in a text file on local disk</strong><ul><li>Hint contains the data itself along with information about which node the data belongs to</li><li>Recover from gossiper — When the coordinator node discovers from the gossiper that a node for which it holds hints has recovered, it forwards the write request for each hint to the target</li><li>Recover from routine call — each node every ten minutes checks to see if the failing node, for which it is holding any hints, has recovered</li></ul></li><li>With consistency level ‘Any,’<ul><li>if all the replica nodes are down, the coordinator node will <strong>write the hints for all the nodes and report success to the client.</strong></li><li>However, this data will <strong>not reappear in any subsequent reads</strong> until one of the replica nodes comes back online, and the coordinator node successfully forwards the write requests to it.</li><li>This is assuming that the coordinator node is up when the replica node comes back.</li><li>This also means that we can lose our data if the coordinator node dies and never comes back. For this reason, we should avoid using the ‘Any’ consistency level</li></ul></li><li>For node offline for quite long<ul><li>Hints can build up considerably on other nodes</li><li>When it back online, other nodes tend to flood that node with write requests</li><li>It would cause issues on the node, as it is already trying to come back after a failure</li><li>To address this, Cassandra <strong>limits the storage of hints to a configurable time window</strong></li><li>By default, set the time window to 3 hours. Post that, older hints will be removed  — now the recovered nodes will have stale data<ul><li>The stale data would be fixed during the read path, it will issue a read repair when it sees the stale data</li></ul></li></ul></li><li>When the cluster cannot meet the consistency level specified by the client, Cassandra fails the write request and does not store a hint .</li></ul><h2 id="3-3-Gossiper"><a href="#3-3-Gossiper" class="headerlink" title="3.3 Gossiper"></a>3.3 Gossiper</h2><h3 id="3-3-1-How-does-Cassandra-use-Gossip-Protocol"><a href="#3-3-1-How-does-Cassandra-use-Gossip-Protocol" class="headerlink" title="3.3.1 How does Cassandra use Gossip Protocol?"></a>3.3.1 How does Cassandra use Gossip Protocol?</h3><ul><li>What’s for?<ul><li>Cassandra uses gossip protocol that allows each node to keep track of state information about the other nodes in the cluster.</li><li>It’s a Peer to Peer communication mechanism in which nodes <strong>periodically exchange state information about themselves and other nodes they know about</strong></li></ul></li><li>How it works?<ul><li>Each node initiates a gossip round every second to exchange state info about themselves with one to three other random nodes</li><li>Each gossip message has a version associated with it, so during a gossip exchange, older info is overwritten with the most current state for a particular node</li></ul></li><li>Generation number<ul><li>Each node stores a generation number which will be incremented every time a node restart</li><li>Node receiving the gossip message can compare the generation number it knows and the gossip message’s generation number</li><li>If the generation number in the gossip message is higher, it knows the node was restarted</li></ul></li><li>Seed nodes<ul><li>For node starting up for the first time</li><li>Assist in gossip convergence, thus guarantee schema/ state changes propagate regularly</li></ul></li></ul><h3 id="3-3-2-Node-Failure-Detection"><a href="#3-3-2-Node-Failure-Detection" class="headerlink" title="3.3.2 Node Failure Detection"></a>3.3.2 Node Failure Detection</h3><ul><li>Disadvantages for heartbeat<ul><li>outputs a boolean value telling us if the system is alive or not;</li><li>there is no middle ground.</li><li>Heartbeating uses a fixed timeout, and if there is no heartbeat from a server, the system, after the timeout, assumes that the server has crashed.</li><li>If we keep the timeout short, the system will be able to detect failures quickly but with many false positives due to slow machines or faulty networks.</li><li>On the other hand, if we keep the timeout long, the false positives will be reduced, but the system will not perform efficiently for being slow in detecting failures.</li></ul></li><li>Use adaptive failure detection mechanism  —— Phi Accrual Failure Detector<ul><li>Use historical heartbeat information to make the threshold adaptive</li><li>It outputs the suspicion level about a server</li><li>As a node’s suspicion level increases, the system can gradually decide to stop sending new requests to it</li><li>It makes the distributed system efficient as it takes into account fluctuations in the network env and other intermittent server issues before declaring a system completely dead</li></ul></li></ul><h2 id="3-4-Anatomy-of-Cassandra’s-Write-Operation"><a href="#3-4-Anatomy-of-Cassandra’s-Write-Operation" class="headerlink" title="3.4 Anatomy of Cassandra’s Write Operation"></a>3.4 Anatomy of Cassandra’s Write Operation</h2><p>Cassandra stores data both <strong>in memory and on disk</strong> to provide both high performance and durability. Every write includes a timestamp, write path involves a lot of components: </p><p><img src="https://i.loli.net/2021/09/17/LrMK7ckIS2zEsU1.png" alt="Write Path"></p><ul><li>Each write is appended to a commit log, which is stored on disk</li><li>It is then written to Memtable in memory</li><li>Periodically, MemTables are flushed to SSTables on the disk</li><li>Periodically, compaction runs to merge SSTables</li></ul><h3 id="3-4-1-Commit-Log"><a href="#3-4-1-Commit-Log" class="headerlink" title="3.4.1 Commit Log"></a>3.4.1 Commit Log</h3><ul><li>When a node receives a write request, it immediately writes data to a commit log</li><li>Commit log is a <strong>write ahead log</strong> stored on disk</li><li>Used as a crash recovery mechanism to support Cassandra’s durability goals</li><li>A write will not be considered successful on the node until it’s <strong>written to the commit log</strong><ul><li>This ensures if a write operation does not make it to the in-memory store, it will still be possible to recover the data</li></ul></li><li>If we shut down the node or it crashes unexpectedly, the commit log can ensure that data is not lost; that’s because if the node restart, the commit log gets replayed</li></ul><h3 id="3-4-2-MemTable"><a href="#3-4-2-MemTable" class="headerlink" title="3.4.2 MemTable"></a>3.4.2 MemTable</h3><ul><li>After written to the commit log, the data is written to a memory resident data structure called memTable<ul><li>Each node has a MemTable in memory for each Cassandra table</li><li>Each MemTable contains data for a specific Cassandra table, and it resembles that table in memory</li><li>Each MemTable accrues writes and <strong>provides reads for data not yet flushed to disk</strong></li><li>Commit log stores all the writes in sequential order, with each new write appended to the end; whereas MemTable stores data in the sorted order of partition key and clustering columns</li><li>After writing data to the commit log and MemTable, the node <strong>sends an acknowledgement to the coordinator</strong> that the data has been successfully written</li></ul></li></ul><h3 id="3-4-3-SStable"><a href="#3-4-3-SStable" class="headerlink" title="3.4.3 SStable"></a>3.4.3 SStable</h3><ul><li>When the number of objects stored in the MemTable reaches a threshold, the contents of the MemTable are <strong>flushed to disk</strong> in a file called <strong>SSTable</strong><ul><li>At this point, a new MemTable is created to store subsequent data</li><li>The flush is non blocking operation</li><li>Multiple Memtables may exist for a single table<ul><li>One current, and the rest waiting to be flushed</li></ul></li><li>When the MemTable is flushed to SStables, <strong>corresponding entries in the commit log</strong> are removed</li></ul></li><li>SStable —Sorted String Table<ul><li>Once a MemTable is flushed to disk as an SStable, it is immutable and cannot be changed later</li><li>Each delete or update is considered as a new write operation</li></ul></li><li>The current data state of a Cassandra table consists of its MemTables in memory and SSTables on the disk.<ul><li>Therefore, on reads, Cassandra will read both SSTables and MemTables to find data values, as the MemTable may contain values that have not yet been flushed to the disk.</li><li>The MemTable works like a write-back cache that Cassandra looks up by key</li></ul></li></ul><p><img src="https://i.loli.net/2021/09/17/Qd7x4M6HRrtuAoZ.png" alt="Whole Write Path"></p><h2 id="3-5-Anatomy-of-Cassandra’s-Read-Operation"><a href="#3-5-Anatomy-of-Cassandra’s-Read-Operation" class="headerlink" title="3.5 Anatomy of Cassandra’s Read Operation"></a>3.5 Anatomy of Cassandra’s Read Operation</h2><p><img src="https://i.loli.net/2021/09/17/wIZKE97YqVNAsrP.png" alt="Whole Read Path"></p><h3 id="3-5-1-Caching"><a href="#3-5-1-Caching" class="headerlink" title="3.5.1 Caching"></a>3.5.1 Caching</h3><ul><li>Row Cache<ul><li>Cache frequently read/ hot rows</li><li>Stores a complete data row, which can be returned directly to the client if requested by a read operation</li><li>Could significantly speed up read access for frequently accessed rows, at the cost of more memory usage</li></ul></li><li>Key Cache<ul><li>Stores a map of recently read partition keys to their <strong>SSTable offsets</strong></li><li>This facilitates faster read access into SSTables and improves the read performance</li><li>Use less memory comparing with row cache and provides a considerable improvement for read operations</li></ul></li><li>Chunk Cache<ul><li>Chunk Cache is used to store umcompressed chunks of data read from SSTable files that are accessed frequently</li></ul></li></ul><h3 id="3-5-2-Read-From-MemTable"><a href="#3-5-2-Read-From-MemTable" class="headerlink" title="3.5.2 Read From MemTable"></a>3.5.2 Read From MemTable</h3><ul><li>When a read request come in, node performs a binary search on the partition key to find the required partition and then return the row</li></ul><h3 id="3-5-3-Read-From-SSTable"><a href="#3-5-3-Read-From-SSTable" class="headerlink" title="3.5.3 Read From SSTable"></a>3.5.3 Read From SSTable</h3><ul><li><p>Bloom Filters</p><ul><li>Each SSTable has a Bloom Filter associated with it, which tells if a particular key is present in it or not</li><li>Used to boost performance of read operations</li><li>It’s a very fast, non deterministic algorithms for testing whether an element is a member of a set</li><li>It’s possible to get a false positive but never a false negative</li><li>Theory<ul><li>It works by <strong>mapping the values in a data set into a bit array</strong> and <strong>condensing a larger data set into a digest string</strong> with a hash function</li><li>Filters are stored in memory and are used to improve performance by reducing the need for disk access on key lookups</li></ul></li></ul></li><li><p>How are SSTables stored on the disk?</p><ul><li><p>Consists of two files</p><ul><li><p>Data File</p><ul><li>Actual data is stored here</li><li>It has partitions and rows associated with those partitions</li><li>Partitions are in sorted order</li></ul></li><li><p>Partition Index File</p><ul><li><p>Stored on disk, partition index file stores the sorted partition keys mapped to their SSTable offsets</p></li><li><p>Enable locating a partition exactly in an SSTable rather than scanning data</p><p><img src="https://i.loli.net/2021/09/17/9gUpTXZyLSksDdK.png" alt="Read via Partition Index File"></p></li></ul></li></ul></li></ul></li><li><p>Partition Index Summary File</p><ul><li><p>It’s stored in memory, stores the summary of the partition index file for performance improvement</p><ul><li><p>Two level index, e.g, search for key=19</p></li><li><p>in partition index summary file, it lays to key range 10 - 21</p></li><li><p>then we could go to byte offset 32,</p></li><li><p>in partition index file , we start from 32, to find partition key 19, and then we could go to 5450</p><p><img src="https://i.loli.net/2021/09/17/efsVEmvGAkIldF6.png" alt="Read via Partition Index Summary File"></p></li></ul></li></ul></li><li><p>Read from KeyCache</p><ul><li><p>As the Key Cache stores a map of recently read partition keys to their SSTable offset, it’s the fastest way to find the required row in the SSTable</p><p>  <img src="https://i.loli.net/2021/09/17/5KPTohGmWpecr1a.png" alt="Read From KeyCache"></p></li></ul></li><li><p>Overall workflow</p><p>  <img src="https://i.loli.net/2021/09/17/2zKlRtS48NQYkud.png" alt="Overall Workflow"></p></li></ul><h2 id="3-6-Compaction"><a href="#3-6-Compaction" class="headerlink" title="3.6 Compaction"></a>3.6 Compaction</h2><h3 id="3-6-1-Why-we-need-compaction-And-How-it-Works"><a href="#3-6-1-Why-we-need-compaction-And-How-it-Works" class="headerlink" title="3.6.1 Why we need compaction? And How it Works?"></a>3.6.1 Why we need compaction? And How it Works?</h3><p><img src="https://i.loli.net/2021/09/17/2DgirVjkeq6AI4T.png" alt="Compaction"></p><ul><li>SSTables are immutable, which helps Cassandra achieve high write speeds</li><li>And flushing from MemTable to SSTable is a continuous process, which means we could have a large number of SSTables lying on the disk</li><li>It’s tedious to scan all these SSTables while reading</li><li>We need compaction thus we could merge multiple related SSTables into a single one to improve reading speed</li><li>During compaction, the data in SSTables is merged, keys are merged, columns are combined, obsolete values are discarded, and a new index is created</li></ul><h3 id="3-6-2-Compaction-Strategies"><a href="#3-6-2-Compaction-Strategies" class="headerlink" title="3.6.2 Compaction Strategies"></a>3.6.2 Compaction Strategies</h3><ul><li>SizeTiered Compaction Strategy<ul><li>Suitable for insert-heavy and general workloads</li><li>Triggered when multiple SSTables of a similar size are present</li></ul></li><li>Leveled Compaction Strategy<ul><li>Optimize read performance</li><li>Groups SSTables into levels, each of which has a fixed size limit which is ten times larger than the previous level</li></ul></li><li>Time Window Compaction Strategy<ul><li>Work on time series data</li><li>Compact SSTables within a configured time window</li><li>Ideal for time series data which is immutable after a fixed time interval</li></ul></li></ul><h3 id="3-6-3-Sequential-Writes"><a href="#3-6-3-Sequential-Writes" class="headerlink" title="3.6.3 Sequential Writes"></a>3.6.3 Sequential Writes</h3><ul><li>Main reason that writes perform so well in Cassandra</li><li>No reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations</li><li>Compaction is intended to amortize the reorganization of data, but it uses sequential I/O to do so, which makes it efficient</li></ul><h2 id="3-7-Tombstones"><a href="#3-7-Tombstones" class="headerlink" title="3.7 Tombstones"></a>3.7 Tombstones</h2><h3 id="3-7-1-What-are-Tombstones"><a href="#3-7-1-What-are-Tombstones" class="headerlink" title="3.7.1 What are Tombstones?"></a>3.7.1 What are Tombstones?</h3><ul><li>Scenario<ul><li>We delete some data for a node that is down or unreachable, it would miss a delete</li><li>When the node com back online later and a repair occurs, the node could resurrect the data due to re-sharing it with other nodes</li><li>To prevent deleted data from being reintroduced, Cassandra used a concept of a Tombstone</li></ul></li><li>Tombstone<ul><li>Similar to the idea of soft delete from the relational database</li><li>When we delete, Cassandra does not delete it right away, instead, it associated a tombstone with it, with Time to Expiry</li><li>It’s a marker to indicate data that has been deleted</li><li>When we execute a delete operation, data is not immediately deleted</li><li>Instead, it’s treated as an update operation that places a tombstone on the value</li><li>Default Time to Expiry is set to 10 days<ul><li>If the node is down longer than this value, it should be treated as failed and replaced</li></ul></li><li>Tombstones are removed as part of compaction</li></ul></li></ul><h3 id="3-7-2-Common-problems-associated-with-Tombstones"><a href="#3-7-2-Common-problems-associated-with-Tombstones" class="headerlink" title="3.7.2 Common problems associated with Tombstones"></a>3.7.2 Common problems associated with Tombstones</h3><ul><li>Takes storage space</li><li>When a table accumulates many tombstones, read queries on that table could become slow and can cause serious performance problems like timeouts.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/09/17/IGdzKBJwD9TbZ6f.png&quot; alt=&quot;Cassandra MindMap.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introdu
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
      <category term="Cassandra" scheme="https://www.llchen60.com/tags/Cassandra/"/>
    
  </entry>
  
  <entry>
    <title>Logical Fallacies</title>
    <link href="https://www.llchen60.com/Logical-Fallacies/"/>
    <id>https://www.llchen60.com/Logical-Fallacies/</id>
    <published>2021-09-12T01:23:39.000Z</published>
    <updated>2021-09-12T01:50:35.560Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Logical-Fallacies"><a href="#Logical-Fallacies" class="headerlink" title="Logical Fallacies"></a>Logical Fallacies</h1><h2 id="Overconfidence"><a href="#Overconfidence" class="headerlink" title="Overconfidence"></a>Overconfidence</h2><ul><li>Overconfidence — wishful thinking bias<ul><li>most people think they are above avg</li><li>overestimate possibilities that they want to happen</li><li><strong>this could explain the trade in financial market</strong></li><li>overconfidence in friends and leaders</li></ul></li></ul><h2 id="Cognitive-Dissonance"><a href="#Cognitive-Dissonance" class="headerlink" title="Cognitive Dissonance"></a>Cognitive Dissonance</h2><ul><li>Cognitive Dissonance  认知失调<ul><li>this concept used to describe the mental discomfort that results from holding two conflicting beliefs, values or attitudes</li><li>People tend to seek consistency in their attitudes and perceptions, so this conflict causes feelings of unease or discomfort</li><li>This inconsistency between <strong>what people believe and how they behave</strong> motivates people to <strong>engage in actions</strong> that will help minimize feelings of discomfort</li><li>when we made decision, most people will still look for info about it, to self prove hisself right… in a lot different aspects… to make themselves happy, and to prove they are make right decision</li><li>disposition effect — gonna avoid that</li><li>what’s the causes for that?<ul><li>Forced Compliance<ul><li>Engaging in behaviors that are opposed to your own beliefs due to external expectations, often for work, school, or a social situation</li></ul></li><li>New Information</li><li>Decisions<ul><li>People make decisions both large and small, on a daily basis</li><li>When faced with two similar choices, people often are left with feelings of dissonance because both options are equally appealing</li><li>Once they make decisions, people need to find a way to <strong>reduce feelings of discomfort</strong></li><li>Accomplish by justifying why their choice was the best option so that they can believe they made the right decision</li></ul></li></ul></li></ul></li></ul><h2 id="Mental-Compartments"><a href="#Mental-Compartments" class="headerlink" title="Mental Compartments"></a>Mental Compartments</h2><ul><li>Mental compartments<ul><li>people don’t look at whole portfolio, in fact, people has two or more portfolio<ul><li>usually they have a safe part and a risky part</li></ul></li></ul></li></ul><h2 id="Attention-Anomalies"><a href="#Attention-Anomalies" class="headerlink" title="Attention Anomalies"></a>Attention Anomalies</h2><ul><li>Attention Anomalies<ul><li>We cannot pay attention to anything</li><li>Attention is fundamental aspect of human intelligence and its limits</li><li>Social Basis for attention<ul><li>We incline to pay more attention to what other s pay attention to</li></ul></li></ul></li></ul><h2 id="Anchoring"><a href="#Anchoring" class="headerlink" title="Anchoring"></a>Anchoring</h2><ul><li>Anchoring<ul><li>A tendency in ambiguous situations to allow one’s decisions to be affected by some anchor</li><li>Our subconscious will do anchoring for us, lol</li><li>subjects unaware of their own anchoring behavior</li><li>stock prices anchored to past values, or to other stock in same market</li></ul></li></ul><h2 id="Representativeness-Heuristic"><a href="#Representativeness-Heuristic" class="headerlink" title="Representativeness Heuristic"></a>Representativeness Heuristic</h2><ul><li>Representativeness Heuristic<ul><li>People judge by similarity to familiar types, without regard to <strong>base rate probabilities</strong><ul><li>For example, we describe a person as artist, and skeptical, then what’s the highest possible occupation of him/ her?<ul><li>two choice: banker, and sculptress</li><li>should be banker, cause there are so many more bank tellers than sculptresses</li></ul></li></ul></li><li>Tendency to see patterns in what is really random walk</li><li>Stock price manipulators try to create patterns to fool investors</li></ul></li></ul><h2 id="Disjunction-Effect"><a href="#Disjunction-Effect" class="headerlink" title="Disjunction Effect"></a>Disjunction Effect</h2><ul><li>inability to make decisions in advance in anticipation of future information</li></ul><h2 id="Magical-Thinking-amp-Quasi-Magical-Thinking"><a href="#Magical-Thinking-amp-Quasi-Magical-Thinking" class="headerlink" title="Magical Thinking  &amp; Quasi Magical Thinking"></a>Magical Thinking  &amp; Quasi Magical Thinking</h2><ul><li>Some coincidence lead you to build superstitious, but there are actually no karma (cause and effect)</li><li>Belief that unrelated events are causally connected despite the absence of any plausible causal link between them, particularly as a result of supernatural effects.</li><li>E.G<ul><li>For voting, though our vote actually has basically 0 possibility to influence president election, but a lot people do it</li><li>For lottery, we somehow put more money if we select the number</li></ul></li></ul><h2 id="Personality-Disorders"><a href="#Personality-Disorders" class="headerlink" title="Personality Disorders"></a>Personality Disorders</h2><ul><li>culture and social contagion — collective memory<ul><li>same effect, same memory, then similar decisions</li></ul></li><li>Antisocial Personality Disorder</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Logical-Fallacies&quot;&gt;&lt;a href=&quot;#Logical-Fallacies&quot; class=&quot;headerlink&quot; title=&quot;Logical Fallacies&quot;&gt;&lt;/a&gt;Logical Fallacies&lt;/h1&gt;&lt;h2 id=&quot;Overc
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>抗逆力</title>
    <link href="https://www.llchen60.com/%E6%8A%97%E9%80%86%E5%8A%9B/"/>
    <id>https://www.llchen60.com/%E6%8A%97%E9%80%86%E5%8A%9B/</id>
    <published>2021-08-28T19:28:59.000Z</published>
    <updated>2021-08-28T19:30:58.113Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>很庆幸听了这场讲座，回想起原先做的一些决定，会囿于固定性思维，因为害怕失败，害怕被证实不够聪明，失去了很多很多成长的机会。感觉无论是工作还是生活，在度过了刚刚开始的熟悉了解阶段以后，后期除了专业技能上的提升之外，更加重要的是mindset的提升 — 关于你是怎么看待自己的，怎么看待你经历的事情。不知道最终答案是什么，但是这篇里面说的东西至少告诉了我想要达到理想的状态，你需要每天做些什么 :)  值得过一段时间回来再看看各种action items 呀</p></blockquote><h1 id="1-心理韧性"><a href="#1-心理韧性" class="headerlink" title="1. 心理韧性"></a>1. 心理韧性</h1><ul><li>高心理韧性是成功者的共性<ul><li>因为对于任何一个成功者来说，磨难是必不可少的一部分</li><li>成功者 坚韧不拔的精神</li><li>心理学研究<ul><li>预测成功的概率 和 坚韧不拔的特质有很明显的正相关，和智商的关系反倒并不是很强</li><li>生存下来的不是最强大的生物，也不是最聪明的生物，而是最能够适应变化的生物</li></ul></li></ul></li></ul><h2 id="1-1-心理学定义"><a href="#1-1-心理学定义" class="headerlink" title="1.1 心理学定义"></a>1.1 心理学定义</h2><ul><li>复原力 resilence<ul><li>人从逆境，冲突，痛楚，失败，压力当中迅速恢复的心理能力</li></ul></li><li>坚毅力 grip</li><li>创伤后的成长 PTG — Post Traumatic Growth<ul><li>不消沉，奋进</li></ul></li></ul><h2 id="1-2-高心理韧性人的特质"><a href="#1-2-高心理韧性人的特质" class="headerlink" title="1.2 高心理韧性人的特质"></a>1.2 高心理韧性人的特质</h2><ul><li>能力<ul><li>适应力</li><li>成长力</li><li>抗挫力</li><li>积极力 — 情绪的调节的方法</li><li>关系力 — 如何建立关系</li><li>控制力 — 淡定从容，自我控制</li></ul></li><li>特质<ul><li>有积极的认知方式  — the power of positive thinking<ul><li>决定我们的幸福指数的不是事情本身，而是我们如何看待这个事情</li></ul></li><li>乐观的情绪调节</li><li>健康的身心状态</li><li>强大的自我效能感  — 感觉自己能成，感觉自己有用  lol<ul><li>结婚能让男性长寿7年 lol</li></ul></li><li>解决问题的行动精神</li><li>良好的人际关系</li></ul></li></ul><h1 id="2-如何提升心理韧性"><a href="#2-如何提升心理韧性" class="headerlink" title="2. 如何提升心理韧性"></a>2. 如何提升心理韧性</h1><h2 id="2-1-自我效能感的提升"><a href="#2-1-自我效能感的提升" class="headerlink" title="2.1 自我效能感的提升"></a>2.1 自我效能感的提升</h2><ul><li>自我效能感的提升 Self Efficacy<ul><li>定义 — 是个人对自己完成某方面工作能力的主观评估，通过两条路径体现出来</li><li>高自我效能感的人，甚至会把压力 挫折 打击当做一种证明自己的能力的机遇</li><li>体现路径<ul><li>结果预期<ul><li>相信自己，认为我可以做到，是一种自我实现的预言</li></ul></li><li>效能预期<ul><li>我认为我能做到不是因为运气好或者环境好，而是因为我的能力</li><li>因此我要施展我的能力，为结果做足准备</li></ul></li></ul></li></ul></li><li>如何去做<ul><li>做出成功的模样<ul><li>装积极，是会变成真积极的</li><li>步伐更快</li><li>说话更多</li><li>做事主动</li><li>穿衣更正式些</li><li>锻炼更频繁些</li></ul></li><li>被成功者接纳<ul><li>与积极的人同行</li><li>替代性强化<ul><li>观察者看到榜样或者他人收到强化，成功了; 从而使得自己也倾向于做出榜样的行为</li></ul></li></ul></li><li>社会支持<ul><li>进化选择的是合作者</li><li>社会网络面积越大，更容易产生优势效应</li><li><strong>弱联系的强势效应</strong><ul><li>弱联系有着很快的低成本和高效能的传播效率</li><li>在六度分隔试验当中，正是层层叠加的弱联系将世界上原本毫不相关的人联系到了一起</li></ul></li></ul></li><li>模拟实战<ul><li>预见</li><li>大脑休闲的时候处于默认模式状态  hh<ul><li>会畅想未来，是一种竞争优势的~</li><li>对于事情进行遇见，是对我们帮助很大的</li></ul></li><li>Visualization  预见想象<ul><li>将自己将要做的事情去提前想象一下</li><li>过一遍自己需要怎么做</li><li>训练越多，意向越清晰</li><li>设想遇到打击，困难的时候你要怎么做</li></ul></li></ul></li><li>不断积累成功<ul><li>人最可怕的是发现自己一成不变</li><li>要去做</li></ul></li></ul></li></ul><h2 id="2-2-培养成长性思维"><a href="#2-2-培养成长性思维" class="headerlink" title="2.2 培养成长性思维"></a>2.2 培养成长性思维</h2><ul><li><p>人的思维模式</p><ul><li>成长性思维  Growth Mindset<ul><li>天赋只是起点</li><li>态度和努力可以决定一切</li><li>可以学会任何我想学会的东西</li><li>喜欢自我挑战</li><li>当我失败的时候，我学到了很多东西</li><li>我希望你表扬我很努力</li><li>如果别人成功了，我会收到别人的启发</li></ul></li><li>固定性思维 — 卓越的包袱<ul><li>我的聪明才智决定了一切</li><li>我擅长某些事，不擅长另外一些事</li><li>我不想尝试我可能不擅长的东西</li><li>如果我失败了，我就无地自容了</li><li>我希望你表扬我很聪明</li><li>如果别人成功了，他会威胁到我</li></ul></li></ul></li><li><p>固定性思维对于人的影响很大</p><ul><li>你会因为认为自己聪明，不敢做更大的挑战，因为一旦失败，你会害怕别人认为你不聪明了 会越来越难达到别人的预期的</li><li>被表扬努力的往往会选择更加困难的任务，也会更愿意通过学习，去尝试解决方案</li><li>卓越的包袱<ul><li>装酷的孩子的包袱</li><li>不愿意去冒险，不愿意去奋斗</li><li>努力愚蠢，装聪明</li><li>精英父母的过高的期望造成的心理压力和心理阴影</li><li>优秀女孩的诅咒，这种包袱往往对女孩的打击更大，她们往往更在意外在的评价，不敢冒险和努力</li><li>we are supposed to be dumb all the way, hhh</li></ul></li></ul></li><li><p>如何培养成长性思维</p><ul><li>改变考核的标准<ul><li>关注进步，而不是结果</li></ul></li><li>改变沟通的方式<ul><li>在评价表现的时候，用暂时不行代替就是不行</li><li>短暂 局部 可以改的</li><li>不要把事情说成稳定的长期的不可改变的</li><li>not yet instead of failed</li></ul></li><li>改变认知的习惯 — Albert Ellis 的认知治疗ABC</li><li>发挥辩证思维的优势 — 从负面体验中吸取成功的经验<ul><li>当一个人出于自我保护而抗拒内心的地狱的时候，他一并切断了通往内在天堂的道路。<ul><li>不承认自己内心的阴暗龌龊，那么就无从改进了</li></ul></li></ul></li></ul></li></ul><ul><li>认知治疗ABC<ul><li>构成<ul><li>A — Activating Events  诱发刺激</li><li>B — Beliefs  信念反应</li><li>C — Consequences  行为后果</li></ul></li><li>原理<ul><li>我们是改不了A的，但是我们可以改B，然后C就会发生变化！！</li><li>关键是你怎么看待A的 ！ 改变认知</li><li>真正困扰我们的并不是发生在我们身上的事情，而是我们围绕这个事情对它编织的故事，和由此引起的身心反应</li></ul></li></ul></li><li>情绪的ABCD理论 — 对于孩子而言<ul><li>出现了ABC以后，给一个机会让其反驳</li><li>让孩子去反驳他当时的念头</li><li>干预B  从而干预C</li></ul></li></ul><h2 id="2-3-提高自我调控的能力"><a href="#2-3-提高自我调控的能力" class="headerlink" title="2.3 提高自我调控的能力"></a>2.3 提高自我调控的能力</h2><ul><li>延迟满足， 自我控制</li><li>自我调控能力是可以锻炼从而获得提升的</li><li>如何进行训练<ul><li>体育锻炼</li><li>正念冥想 — 做事情沉浸其中就好啊！！</li><li>自我挑战</li><li>目标想象</li><li>有效休息</li><li>积极心态</li></ul></li></ul><h1 id="3-组织韧性"><a href="#3-组织韧性" class="headerlink" title="3. 组织韧性"></a>3. 组织韧性</h1><ul><li><p>复原力</p><ul><li>企业遇到困难后，如何回归正常</li></ul></li><li><p>复原后的发展能力</p></li><li><p>影响组织韧性的维度</p><ul><li>组织资本<ul><li>人力资源的保障<ul><li>什么政策</li></ul></li></ul></li><li>组织承诺<ul><li>员工对于组织的感情</li><li>信任</li></ul></li><li>组织领导<ul><li>leader本身的态度，思考，韧性</li></ul></li><li>组织学习</li><li>组织文化<ul><li>组织的传统和信仰</li></ul></li><li>社会网络</li></ul></li><li><p>提升组织韧性的方式</p><ul><li>Staff  选择心理韧性高的人才，锻炼心理韧性<ul><li>积极的自我认识</li><li>提倡积极的思维</li><li>加强关系建设</li><li>未来导向</li><li>乐观主义精神<ul><li>对于路径的乐观</li><li>对于结果的乐观</li></ul></li></ul></li><li>System 创造积极的心理健康环境</li><li>Skill</li></ul></li></ul><h1 id="4-压力的应对技巧"><a href="#4-压力的应对技巧" class="headerlink" title="4. 压力的应对技巧"></a>4. 压力的应对技巧</h1><h2 id="4-1-压力的应激反应"><a href="#4-1-压力的应激反应" class="headerlink" title="4.1 压力的应激反应"></a>4.1 压力的应激反应</h2><ul><li>应激反应的三轴心<ul><li>下丘脑</li><li>垂体</li><li>肾上腺</li></ul></li><li>三个器官会释放压力激素，使得我们的反应是fight or flight lol</li><li>而后激素水平下降</li><li>各种情绪<ul><li>焦虑  为未来的恐慌</li><li>抑郁  为过去伤心</li><li>自残</li></ul></li></ul><h2 id="4-2-与情绪有关的脑区"><a href="#4-2-与情绪有关的脑区" class="headerlink" title="4.2 与情绪有关的脑区"></a>4.2 与情绪有关的脑区</h2><ul><li><p>杏仁核  Amygdala</p><ul><li>会影响我们的情绪</li><li>情绪不好的时候会让杏仁核充血，然后杏仁核温度升高</li></ul></li><li><p>大脑皮层 Cerebral Cortex</p></li><li><p>如何应对压力 — <strong>抑制</strong>杏仁核的活动</p><ul><li>吸入凉气，降低杏仁核的温度  hhh</li><li>香气  让我们产生愉悦的感觉</li><li>写写日记</li></ul></li><li><p>激活大脑的奖励中枢</p><ul><li><p>神经元之间的间隙 靠神经递质连接</p></li><li><p>当奖赏中枢释放神经递质的时候，会释放积极的情绪</p></li><li><p>！！心理活动不是一个一个点，而是一片一片的产生的</p></li><li><p>多巴胺</p><ul><li>庆祝自己的成功 — 让自己的成功和快乐的体验延续一段时间<ul><li>将快乐的体验延续4分钟，就可以在大脑中形成记忆，从而形成一个快乐的神经网络</li></ul></li><li>做自己喜欢做的事情</li><li>享受艺术的美妙</li></ul></li><li><p>血清素</p><ul><li>能够振奋人的心情</li><li>什么时候会分泌<ul><li>体验到自我的价值</li><li>帮助别人的时候</li><li>自尊心的呵护<ul><li>保护自尊心  体现其价值</li></ul></li></ul></li><li>一些行为<ul><li>晒太阳~</li></ul></li></ul></li><li><p>内啡肽</p><ul><li>只有我们身心痛苦的时候，才会释放</li><li>行为<ul><li>有规律的运动</li><li>先苦后甜的体验</li><li>看喜剧，相声~ 烧脑的幽默</li></ul></li></ul></li><li><p>催产素</p><ul><li>男人也有催产素</li><li>主要作用是增加人的爱的感受，而不是为了怀孕</li><li>行为<ul><li>夸奖，赞美</li><li>陪伴</li></ul></li></ul><h2 id="4-3-应对压力的长期策略"><a href="#4-3-应对压力的长期策略" class="headerlink" title="4.3 应对压力的长期策略"></a>4.3 应对压力的长期策略</h2></li><li><p>压力容易让人失控失常</p></li><li><p>培养应对压力的积极习惯</p><ul><li>strength based approach  发挥自己的长处优势  找到自己的优势，然后充分在工作生活当中使用  使用自己的优势！！！</li><li>找到自己的流  find your flow<ul><li>喜欢做的事情，进入心流状态</li></ul></li><li>借助一些科学方法，自修，同修，专修<ul><li>reading &amp; learning</li></ul></li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>彭凯平 演讲  — 抗逆力— 重压下的心理韧性与成功</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;很庆幸听了这场讲座，回想起原先做的一些决定，会囿于固定性思维，因为害怕失败，害怕被证实不够聪明，失去了很多很多成长的机会。感觉无论是工作还是生活，在度过了刚刚开始的熟悉了解阶段以后，后期除了专业技能上的提升之外，更加重要的是mindset的提升 —
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="积极心理学" scheme="https://www.llchen60.com/tags/%E7%A7%AF%E6%9E%81%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Make Body Language Your Super Power</title>
    <link href="https://www.llchen60.com/Make-Body-Language-Your-Super-Power/"/>
    <id>https://www.llchen60.com/Make-Body-Language-Your-Super-Power/</id>
    <published>2021-08-28T02:24:14.000Z</published>
    <updated>2021-08-28T02:25:19.176Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Stand Strong</li><li>Gesture Effectively</li><li>Mind Your Audience</li></ul><h1 id="1-Posture"><a href="#1-Posture" class="headerlink" title="1. Posture"></a>1. Posture</h1><ul><li>Communication begins before you open your mouth</li><li>How to stand<ul><li>no<ul><li>hands in the pocket  — cannot convey strong msg</li><li>hands in the hip — tend to look overwhelming and powerful</li><li>hands in front of family jewels lol</li></ul></li><li>yes<ul><li>base posture — feet should be shoulder width apart<ul><li>that’s the first impression</li></ul></li><li>movement<ul><li>give</li><li>show</li><li>chop — strong msg</li></ul></li><li>palms up has better impact! comparing with palm down and pointing</li></ul></li></ul></li><li>Where to stand<ul><li>face your audience</li><li>move around in the center box</li><li>get rid of potential distraction<ul><li>like window, by nature we are attracted by moving thing, will break the concentration</li></ul></li></ul></li></ul><h1 id="2-Audience"><a href="#2-Audience" class="headerlink" title="2. Audience"></a>2. Audience</h1><ul><li>Speaker need to understand what audience is doing , make sure we are all in the journey</li><li>How to engage with audience more<ul><li>gesture</li><li>notice<ul><li>how your audience sitting</li><li>eye contact</li></ul></li><li>surprise<ul><li>cold call, lol</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.youtube.com/watch?v=cFLjudWTuGQ&ab_channel=StanfordGraduateSchoolofBusiness">https://www.youtube.com/watch?v=cFLjudWTuGQ&amp;ab_channel=StanfordGraduateSchoolofBusiness</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Stand Strong&lt;/li&gt;
&lt;li&gt;Gesture Effectively&lt;/li&gt;
&lt;li&gt;Mind Your Audience&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;1-Posture&quot;&gt;&lt;a href=&quot;#1-Posture&quot; class=&quot;hea
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Distributed Messaging System: Kafka</title>
    <link href="https://www.llchen60.com/Distributed-Messaging-System-Kafka/"/>
    <id>https://www.llchen60.com/Distributed-Messaging-System-Kafka/</id>
    <published>2021-08-24T17:07:33.000Z</published>
    <updated>2021-08-24T17:10:35.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview-of-Messaging-Systems"><a href="#1-Overview-of-Messaging-Systems" class="headerlink" title="1. Overview of Messaging Systems"></a>1. Overview of Messaging Systems</h1><h2 id="1-1-Why-we-need-a-messaging-system"><a href="#1-1-Why-we-need-a-messaging-system" class="headerlink" title="1.1 Why we need a messaging system"></a>1.1 Why we need a messaging system</h2><ul><li><p>Aim:</p><ul><li>Reliably transfer a high throughput of messages between different entities</li></ul></li><li><p>Challenges</p><ul><li>how we handle a spike of messages</li><li>how we divide the work among a set of instances</li><li>how could we receive messages from different types of sources</li><li>what will happen if the service is down?</li></ul></li><li><p>We need messaging systems in distributed architecture due to challenges above</p></li></ul><h2 id="1-2-What-is-a-messaging-system"><a href="#1-2-What-is-a-messaging-system" class="headerlink" title="1.2 What is a messaging system?"></a>1.2 What is a messaging system?</h2><ul><li><p>responsible for transferring data among services /applications/ processes/ servers</p></li><li><p>help decouple different parts of a distributed system by providing an asynchronous way of transferring messaging between the sender and the receiver</p></li><li><p>Two common ways to handle messages</p><ul><li>Queuing<ul><li>msgs are stored sequentially in a queue</li><li>producers push msg to the rear of the queue</li><li>consumers extract the msgs from the front of the queue</li><li>a particular msg can be consumed by a <strong>max of one consumer</strong> only</li></ul></li><li>Publish - Subscribe<ul><li>messages are divided into topics</li><li>a publisher sends a message to a topic</li><li>subscribers subscribe to a topic to receive every message published to that topic</li><li>msg system that stores and maintains the msg named as <strong>message broker</strong></li></ul></li></ul></li></ul><h1 id="2-Kafka"><a href="#2-Kafka" class="headerlink" title="2. Kafka"></a>2. Kafka</h1><h2 id="2-1-General"><a href="#2-1-General" class="headerlink" title="2.1 General"></a>2.1 General</h2><ul><li><strong>publish subscribe based</strong> messaging system</li><li>takes streams of messages from applications known as producers, stores them reliably on a central cluster, and allows those messages to be received by applications that process the messages</li><li>kafka is mainly used for<ul><li>reliably storing a huge amount of data</li><li>enabling high throughput of message transfer between different entities</li><li>streaming real time data</li></ul></li><li>kafka is a distributed commit log — write ahead log<ul><li>append-only data structure that can <strong>persistently store a sequence of records</strong></li><li>all messages are stored <strong>on disk</strong></li><li>since all reads and writes happen <strong>in sequence</strong>, Kafka takes advantage of <strong>sequential disk reads</strong></li></ul></li></ul><h2 id="2-2-Use-Cases"><a href="#2-2-Use-Cases" class="headerlink" title="2.2 Use Cases"></a>2.2 Use Cases</h2><ul><li>Metrics<ul><li>collect and aggregate monitoring data</li></ul></li><li>Log Aggregation<ul><li>collect logs from multiple sources and make them available in a standard format to multiple consumers</li></ul></li><li>Stream Processing<ul><li>the raw data consumed from a topic is transformed, enriched, or aggregated and pushed to a <strong>new topic</strong> for further consumption. This way of data processing is known as stream processing.</li></ul></li><li>Commit Log<ul><li>can be used as an external commit log for any distributed system</li><li>Distributed services can log their transactions to Kafka to keep track of what is happening. This transaction data can be used for replication between nodes and also becomes very useful for disaster recovery, for example, to help failed nodes to recover their states.</li></ul></li><li>Website activity tracking<ul><li>Build a user activity tracking pipeline</li><li>User activities like page clicks, searches, etc., are published to Kafka into separate topics. These topics are available for subscription for a range of use cases, including real-time processing, real-time monitoring, or loading into Hadoop or data warehousing systems for offline processing and reporting</li></ul></li><li>Product Suggestion</li></ul><h1 id="3-High-Level-Architecture"><a href="#3-High-Level-Architecture" class="headerlink" title="3. High Level Architecture"></a>3. High Level Architecture</h1><h2 id="3-1-Common-Terms"><a href="#3-1-Common-Terms" class="headerlink" title="3.1 Common Terms"></a>3.1 Common Terms</h2><ul><li>Brokers<ul><li>A Kafka server</li><li>responsible for reliably storing data provided by the producers and making it available to the consumers</li></ul></li><li>Records<ul><li>A message or an event that get stored in Kafka</li><li>A record contains<ul><li>key</li><li>value</li><li>timestamp</li><li>optional metadata headers</li></ul></li></ul></li><li>Topics<ul><li>Messages are divided into categories called topics</li><li>Each msg that Kafka receives from a producer is associated with a topic</li><li>consumers can subscribe to a topic to get notified when new messages are added to the topic</li><li>a topic can have multiple subscribers that read messages from it</li><li>a topic is identified by its name and must be unique</li><li>mes in a topic can be read as often as needed — message are not deleted after consumption, instead, Kafka <strong>retains messages for a configurable amount of time or until a storage size is exceeded</strong></li></ul></li><li>Producers<ul><li>Applications that publish or write records to Kafka</li></ul></li><li>Consumers<ul><li>Applications that subscribe to read and process data from Kafka topics</li><li>Consumers subscribe to one or more topics and consume published messages by pulling data from the brokers</li><li>In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for</li></ul></li></ul><h2 id="3-2-Architecture"><a href="#3-2-Architecture" class="headerlink" title="3.2 Architecture"></a>3.2 Architecture</h2><p><img src="https://i.loli.net/2021/08/25/Chuwtvg4mNfFU58.png" alt="Overall Architecture"></p><ul><li>Kafka cluster<ul><li>Kafka is run as a cluster of one or more servers, where each server is responsible for running one Kafka broker</li></ul></li><li>ZooKeeper<ul><li>Distributed key value store</li><li>Used for coordination and storing configurations</li><li>Kafka uses ZooKeeper to coordinate between Kafka brokers; ZooKeeper maintains metadata information about the Kafka cluster</li></ul></li></ul><h2 id="3-3-Performance-concern"><a href="#3-3-Performance-concern" class="headerlink" title="3.3 Performance concern"></a>3.3 Performance concern</h2><h3 id="3-3-1-Storing-messages-to-disks"><a href="#3-3-1-Storing-messages-to-disks" class="headerlink" title="3.3.1 Storing messages to disks"></a>3.3.1 Storing messages to disks</h3><ul><li>there is a huge difference in disk performance between <strong>random block access and sequential access</strong>. Random block access is slower because of <strong>numerous disk seeks</strong>, whereas the sequential nature of writing or reading, enables disk operations to be <strong>thousands of times faster</strong> than random access.</li><li>OS level optimization<ul><li>Read Ahead — prefetch large block multiples</li><li>Write Behind — group small logical writes into big physical writes</li><li>PageCache — cache the disk in free RAM</li></ul></li><li>Zero Copy optimization<ul><li>OS copy data from the pageCache directly to a socket, effectively bypassing the kafka broker application entirely</li></ul></li><li>Kafka protocol to group msg together<ul><li>reduce network overhead</li></ul></li></ul><h1 id="4-Dive-Deep-in-Kafka-Cluster"><a href="#4-Dive-Deep-in-Kafka-Cluster" class="headerlink" title="4. Dive Deep in Kafka Cluster"></a>4. Dive Deep in Kafka Cluster</h1><h2 id="4-1-Topic-Partitions"><a href="#4-1-Topic-Partitions" class="headerlink" title="4.1 Topic Partitions"></a>4.1 Topic Partitions</h2><ul><li><p>Topics are partitioned, spread over a number of fragments</p></li><li><p>Each partition can be placed on a separate Kafka broker</p></li><li><p>A new message get appended to one of the topic’s partition</p><ul><li>producer controls which partition it publishes to based on the data</li></ul></li><li><p>One partition is an <strong>ordered sequence</strong> of messages</p><ul><li>producers continually append new messages to partition</li><li>ordering of messages is <strong>maintained at the partition level, not across the topic</strong></li></ul></li><li><p>Unique sequence ID — offset</p><ul><li>It will get assigned to every message that enters a partition</li><li>used to identify every message’s sequential position within a topic’s partition</li><li>offset sequences are unique only to each partition</li><li>to locate a specific message<ul><li>topic</li><li>partition</li><li>offset number</li></ul></li><li>producers can choose to publish a message to any partition<ul><li>if ordering within a partition is not needed, a round robin partition strategy can be used</li><li>Placing each partition on separate Kafka brokers enables multiple consumers to read from a topic in parallel. That means, different consumers can concurrently read different partitions present on separate brokers</li></ul></li></ul></li><li><p>Messages once written to partitions are immutable and cannot be updated.</p></li><li><p>Kafka guarantees that messages with the same key are written to the same partition.</p></li></ul><h2 id="4-2-Dumb-Broker-and-Smart-Consumer"><a href="#4-2-Dumb-Broker-and-Smart-Consumer" class="headerlink" title="4.2 Dumb Broker and Smart Consumer"></a>4.2 Dumb Broker and Smart Consumer</h2><ul><li>Kafka does not keep track of what records are read by the consumer</li><li>Consumers themselves poll kafka for new messages and say what records they want to read<ul><li>this allow them to increment/ decrement the offset they are as they wish</li></ul></li></ul><h2 id="4-3-Leader-and-Follower"><a href="#4-3-Leader-and-Follower" class="headerlink" title="4.3 Leader and Follower"></a>4.3 Leader and Follower</h2><p>Every topic can be replicated to multiple Kafka brokers to make the data fault-tolerant and highly available. Each topic partition has one leader broker and multiple replica (follower) brokers. </p><ul><li>Structure<ul><li>the broker cluster could have multiple brokers, each broker could have multiple partitions which belong to different topics</li><li>Each topic partition would have one lead broker and multiple replica brokers</li></ul></li></ul><h3 id="4-3-1-Leader"><a href="#4-3-1-Leader" class="headerlink" title="4.3.1 Leader"></a>4.3.1 Leader</h3><ul><li>A leader is the node responsible for all reads and writes for the given partition</li><li>Each partition has one kafka broker acting as a leader</li></ul><h3 id="4-3-2-Follower"><a href="#4-3-2-Follower" class="headerlink" title="4.3.2 Follower"></a>4.3.2 Follower</h3><ul><li><p>To handle single point of failure, Kafka replicate partitions and distribute them across multiple broker servers called followers.</p></li><li><p>Each follower’s responsibility is to replicate the leader’s data to serve as a backup partition</p><ul><li><p>any follower can take over the leadership if the leader goes down</p><ul><li><p>from the image below, you could see only the leader take read and write requests, follower acts as replica but not take any read and write reqeusts</p><p><img src="https://i.loli.net/2021/08/25/4t7kVJfv3jliZyK.png" alt="Leader and Follower"></p></li></ul></li></ul></li></ul><h3 id="4-3-3-In-Sync-Replicas"><a href="#4-3-3-In-Sync-Replicas" class="headerlink" title="4.3.3 In Sync Replicas"></a>4.3.3 In Sync Replicas</h3><ul><li>In Sync Replicas means the broker has the latest data for a given partition</li><li>A leader is always an in sync replica</li><li>A follower is an in sync replica only if it has fully caught up to the partition it is following</li><li>Only ISRs are eligible to become partition leaders.</li><li>Kafka can choose the minimum number of ISRs required before the data becomes available for consumers to read</li></ul><h3 id="4-3-4-High-Water-mark"><a href="#4-3-4-High-Water-mark" class="headerlink" title="4.3.4 High Water mark"></a>4.3.4 High Water mark</h3><ul><li><p>To ensure data consistency, the leader broker never returns (or exposes) messages which have not been replicated to a minimum set of ISRs</p></li><li><p>For this, brokers keep track of the high-water mark, which is the highest offset that all ISRs of a particular partition share</p></li><li><p>The leader exposes data only up to the high-water mark offset and propagates the high-water mark offset to all followers</p><p>  <img src="https://i.loli.net/2021/08/25/pyDKzGCRow6O2Wu.png" alt="High Water Mark"></p></li></ul><h1 id="5-Consumer-Group"><a href="#5-Consumer-Group" class="headerlink" title="5. Consumer Group"></a>5. Consumer Group</h1><ul><li>A set of one or more consumers working together in parallel to consume messages from topic partitions, messages are equally divided among all the consumers of a group. with no two consumers receiving the same message</li></ul><h2 id="5-1-How-to-distribute-a-specific-message-to-only-a-single-consumer"><a href="#5-1-How-to-distribute-a-specific-message-to-only-a-single-consumer" class="headerlink" title="5.1 How to distribute a specific message to only a single consumer"></a>5.1 How to distribute a specific message to only a single consumer</h2><ul><li><p>only a single consumer reads messages from any partition within a consumer group</p><ul><li>means only one consumer can work on a partition in a consumer group at a time</li><li>every time a consumer is added to or removed from a group, the consumption is rebalanced within the group</li></ul></li><li><p>with consumer groups, consumers can be parallelized so that multiple consumers can read from multiple partitions on a topic, allowing a very high message processing throughput</p></li><li><p>number of partitions impacts consumers’ maximum parallelism. as there cannot be more consumers than partitions</p></li><li><p>Kafka stores the <strong>current offset per consumer group per topic per partition</strong>, as it would for a single consumer. This means that unique messages are only sent to a single consumer in a consumer group, and the load is balanced across consumers as equally as possible</p></li><li><p><strong>Number of consumers in a group = number of partitions:</strong> each consumer consumes one partition.</p></li><li><p><strong>Number of consumers in a group &gt; number of partitions:</strong> some consumers will be idle.</p></li><li><p><strong>Number of consumers in a group &lt; number of partitions:</strong> some consumers will consume more partitions than others.</p></li></ul><h1 id="6-Kafka-Workflow"><a href="#6-Kafka-Workflow" class="headerlink" title="6. Kafka Workflow"></a>6. Kafka Workflow</h1><h2 id="6-1-Pub-sub-messaging"><a href="#6-1-Pub-sub-messaging" class="headerlink" title="6.1 Pub sub messaging"></a>6.1 Pub sub messaging</h2><ul><li>Producer publish messages on a topic</li><li>Kafka broker stores messages in the partitions configured for that particular topic.<ul><li>If the producer did not specify the partition in which the msg should be stored, the broker ensures that the msg are equally shared between partitions</li><li>If the producer sends two msgs and there are two partitions, Kafka will store those two in two partitions separately.</li></ul></li><li>consumer subscribe to a specific topic</li><li>Kafka will provide the current offset of the topic to the consumer and also saves that offset in the zookeeper</li><li>consumer request kafka at regular intervals for new msgs</li><li>once kafka receives msg from producers, it forward these messages to the consumer</li><li>consumer will receive msg and process it</li><li>once processed, consumer will send an acknowledgement to the kafka broker</li><li>upon receiving the acknowledgement, kafka <strong>increments the offset and updates it in the zooKeeper</strong><ul><li>this info is stored in zooKeeper, thus consumer could read the next msg correctly even during broker outages</li></ul></li><li>consumers can rewind/ skip to the desired offset of a topic at any time and read all the subsequent messages</li></ul><h2 id="6-2-Kafka-workflow-for-consumer-group"><a href="#6-2-Kafka-workflow-for-consumer-group" class="headerlink" title="6.2 Kafka workflow for consumer group"></a>6.2 Kafka workflow for consumer group</h2><ul><li>Producers publish messages on a topic.</li><li>Kafka stores all messages in the partitions configured for that particular topic, similar to the earlier scenario.</li><li>A single consumer subscribes to a specific topic, assume <code>Topic-01</code> with Group ID as <code>Group-1</code>.</li><li>Kafka interacts with the consumer in the same way as pub-sub messaging until a new consumer subscribes to the same topic, <code>Topic-01</code>, with the same Group ID as <code>Group-1</code>.</li><li>Once the new consumer arrives, Kafka switches its operation to share mode, such that each message is passed to only one of the subscribers<br>of the consumer group <code>Group-1</code>. This message transfer is<br>similar to queue-based messaging, as only one consumer of the group<br>consumes a message. Contrary to queue-based messaging, messages are not<br>removed after consumption.</li><li>This message transfer can go on until the number of consumers<br>reaches the number of partitions configured for that particular topic.</li><li>Once the number of consumers exceeds the number of partitions, the<br>new consumer will not receive any message until an existing consumer<br>unsubscribes. This scenario arises because each consumer in Kafka will<br>be assigned a minimum of one partition. Once all the partitions are<br>assigned to the existing consumers, the new consumers will have to wait.</li></ul><h1 id="7-ZooKeeper"><a href="#7-ZooKeeper" class="headerlink" title="7. ZooKeeper"></a>7. ZooKeeper</h1><h2 id="7-1-What-is-ZooKeeper"><a href="#7-1-What-is-ZooKeeper" class="headerlink" title="7.1 What is ZooKeeper"></a>7.1 What is ZooKeeper</h2><ul><li>A distributed configuration and synchronization service</li><li>In Kafka case, help to store basic metadata<ul><li>information about brokers</li><li>topics</li><li>partitions</li><li>partition leader/ followers</li><li>consumer offsets</li></ul></li></ul><h2 id="7-2-Act-as-central-coordinator"><a href="#7-2-Act-as-central-coordinator" class="headerlink" title="7.2 Act as central coordinator"></a>7.2 Act as central coordinator</h2><p>ZooKeeper is used for storing all sorts of metadata about the Kafka cluster:</p><ul><li>It maintains the <strong>last offset position</strong> of each consumer group per partition, so that consumers can quickly recover from the last position in case of a failure (although modern clients store offsets in a<br>separate Kafka topic).</li><li>It tracks the topics, number of partitions assigned to those topics, and leaders’/followers’ location in each partition.</li><li>It also manages the access control lists (ACLs) to different topics in the cluster. ACLs are used to enforce access or authorization.</li></ul><h2 id="7-3-How-to-find-leaders"><a href="#7-3-How-to-find-leaders" class="headerlink" title="7.3 How to find leaders"></a>7.3 How to find leaders</h2><ul><li>The producer connects to any broker and asks for the leader of Partition 1<ul><li>each broker contains metadata</li><li>each brokers will talk to zooKeeper to get the latest metadata</li></ul></li><li>The broker responds with the identification of the leader broker responsible for partition 1</li><li>The producer connects to the leader broker to publish the message</li></ul><h1 id="8-Controller-Broker"><a href="#8-Controller-Broker" class="headerlink" title="8. Controller Broker"></a>8. Controller Broker</h1><ul><li>Within the Kafka cluster, one broker will be elected as the Controller</li><li>Responsibility<ul><li>admin operations<ul><li>creating/ deleting a topic</li><li>adding partitions</li><li>assigning leaders to partitions</li><li>monitoring broker failures</li></ul></li><li>check the health of other brokers in the system periodically</li><li>communicates the result of the partition leader election to other brokers in the system</li></ul></li></ul><h2 id="8-1-Split-brain-issue"><a href="#8-1-Split-brain-issue" class="headerlink" title="8.1 Split brain issue"></a>8.1 Split brain issue</h2><ul><li>some controller has temporary issue, during the period, we assign a new controller, but the previous one auto recover, so we have two controllers and it could bring inconsistency easily.</li><li>Solution:<ul><li>Generation Clock<ul><li>simply a monotonically increasing number to indicate a server’s generation</li><li>If the old leader had an epoch number of ‘1’, the new one would have ‘2’.</li><li>This epoch is included in every request that is sent from the Controller to other brokers.</li><li>This way, brokers can now easily differentiate the real Controller by simply trusting the Controller with the highest number.</li><li>The Controller with the highest number is undoubtedly the latest one, since the epoch number is always increasing.</li><li>This epoch number is stored in ZooKeeper.</li></ul></li></ul></li></ul><h1 id="9-Delivery-Semantics"><a href="#9-Delivery-Semantics" class="headerlink" title="9. Delivery Semantics"></a>9. Delivery Semantics</h1><h2 id="9-1-Producer-Delivery-Semantics"><a href="#9-1-Producer-Delivery-Semantics" class="headerlink" title="9.1 Producer Delivery Semantics"></a>9.1 Producer Delivery Semantics</h2><ul><li>How can a producer know that the data is successfully stored at the leader or that the followers are keeping up with the leader</li><li>Kafka offers three options to denote the <strong>number of brokers</strong> that <strong>must receive the record</strong> before the <strong>producer considers the write as successful</strong><ul><li>Async<ul><li>Producer sends a msg to kafka and does not wait for acknowledgement from the server</li><li>fire-and-forget approach gives the best performance as we can write data to Kafka at network speed, but <strong>no guarantee can be made</strong> that the server has received the record in this case.</li></ul></li><li>Committed to Leader<ul><li>Producer waits for an acknowledgment from the leader.</li><li>This ensures that the data is committed at the leader; it will be slower than the ‘Async’ option, as the data has to be written on disk on the leader.</li><li>Under this scenario, the leader will respond without waiting for acknowledgments from the followers.</li><li>In this case, the record <strong>will be lost if the leader crashes immediately after acknowledging the producer but before the followers have replicated it</strong>.</li></ul></li><li>Committed to Leader and Quorum<ul><li>Producer waits for an acknowledgment from the leader and the quorum. This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This will be the slowest write but guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee.</li></ul></li></ul></li></ul><h2 id="9-2-Consumer-Delivery-Semantics"><a href="#9-2-Consumer-Delivery-Semantics" class="headerlink" title="9.2 Consumer Delivery Semantics"></a>9.2 Consumer Delivery Semantics</h2><ul><li>Ways to provide consistency to the consumer<ul><li>At most once<ul><li>Message may be lost but are never redelivered</li><li>Under this option, the consumer upon receiving a message, commit (or increment) the offset to the broker. Now, if the consumer crashes before fully consuming the message, that message will be lost, as when the consumer restarts, it will receive the next message from the last committed offset.</li></ul></li><li>At least once<ul><li>Messages are never lost but maybe redelivered</li><li>This scenario occurs when the consumer receives a message from Kafka, and it does not immediately commit the offset.</li><li>Instead, it waits till it completes the processing.</li><li>So, if the consumer crashes after processing the message but before committing the offset, it has to reread the message upon restart.</li><li>Since, in this case, the consumer never committed the offset to the broker, the broker will redeliver the same message. Thus, duplicate message delivery could happen in such a scenario.</li></ul></li><li>Exactly once<ul><li>It is very hard to achieve this unless the consumer is working with a transactional system.</li><li>Under this option, the consumer puts the message processing and the offset increment in one transaction.</li><li>This will ensure that the offset increment will happen only if the whole transaction is complete.</li><li>If the consumer crashes while processing, the transaction will be rolled back, and the offset will not be incremented. When the consumer restarts, it can reread the message as it failed to process it last time. This option leads to no data duplication and no data loss but can lead to decreased throughput.</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview-of-Messaging-Systems&quot;&gt;&lt;a href=&quot;#1-Overview-of-Messaging-Systems&quot; class=&quot;headerlink&quot; title=&quot;1. Overview of Messaging Syste
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>货运代理法律风险</title>
    <link href="https://www.llchen60.com/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E6%B3%95%E5%BE%8B%E9%A3%8E%E9%99%A9/"/>
    <id>https://www.llchen60.com/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E6%B3%95%E5%BE%8B%E9%A3%8E%E9%99%A9/</id>
    <published>2021-08-17T04:29:17.000Z</published>
    <updated>2021-08-19T23:01:27.883Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-货运代理法律风险基本内容"><a href="#1-货运代理法律风险基本内容" class="headerlink" title="1. 货运代理法律风险基本内容"></a>1. 货运代理法律风险基本内容</h1><h2 id="1-1-国际货运代理人定义"><a href="#1-1-国际货运代理人定义" class="headerlink" title="1.1 国际货运代理人定义"></a>1.1 国际货运代理人定义</h2><ul><li>历史视角上来看<ul><li>国际货运代理经历了由显明代理人到隐名代理人再到<strong>运输合同当事人</strong>的过程  Fright Agency —→ Freight Forwarder</li><li>研究内容从单纯的代理法律关系向运输法律关系扩展</li><li>现代货运代理的定义 1992 《货运代理》<ul><li>提供并安排货物运输以取得报酬，</li><li>或者为货物合并拼箱并承担将这些货物由收货地运至目的地的<strong>运输责任</strong></li></ul></li><li>2002 《货运代理法》  Legal Classification of Fright Forwarders<ul><li>奠定了国际货运代理制度的FIATA的立法模式下，以货物运输合同的当事人 (as principal) 和非当事人 (except as principal) 来区分国际货物代理企业作为<strong>契约承运人和纯粹代理人</strong>的情况</li></ul></li><li>FIATA (International Federation of Freight Forwarders Association)《国际货运代理业示范法》<ul><li>国际货运代理人是指与客户达成货运代理协定，为其提供各类运输相关服务及其他辅助和咨询服务，或者在前述服务之外还以使用自有运输工具或者签发自己的运输单据的方式为客户承运货物的人</li></ul></li></ul></li><li>无船承运人<ul><li>对于实际货主而言，作为公共承运人与之订立海上货物运输合同</li><li>对于实际承运人而言，又承担着托运人的义务</li></ul></li><li>国际货运代理人定义<ul><li>International Freight Forwarder</li></ul></li><li>对货代的两类界定<ul><li>代理人说<ul><li>将国际货运代理人规定为受委托人的指示为其货物在国际间的运输及其他有关事务提供合理审慎服务的自然人或经济组织，本身业务不涉及货物的承运</li><li>与货主或委托人之间是纯粹的代理关系</li></ul></li><li>双重身份说<ul><li>规定了国际货运代理人是为委托方代办国际货运事务的代理人</li><li>规定了在一定条件下可以<strong>成为运输合同的当事人并对外承担承运人的责任</strong></li></ul></li></ul></li></ul><h2 id="1-2-货运代理法律关系辨析"><a href="#1-2-货运代理法律关系辨析" class="headerlink" title="1.2 货运代理法律关系辨析"></a>1.2 货运代理法律关系辨析</h2><ul><li>法律关系<ul><li>指相关海运国际公约，各国国内法以及行业规范等在调整国际海上货运代理行为的过程中形成的各有关主体间的权利和义务关系</li><li>国际海上货运代理法律关系包括<ul><li>国际货运代理企业作为海运代理人的法律关系</li><li>作为无船承运人的法律关系</li></ul></li></ul></li><li>作为海运代理人的法律关系<ul><li>在提供海上货物运输的相关代理业务的时候，呈现出的关系</li><li>内部委托法律关系<ul><li>国际货运代理企业与托运人之间的委托代理合同法律关系</li></ul></li><li>外部代理法律关系<ul><li>国际货运代理企业为托运人的利益和承运人签订海上货物运输合同而产生的运输合同法律关系</li><li>货代企业往往会以自己的名义代替多笔散货托运人与承运人签订一个总的运输合同，自己再分别同各托运人签订货运代理协议</li></ul></li><li>我国合同法对于包括货运代理合同在内的委托合同采用过错责任原则<ul><li>货代仅在自身确实受托事项存在过错并造成托运人损失的情况下承担违约损害赔偿责任</li><li>且只要在第三方选任上能够证明已经履行了合理和谨慎的义务，对由于第三方造成的托运人损失，可以免于承担责任</li></ul></li></ul></li><li>作为无船承运人的法律关系<ul><li>契约承运人的一种，即不拥有或者不经营船舶，不进行实际的货物运输活动，以签发无船承运人提单(Non Vessel Operating Common Carrier Bill of Loading)的方式明示或者默示对运输负有责任的人，承运责任来源于和托运人签订的货物运输契约，而不是实际的运输行为</li><li>法律关系<ul><li>无船承运人和托运人之间的海上货物运输合同关系</li><li>无船承运人和船公司之间的海上货物运输合同关系<ul><li>货代以托运人的身份和船公司 — 实际承运人签订运输合同并获得海运提单的 (Master Bill of Loading MBL)</li><li><strong>根据海商法， 无船承运人所需要承担的法律责任和实际承运人的责任是一样的</strong></li><li>无船承运人不享有不完全责任制<ul><li>当免责事由发生的时候，船公司可以免于对无船承运人承担责任，而无船承运人无法以相同的事由对托运人免责</li></ul></li><li>当承运人和实际承运人需要对货损货差需要进行赔偿的时候，二者在责任范围内承担连带责任</li><li>对于除了海上运输之外实际托运人提供的其他运输服务，无船承运人均承担严格责任</li></ul></li><li>无船承运人和船公司代理人之间的海上货物运输合同关系</li></ul></li></ul></li></ul><h1 id="2-风险状况分析"><a href="#2-风险状况分析" class="headerlink" title="2. 风险状况分析"></a>2. 风险状况分析</h1><ul><li><p>风险的定义</p><ul><li>由于国际货运代理企业在开展海上业务过程中受到客观法律环境，包括自身在内的各海洋运输相关主体所实施的法律行为的影响，导致其权利义务状态发生改变，从而产生的可能由该企业承担的法律上的不利后果</li><li>法律风险由环境诱因 — 客观法律环境，行为诱因 — 实施的法律行为及二者所引发的不利后果构成<ul><li>法律的不完善和不确定性<ul><li>不完善 — 制定法因为无法避免地滞后于社会的发展而必然存在的漏洞和空白</li><li>不确定性 — 由于法律条文意义晦涩之处需要法官来阐释说明，法官对于法律的理解会有所不同，从而导致了裁判结果不总是一致的</li></ul></li><li>行为诱因<ul><li>货代的一些不规范的法律行为，</li></ul></li></ul></li></ul></li><li><p>货代纠纷案件研究 争议焦点主要在：</p><ul><li>涉案主体间货运代理合同关系的认定<ul><li>因为转委托而引发的对双方间是否存在直接法律关系的异议</li><li>因为货运代理人和无船承运人身份识别而引发的对合同关系性质是货物运输合同法律关系还是货物运输代理合同法律关系的异议</li></ul></li></ul></li><li><p>主要风险分类</p><ul><li>身份认定上的风险<ul><li>指对国际货运代理企业是涉案货物的海运代理人还是无船承运人的认定结果和企业自身对此的认识不同造成的问题</li></ul></li><li>转委托和双方代理上的风险</li><li>涉外法律适用上的风险<ul><li>譬如美国统一商法典，允许以记名提单的方式放货，如果双方在法律适用条款当中约定适用美国法，那么海外代理提单无单放货的行为就不属于过错行为</li></ul></li></ul></li></ul><h1 id="3-风险成因分析"><a href="#3-风险成因分析" class="headerlink" title="3. 风险成因分析"></a>3. 风险成因分析</h1><h2 id="3-1-身份认定上的法律风险成因"><a href="#3-1-身份认定上的法律风险成因" class="headerlink" title="3.1 身份认定上的法律风险成因"></a>3.1 身份认定上的法律风险成因</h2><ul><li>国际货运代理企业是涉案货物的海运代理人还是无船承运人的认定结果的不同，会导致企业需要承担的法律责任和风险完全不同<ul><li>立法上的小混乱<ul><li>因为很有可能货运代理企业同时担当着无船承运人还有代理人的角色</li></ul></li><li>司法裁判的不确定<ul><li>《海上货代纠纷规定》的一些判断逻辑<ul><li>双方之间是否订立了海上货运代理合同，反映出来的是代理协议还是运输协议</li><li>受托人向委托人是否签发了提单，是承运提单还是代理人的分提单</li><li>受托人收取费用的名义是佣金还是运费</li><li>双方以往的交易历史和交易习惯</li></ul></li></ul></li><li>从业者自身行为的不规范<ul><li>是否保留了重要的往来文件，发票和提单</li></ul></li></ul></li></ul><h2 id="3-2-转委托与双方代理上的法律风险成因"><a href="#3-2-转委托与双方代理上的法律风险成因" class="headerlink" title="3.2 转委托与双方代理上的法律风险成因"></a>3.2 转委托与双方代理上的法律风险成因</h2><h3 id="3-2-1-转委托行为的法律风险成因"><a href="#3-2-1-转委托行为的法律风险成因" class="headerlink" title="3.2.1 转委托行为的法律风险成因"></a>3.2.1 转委托行为的法律风险成因</h3><ul><li>什么是转委托<ul><li>受托人将委托人委托其代为处理的事务转交给第三人处理的行为</li></ul></li><li>转委托行为的法律风险成因<ul><li>合同法规定，对于委托事务，除了经过委托人同意或者出现紧急状况可以转委托之外，受托人均应当亲自处理，否则就要为第三人的行为承担责任</li><li>但是对于国际货运代理行业来说，转委托是一个常规方式<ul><li>原因在于海运货代委托人更为看重成本和效率，让货代企业去完全处理每一件委托事务是不经济也不现实的</li></ul></li><li>造成货代企业转委托风险的是是否取得了委托人的同意</li><li>当前《海上货代纠纷规定》明确排除了推定托运人默示同意货运代理人转委托的可能<ul><li>转委托具体权限约定不明的时候，委托人将负有就不明权限想委托人报告的义务</li><li>委托人在受托人指示下与第三人的通常接触行为不能认定为委托人以该行为对转委托的明确同意</li></ul></li></ul></li></ul><h3 id="3-2-2-双方代理行为上的法律风险成因"><a href="#3-2-2-双方代理行为上的法律风险成因" class="headerlink" title="3.2.2 双方代理行为上的法律风险成因"></a>3.2.2 双方代理行为上的法律风险成因</h3><ul><li>什么是双方代理？<ul><li>指在同一法律关系内，一方当事人的代理人同时又接受另一方当事人委托，并为其代理的行为</li><li>由于合同关系中双方是相对的，双方代理会使得本是冲突的合同双方意思表示被代理的个人意志予以替代，偏离了合同的本质属性</li></ul></li><li>海运货代行业需要这样做，因为效率上的提升。但会有法律上的风险</li></ul><h2 id="3-3-涉外法律适用上的风险成因"><a href="#3-3-涉外法律适用上的风险成因" class="headerlink" title="3.3 涉外法律适用上的风险成因"></a>3.3 涉外法律适用上的风险成因</h2><ul><li>作为法院裁判依据的国外法律规定可能会让国际货运代理企业在诉讼中处于不利地位</li><li>涉外商事代理法律关系，会受到途经国家的法律管辖</li><li>如果双方未约定准据法，或涉诉的国际货代企业没有准确理解和把握已约定的准据法，就会大大增加诉讼中的不稳定因素</li><li>英美法系，判例法； 成文法国家，有专门的货运代理法律</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>《国际海上货运代理法律风险研究》  邓大鸣</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-货运代理法律风险基本内容&quot;&gt;&lt;a href=&quot;#1-货运代理法律风险基本内容&quot; class=&quot;headerlink&quot; title=&quot;1. 货运代理法律风险基本内容&quot;&gt;&lt;/a&gt;1. 货运代理法律风险基本内容&lt;/h1&gt;&lt;h2 id=&quot;1-1-国际货运代理人定义&quot;&gt;
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="货运代理" scheme="https://www.llchen60.com/tags/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>国际货运代理行业初探</title>
    <link href="https://www.llchen60.com/%E5%9B%BD%E9%99%85%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E8%A1%8C%E4%B8%9A%E5%88%9D%E6%8E%A2/"/>
    <id>https://www.llchen60.com/%E5%9B%BD%E9%99%85%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E8%A1%8C%E4%B8%9A%E5%88%9D%E6%8E%A2/</id>
    <published>2021-08-15T14:52:04.000Z</published>
    <updated>2021-08-19T23:01:51.975Z</updated>
    
    <content type="html"><![CDATA[<h1 id="国际货运代理行业初探"><a href="#国际货运代理行业初探" class="headerlink" title="国际货运代理行业初探"></a>国际货运代理行业初探</h1><h1 id="1-货运公司业务与工作内容"><a href="#1-货运公司业务与工作内容" class="headerlink" title="1. 货运公司业务与工作内容"></a>1. 货运公司业务与工作内容</h1><h2 id="1-1-主要业务"><a href="#1-1-主要业务" class="headerlink" title="1.1 主要业务"></a>1.1 主要业务</h2><h3 id="1-1-1-Overview"><a href="#1-1-1-Overview" class="headerlink" title="1.1.1 Overview"></a>1.1.1 Overview</h3><blockquote><p>客户委托货运代理运输货物，客户本人并不承担承运责任；货代接受发货人委托后，为发货人提供相关服务，满足发货人的具体要求，同时要求发货人根据服务来支付一定报酬的行业</p></blockquote><h3 id="1-1-2-主营业务"><a href="#1-1-2-主营业务" class="headerlink" title="1.1.2 主营业务"></a>1.1.2 主营业务</h3><ul><li>纯粹代理人的业务<ul><li>取得委托人授权</li><li>负责货物离开港口以及到达港口的公务处理工作<ul><li>报关</li><li>报检</li><li>保险</li></ul></li><li>这个清关环节主要是配合当地海关进行文件准备工作</li></ul></li><li>国际多式联运业务<ul><li>多式联运，发展自集装箱运输</li></ul></li><li>无船承运业务<ul><li>具有无船公共承运人NVOCC (Non Vessel Operating Common Carrier)资质的货代企业可以以承运人的身份接受货载委托</li><li>货物清关的时候用货代企业的提单，承担承运人的责任，完成货物离港到岸的国际海运经营业务</li><li>无船承运人只是契约承运人，而实际完成运输的承运人是货代企业所委托的其他国际船舶运输经营者</li></ul></li><li>物流业务<ul><li>可以针对不同客户提供定制化的供应链解决方案，囊括了货物从生产，出厂，转运，清仓报关，到达目标市场等各个环节</li></ul></li><li>国际快递业务</li></ul><h2 id="1-2-基本工作流程"><a href="#1-2-基本工作流程" class="headerlink" title="1.2 基本工作流程"></a>1.2 基本工作流程</h2><ul><li>海运流程<ul><li>他国买主和中国企业就贸易行为签订贸易合同</li><li>中国工厂确定货运代理企业，并与之商讨出货</li><li>中国货运代理企业和船运公司商讨订舱事宜</li><li>拿到承载货物的船的名字和达到目的地的准确信息</li><li>国内外货运代理企业共同确认货物运输信息，以及到达目的地的时间信息，并将具体信息传递给国外买家</li></ul></li><li>location / key points<ul><li>卖方，出口商地点</li><li>出口单证手续</li><li>边境/ 机场/ 码头交货</li><li>装运港</li><li>船舷</li><li>船上</li><li>船上</li><li>船舷</li><li>到达卸货港</li><li>指定目的地交货; 边境/ 机场/ 码头</li><li>进口单证手续</li><li>卖方/ 进口商地点</li></ul></li></ul><h2 id="1-3-货代企业等级"><a href="#1-3-货代企业等级" class="headerlink" title="1.3 货代企业等级"></a>1.3 货代企业等级</h2><ul><li>一级货代企业<ul><li>需要获得国际货代资格证书</li><li>商务部颁发的，一级货代企业有权从中国不同港口订舱</li></ul></li><li>小型货代公司<ul><li>依托于一级货代企业，来进行订舱的公司</li></ul></li></ul><h1 id="2-国际货运行业发展的外部环境"><a href="#2-国际货运行业发展的外部环境" class="headerlink" title="2. 国际货运行业发展的外部环境"></a>2. 国际货运行业发展的外部环境</h1><ul><li><p>PEST分析方式</p><ul><li>维度<ul><li>政治<ul><li>2010年货代标准化委员会</li><li>2015年《推动共建丝绸之路经济带和21世纪海上丝绸之路的愿景与行动》</li><li>2016年《关于加快国际货运代理物流业健康发展的指导意见》</li><li>《中华人民共和国海商法》</li></ul></li><li>经济</li><li>技术</li><li>社会文化</li></ul></li><li>外部的变化是组织无法控制的，脱离于组织内部，却环绕在组织四周，从不同方面作用在组织内部</li></ul></li><li><p>波特五力竞争理论</p><ul><li>维度<ul><li>替代者<ul><li>货代的供应商</li></ul></li><li>客户</li><li>供应商<ul><li>供应商 尤其是船舶公司航空公司有着比较强的议价能力，尤其对于中小型货代而言</li></ul></li><li>潜在竞争者<ul><li>政策壁垒整体在降低</li><li>传统业务 — 报关报检，码头物流，短途专线运输</li><li>新兴业务 — 供应链管理，非常规货物托运，多式联运</li></ul></li><li>同行业竞争者<ul><li>船舶公司旗下自己设立的货代物流公司<ul><li>有直接客户源</li><li>底价海运费和订舱费减免的优势</li><li>可以通过船公司完善的海外网络指定国内的出口供应商，可以提供更多种的贸易方式<ul><li>贸易方式<ul><li>Exxxx<ul><li>EXW<ul><li>卖方仅在自己的地点为买房备妥货物交付，出工厂以后就没有费用和安全责任了</li></ul></li></ul></li><li>Fxxx  — 卖方需要将货物交到制定的承运人处<ul><li>FCA  货交承运人</li><li>FAS  装运港 船边  交货 — 到码头</li><li>FOB 装运港 船上 交货  — 到船舷<ul><li>价格计算  = (产品含税成本 + 利润 + 国内运输费用 - 出口退税)/ 汇率</li><li>卖方的义务<ul><li>将合同规定的货物交到买房所指派的船上并及时通知买方</li><li>承担货物越过装运港船舷之前的一切风险</li><li>办理货物的出口手续</li><li>提交商业发票等所需的凭证</li></ul></li><li>买方的义务<ul><li>租船订舱，支付运费</li><li>将船名，装货地点和要求交货的时间及时通知卖方</li><li>受领货物，支付货款</li><li>承担货物越过装运港船舷之后的一切风险</li><li>办理货物的进口手续</li></ul></li></ul></li></ul></li><li>Cxxx — 卖方需要订立运输合同，但是对于货物损失的风险以及装船和启运之后发生的意外所产生的额外费用，卖方不承担责任<ul><li>CFR  成本+运费<ul><li>CFR = FOB + 海运费</li></ul></li><li>CIF  成本运费保险<ul><li>CIF = FOB + 海运费 + 海运保险费</li></ul></li><li>CIP  运费/ 保险费付到目的地</li></ul></li><li>Dxxx — 卖方承担将货物交到出口国边境或者目的国所需的全部费用和风险<ul><li>DAF  出口国边境交货</li><li>DES  目的港船上交货</li><li>DEQ  目的港码头交货</li><li>DDU  未完税交货</li><li>DDP  完税后交货</li></ul></li><li>FOB</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>SWOT 战略管理分析</p><ul><li>S - Strength<ul><li>企业内部优势</li></ul></li><li>W - Weakness<ul><li>企业面临的竞争弱势</li></ul></li><li>O - Opportunity<ul><li>发展过程当中的有利外部环境</li></ul></li><li>T  - Threat<ul><li>企业自身业务的外部威胁</li><li>当外部环境不利于企业的发展，企业为了避免外部环境带来的威胁，应当及时调整经营战略</li></ul></li></ul></li></ul><h1 id="3-内部环境分析"><a href="#3-内部环境分析" class="headerlink" title="3. 内部环境分析"></a>3. 内部环境分析</h1><ul><li>价值链分析法<ul><li>将活动分为基础活动以及辅助活动两大类<ul><li>基础活动<ul><li>内部物流活动<ul><li>各个部门和环节之间的合作，如何能直观了解到项目的进展此类问题</li></ul></li><li>生产经营活动<ul><li>生产 销售  供应 财务</li></ul></li><li>外部物流活动<ul><li>货物的周转</li><li>报关 报检 查验 清关 缴税等</li></ul></li><li>服务性活动</li></ul></li><li>辅助活动<ul><li>人力资源管理活动</li><li>改善基础设施条件的活动</li><li>原材料采购活动</li><li>新产品研发活动等</li></ul></li></ul></li><li>只有在一些特定环节，才会真的去创造价值，这些是战略环节，需要在此构建战略优势</li></ul></li></ul><h1 id="4-Other-Notes"><a href="#4-Other-Notes" class="headerlink" title="4. Other Notes"></a>4. Other Notes</h1><ul><li><p>企业战略</p><ul><li>用于整合与重新优化配置的措施及可行性方案</li><li>企业组织结构要跟随企业战略进行调整</li></ul></li><li><p>如何选择合适的竞争战略</p><ul><li>对有吸引力，高潜力的产业的正确选择</li><li>在选择的行业当中确立自己的竞争优势地位</li><li>除此以外还需要从企业内部环境，研究价值链</li></ul></li><li><p>企业内部性的深入研究 — 企业核心能力</p><ul><li>让企业构成其他企业并不具备的能力，同时资源无法在不同的企业之间进行流通</li><li>企业内部的资源的独特性，直接关系到企业利润的获得以及竞争优势的保持</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>1.<a href="https://www.52by.com/article/2584">https://www.52by.com/article/2584</a><br>2. 《小型国际货运代理X公司的发展战略研究》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;国际货运代理行业初探&quot;&gt;&lt;a href=&quot;#国际货运代理行业初探&quot; class=&quot;headerlink&quot; title=&quot;国际货运代理行业初探&quot;&gt;&lt;/a&gt;国际货运代理行业初探&lt;/h1&gt;&lt;h1 id=&quot;1-货运公司业务与工作内容&quot;&gt;&lt;a href=&quot;#1-货运公司业务
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="货运代理" scheme="https://www.llchen60.com/tags/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>System Design Patterns - Quorum</title>
    <link href="https://www.llchen60.com/System-Design-Patterns-Quorum/"/>
    <id>https://www.llchen60.com/System-Design-Patterns-Quorum/</id>
    <published>2021-08-11T04:26:01.000Z</published>
    <updated>2021-08-11T04:26:42.996Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h1><p>In distributed system, data is replicated across multiple servers for fault tolerance and high availability.</p><p>Once system decides to maintain multiple copies of data, another problem arises: how to make sure that all replicas are consistent?? </p><h1 id="2-Dive-Deep"><a href="#2-Dive-Deep" class="headerlink" title="2. Dive Deep"></a>2. Dive Deep</h1><h2 id="2-1-Definition"><a href="#2-1-Definition" class="headerlink" title="2.1 Definition"></a>2.1 Definition</h2><ul><li>Quorum<ul><li>Minimum number of servers on which a distributed operation needs to be performed successfully before declaring the operation’s overall success</li></ul></li></ul><h2 id="2-2-How-it-works"><a href="#2-2-How-it-works" class="headerlink" title="2.2 How it works?"></a>2.2 How it works?</h2><ul><li>Suppose a database is replicated on 5 machines, then quorum refers to the minimum number of machines that perform the same action for a given transaction in order to decide the final operation for that transaction</li><li>So in a set of 5, three machines form the majority quorum, quorum <strong>enforces the consistency requirement</strong> needed for distributed operations</li><li>Quorum Number<ul><li>N / 2 + 1</li></ul></li><li>Quorum is achieved when nodes follow the below protocol R + W &gt; N<ul><li>R  minimum read nodes</li><li>W minimum write nodes</li><li>N  nodes in the quorum group</li></ul></li></ul><h2 id="2-3-Where-is-it-used"><a href="#2-3-Where-is-it-used" class="headerlink" title="2.3 Where is it used?"></a>2.3 Where is it used?</h2><ul><li>Chubby<ul><li>Use paxos for leader election, which use quorum to ensure strong consistency</li></ul></li><li>Cassandra<ul><li>Ensure data consistency, each write request can be configured to be successful only if the data has been written to at least a quorum of replica nodes</li></ul></li><li>Dynamodb<ul><li>Writes to a sloppy quorum of other nodes in the system</li><li>All read/ write operations are performed on the first N healthy nodes from the preference list, which may not always be the first N nodes encountered walking the consistent hashing ring</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Background&quot;&gt;&lt;a href=&quot;#1-Background&quot; class=&quot;headerlink&quot; title=&quot;1. Background&quot;&gt;&lt;/a&gt;1. Background&lt;/h1&gt;&lt;p&gt;In distributed system, data 
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>System Design Patterns - Bloom Filters</title>
    <link href="https://www.llchen60.com/System-Design-Patterns-Bloom-Filters/"/>
    <id>https://www.llchen60.com/System-Design-Patterns-Bloom-Filters/</id>
    <published>2021-08-10T02:33:45.000Z</published>
    <updated>2021-08-10T02:34:11.571Z</updated>
    
    <content type="html"><![CDATA[<h1 id="System-Design-Patterns-Bloom-Filters"><a href="#System-Design-Patterns-Bloom-Filters" class="headerlink" title="System Design Patterns - Bloom Filters"></a>System Design Patterns - Bloom Filters</h1><p>Created: August 8, 2021 10:06 PM<br>Status: Finished<br>Tags: System Design<br>Type: Tech Resource</p><h1 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h1><ul><li>Suppose we have a large set of structured data(identified by record IDs) stored in a set of data files, and we want to know which file might contain our required data<ul><li>we don’t want to read each file, as it’s slow and we have to read a lot of data from the disk</li></ul></li><li>Solution 1: Build an index on each data file and store it in a separate index file, to map each record ID to its offset in the data file; each index file will be sorted on the record ID. Then we could do a binary search in index file</li><li>Solution 2: We could use Bloom Filters</li></ul><h1 id="2-How-does-Bloom-Filter-work"><a href="#2-How-does-Bloom-Filter-work" class="headerlink" title="2. How does Bloom Filter work?"></a>2. How does Bloom Filter work?</h1><ul><li>The Bloom filter data structure tells whether an element <strong>may be in a set, or definitely is not</strong><ul><li>which means the only possible errors are false positives</li></ul></li><li>How it looks<ul><li>An empty bloom filter is a bit array of m bits, all set to 0</li><li>There are also k different hash functions, each of which maps a set element to one of the m bit positions</li></ul></li><li>Workflow<ul><li>To add an element, feed it to the hash functions to get k bit positions, and set the bits at these positions to 1</li><li>To test if an element is in the set, feed it to the hash functions to get k bit positions<ul><li>if any of the bits at these positions is 0, the element is definitely not in the set</li><li>if all are 1, then the element may be in the set</li></ul></li></ul></li></ul><h1 id="3-When-will-we-use-Bloom-Filter"><a href="#3-When-will-we-use-Bloom-Filter" class="headerlink" title="3. When will we use Bloom Filter?"></a>3. When will we use Bloom Filter?</h1><ul><li>In BigTable, any read operation has to read from all SSTables that make up a tablet<ul><li>if these SSTables are not in memory, the read operation may end up doing many disk accesses</li><li>BigTable uses bloom filters to reduce the number of disk accesses</li><li>Store of BloomFilter could drastically reduces the number of disk seeks, thereby improving read performance</li></ul></li></ul><h1 id="4-How-to-use-Bloom-Filter-in-Java"><a href="#4-How-to-use-Bloom-Filter-in-Java" class="headerlink" title="4. How to use Bloom Filter in Java?"></a>4. How to use Bloom Filter in Java?</h1><ul><li>We could use BloomFilter class from the Guava library to achieve this</li></ul><pre><code class="jsx">BloomFilter&lt;Integer&gt; filter = BloomFilter.create(    Funnels.integerFunnel(),     500,     0.01);</code></pre><ul><li>How to implement from scratch   <a href="https://www.inlighting.org/archives/java-implement-bloom-filter/">https://www.inlighting.org/archives/java-implement-bloom-filter/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;System-Design-Patterns-Bloom-Filters&quot;&gt;&lt;a href=&quot;#System-Design-Patterns-Bloom-Filters&quot; class=&quot;headerlink&quot; title=&quot;System Design Patter
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis集合统计模式</title>
    <link href="https://www.llchen60.com/Redis%E9%9B%86%E5%90%88%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    <id>https://www.llchen60.com/Redis%E9%9B%86%E5%90%88%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F/</id>
    <published>2021-08-08T23:32:13.000Z</published>
    <updated>2021-08-08T23:32:56.696Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis集合统计模式"><a href="#Redis集合统计模式" class="headerlink" title="Redis集合统计模式"></a>Redis集合统计模式</h1><h1 id="1-聚合统计"><a href="#1-聚合统计" class="headerlink" title="1. 聚合统计"></a>1. 聚合统计</h1><h2 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h2><ul><li>统计多个集合元素的聚合结果<ul><li>交集统计 — 统计多个集合的共有元素</li><li>差集统计 — 统计其中一个集合独有的元素</li><li>并集统计 — 统计多个集合的所有元素</li></ul></li><li>聚合统计可以使用Set类型来做</li><li>但是Set的差集，并集，交集的计算复杂度比较高，在数据量比较大的情况下，直接执行可能会导致Redis实例阻塞。</li><li>可以从主从集群当中选择一个从库，使其专门负责聚合计算，或者将数据读取到客户端，在客户端完成聚合统计</li></ul><h2 id="1-2-案例分析"><a href="#1-2-案例分析" class="headerlink" title="1.2 案例分析"></a>1.2 案例分析</h2><ul><li>统计一个手机App的每天的新增用户数和第二天的留存用户数</li><li>用一个集合记录所有登陆过App的用户Id<ul><li>key — user:id</li><li>value — set类型 记录用户id</li></ul></li><li>另外一个集合记录每天用户set<ul><li>key — user:id:20210808</li><li>value — set类型  记录用户id</li></ul></li></ul><pre><code class="jsx">// 差值统计SUNIONSTORE user:id user:id user:id:20210808// 计算新用户SDIFFSTORE user:new user:id:20210808 user:id // 计算留存用户SINTERSTOPRE user:id:rem user:id:20210808 user:id:20210809</code></pre><h1 id="2-排序统计"><a href="#2-排序统计" class="headerlink" title="2. 排序统计"></a>2. 排序统计</h1><h2 id="2-1-概念"><a href="#2-1-概念" class="headerlink" title="2.1 概念"></a>2.1 概念</h2><ul><li>需要能对输出进行排序，Redis常用的4个集合类型当中 (List, Hash, Set, Sorted Set)， List和Sorted Set属于有序集合</li><li>List按照元素进入List的顺序排序，而Sorted Set可以根据元素的权重来排序</li></ul><h2 id="2-2-案例"><a href="#2-2-案例" class="headerlink" title="2.2 案例"></a>2.2 案例</h2><ul><li>电商网站上提供最新评论列表的场景</li><li>List在这个场景里面的问题<ul><li>因为根据位置排序，当有新的评价加进来，那么可能会有一些评价会在不同页面重复出现</li></ul></li><li>Sorted Set不存在这个问题，因为它是根据元素的实际权重来排序和获取数据的<ul><li>我们可以按照评论时间的先后给每条评论设置一个权重值，然后将评论保存到Sorted Set当中</li><li>ZRANGEBYSCORE命令就可以按照权重排序以后返回元素</li></ul></li></ul><h1 id="3-二值状态统计"><a href="#3-二值状态统计" class="headerlink" title="3. 二值状态统计"></a>3. 二值状态统计</h1><h2 id="3-1-概念"><a href="#3-1-概念" class="headerlink" title="3.1 概念"></a>3.1 概念</h2><ul><li>指集合元素的取值只有0和1两种</li><li>计算海量二值状态数据的时候，bitmap可以有效减少所需的内存空间</li></ul><h2 id="3-2-案例"><a href="#3-2-案例" class="headerlink" title="3.2 案例"></a>3.2 案例</h2><ul><li>签到统计<ul><li>每个用户一天的签到用一个bit位就能表示</li><li>因此并不需要非常复杂的数据类型，使用bitmap就可以了</li></ul></li><li>Redis提供了Bitmap类型<ul><li>GETBIT</li><li>SETBIT<ul><li>将某一位设置为1</li></ul></li><li>BITCOUNT<ul><li>用来统计所有1的个数</li></ul></li></ul></li></ul><pre><code class="jsx">// 记录用户8 3 签到了SETBIT uid:sign:3000:202008 2 1 // 检查是否8 3 签到了GETBIT uid:sign:3000:202008 2 // 统计该用户8月份的签到次数BITCOUNT uid:sign:3000:202008 </code></pre><ul><li>如何统计一亿个用户连续10天的签到情况<ul><li>将每天日期作为key，每个key对应一个1亿位的bitmap  每一个bit对应一个用户当天的签到情况</li><li>对10个bitmap做与操作</li><li>然后用BITCOUNT统计下最终生成的Bitmap当中1的个数</li></ul></li></ul><h1 id="4-基数统计"><a href="#4-基数统计" class="headerlink" title="4. 基数统计"></a>4. 基数统计</h1><h2 id="4-1-概念"><a href="#4-1-概念" class="headerlink" title="4.1 概念"></a>4.1 概念</h2><ul><li>基数统计指统计一个集合中不重复的元素的个数</li></ul><h2 id="4-2-案例"><a href="#4-2-案例" class="headerlink" title="4.2 案例"></a>4.2 案例</h2><ul><li>统计一个网页的UV<ul><li>需要去重，一个用户一天内的多次访问只能算一次</li></ul></li><li>可以使用SET或者HASH类型来进行记录，但是会消耗很大的内存空间</li><li>可以使用HyperLogLog<ul><li>用于统计基数的数据集合类型</li><li>优势在于当集合元素数量非常多的时候，计算基数所需的空间总是固定的，而且还很小</li><li>Redis中每个HyperLogLog只需要使用12KB内存，就可以计算接近2^64个元素的基数</li></ul></li><li>HyperLogLog的统计规则是基于概率完成的，因此其给出的统计结果是有一定误差的，标准误算率为0.81% ; 如果应用场景是必须非常精确，那就还需要使用Set或者Hash类型</li></ul><pre><code class="jsx">PFADD page1:uv  user1 user2 user3 user4 PFCOUNT page1:uv </code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis集合统计模式&quot;&gt;&lt;a href=&quot;#Redis集合统计模式&quot; class=&quot;headerlink&quot; title=&quot;Redis集合统计模式&quot;&gt;&lt;/a&gt;Redis集合统计模式&lt;/h1&gt;&lt;h1 id=&quot;1-聚合统计&quot;&gt;&lt;a href=&quot;#1-聚合统计&quot; cla
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>DynamoDB Architecture (Paper Reading)</title>
    <link href="https://www.llchen60.com/DynamoDB-Architecture-Paper-Reading/"/>
    <id>https://www.llchen60.com/DynamoDB-Architecture-Paper-Reading/</id>
    <published>2021-08-07T20:14:38.000Z</published>
    <updated>2021-08-08T18:51:22.531Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>Designed to be always on</li><li>Dynamo falls within the category of AP systems (available and partition tolerant) and is designed for high availability and partition tolerance at the expense of strong consistency</li></ul><h2 id="1-1-Design-Goals"><a href="#1-1-Design-Goals" class="headerlink" title="1.1 Design Goals"></a>1.1 Design Goals</h2><ul><li>Scalable<ul><li>System need to be highly scalable. We should be able to throw a machine into the system to see proportional improvement</li></ul></li><li>Decentralized<ul><li>To avoid single points of failures and performance bottlenecks, there should not be any central/ leader process</li></ul></li><li>Eventually Consistent<ul><li>Data can be optimistically replicated to become eventually consistent</li></ul></li></ul><h2 id="1-2-Use-cases"><a href="#1-2-Use-cases" class="headerlink" title="1.2 Use cases"></a>1.2 Use cases</h2><ul><li>eventually consistent database</li></ul><h2 id="1-3-System-APIs"><a href="#1-3-System-APIs" class="headerlink" title="1.3 System APIs"></a>1.3 System APIs</h2><ul><li>put(key, context, object)<ul><li>find the nodes where the object associated with the given key should locate</li><li>context is a value that is returned with a get operation and then sent back with the put operation</li><li>context is always stored along with the object</li><li>used like a cookie to verify the validity of the object supplied in the put request</li></ul></li><li>get(key)<ul><li>find the nodes where the object associated with the given key is located</li><li>return a single object or a list of objects with conflicting versions along with a context</li><li>context contains encoded metadata about the object, and <strong>version of the object</strong></li></ul></li></ul><h1 id="2-High-Level-Design"><a href="#2-High-Level-Design" class="headerlink" title="2. High Level Design"></a>2. High Level Design</h1><h2 id="2-1-Data-Distribution"><a href="#2-1-Data-Distribution" class="headerlink" title="2.1 Data Distribution"></a>2.1 Data Distribution</h2><h3 id="2-1-1-What-is-it"><a href="#2-1-1-What-is-it" class="headerlink" title="2.1.1 What is it"></a>2.1.1 What is it</h3><ul><li>Consistent hashing to distribute its data among nodes</li><li>Also make it easy to add/ remove nodes from a dynamo cluster</li></ul><h3 id="2-1-2-Challenge"><a href="#2-1-2-Challenge" class="headerlink" title="2.1.2 Challenge"></a>2.1.2 Challenge</h3><ul><li>how do we know on which node a particular piece of data will be stored?</li><li>when we add/ remove nodes, how do we know what data will be moved from existing nodes to the new nodes?</li><li>how can we minimize data movement when nodes join or leave?</li></ul><h3 id="2-1-3-Consistent-Hashing"><a href="#2-1-3-Consistent-Hashing" class="headerlink" title="2.1.3 Consistent Hashing"></a>2.1.3 Consistent Hashing</h3><ul><li><p>Represents the data managed by a cluster as a ring</p></li><li><p>Each node in the ring is assigned a range of data</p></li><li><p>Token</p><ul><li><p>The start of the range is called a token</p></li><li><p>each node will be assigned with one token</p><p>  <img src="https://i.loli.net/2021/08/08/8jFPMEDNmn4cXaf.png" alt=""></p></li></ul></li><li><p>Process for a put or get request</p><ul><li>DDB performs a MD5 hashing algorithm to the key</li><li>Output determines within which range the data lies —→ which node the data will be stored</li></ul></li><li><p>Problems for only use physical nodes</p><ul><li>for adding or removing nodes, it only influence the next node, but it would cause uneven distribution of traffic</li><li>recomputing the tokens causing a significant administrative overhead for a large cluster</li><li>Since each node is assigned one large range, if the data is not evenly distributed, some nodes can become hotspots</li><li>Since each node’s data is replicated on a fixed number of nodes (discussed later), when we need to rebuild a node, only its replica nodes can provide the data. This puts a lot of pressure on the replica nodes and can lead to service degradation</li></ul></li></ul><h3 id="2-1-4-Virtual-Nodes"><a href="#2-1-4-Virtual-Nodes" class="headerlink" title="2.1.4 Virtual Nodes"></a>2.1.4 Virtual Nodes</h3><p><img src="https://i.loli.net/2021/08/08/YWwxt9eMmToQ3qn.png" alt=""></p><ul><li><p>Hash range is divided into multiple smaller ranges, and each physical node is assigned multiple of these smaller ranges</p></li><li><p>Each of these subranges is called a Vnode</p></li><li><p>Vnodes are randomly distributed across the cluster and are generally non contiguous (不连续的)</p><p>  <img src="https://i.loli.net/2021/08/08/RVgrPm8T19nbKZB.png" alt=""></p></li><li><p>Benefits of Vnodes</p><ul><li>Help spread the load more evenly across the physical nodes on the cluster by dividing the hash range into smaller subranges<ul><li>speeds up the rebalancing process after adding or removing nodes</li><li>When a new node is added, it receives many Vnodes from the existing nodes to maintain a balanced cluster. Similarly, when a node needs to be rebuilt, instead of getting data from a fixed number of replicas, many nodes participate in the rebuild process.</li></ul></li><li>Vnodes make it easier to maintain a cluster containing heterogeneous machines. This means, with Vnodes, we can assign a high number of ranges to a powerful server and a lower number of ranges to a less powerful server</li><li>Since Vnodes help assign smaller ranges to each physical node, the probability of hotspots is much less than the basic Consistent Hashing scheme which uses one big range per node</li></ul></li></ul><h2 id="2-2-Data-Replication-and-Consistency"><a href="#2-2-Data-Replication-and-Consistency" class="headerlink" title="2.2 Data Replication and Consistency"></a>2.2 Data Replication and Consistency</h2><ul><li>Data is replicated optimistically</li><li>Dynamo provides eventual consistency</li></ul><h3 id="2-2-1-Optimistic-Replication"><a href="#2-2-1-Optimistic-Replication" class="headerlink" title="2.2.1 Optimistic Replication"></a>2.2.1 Optimistic Replication</h3><ul><li><p>To ensure high availability and durability, Dynamo replicates each data item on multiple N nodes in the system where the value N is equivalent to the <strong>replication factor</strong>, also is configured per instance of Dynamo</p></li><li><p>Each key is assigned to a coordinator node, which first stores the data locally and then replicates it to N-1 clockwise successor nodes on the ring</p><ul><li>Thus each node owns the region on the ring between it and its Nth predecessor</li></ul></li><li><p>Replication is done asynchronously and Dynamo provides an eventually consistent model</p></li><li><p>It’s called optimistic replication, as the replicas are not guaranteed to be identical at all times</p><p>  <img src="https://i.loli.net/2021/08/08/QaHhmPTI6XcYfwu.png" alt=""></p></li><li><p>Preference List</p><ul><li>List of nodes responsible for storing a particular key</li><li>Dynamo is designed so that every node in the system can determine which nodes should be in the list for any specific key</li><li>The list contains more than N nodes to account for failures and skip virtual nodes on the ring so that the list only contains distinct physical nodes</li></ul></li></ul><h2 id="2-3-Handling-Temporary-Failures"><a href="#2-3-Handling-Temporary-Failures" class="headerlink" title="2.3 Handling Temporary Failures"></a>2.3 Handling Temporary Failures</h2><ul><li>To handle temporary failures, dynamo replicates data to a <strong>sloppy quorum</strong> of other nodes in the system instead of a strict majority quorum</li></ul><h3 id="2-3-1-Quorum-Approach"><a href="#2-3-1-Quorum-Approach" class="headerlink" title="2.3.1 Quorum Approach"></a>2.3.1 Quorum Approach</h3><ul><li>Traditional quorum approach<ul><li>any distributed system becomes unavailable during server failures or network partitions and would have reduced availability even under simple failure conditions</li></ul></li><li>Sloppy quorum<ul><li>all read/ write operations are performed on the first N healthy nodes from the preference list. may not always be the first N nodes encountered while moving clockwise on the consistent hashing ring</li></ul></li></ul><h3 id="2-3-2-Hinted-Handoff"><a href="#2-3-2-Hinted-Handoff" class="headerlink" title="2.3.2 Hinted Handoff"></a>2.3.2 Hinted Handoff</h3><ul><li>When a node is unreachable, another node can accept writes on its behalf</li><li>Write is then kept in a local buffer and sent out once the destination node is reachable again</li><li>Problem<ul><li>Sloppy quorum is not a strict majority, the data can and will diverge</li><li>It is possible for two concurrent writes to the same key to be accepted by non-overlapping sets of nodes. This means that multiple conflicting values against the same key can exist in the system, and we can get stale or conflicting data while reading. Dynamo allows this and resolves these conflicts using Vector Clocks.</li></ul></li></ul><h2 id="2-4-Inter-node-communication-and-failure-detection"><a href="#2-4-Inter-node-communication-and-failure-detection" class="headerlink" title="2.4 Inter node communication and failure detection"></a>2.4 Inter node communication and failure detection</h2><ul><li>Use gossip protocol to keep track of the cluster state</li></ul><h3 id="2-4-1-Gossip-Protocol"><a href="#2-4-1-Gossip-Protocol" class="headerlink" title="2.4.1 Gossip Protocol"></a>2.4.1 Gossip Protocol</h3><ul><li><p>Enable each node to keep track of state information about the other nodes in the cluster</p><ul><li>which nodes are reachable</li><li>what key ranges they are responsible for</li></ul></li><li><p>Gossip Protocol</p><ul><li><p>Peer to peer communication mechanism</p></li><li><p>nodes periodically exchange state information about themselves and other nodes they know about</p></li><li><p>each node initiate a gossip round every second with a random node</p><p>  <img src="https://i.loli.net/2021/08/08/hgBPER5e2kuxvMw.png" alt=""></p></li></ul></li><li><p>External discovery through seed nodes</p><ul><li>An administrator joins node A to the ring and then joins node B to the ring. Nodes A and B consider themselves part of the ring, yet neither would be immediately aware of each other. To prevent these logical partitions, Dynamo introduced the concept of seed nodes. Seed nodes are fully functional nodes and can be obtained either from a static configuration or a configuration service. This way, all nodes are aware of seed nodes. Each node communicates with seed nodes through gossip protocol to reconcile membership changes; therefore, logical partitions are highly unlikely.</li></ul></li></ul><h2 id="2-5-Conflict-Resolution-and-Handling-permanent-failures"><a href="#2-5-Conflict-Resolution-and-Handling-permanent-failures" class="headerlink" title="2.5 Conflict Resolution and Handling permanent failures"></a>2.5 Conflict Resolution and Handling permanent failures</h2><h3 id="2-5-1-Clock-Skew"><a href="#2-5-1-Clock-Skew" class="headerlink" title="2.5.1 Clock Skew"></a>2.5.1 Clock Skew</h3><ul><li>Dynamo resolves potential conflicts using below mechanisms<ul><li>use vector clocks to keep track of value history and reconcile divergent histories at read time</li><li>in the background, dynamo use an <strong>anti entropy mechanism</strong> like <strong>Merkle trees</strong> to handle permanent failures</li></ul></li></ul><h3 id="2-5-2-Vector-Clock"><a href="#2-5-2-Vector-Clock" class="headerlink" title="2.5.2 Vector Clock"></a>2.5.2 Vector Clock</h3><ul><li><p>Used to capture causality between different versions of the same object</p></li><li><p>A vector clock is a <code>node, counter</code> pair</p></li><li><p>Each version of every object associate with a vector clock</p><ul><li>one can determine whether two versions of an object are on parallel branches or have a causal ordering by examining vector clocks</li><li>If the counters on the first object’s clock are less-than-or-equal to all of the nodes in the second clock, then the first is an ancestor of the second and can be forgotten. Otherwise, the two changes are considered to be in conflict and require reconciliation.</li></ul></li><li><p>  <img src="https://i.loli.net/2021/08/08/9fKB418IrMezTun.png" alt=""></p></li><li><p>Issue occur when there are network partition, that same data cannot be shared / communicated via different servers</p></li><li><p>In this case, DynamoDB will return it back and let client reads and reconciles</p></li></ul><h3 id="2-5-3-Conflict-free-replicated-data-types-CRDTs"><a href="#2-5-3-Conflict-free-replicated-data-types-CRDTs" class="headerlink" title="2.5.3 Conflict free replicated data types (CRDTs)"></a>2.5.3 Conflict free replicated data types (CRDTs)</h3><ul><li>we need to model our data in such a way that concurrent changes can be applied to the data in any order and will produce the same end result</li></ul><h2 id="2-6-put-and-get-Operations"><a href="#2-6-put-and-get-Operations" class="headerlink" title="2.6 put() and get() Operations"></a>2.6 put() and get() Operations</h2><h3 id="2-6-1-Strategies-for-choosing-the-coordinator-node"><a href="#2-6-1-Strategies-for-choosing-the-coordinator-node" class="headerlink" title="2.6.1 Strategies for choosing the coordinator node"></a>2.6.1 Strategies for choosing the coordinator node</h3><ul><li><p>Strategies</p><p>  <img src="https://i.loli.net/2021/08/08/MB8qFbQGdwhoZAJ.png" alt=""></p><ul><li>Clients can route their requests through a generic load balancer<ul><li>client is unaware of the dynamo ring<ul><li>helps scalability</li><li>make ddb architecture loosely coupled</li></ul></li><li>it’s possible node it select is not part of the perference list, this will result in an extra hop</li></ul></li><li>Clients can use a partition aware client library that routes the request to the appropriate coordinator nodes with lower latency<ul><li>helps to achieve lower latency  — achieve zero hop</li><li>DDB doesn’t have much control over the load distribution and request handling</li></ul></li></ul></li></ul><h3 id="2-6-2-Consistency-Protocol"><a href="#2-6-2-Consistency-Protocol" class="headerlink" title="2.6.2 Consistency Protocol"></a>2.6.2 Consistency Protocol</h3><ul><li>R W is the min number of nodes that must participate in a successful read/ write operation</li><li>R + W &gt; N yields a quorun like system</li><li>A Common (N,R,WN, R, WN,R,W) configuration used by Dynamo is (3, 2, 2)</li><li>In general, low values of WWW and RRR increase the risk of inconsistency, as write requests are deemed successful and returned to the clients even if a majority of replicas have not processed them. This also introduces a vulnerability window for durability when a write request is successfully returned to the client even though it has been persisted at only a small number of nodes</li></ul><h3 id="2-6-3-put-process"><a href="#2-6-3-put-process" class="headerlink" title="2.6.3  put() process"></a>2.6.3  <code>put()</code> process</h3><ul><li>the coordinator generates a new data version and vector clock component</li><li>saves new data locally</li><li>sends the write request to N-1 highest ranked healthy nodes from the preference list</li><li>the put() operation is considered successful after receiving W - 1 confirmation</li></ul><h3 id="2-6-4-get-process"><a href="#2-6-4-get-process" class="headerlink" title="2.6.4 get() process"></a>2.6.4 <code>get()</code> process</h3><ul><li>coordinator requests the data version from N - 1 highest ranked healthy nodes from the preference list</li><li>waits until R - 1 replies</li><li>coordinator handles causal data versions through a vector clock</li><li>returns all relevant data versions to the caller</li></ul><h3 id="2-6-5-Request-handling-through-state-machine"><a href="#2-6-5-Request-handling-through-state-machine" class="headerlink" title="2.6.5 Request handling through state machine"></a>2.6.5 Request handling through state machine</h3><ul><li>Each client request results in creating a state machine on the node that received the client request</li><li>A read operation workflow would be:<ul><li>send read requests to the nodes</li><li>wait for the minimum number of required responses</li><li>if too few replies were received within a given time limit, fail the request</li><li>otherwise, gather all the data versions and determine the ones to be returned</li><li>if versioning is enabled, perform syntactic reconciliation and generate an opaque write context that contains the vector clock that subsumes all the remaining versions</li><li>At this point, read response has been returned to the caller</li><li>the state machine waits for a short period to receive any outstanding responces</li><li>if stale versions were returned in any of the responses, the coordinator updates those nodes with the latest version — READ REPAIR</li></ul></li></ul><h2 id="2-7-Anti-entropy-Through-Merkle-Trees"><a href="#2-7-Anti-entropy-Through-Merkle-Trees" class="headerlink" title="2.7 Anti-entropy Through Merkle Trees"></a>2.7 Anti-entropy Through Merkle Trees</h2><ul><li>Vector clocks are useful to remove conflicts while serving read requests</li><li>But if a replica falls significantly behind others, it might take a very long time to resolve conflicts using just vector clocks</li></ul><hr><p>—&gt; we need to quickly compare two copies of a range of data residing on different replicas and figure out exactly which parts are different </p><h3 id="2-7-1-What-are-MerkleTrees"><a href="#2-7-1-What-are-MerkleTrees" class="headerlink" title="2.7.1 What are MerkleTrees?"></a>2.7.1 What are MerkleTrees?</h3><ul><li><p>Dynamo use Merkel Trees to compare replicas of a range</p></li><li><p>A merkle tree is a binary tree of hashes, where each internal node is the hash of its two children, each leaf node is a hash of a portion of the original data</p></li><li><p>Then compare the merkle tree come to be super easy, just compare the root hashes of both trees, if equal, then stop; else, recurse on the left and right children</p></li><li><p>The principal advantage of using a Merkle tree is that each branch of the tree can be checked independently without requiring nodes to download the entire tree or the whole data set. Hence, Merkle trees minimize the amount of data that needs to be transferred for synchronization and reduce the number of disk reads performed during the anti-entropy process.</p></li><li><p>The disadvantage of using Merkle trees is that many key ranges can change when a node joins or leaves, and as a result, the trees need to be recalculated.</p></li></ul><h2 id="2-8-Dynamo-Characteristics"><a href="#2-8-Dynamo-Characteristics" class="headerlink" title="2.8 Dynamo Characteristics"></a>2.8 Dynamo Characteristics</h2><h3 id="2-8-1-Dynamo’s-Node-Responsibilities"><a href="#2-8-1-Dynamo’s-Node-Responsibilities" class="headerlink" title="2.8.1 Dynamo’s Node Responsibilities"></a>2.8.1 Dynamo’s Node Responsibilities</h3><ul><li>Each node serves three functions:<ul><li>Managing <code>get()</code> and <code>put()</code> requests<ul><li>A node may act as a coordinator and manage all operations for a particular key</li><li>A node also could forward the request to the appropriate node</li></ul></li></ul></li><li>Keep track of membership and detecting failures<ul><li>Every node uses gossip protocol to keep track of other nodes in the system and their associated hash ranges</li></ul></li><li>Local persistent storage<ul><li>Each node is responsible for being either the primary or replica store for keys that hash to a specific range of values</li><li>These pairs are stored within that node using various storage systems depending on application needs</li><li>E.G<ul><li>BerkeleyDB Transactional Data Store</li><li>MySQL</li><li>In memory buffer backed by persistent storage</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Designed to be always on&lt;/li&gt;

      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
      <category term="DynamoDB" scheme="https://www.llchen60.com/tags/DynamoDB/"/>
    
  </entry>
  
  <entry>
    <title>数据库索引</title>
    <link href="https://www.llchen60.com/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/"/>
    <id>https://www.llchen60.com/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/</id>
    <published>2021-07-11T04:56:10.000Z</published>
    <updated>2021-07-11T04:56:48.749Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>索引的实现，底层一般会依赖于哪些数据结构？</p></blockquote><h1 id="1-为什么需要索引"><a href="#1-为什么需要索引" class="headerlink" title="1. 为什么需要索引"></a>1. 为什么需要索引</h1><ul><li>为了能够尽快得到想要的数据 — 提高性能</li><li>选取高效的数据结构，在针对访问特性有比较好的访问速度的同时，减少空间上的占用<ul><li>如何节省存储空间</li><li>如何提高数据增删改查的执行效率</li></ul></li></ul><h1 id="2-功能性需求"><a href="#2-功能性需求" class="headerlink" title="2. 功能性需求"></a>2. 功能性需求</h1><ul><li>格式化数据还是非格式化数据？<ul><li>结构化数据<ul><li>比如MySQL数据</li></ul></li><li>非结构化数据<ul><li>比如搜索引擎当中的网页</li><li>一般需要进行预处理，提取出查询关键词，对关键词建立索引</li></ul></li></ul></li><li>静态数据还是动态数据？<ul><li>静态数据<ul><li>意味着没有增删改查</li><li>只需要考虑查询效率即可</li></ul></li><li>动态数据<ul><li>当原始数据更新的同时，我们还需要动态的更新索引</li></ul></li></ul></li><li>索引存储在内存还是硬盘？<ul><li>存储在内存<ul><li>访问速度快</li><li>原始数据量大的前提下，对应的索引也会非常大</li><li>因为内存有限，我们将不得不存放到内存当中</li></ul></li><li>存储在硬盘</li><li>部分存储在内存，部分在硬盘</li></ul></li><li>单值查找还是区间查找<ul><li>单值查找<ul><li>查询关键词等于某个值的数据</li></ul></li><li>区间查找<ul><li>查找关键词处于某个区间值的数据</li></ul></li></ul></li><li>单关键词还是多关键词组合的查找<ul><li>多关键词查询<ul><li>对于结构化数据的，<ul><li>实现针对多个关键词的组合建立索引</li></ul></li><li>对于非结构化数据的<ul><li>建立针对单个关键词的索引</li><li>然后通过集合操作，计算出查询结果</li></ul></li></ul></li></ul></li></ul><h1 id="3-非功能性需求"><a href="#3-非功能性需求" class="headerlink" title="3. 非功能性需求"></a>3. 非功能性需求</h1><ul><li>无论存放在内存还是磁盘当中，对于存储空间的消耗不能过大</li><li>在考虑索引查询效率的同时，还要考虑索引的维护成本</li></ul><h1 id="4-构建索引常用的数据结构"><a href="#4-构建索引常用的数据结构" class="headerlink" title="4. 构建索引常用的数据结构"></a>4. 构建索引常用的数据结构</h1><ul><li>散列表<ul><li>构建内存索引</li><li>如Redis Memcache</li></ul></li><li>红黑树<ul><li>构建内存索引</li><li>Ext文件系统中对磁盘块的索引</li></ul></li><li>B+树<ul><li>比起红黑树，更适合在磁盘当中构建索引</li><li>多叉树，比起红黑树，高度更低，IO更少</li><li>大部分关系型数据库的索引，会使用B+树来实现<ul><li>MySQL</li><li>Oracle</li></ul></li></ul></li><li>跳表<ul><li>Redis中的有序集合</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;索引的实现，底层一般会依赖于哪些数据结构？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;1-为什么需要索引&quot;&gt;&lt;a href=&quot;#1-为什么需要索引&quot; class=&quot;headerlink&quot; title=&quot;1. 为什么需要索引&quot;&gt;&lt;/a&gt;1.
      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>B+树</title>
    <link href="https://www.llchen60.com/B-%E6%A0%91/"/>
    <id>https://www.llchen60.com/B-%E6%A0%91/</id>
    <published>2021-07-11T04:25:25.000Z</published>
    <updated>2021-07-11T04:27:41.539Z</updated>
    
    <content type="html"><![CDATA[<h1 id="B-树"><a href="#B-树" class="headerlink" title="B+树"></a>B+树</h1><h1 id="1-什么时候会用到？"><a href="#1-什么时候会用到？" class="headerlink" title="1. 什么时候会用到？"></a>1. 什么时候会用到？</h1><h2 id="1-1-数据库索引"><a href="#1-1-数据库索引" class="headerlink" title="1.1 数据库索引"></a>1.1 数据库索引</h2><p>工作中为了加快数据库中数据的查找速度，我们常用的处理思路是对表中数据创建索引，而数据库的索引底层就使用了B+树的结构</p><p>对于数据库，我们希望通过索引实现查询数据的效率的提升；同时不要消耗太多的内存空间。而数据库索引查找的过程当中，其特点是：</p><ul><li>需要进行直接查找</li><li>需要支持按照一定区间的快速查找</li></ul><h2 id="1-2-Overview"><a href="#1-2-Overview" class="headerlink" title="1.2 Overview"></a>1.2 Overview</h2><ul><li>B + 树特征<ul><li>每个节点中子节点的个数不能超过 m，也不能小于 m/2；</li><li>根节点的子节点个数可以不超过 m/2，这是一个例外；</li><li>m 叉树只存储索引，并不真正存储数据，这个有点儿类似跳表；</li><li>通过链表将叶子节点串联在一起，这样可以方便按区间查找；</li><li>一般情况下，根节点会被存储在内存中，其他节点存储在磁盘中</li></ul></li><li>B树<ul><li>节点中存储数据</li><li>叶子节点并没有使用链表来串联</li><li>是一个每个节点的子节点个数不少于m/2的m叉树</li></ul></li></ul><h1 id="2-为什么在数据库索引的时候会使用到B-树呢？"><a href="#2-为什么在数据库索引的时候会使用到B-树呢？" class="headerlink" title="2. 为什么在数据库索引的时候会使用到B+树呢？"></a>2. 为什么在数据库索引的时候会使用到B+树呢？</h1><h2 id="2-1-使用二叉查找树来实现索引？"><a href="#2-1-使用二叉查找树来实现索引？" class="headerlink" title="2.1 使用二叉查找树来实现索引？"></a>2.1 使用二叉查找树来实现索引？</h2><ul><li>从二叉查找树开始说起 我们可以改一下<ul><li>树中的节点不存储数据本身，只作为索引</li><li>每个叶子节点串在一条链表上</li><li>链表当中的数据从小到大有序</li></ul></li><li>对于区间查找的支持程度<ul><li>先在树中遍历，然后到叶子节点以后再顺着链表来往后遍历</li><li>直到链表当中的结点数据值大于区间的终止值为止</li></ul></li><li>问题<ul><li>占用的内存空间非常大<ul><li>解决方案<ul><li>放到硬盘上，但是访问速度会变慢很多了</li><li>内存访问速度在纳秒级别</li><li>硬盘访问速度在毫秒级别</li></ul></li><li>解决方案的问题在于<ul><li>磁盘IO操作的次数等于树的高度</li><li>我们需要尽可能的减少树的高度来减少磁盘IO的次数</li></ul></li></ul></li></ul></li></ul><h2 id="2-2-使用m叉树来实现"><a href="#2-2-使用m叉树来实现" class="headerlink" title="2.2 使用m叉树来实现"></a>2.2 使用m叉树来实现</h2><ul><li>使用m叉树的好处<ul><li>减少了访问磁盘的IO次数</li></ul></li></ul><pre><code class="jsx">/** * 这是B+树非叶子节点的定义。 * * 假设keywords=[3, 5, 8, 10] * 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF) * 5个区间分别对应：children[0]...children[4] * * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = (m-1)*4[keywordss大小]+m*8[children大小] */public class BPlusTreeNode &#123;  public static int m = 5; // 5叉树  public int[] keywords = new int[m-1]; // 键值，用来划分数据区间  public BPlusTreeNode[] children = new BPlusTreeNode[m];//保存子节点指针&#125;/** * 这是B+树中叶子节点的定义。 * * B+树中的叶子节点跟内部节点是不一样的, * 叶子节点存储的是值，而非区间。 * 这个定义里，每个叶子节点存储3个数据行的键值及地址信息。 * * k值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = k*4[keyw..大小]+k*8[dataAd..大小]+8[prev大小]+8[next大小] */public class BPlusTreeLeafNode &#123;  public static int k = 3;  public int[] keywords = new int[k]; // 数据的键值  public long[] dataAddress = new long[k]; // 数据地址  public BPlusTreeLeafNode prev; // 这个结点在链表中的前驱结点  public BPlusTreeLeafNode next; // 这个结点在链表中的后继结点&#125;</code></pre><ul><li>m值的设定<ul><li>操作系统是按页来读取数据的，一页通常大小为4KB</li><li>一次会读取一页的数据</li><li>如果要读取的数据量超过一页的大小，就会触发多次IO操作了</li><li>因此我们选定m大小的时候尽量让每个节点的大小等于一个页的大小</li><li>这样的话读取一个节点只需要一次磁盘IO操作即可</li></ul></li></ul><p><img src="https://i.loli.net/2021/07/11/148A67p9Y3dre2g.png" alt="B+树数据存储"></p><h2 id="2-3-索引的问题-导致写入效率下降"><a href="#2-3-索引的问题-导致写入效率下降" class="headerlink" title="2.3 索引的问题 - 导致写入效率下降"></a>2.3 索引的问题 - 导致写入效率下降</h2><ul><li>数据写入过程，会涉及到索引的更新，这是导致索引写入变慢的主要原因</li><li>场景描述<ul><li>m值是提前计算好的</li><li>向数据库写入过程当中，有可能会使得索引当中某些节点的子节点的个数超过m</li><li>这个节点的大小超过一个页的大小，那么读取这样一个节点的时候，就会导致多次磁盘IO操作，这是我们极力避免的</li></ul></li><li>解决方案<ul><li>将这个节点分裂成两个节点</li><li>这个问题会向上传导，即上层父节点的子节点个数有可能超过m个了</li><li>因此我们就需要向上递归来重构这棵B+树了</li><li>正是因为我们需要保证B+树索引是一个m叉树，所以索引的存在会导致数据库写入速度降低</li></ul></li></ul><p><img src="https://i.loli.net/2021/07/11/9qAHTSmnlUeVPjb.png" alt="B+树索引修改"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;B-树&quot;&gt;&lt;a href=&quot;#B-树&quot; class=&quot;headerlink&quot; title=&quot;B+树&quot;&gt;&lt;/a&gt;B+树&lt;/h1&gt;&lt;h1 id=&quot;1-什么时候会用到？&quot;&gt;&lt;a href=&quot;#1-什么时候会用到？&quot; class=&quot;headerlink&quot; title=&quot;1
      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis哨兵机制</title>
    <link href="https://www.llchen60.com/Redis%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/"/>
    <id>https://www.llchen60.com/Redis%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/</id>
    <published>2021-07-06T03:07:59.000Z</published>
    <updated>2021-07-05T19:22:25.605Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis多机数据库-哨兵机制"><a href="#Redis多机数据库-哨兵机制" class="headerlink" title="Redis多机数据库-哨兵机制"></a>Redis多机数据库-哨兵机制</h1><p>主从库的集群模式使得当从库发生故障以后，客户端可以继续向主库或者其他从库发送请求，进行相关的操作；但是如果主库发生了故障，那会直接影响到从库的同步。无论是写中断还是从库无法进行数据同步都是Redis所不能接受的。因此我们需要一些机制，来能够<strong>将一个从库切换为主库</strong>，这就涉及到了Redis的哨兵机制。</p><h1 id="1-哨兵机制的基本流程"><a href="#1-哨兵机制的基本流程" class="headerlink" title="1. 哨兵机制的基本流程"></a>1. 哨兵机制的基本流程</h1><ul><li>哨兵可以理解为一个运行在特殊模式下的<strong>Redis进程</strong>，其在主从库实例运行的同时也在运行</li><li>哨兵主要的三个任务为：<ul><li>监控 — 决策：判断主库是否处于下线状态<ul><li>周期性的ping主库，检测其是否仍然在线运行</li><li>如果从库没有在规定时间内响应哨兵的Ping命令，哨兵就会将其标记为下线状态</li><li>对主库来说同理，在判定主库下线以后会开始一个自动切换主库的流程</li></ul></li><li>选主 — 决策：决定选择哪个从库实例作为主库<ul><li>主库挂了以后，哨兵就需要从很多从库里按照一定的规则选择一个从库实例，将其作为新的主库</li></ul></li><li>通知<ul><li>将新主库的连接信息发给其他从库，让它们执行replicaof命令，与新主库建立连接，并进行数据复制</li><li>哨兵会将新主库的连接信息通知给客户端，让它们将请求操作发到新主库当中</li></ul></li></ul></li></ul><p><img src="https://i.loli.net/2021/01/07/z7o6Kkdfp2IaPsx.png" alt="哨兵机制任务"></p><p>哨兵三大任务</p><h1 id="2-判断主库的下线状态"><a href="#2-判断主库的下线状态" class="headerlink" title="2. 判断主库的下线状态"></a>2. 判断主库的下线状态</h1><h2 id="2-1-哨兵集群使用原因"><a href="#2-1-哨兵集群使用原因" class="headerlink" title="2.1 哨兵集群使用原因"></a>2.1 哨兵集群使用原因</h2><h3 id="2-1-1-为什么需要哨兵集群？"><a href="#2-1-1-为什么需要哨兵集群？" class="headerlink" title="2.1.1 为什么需要哨兵集群？"></a>2.1.1 为什么需要哨兵集群？</h3><ul><li>如果哨兵发生误判，后续的选主和通知操作都会带来额外的计算和通信的开销</li><li>误判通常发生在<ul><li>集群网络压力较大</li><li>网络拥塞</li><li>主库本身压力较大的情况</li></ul></li><li>哨兵机制也是类似的，它通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。</li></ul><h3 id="2-1-2-如何使用哨兵集群？"><a href="#2-1-2-如何使用哨兵集群？" class="headerlink" title="2.1.2 如何使用哨兵集群？"></a>2.1.2 如何使用哨兵集群？</h3><ul><li>简单来说，“客观下线”的标准就是，当有 N 个哨兵实例时，最好要有 N/2 + 1 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来，就可以减少误判的概率，也能避免误判带来的无谓的主从库切换。（当然，有多少个实例做出“主观下线”的判断才可以，可以由 Redis 管理员自行设定）。</li></ul><h2 id="2-2-哨兵集群原理-—-基于PubSub机制"><a href="#2-2-哨兵集群原理-—-基于PubSub机制" class="headerlink" title="2.2 哨兵集群原理 — 基于PubSub机制"></a>2.2 哨兵集群原理 — 基于PubSub机制</h2><h3 id="2-2-1-pubsub机制"><a href="#2-2-1-pubsub机制" class="headerlink" title="2.2.1 pubsub机制"></a>2.2.1 pubsub机制</h3><p>哨兵实例之间的相互发现是基于Redis提供的pubsub机制的，<strong>哨兵只要和主库建立起连接</strong>，就可以在主库上发布消息了</p><ul><li>可以选择发布自己的连接信息到主库上</li><li>也可以从主库上订阅消息，获得其他哨兵发布的连接信息</li><li>当多个哨兵实例都在主库上做了发布和订阅操作之后，他们之间就能知道彼此的IP地址和端口</li></ul><h3 id="2-2-2-频道"><a href="#2-2-2-频道" class="headerlink" title="2.2.2 频道"></a>2.2.2 频道</h3><ul><li>Redis通过频道来区分不同应用的消息，对这些消息进行分门别类的管理。频道就是指消息的类别，当消息类别相同时，就会属于同一个频道，否则属于不同的频道。</li><li>主库会构建一个叫做 <code>__sentinel__:hello</code> 的频道，来让各个哨兵互相发现</li><li>只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换</li></ul><p><img src="https://i.loli.net/2021/01/11/LIhj62iuDBbPEve.png" alt="pubsub机制"></p><p>频道订阅机制</p><ul><li>哨兵1 向频道hello发送信息，因为哨兵2 哨兵3 subscribe了hello频道，他们就能从这个频道获取到哨兵1的IP地址和端口号信息</li></ul><h3 id="2-2-3-哨兵和从库的连接沟通"><a href="#2-2-3-哨兵和从库的连接沟通" class="headerlink" title="2.2.3 哨兵和从库的连接沟通"></a>2.2.3 哨兵和从库的连接沟通</h3><ul><li><p>哨兵向主库发出INFO命令</p></li><li><p>主库收到命令后，就会将从库列表返回给哨兵</p></li><li><p>接着哨兵就可以根据从库列表中的信息，和每个从库建立连接，并在这个连接上持续对从库进行监控</p><p>  <img src="https://i.loli.net/2021/01/11/sPTRS1mkhU2lLQn.png" alt="哨兵与从库之间的连接"></p><p>  哨兵和从库的连接</p></li><li><p>哨兵除了上述的和主库之间的连接，获取从库列表，并和从库们建立连接之外，还承担着在发生主库更换以后，将新主库的信息告诉客户端这个任务</p></li></ul><h2 id="2-3-客户端事件通知机制"><a href="#2-3-客户端事件通知机制" class="headerlink" title="2.3 客户端事件通知机制"></a>2.3 客户端事件通知机制</h2><ul><li><p>哨兵是一个运行在特定模式下的Redis实例，只是它不服务请求操作，只是完成监控，选主和通知的任务</p></li><li><p>因此每个哨兵实例也提供pubsub机制，客户端可以从哨兵订阅消息</p><ul><li><p>哨兵提供了很多的消息订阅频道，不同频道包含了主从库切换过程中的不同关键事件</p><p>  <img src="https://i.loli.net/2021/07/06/lBP5xJYieohTZXw.png" alt="哨兵常用的消息订阅频道"></p></li><li><p>客户端可以执行订阅命令，来订阅不同的频道，然后来获取不同的事件信息</p><ul><li>当哨兵将新的主库选出来以后，客户端会看到switch-master事件，事件中会包含新的主库的IP地址还有端口信息</li><li>此时客户端就会使用新主库地址和端口来进行通信了</li></ul></li></ul></li></ul><h1 id="3-如何选定新主库？"><a href="#3-如何选定新主库？" class="headerlink" title="3. 如何选定新主库？"></a>3. 如何选定新主库？</h1><blockquote><p>哨兵筛选新主库的过程称为筛选+打分</p></blockquote><ul><li>筛选 — 按照一定条件筛选，将不符合条件的从库去掉<ul><li>确保从库仍然在线运行</li><li>判断其之前的网络状态 看该从库和主库之间是否经常断联，出现网络相关的问题<ul><li>通过使用down-after-milliseconds属性，这是我们认为的最大主从间的连接超时，如果超过这个时间我们就认为断联了，超过一定次数就认为从库网络状况不好</li></ul></li></ul></li><li>打分 — <strong>只要有得分最高的，那么就在当前轮停止并且认定其为主库</strong><ul><li>从库优先级 — 手动设置的<ul><li>用户可以通过slave-priority配置项，给不同的从库设置不同的优先级<ul><li>譬如：两个从库内存大小不一样，我们就可以手动给内存大的实例设置一个高优先级</li></ul></li></ul></li><li>从库复制进度<ul><li>选择和旧主库<strong>同步最为接近</strong>的那个从库作为主库</li><li>如何判断从库和旧主库的同步进度？<ul><li>主从库之间命令传播机制里面的master_repl_offset 和slave_repl_offset</li><li>看二者的接近程度</li></ul></li></ul></li><li>从库ID号<ul><li>当优先级和复制进度都相同的情况下，ID号最小的从库得分最高，被选为新主库</li></ul></li></ul></li></ul><h1 id="4-由哪个哨兵来执行主从切换？"><a href="#4-由哪个哨兵来执行主从切换？" class="headerlink" title="4. 由哪个哨兵来执行主从切换？"></a>4. 由哪个哨兵来执行主从切换？</h1><ul><li><h2 id="任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应"><a href="#任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应" class="headerlink" title="任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应"></a>任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应</h2><pre><code>  ![哨兵沟通，确定主库是否下线](https://i.loli.net/2021/01/11/HpT5MAdKX9fmo2S.png)  is master down by addr</code></pre><ul><li><p>一个哨兵获得了<strong>仲裁所需的赞成票数后，就可以标记主库为客观下线</strong></p><ul><li>这个所需的赞成票数是通过哨兵配置文件中的quorum配置项设定的</li></ul></li><li><p>当获得了所需赞成票数以后，这个哨兵会再给其他哨兵发送命令，希望由自己来执行主从切换，并让所有其他哨兵进行投票，这个过程称为<strong>Leader选举</strong>。</p></li><li><p>在投票过程中，任何一个想成为 Leader 的哨兵，要满足两个条件：</p><ul><li>第一，拿到半数以上的赞成票；</li><li>第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。以 3 个哨兵为例，假设此时的 quorum 设置为 2，那么，任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以了。</li></ul><p><img src="https://i.loli.net/2021/01/11/RAaq7KZr2LSUdVx.png" alt="哨兵投票，选举leader"></p><p>票选执行主从切换哨兵的过程</p></li></ul><ol><li>在 T1 时刻，S1 判断主库为“客观下线”，它想成为 Leader，就先给自己投一张赞成票，然后分别向 S2 和 S3 发送命令，表示要成为 Leader。</li><li>在 T2 时刻，S3 判断主库为“客观下线”，它也想成为 Leader，所以也先给自己投一张赞成票，再分别向 S1 和 S2 发送命令，表示要成为 Leader。</li><li>在 T3 时刻，S1 收到了 S3 的 Leader 投票请求。<strong>因为 S1 已经给自己投了一票 Y，所以它不能再给其他哨兵投赞成票了，所以 S1 回复 N 表示不同意</strong>。同时，S2 收到了 T2 时 S3 发送的 Leader 投票请求。因为 S2 之前没有投过票，它会给第一个向它发送投票请求的哨兵回复 Y，给后续再发送投票请求的哨兵回复 N，所以，在 T3 时，S2 回复 S3，同意 S3 成为 Leader。</li><li>在 T4 时刻，S2 才收到 T1 时 S1 发送的投票命令。因为 S2 已经在 T3 时同意了 S3 的投票请求，此时，S2 给 S1 回复 N，表示不同意 S1 成为 Leader。发生这种情况，是因为 S3 和 S2 之间的网络传输正常，而 S1 和 S2 之间的网络传输可能正好拥塞了，导致投票请求传输慢了。</li><li>在 T5 时刻，S1 得到的票数是来自它自己的一票 Y 和来自 S2 的一票 N。而 S3 除了自己的赞成票 Y 以外，还收到了来自 S2 的一票 Y。此时，S3 不仅获得了半数以上的 Leader 赞成票，也达到预设的 quorum 值（quorum 为 2），所以它最终成为了 Leader。接着，S3 会开始执行选主操作，而且在选定新主库后，会给其他从库和客户端通知新主库的信息。</li></ol><ul><li><p>如果 S3 没有拿到 2 票 Y，那么这轮投票就不会产生 Leader。<strong>哨兵集群会等待一段时间（也就是哨兵故障转移超时时间的 2 倍）</strong>，再重新选举。这是因为，哨兵集群能够进行成功投票，很大程度上依赖于选举命令的正常网络传播。<strong>如果网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票</strong>。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。需要注意的是，如果哨兵集群只有 2 个实例，此时，一个哨兵要想成为 Leader，必须获得 2 票，而不是 1 票。所以，如果有个哨兵挂掉了，那么，此时的集群是无法进行主从库切换的。因此，通常我们至少会配置 3 个哨兵实例。</p><h1 id="5-FAQs"><a href="#5-FAQs" class="headerlink" title="5. FAQs"></a>5. FAQs</h1><h2 id="5-1-哨兵在操作主从切换的过程当中，客户端能否正常进行请求操作？"><a href="#5-1-哨兵在操作主从切换的过程当中，客户端能否正常进行请求操作？" class="headerlink" title="5.1 哨兵在操作主从切换的过程当中，客户端能否正常进行请求操作？"></a>5.1 哨兵在操作主从切换的过程当中，客户端能否正常进行请求操作？</h2></li><li><p>如果客户端使用了读写分离，那么<strong>读请求</strong>可以在从库上正常执行，不会受到影响。但是由于此时主库已经挂了，而且哨兵还没有选出新的主库，所以在这期间<strong>写请求会失败</strong>，失败持续的时间 = 哨兵切换主从的时间 + 客户端感知到新主库 的时间。</p></li><li><p>如果不想让业务感知到异常，客户端只能把写失败的请求先缓存起来或<strong>写入消息队列中间件</strong>中，等哨兵切换完主从后，再把这些写请求发给新的主库，但这种场景只适合对写入请求返回值不敏感的业务，而且还需要业务层做适配，另外主从切换时间过长，也会导致客户端或消息队列中间件缓存写请求过多，切换完成之后重放这些请求的时间变长。</p></li><li><p>哨兵检测主库多久没有响应就提升从库为新的主库，这个时间是可以配置的（down-after-milliseconds参数）。配置的时间越短，哨兵越敏感，哨兵集群认为主库在短时间内连不上就会发起主从切换，这种配置很可能因为网络拥塞但主库正常而发生不必要的切换，当然，当主库真正故障时，因为切换得及时，对业务的影响最小。如果配置的时间比较长，哨兵越保守，这种情况可以减少哨兵误判的概率，但是主库故障发生时，业务写失败的时间也会比较久，缓存写请求数据量越多。</p></li><li><p>应用程序不感知服务的中断，还需要哨兵和客户端做些什么？当哨兵完成主从切换后，客户端需要及时感知到主库发生了变更，然后把缓存的写请求写入到新库中，保证后续写请求不会再受到影响，具体做法如下：</p><ul><li>哨兵提升一个从库为新主库后，哨兵会把新主库的地址写入自己实例的pubsub（switch-master）中。客户端需要订阅这个pubsub，当这个pubsub有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。</li><li>如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见，客户端也需要支持主动去获取最新主从的地址进行访问。</li><li>所以，客户端需要访问主从库时，不能直接写死主从库的地址了，而是需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name命令），这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。</li><li>一般Redis的SDK都提供了通过哨兵拿到实例地址，再访问实例的方式，我们直接使用即可，不需要自己实现这些逻辑。当然，对于只有主从实例的情况，客户端需要和哨兵配合使用，而在分片集群模式下，这些逻辑都可以做在proxy层，这样客户端也不需要关心这些逻辑了，Codis就是这么做的</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1></li></ul><ol><li>极客时间Redis课程</li><li>Redis设计与实现</li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis多机数据库-哨兵机制&quot;&gt;&lt;a href=&quot;#Redis多机数据库-哨兵机制&quot; class=&quot;headerlink&quot; title=&quot;Redis多机数据库-哨兵机制&quot;&gt;&lt;/a&gt;Redis多机数据库-哨兵机制&lt;/h1&gt;&lt;p&gt;主从库的集群模式使得当从库发生故障以
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis Cluster</title>
    <link href="https://www.llchen60.com/Redis-Cluster/"/>
    <id>https://www.llchen60.com/Redis-Cluster/</id>
    <published>2021-07-06T00:08:09.000Z</published>
    <updated>2021-07-09T22:19:57.371Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis-Cluster"><a href="#Redis-Cluster" class="headerlink" title="Redis Cluster"></a>Redis Cluster</h1><h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>Redis集群是Redis提供的分布式数据库方案，集群通过分片 - sharding来进行数据共享，并提供复制和故障转移功能</li></ul><h1 id="2-节点"><a href="#2-节点" class="headerlink" title="2. 节点"></a>2. 节点</h1><ul><li>概述<ul><li>一个redis集群通常由多个节点 node 组成</li><li>开始的时候每个节点都是相互独立的，相当于各自在自己的集群当中，我们需要将各个独立的节点连接起来，构成一个包含多个节点的集群</li><li><code>CLUSTER MEET &lt;ip&gt; &lt;port&gt;</code> 我们可以用这个指令来连接各个节点<ul><li>以节点A 收到命令，要加节点B为例<ul><li>A和B 握手，确认彼此存在</li><li>A为节点B创建一个clusterNode结构，并添加到自己的clusterState.nodes字典里面</li><li>A根据IP 还有端口，向B发送MEET消息</li><li>节点B 收到信息，为A创建clusterNode结构，并添加到自己的 <code>clustState.nodes</code> 字典里面</li><li>B向A发出PONG消息，让A知道自己成功接收到了</li><li>A向B发出PING消息</li><li>节点B收到，确认A收到了自己的PONG  整个过程结束</li></ul></li></ul></li></ul></li><li>启动节点<ul><li>Redis服务器根据 <code>cluster-enabled</code> 配置选项判断是否开启服务器的集群模式</li></ul></li><li>集群模式和standalone模式的节点区别<ul><li>相同之处<ul><li>单机模式下的功能照旧<ul><li>文件事件处理器</li><li>时间事件处理器</li><li>持久化</li><li>Pubsub</li><li>复制模块</li><li>lua脚本</li></ul></li></ul></li><li>不同点<ul><li>集群模式下的数据会被保存到clusterNode, clusterLink以及clusterState结构里面<ul><li>下面是三个结构的代码实现</li><li>以及一个有三个节点的clusterState结构</li></ul></li></ul></li></ul></li></ul><pre><code class="jsx">struct clusterNode &#123;    mstime_t ctime;    char name[REDIS_CLUSTER_NAMELEN];    // 记录节点角色（主/从); 记录节点目前所处状态(在线/ 下线)    int flags;    // 节点当前配置记录    unit64_t configEpoch;    char ip[REDIS_IP_STR_LEN];    int port;    // 保存连接节点需要的信息    clusterLink *link;    // ....&#125;</code></pre><pre><code class="jsx">typedef struct clusterLink &#123;    mstime_t ctime;    // TCP 套接字描述符    int fd;    // 输出缓冲区，保存等待要发送给其他节点的消息    sds sndbuf;    // 输入缓冲区，保存着从其他节点接收到的消息    sds rcvbuf;    // 与这个连接相关联的节点    struct clusterNode *node;    // ....&#125; clusterLink; </code></pre><pre><code class="jsx">typedef struct clusterState &#123;    clustNode *myself;    unit64_t currentEpoch;    // 集群当前状态    int state;    // 集群中至少处理着一个槽的节点的数量    int size;    // 集群节点名单    dict *nodes;    // ....&#125; clusterState; </code></pre><p><img src="https://i.loli.net/2021/07/06/XxiOA9Qjz763K1Y.png" alt="clusterState示意图"></p><h1 id="3-槽指派"><a href="#3-槽指派" class="headerlink" title="3. 槽指派"></a>3. 槽指派</h1><ul><li>Redis集群通过分片的方式保存数据库当中的键值对<ul><li>整个数据库被分为了16384个slot</li><li>每个键都属于这些槽其中之一</li><li>集群当中的每个节点可以处理0个或者最多16384个槽</li><li>当16383个槽都有节点在处理的时候，集群处在上线状态</li><li>反之，如果有任何一个槽没有得到处理，那么集群就在下线状态</li></ul></li><li>如何指派槽<ul><li><code>CLUSTER ADDSLOTS &lt;slot&gt; [slot...]</code></li></ul></li></ul><h2 id="3-1-如何记录槽指派的信息"><a href="#3-1-如何记录槽指派的信息" class="headerlink" title="3.1 如何记录槽指派的信息"></a>3.1 如何记录槽指派的信息</h2><ul><li><p>clusterNode结构里面有slots属性和numslot属性</p><ul><li><p>slots</p><ul><li><p>bit array</p></li><li><p>根据索引i上的二进制位的值来判断节点是否负责处理槽i</p><p>  <img src="https://i.loli.net/2021/07/06/Zthejck4Xz2bPKl.png" alt="用bit array记录槽指派信息"></p></li></ul></li></ul></li></ul><pre><code class="jsx">struct clusterNode &#123;    // ...    unsigned char slots[16384/8];    int numslots; &#125;</code></pre><h2 id="3-2-如何传播-槽指派的信息"><a href="#3-2-如何传播-槽指派的信息" class="headerlink" title="3.2 如何传播 槽指派的信息"></a>3.2 如何传播 槽指派的信息</h2><ul><li>一个节点除了要记录自己负责处理的槽以外，还要将自己的slots通过消息发送给集群中的其他节点，以此告知其他结点自己目前负责的槽位</li><li>而后槽信息会被存储在clusterState 里面<ul><li>指向的是一个clusterNode结构！</li></ul></li><li>这里对比 <code>clusterState.slots</code> 以及 <code>clusterNode.slots</code> ， 前者记录了所有槽的指派信息，后者记录了当前的clusterNode结构所代表的节点的槽指派信息  一个记录了所有节点，一个记录了部分信息</li></ul><pre><code class="jsx">typedef struct clusterState &#123;    // ...    clusterNode *slots[16384];    // ... &#125;</code></pre><p><img src="https://i.loli.net/2021/07/06/4n1tgixR2NXOHdr.png" alt="槽指派信息的传播"></p><h1 id="4-在集群中执行命令"><a href="#4-在集群中执行命令" class="headerlink" title="4. 在集群中执行命令"></a>4. 在集群中执行命令</h1><h2 id="4-1-发送指令过程"><a href="#4-1-发送指令过程" class="headerlink" title="4.1 发送指令过程"></a>4.1 发送指令过程</h2><ul><li><p>当客户端向节点发送和数据库键有关的命令的时候，接收命令的节点会计算出命令要处理的数据库键属于哪一个槽</p></li><li><p>检查这个槽是否指派给了自己</p><ul><li><p>如果正好指派了，节点直接执行这个命令</p></li><li><p>如果没有，节点向客户端发送一个MOVED错误，指引客户端转向正确的节点，并再次发送之前想要执行的指令</p><p>  <img src="https://i.loli.net/2021/07/06/MamzjpOhCXvwNI4.png" alt="moved执行逻辑"></p><p>  <img src="https://i.loli.net/2021/07/06/3rBPwlGVoITAJn7.png" alt="节点返回MOVED,IP以及端口号"></p><p>  <img src="https://i.loli.net/2021/07/06/zIOFRiGWZutUce9.png" alt="节点转向"></p></li></ul></li></ul><h2 id="4-2-slots-to-keys跳跃表"><a href="#4-2-slots-to-keys跳跃表" class="headerlink" title="4.2 slots_to_keys跳跃表"></a>4.2 slots_to_keys跳跃表</h2><ul><li>当有新的键值对增删的操作时，节点会用clusterState结构中的slots_to_keys跳跃表来保存槽和键之间的关系</li></ul><pre><code class="jsx">typedef struct clusterState &#123;    // ...    zskiplist *slots_to_keys;    // ...&#125; clusterState; </code></pre><h1 id="5-重新分片操作"><a href="#5-重新分片操作" class="headerlink" title="5. 重新分片操作"></a>5. 重新分片操作</h1><h2 id="5-1-重分片流程"><a href="#5-1-重分片流程" class="headerlink" title="5.1 重分片流程"></a>5.1 重分片流程</h2><ul><li><p>将任意数量已经指派给某个节点的槽改为指派给另外一个节点，相关槽所属的键值对也会从源节点被移动到目标节点</p><p>  <img src="https://i.loli.net/2021/07/06/t5wivjMnQuIT2rG.png" alt="重分片"></p></li></ul><h2 id="5-2-ASK错误"><a href="#5-2-ASK错误" class="headerlink" title="5.2 ASK错误"></a>5.2 ASK错误</h2><ul><li>发生了重分片的过程当中，一部分键已经转移到了目标节点，另外一部分还在源节点</li><li>这种情况下，当接收到请求之后<ul><li>源节点首先在自己的数据库里面查找指定的键，如果找到，直接执行</li><li>如果没有找到，源节点会向客户端发送一个ASK，指引客户端转向正在导入槽的目标节点，并再次发送之前想要执行的命令</li></ul></li></ul><p><img src="https://i.loli.net/2021/07/06/nD7Ybyk1BHGzOC4.png" alt="ASK判断逻辑"></p><h1 id="6-复制与故障转移"><a href="#6-复制与故障转移" class="headerlink" title="6. 复制与故障转移"></a>6. 复制与故障转移</h1><ul><li><p>Redis集群当中的节点分为主节点还有从节点</p></li><li><p>主节点用于处理槽，而从节点用于复制某个主节点，并在被复制的主节点下线以后，代替下线主节点继续处理命令请求</p></li><li><p>通过 <code>CLUSTER REPLICATE &lt;node_id&gt;</code> 来让接受命令的节点成为从节点，并开始对主节点进行复制</p><ul><li>从节点会在自己的 clusterState.nodes字典里 找到主节点对象的clusterNode结构，并将自己的 <code>clusterState.myself.slaveof</code>指针指向这个结构，以此来记录这个节点正在复制的主节点</li><li>而后修改在 <code>clusterStat.myslef.flags</code>中的属性  关闭原来的REDIS_NODE_MASTER标识，打开REDIS_NODE_SLAVE标识，表示这个节点已经由原来的主节点变成了从节点</li><li>根据 <code>clusterState.myself.slaveof</code>指向的clusterNode结构的IP还有端口号，开始对主节点的复制工作</li></ul></li><li><p>故障检测</p><ul><li>集群中每个节点会定期向其他节点发送PING消息，以此检测对方是否在线</li><li>如果接收PING消息的节点没有在规定时间内，返回PONG消息，那么没发回消息的节点会被标记为probable fail</li><li>集群中的各个节点会通过互相发送消息的方式来交换集群中各个节点的状态信息<ul><li>在线</li><li>疑似下线状态 PFAIL</li><li>已下线状态 FAIL</li></ul></li><li>主节点会记录各个节点的报告如果半数以上的负责处理槽的主节点都将某个主节点x报告为疑似下线，那么这个主节点会被标记为已下线</li><li>该消息会向集群广播出去</li></ul></li><li><p>故障转移</p><ul><li>在下线的主节点的从节点里面，选出一个从节点<ul><li>通过选举产生</li><li>当从节点发现主节点进入已下线状态的时候，从节点会向集群广播一条 <code>CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST</code> 消息，要求所有收到这条消息并且有投票权的主节点向这个从节点投票</li><li>如果一个主节点有投票权(负责处理槽) ，且还未投票给其他从节点，那么就会向要求投票的从节点返回一条 <code>CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK</code></li><li>每个参与选举的从节点统计收获到的ACK的数量</li><li>具有投票权的主节点数量为N  那么从节点需要收集到大于等于 N/2 +1 张支持票来获得主节点的支持</li><li>如果没有节点获得大于等于 N/2 +1 张支持票，那么就重新开始选举</li></ul></li><li>被选中的从节点会执行SLAVEOF no one命令，成为新的主节点</li><li>新主节点会撤销所有对已下线的主节点的槽指派，并将这些槽全部指派给自己</li><li>新的主节点向集群广播一条PONG消息，使得集群中的其他结点知道这个节点已经变成了主节点</li></ul></li></ul><h1 id="7-消息"><a href="#7-消息" class="headerlink" title="7. 消息"></a>7. 消息</h1><p>节点发送的消息主要有以下5种： </p><ul><li>MEET<ul><li>要求接受者加入到发送者当前所处的集群当中</li></ul></li><li>PING<ul><li>集群里的每个节点默认每隔一秒从已知节点列表中随机挑选5个结点</li><li>然后对5个当中最长时间没有发送过PING消息的节点发送PING消息</li></ul></li><li>PONG<ul><li>来告知发送者成功收到了MEET或者PING消息</li><li>也可以通过集群广播PONG 来让其他节点刷新对自己的认知</li></ul></li><li>FAIL<ul><li>当主节点A判断另一个主节点B进入FAIL状态了以后</li><li>节点A会向集群广播一条关于节点B FAIL的消息</li><li>所有收到消息的节点会将节点B标记为已下线</li></ul></li><li>PUBLISH<ul><li>当节点接到一个PUBLISH命令的时候，节点会执行，并向集群广播一条PUBLISH消息</li><li>所有接收到的节点都会执行相同的PUBLISH命令</li></ul></li></ul><h1 id="8-Redis-集群扩展与调优"><a href="#8-Redis-集群扩展与调优" class="headerlink" title="8. Redis 集群扩展与调优"></a>8. Redis 集群扩展与调优</h1><blockquote><p>实际案例： 5000万个键值对，每个键值对512B, 如何选择云主机的内存容量？</p></blockquote><ul><li><p>占用的内存空间</p><ul><li>50MM x 512B = 25GB</li></ul></li><li><p>使用32GB主机，但是发现Redis响应有时候会非常慢</p><ul><li>查询latest_fork_usec 指标，发现快到秒级别了</li></ul></li><li><p>Redis持久化机制</p><ul><li>RDB持久化过程当中，Redis会fork子进程来完成</li><li>fork用时和Redis的数据量正相关</li><li>fork在执行时会阻塞主线程<ul><li>数据量越大，fork操作造成的主线程阻塞时间就会越长</li><li>当我们对25GB的数据进行持久化的时候，数据量比较大，后台运行的子进程在fork创建时阻塞了主线程，导致Redis响应变慢</li></ul></li></ul></li><li><p>在上述的例子里面，使用单一redis instance已经不够了，因为我们要通过数据切片来实现</p></li></ul><h2 id="8-1-如何保存更多的数据"><a href="#8-1-如何保存更多的数据" class="headerlink" title="8.1 如何保存更多的数据"></a>8.1 如何保存更多的数据</h2><ul><li>纵向扩展<ul><li>升级单个Redis实例的资源配置<ul><li>增加内存容量</li><li>增加磁盘容量</li><li>使用更高配置的CPU</li></ul></li></ul></li><li>横向扩展<ul><li>横向增加当前Redis实例的个数</li></ul></li></ul><h2 id="8-2-数据切片和实例的对象分布关系"><a href="#8-2-数据切片和实例的对象分布关系" class="headerlink" title="8.2 数据切片和实例的对象分布关系"></a>8.2 数据切片和实例的对象分布关系</h2><ul><li>Redis cluster采用哈希槽来处理数据和实例之间的映射关系<ul><li>一个切片集群有16384个哈希槽</li><li>每个键值都会根据它的key，被映射到一个哈希槽当中</li></ul></li><li>具体映射过程<ul><li>根据键值对的key，按照CRC16算法计算一个16bit的值</li><li>用这个16bit的值对16384取模</li></ul></li><li>reids如何初始化各个实例来分配这些槽的？<ul><li>部署的时候，会用到cluster create命令创建集群</li><li>Redis会在此时将这些槽平均分别在集群实例当中</li></ul></li></ul><h2 id="8-3-客户端如何定位数据的"><a href="#8-3-客户端如何定位数据的" class="headerlink" title="8.3 客户端如何定位数据的"></a>8.3 客户端如何定位数据的</h2><ul><li><p>客户端定位键值对的时候，client library是可以计算出哈希槽是哪个的，但是如何定位到在哪个实例上是个问题</p><ul><li>拿到实例发过来的信息，缓存到本地上</li></ul></li><li><p>最开始</p><ul><li>每个实例都只知道自己被分配了哪些哈希槽的信息，并不知道其他实例拥有的哈希槽信息</li></ul></li><li><p>客户端和集群实例建立连接后</p><ul><li>实例会把哈希槽的分配信息发回给客户端</li><li>各个Redis实例会互相连通，完成哈希槽信息的扩散</li></ul></li><li><p>Redis Cluster的重定向机制</p><ul><li>为什么需要<ul><li>客户端的缓存没有拿到最新的信息<ul><li>集群中实例有新增或者删除</li><li>Redis 重新分配哈希槽</li></ul></li></ul></li><li>如何实现的<ul><li>客户端给一个实例发送读写操作，但是这个实例没有相应的数据</li><li>实例发回一个MOVED响应，包括了新实例的访问地址</li></ul></li></ul></li></ul><p>Redis使用集群方案就是为了解决单个节点数据量大、写入量大产生的性能瓶颈的问题。多个节点组成一个集群，可以提高集群的性能和可靠性，但随之而来的就是集群的管理问题，最核心问题有2个：请求路由、数据迁移（扩容/缩容/数据平衡）。</p><p>1、请求路由：一般都是采用哈希槽的映射关系表找到指定节点，然后在这个节点上操作的方案。</p><p>Redis</p><p>Cluster在每个节点记录完整的映射关系(便于纠正客户端的错误路由请求)，同时也发给客户端让客户端缓存一份，便于客户端直接找到指定节点，客户端与服务端配合完成数据的路由，这需要业务在使用Redis<br> Cluster时，必须升级为集群版的SDK才支持客户端和服务端的协议交互。</p><p>其他Redis集群化方案例如Twemproxy、Codis都是中心化模式（增加Proxy层），客户端通过Proxy对整个集群进行操作，Proxy后面可以挂N多个Redis实例，Proxy层维护了路由的转发逻辑。操作Proxy就像是操作一个普通Redis一样，客户端也不需要更换SDK，而Redis<br> Cluster是把这些路由逻辑做在了SDK中。当然，增加一层Proxy也会带来一定的性能损耗。</p><p>2、数据迁移：当集群节点不足以支撑业务需求时，就需要扩容节点，扩容就意味着节点之间的数据需要做迁移，而迁移过程中是否会影响到业务，这也是判定一个集群方案是否成熟的标准。</p><p>Twemproxy不支持在线扩容，它只解决了请求路由的问题，扩容时需要停机做数据重新分配。而Redis</p><p>Cluster和Codis都做到了在线扩容（不影响业务或对业务的影响非常小），重点就是在数据迁移过程中，客户端对于正在迁移的key进行操作时，集群如何处理？还要保证响应正确的结果？</p><p>Redis</p><p>Cluster和Codis都需要服务端和客户端/Proxy层互相配合，迁移过程中，服务端针对正在迁移的key，需要让客户端或Proxy去新节点访问（重定向），这个过程就是为了保证业务在访问这些key时依旧不受影响，而且可以得到正确的结果。由于重定向的存在，所以这个期间的访问延迟会变大。等迁移完成之后，Redis</p><p>Cluster每个节点会更新路由映射表，同时也会让客户端感知到，更新客户端缓存。Codis会在Proxy层更新路由表，客户端在整个过程中无感知。</p><p>除了访问正确的节点之外，数据迁移过程中还需要解决异常情况（迁移超时、迁移失败）、性能问题（如何让数据迁移更快、bigkey如何处理），这个过程中的细节也很多。</p><p>Redis Cluster的数据迁移是同步的，迁移一个key会同时阻塞源节点和目标节点，迁移过程中会有性能问题。而Codis提供了异步迁移数据的方案，迁移速度更快，对性能影响最小，当然，实现方案也比较复杂。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>极客时间-Redis核心技术与实战</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis-Cluster&quot;&gt;&lt;a href=&quot;#Redis-Cluster&quot; class=&quot;headerlink&quot; title=&quot;Redis Cluster&quot;&gt;&lt;/a&gt;Redis Cluster&lt;/h1&gt;&lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Redis多机数据库-数据同步</title>
    <link href="https://www.llchen60.com/Redis%E5%A4%9A%E6%9C%BA%E6%95%B0%E6%8D%AE%E5%BA%93-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/"/>
    <id>https://www.llchen60.com/Redis%E5%A4%9A%E6%9C%BA%E6%95%B0%E6%8D%AE%E5%BA%93-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/</id>
    <published>2021-07-05T03:00:14.000Z</published>
    <updated>2021-07-05T03:01:36.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis多机数据库-数据同步"><a href="#Redis多机数据库-数据同步" class="headerlink" title="Redis多机数据库-数据同步"></a>Redis多机数据库-数据同步</h1><h1 id="1-Redis的高可靠性"><a href="#1-Redis的高可靠性" class="headerlink" title="1. Redis的高可靠性"></a>1. Redis的高可靠性</h1><p>Redis的高可靠性体现在两个方面：</p><ul><li>数据尽量少丢失<ul><li>AOF</li><li>RDB</li></ul></li><li>服务尽量少中断<ul><li>增加副本冗余量 — 将一份数据同时保存在多个实例上</li></ul></li></ul><h1 id="2-数据同步-—-主从库模式"><a href="#2-数据同步-—-主从库模式" class="headerlink" title="2. 数据同步 — 主从库模式"></a>2. 数据同步 — 主从库模式</h1><ul><li><p>主从库之间采用的是读写分离的方式</p><ul><li><p>读操作</p><ul><li>主库，从库都可以接收</li></ul></li><li><p>写操作</p><ul><li><p>首先到主库执行</p></li><li><p>然后主库将写操作同步给从库</p><p>  <img src="https://i.loli.net/2021/01/04/8wE4dPDgFxqRX6r.png" alt="https://i.loli.net/2021/01/04/8wE4dPDgFxqRX6r.png"></p><p>  主从读写分离</p></li></ul></li></ul></li><li><p>主从库的好处是修改操作都只会在一个库实现</p><ul><li>可以减少加锁，实例间协商这类开销</li></ul></li></ul><h2 id="2-1-主从库之间如何进行第一个同步？"><a href="#2-1-主从库之间如何进行第一个同步？" class="headerlink" title="2.1 主从库之间如何进行第一个同步？"></a>2.1 主从库之间如何进行第一个同步？</h2><ul><li><p>多个Redis实例之间通过replicaof/ slaveof命令形成主库和从库的关系，然后按照三个阶段完成数据的第一次同步：</p><p>  <img src="https://i.loli.net/2021/01/04/KZemoVB6CFjNlJI.png" alt="https://i.loli.net/2021/01/04/KZemoVB6CFjNlJI.png"></p><p>  主从首次同步过程</p></li><li><p>第一阶段</p><ul><li><p>主从库之间建立连接，协商同步</p></li><li><p>为全量复制做准备</p></li><li><p>从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复以后，主从库间就可以开始同步了</p><ul><li><p>从库给主库发送psync命令，表示要进行数据同步</p></li><li><p>主库根据这个命令的参数来启动复制</p><ul><li>psync命令包含主库的runId和复制进度的offset两个参数<ul><li>runID — Redis实例启动的时候自动随机生成的ID，用来唯一标识当前实例<ul><li>runId很关键，比如从服务器断线重连主服务器以后，会发送之前保存的主服务器的运行ID，如果ID一致，说明前后连接的是同一个主服务器，那么就可以继续尝试执行部分的重同步操作</li><li>相反，如果运行ID不同，那么就必须通过RDB完成整个重同步操作</li></ul></li><li>offset 此时设为-1，表示第一次复制</li></ul></li></ul></li><li><p>主库收到psync命令后，使用FULLRESYNC响应命令，包括了主库的runID还有主库目前的复制进度offset，返回给从库</p><ul><li>从库记录下两个参数</li></ul></li><li><p>FULLRESYNC表示第一次复制使用的是全量复制</p></li><li><p>主库与此同时执行BGSAVE命令，在后台生成一个RDB文件，并使用一个缓冲区记录从现在开始执行的所有写命令</p><p>  <img src="https://i.loli.net/2021/07/05/Kmw8ZsWuzGki25V.png" alt="主从服务器同步过程"></p></li></ul></li></ul></li><li><p>第二阶段</p><ul><li>主库将所有数据同步给从库</li><li>从库收到数据后，在本地完成数据加载 — 依赖于内存快照生成的RDB文件<ul><li>主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。</li><li>从库接收到 RDB 文件后，会先<strong>清空当前数据库</strong>，然后加载 RDB 文件。<ul><li>这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空</li></ul></li></ul></li><li>在做数据同步的过程中，主库不会被阻塞。对于这个过程中接收到的正常请求，写操作会记录在主库的<strong>Replication Buffer</strong>当中</li></ul></li><li><p>第三阶段</p><ul><li>主库会将第二阶段新收到的修改命令，再发给从库</li><li>当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了</li></ul></li></ul><h2 id="2-2-主从级联方式分担全量复制时的主库压力"><a href="#2-2-主从级联方式分担全量复制时的主库压力" class="headerlink" title="2.2 主从级联方式分担全量复制时的主库压力"></a>2.2 主从级联方式分担全量复制时的主库压力</h2><ul><li><p>现状/ 问题</p><ul><li>一次全量复制主库需要完成两个耗时操作<ul><li>生成RDB文件和传输RDB文件</li></ul></li><li>如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于 fork 子进程生成 RDB 文件，进行数据全量同步。fork 这个操作会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。</li><li>传输 RDB 文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力</li></ul></li><li><p>解决方案 — 主从从模式</p><ul><li><p>我们在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。然后，我们可以再选择一些从库（例如三分之一的从库），在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系。replicaof 所选从库的IP 6379</p><p><img src="https://i.loli.net/2021/01/04/eihQpmN6FJdRxLy.png" alt="https://i.loli.net/2021/01/04/eihQpmN6FJdRxLy.png"></p></li></ul></li></ul><h2 id="2-3-突发情况下-暂时断网-的增量复制"><a href="#2-3-突发情况下-暂时断网-的增量复制" class="headerlink" title="2.3 突发情况下(暂时断网)的增量复制"></a>2.3 突发情况下(暂时断网)的增量复制</h2><ul><li><p>旧版redis在断开重连以后从服务器会向主服务器发出SYNC命令，从头开始进行bootstrap，时间会非常长，非常低效</p></li><li><p>新版本使用PSYNC</p><ul><li>PSYNC具有full resynchronization, partial resynchronization两种模式</li><li>partial resync使得断线重连以后可以通过增量来做复制，而不是用RDB重头开始</li></ul></li><li><p>网络断了以后我们需要一种开销相对合理的复制方式，即增量复制</p><ul><li>将主从库断联期间主库收到的命令，同步给从库</li></ul></li><li><p>增量复制的时候，主从库之间依靠repl_backlog_buffer这个缓冲区来做同步</p></li><li><p>整个过程如下：</p><ul><li><p>当主从库断连后，主库会把断连期间收到的写操作命令，写入 <strong>replication buffer</strong>，同时也会把这些操作命令也写入 <strong>repl_backlog_buffer</strong> 这个缓冲区。</p></li><li><p>repl_backlog_buffer 是一个<strong>环形缓冲区</strong>，主库会记录自己写到的位置，从库则会记录自己已经读到的位置。</p></li><li><p>刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这个偏移距离的大小，对主库来说，对应的偏移量就是 master_repl_offset。主库接收的新写操作越多，这个值就会越大。</p></li><li><p>同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量 slave_repl_offset 也在不断增加。正常情况下，这两个偏移量基本相等。</p></li><li><p>主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的 slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset 之间的差距</p></li><li><p>在网络断连阶段，主库可能会收到新的写操作命令，所以，一般来说，master_repl_offset 会大于 slave_repl_offset。此时，主库只用把 master_repl_offset 和 slave_repl_offset 之间的命令操作同步给从库就行。就像刚刚示意图的中间部分，主库和从库之间相差了 put d e 和 put d f 两个操作，在增量复制时，主库只需要把它们同步给从库，就行了。</p><p><img src="https://i.loli.net/2021/01/04/w3TLhzRgOH2A65d.png" alt="https://i.loli.net/2021/01/04/w3TLhzRgOH2A65d.png"></p></li></ul></li></ul><blockquote><p>因为 repl_backlog_buffer 是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致。</p></blockquote><p>我们要想办法避免这一情况，一般而言，我们可以调整 repl_backlog_size 这个参数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：<strong>缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小</strong>。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 repl_backlog_size = 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值</p><ul><li>repl_backlog_buffer<ul><li>是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销</li><li>如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量同步，所以repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量同步的概率</li><li>而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer</li></ul></li><li>replication_buffer<ul><li>Redis和客户端通信也好，和从库通信也好，Redis都需要给分配一个 内存buffer进行数据交互</li><li>客户端是一个client，从库也是一个client，我们每个client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的</li><li>Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互。</li><li>所以主从在增量同步时，从库作为一个client，也会分配一个buffer，只不过这个buffer专门用来传播用户的写命令到从库，保证主从数据一致，我们通常把它叫做replication buffer</li><li>这个buffer需要做大小的限制<ul><li>如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个buffer就会持续增长，消耗大量的内存资源，甚至OOM</li><li>所以Redis提供了<strong>client-output-buffer-limit</strong>参数限制这个buffer的大小，如果超过限制，主库会强制断开这个client的连接，也就是说从库处理慢导致主库内存buffer的积压达到限制后，主库会强制断开从库的连接，此时主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴，这种情况需要格外注意。</li></ul></li></ul></li></ul><h2 id="2-4-主从全量同步-RDB-vs-AOF"><a href="#2-4-主从全量同步-RDB-vs-AOF" class="headerlink" title="2.4 主从全量同步 RDB vs AOF"></a>2.4 主从全量同步 RDB vs AOF</h2><p>1、RDB文件内容是<strong>经过压缩的二进制数据（不同数据类型数据做了针对性优化）</strong>，文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。在主从全量数据同步时，传输RDB文件可以尽量降低对主库机器网络带宽的消耗，从库在加载RDB文件时，一是文件小，读取整个文件的速度会很快，二是因为<strong>RDB文件存储的都是二进制数据，从库直接按照RDB协议解析还原数据即可</strong>，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量同步的成本最低。</p><p>2、假设要使用AOF做全量同步，意味着必须打开AOF功能，打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能。而RDB只有在需要定时备份和主从全量同步数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的。</p><h1 id="3-复制过程的具体实现"><a href="#3-复制过程的具体实现" class="headerlink" title="3. 复制过程的具体实现"></a>3. 复制过程的具体实现</h1><ul><li>设置主服务器的地址和端口</li><li>建立套接字连接<ul><li>从服务器根据命令所设置的IP和端口，创建连向主服务器的套接字连接</li><li>如果连接成功，会为这个套接字关联一个专门用于处理复制工作的文件事件处理器</li><li>从服务器这个时候相当于主服务器的客户端</li></ul></li><li>建立完成以后，从PING主<ul><li>检查套接字读写正常</li><li>检查主服务器能否正常处理命令请求</li></ul></li><li>身份验证<ul><li>masterauth</li></ul></li><li>验证成功以后从服务器执行命令  <code>REPLCONF listerning-port &lt;port-numer&gt;</code> 向主服务器发送从服务器的监听端口号</li><li>主服务器接收到以后，会将端口号记录在从服务器所对应的客户端状态的属性当中 <code>slave_listening_port</code></li><li>同步<ul><li>从向主发PSYNC命令，执行同步操作</li></ul></li><li>命令传播</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>极客时间</li><li>redis设计与实现</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis多机数据库-数据同步&quot;&gt;&lt;a href=&quot;#Redis多机数据库-数据同步&quot; class=&quot;headerlink&quot; title=&quot;Redis多机数据库-数据同步&quot;&gt;&lt;/a&gt;Redis多机数据库-数据同步&lt;/h1&gt;&lt;h1 id=&quot;1-Redis的高可靠性&quot;
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis持久化机制-RDB</title>
    <link href="https://www.llchen60.com/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6-RDB/"/>
    <id>https://www.llchen60.com/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6-RDB/</id>
    <published>2021-07-04T22:17:10.000Z</published>
    <updated>2021-07-04T22:18:28.838Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-AOF数据恢复存在的问题"><a href="#1-AOF数据恢复存在的问题" class="headerlink" title="1. AOF数据恢复存在的问题"></a>1. AOF数据恢复存在的问题</h1><ul><li>AOF方法每次执行记录的是操作命令，需要持久化的数据量不大</li><li>但是也因为记录的是操作命令，而不是实际数据，所以用AOF方法进行故障恢复的时候，需要逐一把操作日志都执行一遍<ul><li>如果操作日志很多，Redis的恢复就会很缓慢，可能影响到正常</li></ul></li></ul><h1 id="2-内存快照Overview"><a href="#2-内存快照Overview" class="headerlink" title="2. 内存快照Overview"></a>2. 内存快照Overview</h1><ul><li>内存快照可以解决上述的问题<ul><li>内存快照指的是记录下内存中的数据在某一时刻的状态</li><li>将某一时刻的状态以文件的形式写到磁盘上 这样即使宕机，快照文件也不会丢失，数据的可靠性也就有了保证</li><li>快照文件称为RDB文件，RDB — Redis DataBase</li></ul></li><li>RDB特征<ul><li>记录的是某一个时刻的数据，并不是操作</li><li>因此在数据恢复的时候，我们可以将RDB文件直接读入内存，很快完成恢复</li></ul></li><li>什么时候会实现RDE的载入？<ul><li>只要Redis服务器在启动的时候检测到RDB文件存在，就会自动载入RDB文件</li><li>如果服务器开启了AOF持久化功能，因为AOF文件更新频率一般比RDB高很多，所以服务器会优先使用AOF文件来还原数据库状态、</li><li>只有当AOF功能处于关闭状态的时候，服务器才会使用RDB文件来还原数据库状态</li></ul></li><li>如何工作的<ul><li>我们可以设置一系列规则，被保存在saveparams里面<ul><li>seconds</li><li>changes</li><li>—&gt; 当在xx秒里 有超过xxx更新数量的时候会触发RDB</li></ul></li><li>dirty计数器<ul><li>记录在上次成功执行了SAVE或者BGSAVE命令之后，服务器对数据库状态进行了多少次修改</li></ul></li><li>lastsave属性<ul><li>UNIX时间戳，记录了上次成功执行的时间</li></ul></li><li>Redis的serverCron函数默认每隔100ms执行一次，来维护服务器<ul><li>其中一项工作就是检查save选项设置的保存条件是否满足</li><li>如果满足就执行BGSAVE命令</li></ul></li></ul></li></ul><h2 id="2-1-给哪些数据做快照？"><a href="#2-1-给哪些数据做快照？" class="headerlink" title="2.1 给哪些数据做快照？"></a>2.1 给哪些数据做快照？</h2><ul><li>Redis的数据都在内存当中，为了提供所有数据的可靠性保证，其执行的是<strong>全量快照</strong><ul><li>即将内存中的所有数据都记录到磁盘当中</li><li>与之一起来的问题就是，当需要对内存的全量数据做快照的时候，将其全部写入磁盘会花费很多时间</li><li>而且全量数据越多，RDB文件就越大，往磁盘上写数据的时间开销就越大</li><li>而Redis的单线程模型决定了我们要尽量避免阻塞主线程的操作</li></ul></li><li>Redis生成RDB文件的命令<ul><li>save<ul><li>在主线程中执行，会导致阻塞</li><li>在服务器进程阻塞期间，服务器不能处理任何命令请求</li></ul></li><li>bgsave<ul><li>创建一个子进程，专门用于写入RDB文件，可以避免对于主线程的阻塞 — 是Redis RDB的文件生成的默认配置</li></ul></li></ul></li></ul><h2 id="2-2-做快照的时候数据是否能够被增删改？"><a href="#2-2-做快照的时候数据是否能够被增删改？" class="headerlink" title="2.2 做快照的时候数据是否能够被增删改？"></a>2.2 做快照的时候数据是否能够被增删改？</h2><ul><li><p>我们需要使系统在进行快照的时候仍然能够接受修改请求，要不然会严重影响系统的执行效率</p></li><li><p>Redis会借助操作系统提供的<strong>写时复制技术 — copy on write</strong>，在执行快照的同时，正常处理写操作</p><ul><li><p>copy on write</p><ul><li><p>copy operation is deferred until the first write,</p></li><li><p>could significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations</p><p><a href="https://en.wikipedia.org/wiki/Copy-on-write">Copy-on-write</a></p><p><img src="https://i.loli.net/2021/01/03/I8kwNqF41KlezWL.png" alt="Copy On Write的实现"></p><p>Copy on Write实现</p></li></ul></li></ul></li><li><p>例图当中键值对C发生了改变，那么bgsave子进程还会对原键值对C 进行snapshot，然后过程当中的写操作会被写到副本里面</p></li></ul><h2 id="2-3-多久做一次快照？"><a href="#2-3-多久做一次快照？" class="headerlink" title="2.3 多久做一次快照？"></a>2.3 多久做一次快照？</h2><ul><li>尽管bgsave执行时不阻塞主线程，但是频繁的执行全量快照，会带来两方面的开销<ul><li>磁盘带宽压力<ul><li>频繁将全量数据写入磁盘，会给磁盘带来很大的压力</li><li>多个快照竞争有限的磁盘贷款，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环</li></ul></li><li>fork操作的阻塞<ul><li>bgsave子进程需要通过fork操作从主线程创建出来</li><li>fork创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间就越长</li></ul></li></ul></li></ul><h2 id="2-4-RDB文件结构"><a href="#2-4-RDB文件结构" class="headerlink" title="2.4 RDB文件结构"></a>2.4 RDB文件结构</h2><ul><li>一个RDB文件分成以下几个部分<ul><li>REDIS<ul><li>用来检测载入的文件是否为RDB文件</li></ul></li><li>db_version<ul><li>记录RDB文件的版本号</li></ul></li><li>databased<ul><li>包含任意多个数据库，以及每个数据库中的键值对数据</li></ul></li><li>EOF<ul><li>1字节</li><li>标志着RDB文件正文部分的结束</li></ul></li><li>check_sum<ul><li>通过对上面四个部分的内容进行计算得出的</li><li>载入RDB文件的时候，会将载入数据所计算出来的校验和与check_sum所记录的进行对比</li></ul></li></ul></li></ul><h1 id="3-AOF和RDB混用模式"><a href="#3-AOF和RDB混用模式" class="headerlink" title="3. AOF和RDB混用模式"></a>3. AOF和RDB混用模式</h1><ul><li>为什么要混用<ul><li>AOF执行速度会比较慢</li><li>RDB的全量复制频率难以把控，太低，会容易丢失数据；太高，系统开销会很大</li></ul></li><li>如何实现的<ul><li>RDB以一定的频率来执行</li><li>在两次快照之间，使用AOF日志记录这期间所有的命令操作</li></ul></li></ul><p><img src="https://i.loli.net/2021/01/03/C98RNZ7PanDWyUr.png" alt="RDB增量快照的实现"></p><p>AOF &amp; RDB Mix</p><ul><li>如上图所示，到了第二次做全量快照的时候，就可以清空AOF日志，因为所有的操作都已经保存到了第二次的全量快照当中了</li></ul><h1 id="4-实际场景探究"><a href="#4-实际场景探究" class="headerlink" title="4. 实际场景探究"></a>4. 实际场景探究</h1><blockquote><p>我们使用一个 2 核 CPU、4GB 内存、500GB 磁盘的云主机运行 Redis，Redis 数据库的数据量大小差不多是 2GB，我们使用了 RDB 做持久化保证。当时 Redis 的运行负载以修改操作为主，写读比例差不多在 8:2 左右，也就是说，如果有 100 个请求，80 个请求执行的是修改操作。你觉得，在这个场景下，用 RDB 做持久化有什么风险吗？</p></blockquote><ul><li>内存资源风险<ul><li>Redis fork子进程做RDB持久化，由于写的比例为80%，那么在持久化过程中，“写实复制”会重新分配整个实例80%的内存副本，大约需要重新分配1.6GB内存空间，这样整个系统的内存使用接近饱和，</li><li>如果此时父进程又有大量新key写入，很快机器内存就会被吃光，如果机器开启了Swap机制，那么Redis会有一部分数据被换到磁盘上，当Redis访问这部分在磁盘上的数据时，性能会急剧下降，已经达不到高性能的标准（可以理解为武功被废）。如果机器没有开启Swap，会直接触发OOM，父子进程会面临被系统kill掉的风险。<ul><li>swap 机制<ul><li>将一块磁盘或者一个本地文件当做内存来使用<ul><li>换入<ul><li>当进程再次访问内存的时候，从磁盘读取数据到内存当中</li></ul></li><li>换出<br><a href="https://blog.csdn.net/qq_24436765/article/details/103822548">Linux系统的swap机制_囚牢-峰子的博客-CSDN博客</a><ul><li>将进程暂时不用的内存数据保存到磁盘上，再释放内存给其他进程使用</li><li>当进程再次访问内存的时候，从磁盘读取数据到内存中</li></ul></li></ul></li></ul></li></ul></li></ul></li><li>CPU资源风险<ul><li>虽然子进程在做RDB持久化，但生成RDB快照过程会消耗大量的CPU资源，</li><li>虽然Redis处理处理请求是单线程的，但Redis Server还有其他线程在后台工作，例如AOF每秒刷盘、异步关闭文件描述符这些操作。</li><li>由于机器只有2核CPU，这也就意味着父进程占用了超过一半的CPU资源，此时子进程做RDB持久化，可能会产生CPU竞争，导致的结果就是父进程处理请求延迟增大，子进程生成RDB快照的时间也会变长，整个Redis Server性能下降。</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>极客时间</li><li>Redis设计与实现</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-AOF数据恢复存在的问题&quot;&gt;&lt;a href=&quot;#1-AOF数据恢复存在的问题&quot; class=&quot;headerlink&quot; title=&quot;1. AOF数据恢复存在的问题&quot;&gt;&lt;/a&gt;1. AOF数据恢复存在的问题&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;AOF方法每次执行记录的是操
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis持久化机制-AOF</title>
    <link href="https://www.llchen60.com/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6-AOF/"/>
    <id>https://www.llchen60.com/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6-AOF/</id>
    <published>2021-07-04T19:02:48.000Z</published>
    <updated>2021-07-04T19:05:05.587Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis持久化机制-AOF"><a href="#Redis持久化机制-AOF" class="headerlink" title="Redis持久化机制-AOF"></a>Redis持久化机制-AOF</h1><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><p>Redis很大的一个应用场景就是缓存，因为速度很快，通过将后端数据库中的数据存储在内存当中，然后直接从内存中读取数据。</p><p>但是这样做的一个问题，是如果服务器宕机，内存中的数据将会全部丢失掉。对于恢复数据，我们可能的解决方案是：</p><ul><li>从后端数据库访问<ul><li>对数据库的频繁访问会给数据库造成巨大的压力</li><li>会导致应用程序响应速度变慢</li></ul></li><li>理念 - 不从后端数据库读取，实现数据的持久化<ul><li>AOF日志</li><li>RDB快照</li></ul></li></ul><h1 id="2-AOF日志的实现"><a href="#2-AOF日志的实现" class="headerlink" title="2. AOF日志的实现"></a>2. AOF日志的实现</h1><h2 id="2-1-什么是AOF"><a href="#2-1-什么是AOF" class="headerlink" title="2.1 什么是AOF"></a>2.1 什么是AOF</h2><ul><li><p>AOF - Append Only File</p></li><li><p>写后日志</p><ul><li><p>Redis先执行命令，将数据写入内存，然后才记录日志</p><p>  [写后日志]](<a href="https://i.loli.net/2020/12/29/bNUOftoVI19G8Wj.png">https://i.loli.net/2020/12/29/bNUOftoVI19G8Wj.png</a>)</p></li></ul></li></ul><h2 id="2-2-AOF记录了什么"><a href="#2-2-AOF记录了什么" class="headerlink" title="2.2 AOF记录了什么"></a>2.2 AOF记录了什么</h2><ul><li><p>传统数据库日志</p><ul><li>记录修改后的数据</li></ul></li><li><p>AOF</p><ul><li><p>写后日志</p></li><li><p>记录Redis收到的每一条指令，这些命令以文本形式保存</p></li><li><p><strong>AOF记录日志的时候，不会进行语法检查的</strong>！ 因此，如果先记录日志，再做执行的话，日志当中就有可能记录错误的命令，在使用日志恢复数据的时候，就有可能出错</p><ul><li><p>还是对于速度和保证性的tradeoff</p><p><img src="https://i.loli.net/2020/12/29/qn9adxRcv2ZSJiD.png" alt="AOF文件EG"></p><p>AOF日志范例</p></li></ul></li><li><p>写后日志可以避免出现记录错误命令的情况</p></li><li><p>而且因为是在命令执行后才记录日志，所以不会阻塞当前的写操作</p></li></ul></li></ul><h3 id="2-2-1-写后日志的风险"><a href="#2-2-1-写后日志的风险" class="headerlink" title="2.2.1 写后日志的风险"></a>2.2.1 写后日志的风险</h3><ul><li>如果刚执行完一个命令，还没有记录日志就宕机了，那么命令和相应的数据都有丢失的风险。</li><li>AOF虽然避免了对当前命令的阻塞，但是可能会给下一个操作带来阻塞风险<ul><li>因为AOF日志也是在主线程中执行，如果将日志文件写入磁盘的时候，磁盘写压力大，会导致写盘非常慢</li></ul></li></ul><p>解决方案： 需要控制写命令执行完成后AOF日志写回磁盘的时机</p><h2 id="2-3-AOF文件载入与数据还原"><a href="#2-3-AOF文件载入与数据还原" class="headerlink" title="2.3 AOF文件载入与数据还原"></a>2.3 AOF文件载入与数据还原</h2><ul><li>详细步骤<ul><li>创建一个不带网络连接的fake client<ul><li>因为Redis命令只能在客户端的上下文当中来执行</li></ul></li><li>从AOF文件当中分析并读取一条写命令</li><li>使用伪客户端来执行被读出的写命令</li><li>重复执行上述的读取和执行指令，直到处理完毕</li></ul></li></ul><h1 id="3-单点研究"><a href="#3-单点研究" class="headerlink" title="3. 单点研究"></a>3. 单点研究</h1><h2 id="3-1-写回策略"><a href="#3-1-写回策略" class="headerlink" title="3.1 写回策略"></a>3.1 写回策略</h2><ul><li>Redis的服务器进程是一个事件循环，这个循环当中的文件事件负责<ul><li>接收客户端的命令请求</li><li>向客户端发送命令回复</li></ul></li><li>时间事件负责接收客户端的命令请求</li></ul><pre><code class="jsx">def eventLoop():     while true:         // 处理文件事件，可能会有新内容追加到aof_but缓冲区        processFileEvents()        processTimeEvents()        flushAppendOnlyFile()</code></pre><ul><li><p>可用的写回策略 - AOF当中的appendfsync的三个可选值</p><ul><li><p>Always 同步写回</p><ul><li>每个写命令执行完，立刻同步将日志写回磁盘</li></ul></li><li><p>EverySec 每秒写回</p><ul><li>每个写命令执行完，只是先把日志写到<strong>AOF文件的内存缓冲区</strong>，每隔一秒将缓冲区中的内容写入磁盘</li></ul></li><li><p>No 操作系统控制的写回</p><ul><li>每个写命令执行完，只是将日志写到AOF文件的缓冲区，由操作系统决定何时将缓冲区内容写回磁盘</li></ul><p><img src="https://i.loli.net/2020/12/29/RkfhClbVDv5JKzp.png" alt="appendfsync 规定的写回策略"></p><p>写回策略对比</p></li></ul></li><li><p>写回策略的选择 — 根据对于性能和可靠性的要求，来选择选用哪一种写回策略</p><ul><li>想要获得高性能，选用No策略</li><li>想要高可靠性的保证，选用Always策略</li><li>如果允许数据有一点丢失，又希望性能不受太大的影响，选用EverySec策略</li></ul></li></ul><h2 id="3-2-如何处理过大的日志文件-—-AOF重写机制"><a href="#3-2-如何处理过大的日志文件-—-AOF重写机制" class="headerlink" title="3.2 如何处理过大的日志文件 — AOF重写机制"></a>3.2 如何处理过大的日志文件 — AOF重写机制</h2><p>日志过大会产生性能问题，主要在以下三个方面： </p><ol><li><p>文件系统本身对文件大小的限制，无法保存过大的文件 </p></li><li><p>如果文件太大，再向里面追加命令记录，效率会降低 </p></li><li><p>如果发生宕机，AOF中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程会非常缓慢，这就会影响到Redis的正常使用</p></li></ol><h3 id="3-2-1-重写可以优化日志大小的原理"><a href="#3-2-1-重写可以优化日志大小的原理" class="headerlink" title="3.2.1 重写可以优化日志大小的原理"></a>3.2.1 重写可以优化日志大小的原理</h3><ul><li><p>AOF重写机制</p><ul><li><p>重写的时候，根据数据库现状创建一个新的AOF文件</p><ul><li>读取数据库所有的键值对</li><li>针对每一个键值对用一条命令记录它的写入</li><li>需要回复的时候，直接执行这条命令</li></ul></li><li><p>重写可以使得日志文件变小，因为可以压缩多条指令到一条</p><ul><li><p>即AOF日志是用来做恢复的，我不需要记录每一步的中间状态，只要知道最终对应的key的value是多少就好</p><p>  <img src="https://i.loli.net/2020/12/29/R7fgD2tVBvZUk8z.png" alt="AOF重写E.G"></p><p>  重写原理</p></li></ul></li></ul></li></ul><h3 id="3-2-2-重写如何避免阻塞？"><a href="#3-2-2-重写如何避免阻塞？" class="headerlink" title="3.2.2 重写如何避免阻塞？"></a>3.2.2 重写如何避免阻塞？</h3><ul><li><p>AOF日志由主线程写回 — 是在执行了主操作以后，直接call的AOF的方法来进行执行的，而重写过程是由<strong>后台子进程bgrewriteaof</strong>来完成的，是为了避免阻塞主线程，导致数据库性能的下降</p></li><li><p>重写的整个流程</p><ul><li><p>一处拷贝</p><ul><li>每次执行重写的时候，主线程fork到bgrewriteaof子进程</li><li>主线程的内存会被拷贝一份到bgrewriteaof子进程当中，其中会包含数据库的最新数据</li><li>然后该子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志</li></ul></li><li><p>两处日志</p><ul><li><p>主线程当中的AOF日志</p><ul><li>但有新的操作进入，Redis会将该操作写到AOF日志缓冲区</li><li>这样即使宕机，AOF日志的操作仍齐全，可以用来做恢复</li></ul></li><li><p>AOF重写日志</p><ul><li><p>该操作同时也会被写入到重写日志的缓冲区</p></li><li><p>等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以<strong>用新的 AOF 文件替代旧文件</strong>了。</p><p><img src="https://i.loli.net/2020/12/29/6vco3pLJNDBwU52.png" alt="AOF重写缓冲"></p></li></ul></li><li><p>如何解决在AOF重写过程当中数据库新的更新导致的服务器当前的数据库状态和重写后AOF文件所保存的数据库状态不一致的问题？？</p><ul><li>AOF重写缓冲区会在服务器创建了子进程之后开始使用</li><li>当redis服务器执行一个写命令之后，它会同时将其发给AOF缓冲区还有AOF重写缓冲区</li><li>当子进程完成了AOF重写工作之后，会向父进程发出信号，父进程接收到以后，会调用一个信号处理函数，而后：<ul><li>将AOF重写缓冲区中的所有内容写入到新AOF文件里</li><li>对新的AOF文件改名，覆盖现有的AOF文件，完成新旧两个AOF文件的替换</li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="3-3-AOF-日志重写过程当中的阻塞风险"><a href="#3-3-AOF-日志重写过程当中的阻塞风险" class="headerlink" title="3.3 AOF 日志重写过程当中的阻塞风险"></a>3.3 AOF 日志重写过程当中的阻塞风险</h2><ul><li>Fork子进程的过程<ul><li>fork并不会一次性拷贝所有内存数据给子进程，采用的是操作系统提供的copy on write机制<ul><li>copy on write机制就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞的问题<ul><li>fork()之后，kernel把父进程中所有的内存页的权限都设为read-only，然后子进程的地址空间指向父进程。当父子进程都只读内存时，相安无事。当其中某个进程写内存时，CPU硬件检测到内存页是read-only的，于是触发页异常中断（page-fault），陷入kernel的一个中断例程。中断例程中，kernel就会<strong>把触发的异常的页复制一份</strong>，于是父子进程各自持有独立的一份。</li></ul></li></ul></li><li>fork子进程需要先拷贝进程必要的数据结构<ul><li>拷贝内存页表 — 即虚拟内存和物理内存的映射索引表</li><li>这个拷贝过程会消耗大量的CPU资源，并且拷贝完成之前整个进程是会阻塞的</li><li>阻塞时间取决于整个实例的内存大小<ul><li>实例越大，内存页表也越大，fork阻塞时间就会越久</li></ul></li></ul></li><li>在完成了拷贝内存页表之后，子进程和父进程指向的是相同的内存地址空间<ul><li>这个时候虽然产生了子进程，但是并没有申请和父进程相同的内存大小</li><li>真正的内存分离是<strong>在写发生的时候，这个时候才会真正拷贝内存的数据</strong></li></ul></li></ul></li><li>AOF重写过程中父进程产生写入的过程<ul><li>Fork出的子进程当前状态是指向了和父进程相同的内存地址空间，这个时候子进程就可以执行AOF重写，将内存中的所有数据写入到AOF文件里</li><li>但是同时父进程仍然会有流量写入<ul><li>如果父进程操作的是一个已经存在的key，那么这个时候父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间，这样逐渐地，父子进程内存数据开始分离</li><li>父子进程逐渐拥有各自独立的内存空间。因为内存分配是以页为单位进行分配的，默认4k，如果父进程此时操作的是一个bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险</li><li>如果操作系统开启了内存大页机制(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在Redis机器上需要关闭Huge Page机制。Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间</li></ul></li></ul></li></ul><h2 id="3-4-AOF重写日志为什么不共享AOF本身的日志？"><a href="#3-4-AOF重写日志为什么不共享AOF本身的日志？" class="headerlink" title="3.4 AOF重写日志为什么不共享AOF本身的日志？"></a>3.4 AOF重写日志为什么不共享AOF本身的日志？</h2><p>AOF重写不复用AOF本身的日志，一个原因是父子进程写同一个文件必然会产生竞争问题，控制竞争就意味着会影响父进程的性能。二是如果AOF重写过程中失败了，那么原本的AOF文件相当于被污染了，无法做恢复使用。所以Redis AOF重写一个新文件，重写失败的话，直接删除这个文件就好了，不会对原先的AOF文件产生影响。等重写完成之后，直接替换旧文件即可</p><h2 id="3-5-如何触发AOF重写？"><a href="#3-5-如何触发AOF重写？" class="headerlink" title="3.5 如何触发AOF重写？"></a>3.5 如何触发AOF重写？</h2><p>有两个配置项在控制AOF重写的触发时机：</p><ol><li>auto-aof-rewrite-min-size: 表示运行AOF重写时文件的最小大小，默认为64MB</li><li>auto-aof-rewrite-percentage: 这个值的计算方法是：当前AOF文件大小和上一次重写后AOF文件大小的差值，再除以上一次重写后AOF文件大小。也就是当前AOF文件比上一次重写后AOF文件的增量大小，和上一次重写后AOF文件大小的比值。</li></ol><p>AOF文件大小同时超出上面这两个配置项时，会触发AOF重写。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://time.geekbang.org/column/article/271754">https://time.geekbang.org/column/article/271754</a></li><li><a href="https://juejin.cn/post/6844903702373859335">https://juejin.cn/post/6844903702373859335</a> </li><li>Redis设计与实现</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis持久化机制-AOF&quot;&gt;&lt;a href=&quot;#Redis持久化机制-AOF&quot; class=&quot;headerlink&quot; title=&quot;Redis持久化机制-AOF&quot;&gt;&lt;/a&gt;Redis持久化机制-AOF&lt;/h1&gt;&lt;h1 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis IO 模型</title>
    <link href="https://www.llchen60.com/Redis-IO-%E6%A8%A1%E5%9E%8B/"/>
    <id>https://www.llchen60.com/Redis-IO-%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-07-02T18:54:57.000Z</published>
    <updated>2021-07-02T18:59:13.678Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis-IO-模型"><a href="#Redis-IO-模型" class="headerlink" title="Redis IO 模型"></a>Redis IO 模型</h1><blockquote><p>为什么单线程的Redis会那么快？</p></blockquote><h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><p>这里的单线程主要指Redis的网络IO和键值对读写是由一个线程来完成的，而Redis的其他功能，比如持久化，异步删除，集群数据同步等，是由额外的线程执行的。</p><h1 id="2-为什么要使用单线程？"><a href="#2-为什么要使用单线程？" class="headerlink" title="2. 为什么要使用单线程？"></a>2. 为什么要使用单线程？</h1><ul><li><p>使用多线程的开销</p><ul><li><p>使用多线程，一定程度上可以增加系统的吞吐率/ 拓展性</p></li><li><p>但是值得注意的是多线程本身有开销，并不是线程增多吞吐率会线性增长的。达到了某个线程数之后，系统吞吐率的增长就会开始迟缓了，有时甚至会出现下降的情况</p><p><img src="https://i.loli.net/2020/12/24/UNVLxyIRokWmjTQ.png" alt="吞吐率Metrics"></p><p>吞吐率随着线程数增长的变化</p></li><li><p>出现这种情况的原因在于</p><ul><li>系统中通常会存在被<strong>多线程同时访问的共享资源</strong> — 比如一个共享的数据结构</li><li>当有多个线程要修改这个共享资源的时候，为了保证共享资源的正确性，就需要有额外的机制进行保证。这会带来额外的开销</li></ul></li></ul></li><li><p>Redis采用单线程就是希望能够避免这种共享资源放锁的情况</p><ul><li>而且CPU往往不是Redis的瓶颈，瓶颈很可能是机器内存或者网络带宽</li></ul></li></ul><h1 id="3-单线程Redis是如何实现低延时的"><a href="#3-单线程Redis是如何实现低延时的" class="headerlink" title="3. 单线程Redis是如何实现低延时的"></a>3. 单线程Redis是如何实现低延时的</h1><ul><li>High Level Takeaway<ul><li>内存上完成操作</li><li>高效的数据结构</li><li>多路复用机制 — 网络IO能够并发处理大量的客户端请求</li></ul></li></ul><h2 id="3-1-基本IO模型和阻塞点"><a href="#3-1-基本IO模型和阻塞点" class="headerlink" title="3.1  基本IO模型和阻塞点"></a>3.1  基本IO模型和阻塞点</h2><p>以前面的SimpleKV为例，为了处理一个Get请求，数据库需要：</p><ol><li>监听客户端请求(bind/ listen)</li><li>和客户端建立连接 (accept)</li><li>从socket中读取请求(recv)</li><li>解析客户端发送请求(parse)</li><li>根据请求类型读取键值数据(get)</li><li>从客户端返回结果，即向socket中写回数据(send)</li></ol><p><img src="https://i.loli.net/2020/12/24/1pFsDaMO452fGkQ.png" alt="处理IO流程"></p><p>Get请求处理示意图</p><p>在上述的整个过程当中，如果Redis监听到客户端请求，但没有成功建立连接的时候，<strong>会阻塞在accept函数上，导致其他的客户端无法建立连接</strong>。这种基本IO模型效率会非常低，因为是阻塞式的，任何一个请求出现了任何一个问题，都会导致其他的请求无法成功完成。</p><h2 id="3-2-非阻塞模式"><a href="#3-2-非阻塞模式" class="headerlink" title="3.2 非阻塞模式"></a>3.2 非阻塞模式</h2><p>Socket网络模型的非阻塞模式体现在不同操作调用后会<strong>返回不同的套接字类型</strong>。socket() 方法会返回主动套接字，然后调用 listen() 方法，将主动套接字转化为监听套接字，此时，可以监听来自客户端的连接请求。最后，调用 accept() 方法接收到达的客户端连接，并返回已连接套接字。</p><p>这样子可以实现非阻塞，值得注意的是我们需要一些机制来监听套接字，有数据到达的时候再通知数据库线程</p><h2 id="3-3-基于多路复用的高性能I-O模型"><a href="#3-3-基于多路复用的高性能I-O模型" class="headerlink" title="3.3  基于多路复用的高性能I/O模型"></a>3.3  基于多路复用的高性能I/O模型</h2><ul><li>是什么<ul><li>指让一个线程能够处理多个IO流</li><li>select epoll机制<ul><li>在内核中，存在多个监听套接字和已连接套接字</li><li>内核会一直监听这些套接字上的连接请求或者数据请求</li></ul></li></ul></li><li>为什么使用I/O多路复用这种技术<ul><li>解决单线程下阻塞操作的问题</li></ul></li><li>如何实现的<ul><li>select epoll方法同时监控多个文件描述符FD的读写情况，当某些FD可读/ 可写的时候，该方法就会返回可读/ 写的FD个数<br><a href="https://cloud.tencent.com/developer/article/1639569">IO多路复用：Redis中经典的Reactor设计模式</a><ul><li>将用户Socket对应的FD注册进epoll，然后epoll告诉那些需要进行读写操作的socket，只处理那些活跃的，有变化的socket FD</li></ul></li></ul></li></ul><p><a href="https://draveness.me/redis-io-multiplexing/">https://draveness.me/redis-io-multiplexing/</a></p><p><a href="https://cloud.tencent.com/developer/article/1639569">https://cloud.tencent.com/developer/article/1639569</a></p><p><a href="https://blog.csdn.net/u014590757/article/details/79860766">https://blog.csdn.net/u014590757/article/details/79860766</a></p><ul><li><p>一个线程处理多个IO流 — select / epoll机制</p><p>  <img src="https://i.loli.net/2020/12/24/QUKfj9ExTgy4tMN.png" alt="select/ epoll机制流程"></p><p>  epoll机制</p><ul><li><p>允许内核中，同时存在多个监听套接字和已连接套接字</p></li><li><p>内核会一直监听这些套接字上的连接请求或数据请求</p></li><li><p>一旦有请求到达，就会交给Redis线程处理</p><p><img src="https://i.loli.net/2020/12/24/4Cp7TQMZ3csAUIm.png" alt="epoll过程"></p><p>多路复用全程</p></li></ul></li><li><p>select/ epoll 一旦检测到FD上有请求到达，就会触发相应的事件</p><ul><li>事件会被放到一个事件队列，Redis单线程对该事件队列不断进行处理</li></ul></li></ul><h2 id="3-3-单线程处理的性能瓶颈"><a href="#3-3-单线程处理的性能瓶颈" class="headerlink" title="3.3 单线程处理的性能瓶颈"></a>3.3 单线程处理的性能瓶颈</h2><p>1、任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。耗时的操作包括以下几种：</p><p>a、操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时；</p><p>b、使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据；</p><p>c、大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长；</p><p>d、淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长；</p><p>e、AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能；</p><p>f、主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久；</p><p>2、并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。</p><p>针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。</p><p>针对问题2，Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis-IO-模型&quot;&gt;&lt;a href=&quot;#Redis-IO-模型&quot; class=&quot;headerlink&quot; title=&quot;Redis IO 模型&quot;&gt;&lt;/a&gt;Redis IO 模型&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;为什么单线程的Redis会那么快？&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
      <category term="Redis" scheme="https://www.llchen60.com/tags/Redis/"/>
    
  </entry>
  
</feed>
