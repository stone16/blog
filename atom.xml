<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leilei&#39;s Blog | 磊磊的博客</title>
  
  <subtitle>Because it&#39;s there</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.llchen60.com/"/>
  <updated>2022-07-15T12:59:24.908Z</updated>
  <id>https://www.llchen60.com/</id>
  
  <author>
    <name>Leilei Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>How to use Sprint transactional annotation</title>
    <link href="https://www.llchen60.com/How-to-use-Sprint-transactional-annotation/"/>
    <id>https://www.llchen60.com/How-to-use-Sprint-transactional-annotation/</id>
    <published>2022-07-15T12:58:18.000Z</published>
    <updated>2022-07-15T12:59:24.908Z</updated>
    
    <content type="html"><![CDATA[<h1 id="How-to-use-Sprint-transactional-annotation"><a href="#How-to-use-Sprint-transactional-annotation" class="headerlink" title="How to use Sprint transactional annotation"></a>How to use Sprint transactional annotation</h1><h1 id="1-Attributes"><a href="#1-Attributes" class="headerlink" title="1. Attributes"></a>1. Attributes</h1><ul><li><code>value</code> and <code>transactionManager</code><ul><li>used to provide a TransactionManager reference to be used when handling the transaction for the annotated block</li></ul></li><li><code>propagation</code><ul><li>define how the transaction boundaries propagate to other methods that will be called either directly or indirectly from within the annotated block</li><li>default is <code>REQUIRED</code><ul><li>means a transaction is started if no transaction is already available</li><li>otherwise use the current running thread</li></ul></li></ul></li><li><code>timeout</code> and <code>timeoutString</code><ul><li>define the max number of seconds the current method is allowed to run</li></ul></li><li><code>readOnly</code><ul><li>defines if the current transaction is read-only or read-write</li></ul></li><li><code>rollbackFor</code> and <code>rollbackForClassName</code><ul><li>define one or more Throwable classes for which the current transaction will be rolled back</li><li>default is RuntimeException or an Error is thrown, but not for a checked Exception</li></ul></li><li><code>noRollbackFor</code> and <code>noRollbackForClassName</code><ul><li>used for one or more RuntimeException</li></ul></li></ul><h1 id="2-Transaction-Override"><a href="#2-Transaction-Override" class="headerlink" title="2. Transaction Override"></a>2. Transaction Override</h1><ul><li><code>addStatementReportOperation</code> is using the serializeble level, which override the class level readonly transaction</li></ul><pre><code class="java">@Service@Transactional(readOnly = true)public class OperationService &#123;    @Transactional(isolation = Isolation.SERIALIZABLE)    public boolean addStatementReportOperation(        String statementFileName,        long statementFileSize,        int statementChecksum,        OperationType reportType) &#123;        ...    &#125;&#125;</code></pre><ul><li>we use readOnly in class level because Spring could perform some read only optimization<ul><li>we could save memory when loading read only entities since the loaded state is discarded right away, and not kept for the whole duration of the currently running persistence context</li><li>also, in the cluster, read only data source could be redirected to DB replica instead of DB primary, this could also reduce the burden for primary</li></ul></li></ul><ol><li><a href="https://stackoverflow.com/questions/10394857/how-to-use-transactional-with-spring-data#:~:text=Thus%20we%20recommend%20using%20%40Transactional,re%2Ddecorated%20in%20that%20interface">https://stackoverflow.com/questions/10394857/how-to-use-transactional-with-spring-data#:~:text=Thus we recommend using %40Transactional,re-decorated in that interface</a> </li><li><a href="https://vladmihalcea.com/spring-read-only-transaction-hibernate-optimization/">https://vladmihalcea.com/spring-read-only-transaction-hibernate-optimization/</a>  </li><li><a href="https://vladmihalcea.com/spring-transactional-annotation/">https://vladmihalcea.com/spring-transactional-annotation/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;How-to-use-Sprint-transactional-annotation&quot;&gt;&lt;a href=&quot;#How-to-use-Sprint-transactional-annotation&quot; class=&quot;headerlink&quot; title=&quot;How to u
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Spring" scheme="https://www.llchen60.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>PG::DuplicatePstatement Error prepared statement axxx already exists</title>
    <link href="https://www.llchen60.com/PG-DuplicatePstatement-Error-prepared-statement-axxx-already-exists/"/>
    <id>https://www.llchen60.com/PG-DuplicatePstatement-Error-prepared-statement-axxx-already-exists/</id>
    <published>2022-07-01T14:04:04.000Z</published>
    <updated>2022-07-01T14:05:31.350Z</updated>
    
    <content type="html"><![CDATA[<p>That’s an interesting issue we faced in production during one deployment which only contain some frontend changes. </p><p>Upon check, this happens in such scenario: </p><pre><code class="java">A prepared statement is generated in postgresql, but never stored in rails. Since the code was interrupted before storing the statement, the @counter variable was never incremented even though it was used to generate a prepared statement.</code></pre><p>That pretty much described the issue, prepared statement on postgres side is a server side object that can be used to optimize performance. When the <code>PREPARE</code><br> statement is executed, the specified statement is parsed, analyzed, and rewritten. When an <code>EXECUTE</code>command is subsequently issued, the prepared statement is planned and executed.  </p><p>When the identifiers already bound to existing prepared statements but rails does not realize it, this issue will be happened. </p><p>here is the fix </p><p><a href="https://github.com/rails/rails/pull/41356/files">https://github.com/rails/rails/pull/41356/files</a></p><pre><code class="java">def next_key     &quot;a#&#123;@counter + 1&#125;&quot;end def next_key     &quot;a#&#123;@counter += 1&#125;&quot;end This change make the postgres prepared statement counter before makeing a prepared statementThus if the statemnt is aborted in rails side, app won&#39;t end up in perpetual crash state </code></pre><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://github.com/rails/rails/issues/1627">https://github.com/rails/rails/issues/1627</a></li><li><a href="https://github.com/rails/rails/pull/25827">https://github.com/rails/rails/pull/25827</a></li><li><a href="https://github.com/rails/rails/pull/17607">https://github.com/rails/rails/pull/17607</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;That’s an interesting issue we faced in production during one deployment which only contain some frontend changes. &lt;/p&gt;
&lt;p&gt;Upon check, th
      
    
    </summary>
    
    
      <category term="Ruby" scheme="https://www.llchen60.com/categories/Ruby/"/>
    
    
      <category term="Postgres" scheme="https://www.llchen60.com/tags/Postgres/"/>
    
  </entry>
  
  <entry>
    <title>Equity 101</title>
    <link href="https://www.llchen60.com/Equity-101/"/>
    <id>https://www.llchen60.com/Equity-101/</id>
    <published>2022-04-02T09:08:11.000Z</published>
    <updated>2022-04-02T09:09:25.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-What-is-equity"><a href="#1-What-is-equity" class="headerlink" title="1. What is equity?"></a>1. What is equity?</h1><ul><li>A form of compensation<ul><li>give you partial ownership in the company<ul><li>shares<ul><li>options</li><li>RSUs</li></ul></li></ul></li><li>can have seriously life changing result</li></ul></li><li>Liquidity<ul><li>the ability to sell your shares</li></ul></li></ul><h1 id="2-Cap-Table"><a href="#2-Cap-Table" class="headerlink" title="2. Cap Table"></a>2. Cap Table</h1><ul><li>A document that details who has ownership in a company</li><li>records<ul><li>how and when you’ll get your shares</li><li>how many shares you’ll get</li><li>how you’ll be paid out</li></ul></li><li>employee option pool<ul><li>investors require it, to hire talented people</li><li>let talented stay longer</li></ul></li><li>different types of equity<ul><li>preferred shares — investors</li><li>stock options — employees</li><li>advisory shares — advisors</li></ul></li></ul><h1 id="3-Equity-grants"><a href="#3-Equity-grants" class="headerlink" title="3. Equity grants"></a>3. Equity grants</h1><ul><li><p>equity grant agreements</p><ul><li>how many shares you are getting</li><li>how much each share is worth</li><li>how long it will take to vest all your shares</li></ul></li><li><p>types of equity</p><ul><li>stock option — early stage<ul><li>you have the option to buy shares, but not obligated to</li><li>only vest on a single trigger</li><li>taxed when you buy/ sell your shares</li><li>option types<ul><li>incentive stock option<ul><li>pay tax when you sell your shares</li></ul></li><li>non qualified stock option<ul><li>pay taxes when you buy and sell shares</li></ul></li></ul></li></ul></li><li>restricted stock units - large startup<ul><li>you receive shares that you own automatically</li><li>often vest on a double trigger</li><li>taxed when get vested</li></ul></li></ul></li></ul><h1 id="4-How-vesting-work"><a href="#4-How-vesting-work" class="headerlink" title="4. How vesting work"></a>4. How vesting work</h1><ul><li>Earning the right to buy your shares<ul><li>when you are taxed</li><li>when you are able to buy shares</li></ul></li><li>vesting structures<ul><li>time based vesting</li><li>event based vesting</li><li>hybrid vesting</li></ul></li><li>option outstanding<ul><li>options has not been granted</li></ul></li></ul><h1 id="5-How-exercising-equity-works"><a href="#5-How-exercising-equity-works" class="headerlink" title="5. How exercising equity works"></a>5. How exercising equity works</h1><ul><li>Being granted stock options doesn’t mean you’re automatically a shareholder in a company.</li><li>Stock option not means share, you need to buy it</li><li>exercise<ul><li>means you purchase stock at a specified price</li><li>purchase your options</li><li>then company give stock certificate</li></ul></li><li>strike price<ul><li>option price you could leverage on to buy stocks</li><li>fixed price</li></ul></li><li>409A valuation<ul><li>an analysis of your company’s financials, which determines the value of the company’s shares</li><li>usually done every 12 months</li><li>Fair Market Value - FMV<ul><li>the value of one share of common stock</li></ul></li></ul></li><li>Spread/ Gain<ul><li>the difference between stock price and strike price</li></ul></li><li>PTEP - post termination exercise period<ul><li>the amount of time you have to exercise your options after you leave the company</li></ul></li><li>early exercise<ul><li>company let you buy everything today</li><li>then you just need to pay taxes at strike price</li><li>83b election<ul><li>a form sent to IRS</li><li>you want to pay tax now</li></ul></li></ul></li></ul><h1 id="6-How-to-sell-shares"><a href="#6-How-to-sell-shares" class="headerlink" title="6. How to sell shares"></a>6. How to sell shares</h1><ul><li>How and when can you sell your shares<ul><li>think about<ul><li>any restrictions on selling?</li><li>do you need the cash?</li><li>is the value increasing?</li><li>only chance to sell?</li></ul></li></ul></li><li>ways<ul><li>public company<ul><li>public shares pretty liquid,<ul><li>as it’s easy to convert them into cash at market price</li></ul></li></ul></li><li>private company<ul><li>strongly restrict sell usually</li><li>ways<ul><li>IPO<ul><li>usually 6 month lock up period after going public</li></ul></li><li>M&amp;A<ul><li>bought or merge   acquisition</li><li>another company buys your shares<ul><li>pay for cash or other stocks</li></ul></li></ul></li><li>tender offer<ul><li>company buy from employee</li><li>could happen while the company still private</li></ul></li></ul></li></ul></li></ul></li></ul><h1 id="7-How-are-equity-grants-taxed"><a href="#7-How-are-equity-grants-taxed" class="headerlink" title="7. How are equity grants taxed?"></a>7. How are equity grants taxed?</h1><ul><li>ISO shares<ul><li>when sell it, tax rate come to be different</li></ul></li><li>qualifying disposition<ul><li>1 year after exercise</li><li>2 years after option grant date</li></ul></li><li>long term capital gains tax<ul><li>0% 0 20%</li></ul></li><li>short term capital gains<ul><li>taxed at ordinary income rates</li></ul></li><li>AMT<ul><li>alternative minimum tax</li><li>a low threshold, or floor, defining the minimum amount of taxes you are obligated to pay</li></ul></li></ul><h1 id="8-Valuations"><a href="#8-Valuations" class="headerlink" title="8. Valuations"></a>8. Valuations</h1><ul><li>dilution<ul><li>affect the percentage you have</li></ul></li><li>how valuation works<ul><li>term sheet<ul><li>a legal document defining the terms of an investment</li></ul></li><li>valuation<ul><li>an appraisal of a company’s worth</li><li>affect<ul><li>how many shares</li><li>price per shares</li><li>percentage</li></ul></li></ul></li></ul></li><li>stage<ul><li>pre money valuation<ul><li>post money matters you</li><li>the employee stock option pool—that percentage of the company that gets carved out for new hires and new employees? Well, if you think about how dilution works, when an option pool gets carved out, it reduces the ownership percentage of all of the other shareholders. So that’s why a lot of the time, when an investor puts money into a company, they’ll require that this option pool gets sliced out of the company on a pre-money basis.</li></ul></li><li>investment</li><li>post money valuation</li></ul></li><li>how valuations are calculated<ul><li>valuation for startup come to be complex,<ul><li>revenue, profit, tax, etc.</li><li>growth rate</li><li>user count</li><li>industry</li><li>founding team</li></ul></li><li>section 409A<ul><li>let 3rd party help do valuation for the startup</li><li>every 12 months</li><li>or every time do a new fund raising</li></ul></li></ul></li></ul><h1 id="9-Who-can-invest-in-startups"><a href="#9-Who-can-invest-in-startups" class="headerlink" title="9. Who can invest in startups"></a>9. Who can invest in startups</h1><ul><li>For private company<ul><li>no need to release any info to public</li><li>less info and more risk comparing with public company investment</li></ul></li><li>who can invest in a startup<ul><li>accredited investor<ul><li>more relevant to individuals</li><li>criteria (either one of them)<ul><li>net worth  over a million, not include the primary living place;</li><li>income  net income 200k per year for last 2 years. reasonable expectation in following year; for a family is 300k</li><li>passing a financial exam, hold a license</li></ul></li></ul></li><li>qualified purchaser<ul><li>more relevant for funds, institution,</li><li>all about investment, purchasing power</li><li>an entity that has a significant amount of investment capital — their purchasing power</li></ul></li></ul></li></ul><h1 id="10-How-venture-capital-works"><a href="#10-How-venture-capital-works" class="headerlink" title="10. How venture capital works"></a>10. How venture capital works</h1><ul><li>venture capital system<ul><li>equity financing<ul><li>angel investors</li><li>venture capital fund<ul><li>invest other people’s money through fund</li></ul></li></ul></li><li>debt financing</li></ul></li><li>type of fundraising instruments<ul><li>convertible instrument<ul><li>investor iou, convert the note into shares, owns equity in the future<ul><li>control - startup not lose control in this way</li></ul></li><li>used for very early stage of startups</li></ul></li><li>priced round<ul><li>used at later stages, when the company is growing</li></ul></li></ul></li><li>type of investors<ul><li>accredited investor - angel investor</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://carta.com/equity/learn/what-is-equity/">https://carta.com/equity/learn/what-is-equity/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-What-is-equity&quot;&gt;&lt;a href=&quot;#1-What-is-equity&quot; class=&quot;headerlink&quot; title=&quot;1. What is equity?&quot;&gt;&lt;/a&gt;1. What is equity?&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;A fo
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="Equity" scheme="https://www.llchen60.com/tags/Equity/"/>
    
      <category term="RSU" scheme="https://www.llchen60.com/tags/RSU/"/>
    
  </entry>
  
  <entry>
    <title>System.nanoTime vs System.currentTimeMillis</title>
    <link href="https://www.llchen60.com/System-nanoTime-vs-System-currentTimeMillis/"/>
    <id>https://www.llchen60.com/System-nanoTime-vs-System-currentTimeMillis/</id>
    <published>2022-03-28T10:13:34.000Z</published>
    <updated>2022-03-28T10:14:08.401Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Monotonic-Clock"><a href="#1-Monotonic-Clock" class="headerlink" title="1. Monotonic Clock"></a>1. Monotonic Clock</h1><ul><li>suitable for measuring a duration, such as a timeout or a service’s response time<ul><li><code>clock_gettime(CLOCK_MONOTONIC)</code></li><li><code>System.nanoTime()</code></li></ul></li><li>They are guaranteed to always move forward</li><li>However, the <em>absolute</em> value of the clock is meaningless: it might be the number of nanoseconds since the computer was started, or something similarly arbitrary. In particular, it makes no sense to compare monotonic clock values from two different computers, because they don’t mean the same thing.<ul><li>On a server with multiple CPU sockets, there may be a separate timer per CPU, which is not necessarily synchronized with other CPUs</li></ul></li></ul><h1 id="2-Time-of-day-clock"><a href="#2-Time-of-day-clock" class="headerlink" title="2. Time of day clock"></a>2. Time of day clock</h1><ul><li>It returns the current date and time according to some calendar<ul><li><code>clock_gettime(CLOCK_REALTIME)</code></li><li><code>System.currentTimeMillis()</code><ul><li>return the number of seconds since the epoch</li></ul></li></ul></li><li>this time is synchronized with NTP, which means a timestamp from one machine ideally means the same as a timestamp on another machine</li><li>Oddies<ul><li>In particular, if the local clock is too far ahead of the NTP server, it may be forcibly reset and appear to jump back to a previous point in time. These jumps, as well as similar jumps caused by leap seconds, make time-of-day clocks unsuitable for measuring elapsed time</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Monotonic-Clock&quot;&gt;&lt;a href=&quot;#1-Monotonic-Clock&quot; class=&quot;headerlink&quot; title=&quot;1. Monotonic Clock&quot;&gt;&lt;/a&gt;1. Monotonic Clock&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;su
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Java" scheme="https://www.llchen60.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Transaction Understanding</title>
    <link href="https://www.llchen60.com/Transaction-Understanding/"/>
    <id>https://www.llchen60.com/Transaction-Understanding/</id>
    <published>2022-03-25T03:16:10.000Z</published>
    <updated>2022-03-25T13:52:19.167Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transaction-Understanding"><a href="#Transaction-Understanding" class="headerlink" title="Transaction Understanding"></a>Transaction Understanding</h1><h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><h2 id="1-1-Things-can-go-wrong"><a href="#1-1-Things-can-go-wrong" class="headerlink" title="1.1 Things can go wrong"></a>1.1 Things can go wrong</h2><ul><li>db software or hardware could fail at any time</li><li>application could crash at any time</li><li>interruptions in the network can unexpectedly cut off the application from the db, or one db node from another</li><li>several clients may write to the db at the same time, overwriting each other’s change</li><li>race conditions between clients can cause surprising bugs</li></ul><h2 id="1-2-Transaction"><a href="#1-2-Transaction" class="headerlink" title="1.2 Transaction"></a>1.2 Transaction</h2><ul><li>Transaction is the mechanism for us to simplify those issues<ul><li>Group several reads and writes together is a logical unit</li><li>either the entire transaction succeeds or it fails<ul><li>application can safely retry if it fails</li></ul></li></ul></li><li>Transaction makes error handling much more easier<ul><li>no need to worry about partial failure</li></ul></li><li>Transaction is created with a purpose, to simplify the programming model for applications accessing a database<ul><li>database take care of such issue</li></ul></li><li>for transaction, we need to understand<ul><li>what safety guarantees transactions can provide</li><li>what costs are associated with them</li></ul></li></ul><h1 id="2-ACID"><a href="#2-ACID" class="headerlink" title="2. ACID"></a>2. ACID</h1><ul><li>ACID are the safety guarantees provided by transactions<ul><li>Atomicity</li><li>Consistency</li><li>Isolation</li><li>Durability</li></ul></li></ul><h2 id="2-1-Atomicity"><a href="#2-1-Atomicity" class="headerlink" title="2.1 Atomicity"></a>2.1 Atomicity</h2><aside>💡 The ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity</aside><ul><li>Atomic means sth <strong>cannot be</strong> broken down into smaller parts</li><li>Describes what happens if a client want to make several writes, but a fault occurs after some of the writes have been processed<ul><li>if the writes are grouped together into an atomic transaction</li><li>the transaction cannot be completed due to a fault,</li><li>then the transaction is aborted</li><li>the database must discard or undo any writes it has made so far in that transaction</li></ul></li><li>The ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity</li></ul><h2 id="2-2-Consistency"><a href="#2-2-Consistency" class="headerlink" title="2.2 Consistency"></a>2.2 Consistency</h2><ul><li>Consistency refers to an application specific notion of the database being in a good state</li><li>The idea of ACID consistency is that you have certain statements about your data (<em>invariants</em>) that must always be true—for example, in an accounting system, credits and debits across all accounts must always be balanced</li><li>But this ususally is application code responsibility to define what data is valid or invalid</li></ul><h2 id="2-3-Isolation"><a href="#2-3-Isolation" class="headerlink" title="2.3 Isolation"></a>2.3 Isolation</h2><ul><li>Isolation means that concurrently executing transactions are isolated from each other: they cannot step on each other’s toe</li></ul><h2 id="2-4-Durability"><a href="#2-4-Durability" class="headerlink" title="2.4 Durability"></a>2.4 Durability</h2><ul><li>Purpose of a db system is to provide a safe place where data can be stored without fear of losing it</li><li><em>Durability</em> is the promise that once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes.</li><li>In a single node database<ul><li>means data has been written to nonvolatile storage such as a hard drive or SSD</li><li>could also involves a write ahead log</li></ul></li><li>In a replicated db,<ul><li>means data has been successfully copied to some number of nodes</li></ul></li></ul><h1 id="3-Single-and-Multi-Objects-Operations-Atomicity-Isolation"><a href="#3-Single-and-Multi-Objects-Operations-Atomicity-Isolation" class="headerlink" title="3. Single and Multi Objects Operations - Atomicity + Isolation"></a>3. Single and Multi Objects Operations - Atomicity + Isolation</h1><h2 id="3-1-Multi-Object-Transactions"><a href="#3-1-Multi-Object-Transactions" class="headerlink" title="3.1 Multi Object Transactions"></a>3.1 Multi Object Transactions</h2><h3 id="3-1-1-Why-we-need-it"><a href="#3-1-1-Why-we-need-it" class="headerlink" title="3.1.1 Why we need it"></a>3.1.1 Why we need it</h3><ul><li>Need some way to determine which read and write operations belong to the same transaction<ul><li>In relational db, typically done based on the <strong>client’s TCP connection to the database server</strong></li><li>on any particular connection, everything between a <strong>BEGIN TRANSACTION and a COMMIT statement</strong> is considered to be part of the same transaction</li></ul></li></ul><h3 id="3-1-2-Scenarios"><a href="#3-1-2-Scenarios" class="headerlink" title="3.1.2 Scenarios"></a>3.1.2 Scenarios</h3><ul><li>In a relational data model, a row in one table often has a foreign key reference to a row in another table<ul><li>multi object transactions allow you to ensure that these references remain valid</li></ul></li><li>In a document data model, the fields that need to be updated together are often <strong>within the same document</strong>, which is treated as a single object—no multi-object transactions are needed when updating a single document. However, document databases lacking join functionality also encourage denormalization. When denormalized information needs to be updated, you need to update several documents in one go. Transactions are very useful in this situation to prevent denormalized data from going out of sync</li><li>In db with secondary indexes, indexes also need to be updated every time you change a value<ul><li>These indexes are different database objects from a transaction point of view: for example, without transaction isolation, it’s possible for a record to appear in one index but not another, because the update to the second index hasn’t happened yet.</li></ul></li></ul><h2 id="3-2-Single-Object-Writes"><a href="#3-2-Single-Object-Writes" class="headerlink" title="3.2 Single Object Writes"></a>3.2 Single Object Writes</h2><ul><li>Atomicity and isolation also apply when a single object is being changed<ul><li>all storage engines universally aim to provide atomicity and isolation on the level of a single object on one node</li></ul></li><li>Atomicity<ul><li>Using a log for crash recovery</li><li>increment operation</li><li>compare and set</li></ul></li><li>Isolation<ul><li>Using a lock on each object</li></ul></li></ul><h2 id="3-3-Handling-errors-and-aborts"><a href="#3-3-Handling-errors-and-aborts" class="headerlink" title="3.3 Handling errors and aborts"></a>3.3 Handling errors and aborts</h2><ul><li>ACID DB Philosophy</li></ul><aside>💡 if the database is in danger of violating its guarantee of atomicity, isolation, or durability, it would rather abandon the transaction entirely than allow it to remain half-finished.</aside><ul><li>Aborts of transaction point is to safely retry, thus we should have some retry mechanism build for such scenario</li></ul><h1 id="4-Isolation-Levels"><a href="#4-Isolation-Levels" class="headerlink" title="4. Isolation Levels"></a>4. Isolation Levels</h1><p>Concurrency bugs are hard to find by testing, cause they are rare, and difficult to reproduce. For such reasons, databases have long tried to hide concurrency issues from application developers by providing transaction isolation </p><p>In theory, isolation should make your life easier by letting you pretend that no concurrency is happening: <em>serializable</em> isolation means that the database guarantees that transactions have the same effect as if they ran <em>serially</em></p><p>In practice, isolation has a performance cost, and many databased don’t want to pay that price. Thus it’s common for systems to use weaker levels of isolation, which protect against some concurrency issues, but not all </p><h2 id="4-1-Read-Committed"><a href="#4-1-Read-Committed" class="headerlink" title="4.1 Read Committed"></a>4.1 Read Committed</h2><h3 id="4-1-1-Guarantees"><a href="#4-1-1-Guarantees" class="headerlink" title="4.1.1 Guarantees"></a>4.1.1 Guarantees</h3><ul><li>when reading from the database, you will only see data that has been committed(no dirty reads)</li><li>when writing to the database, you will only overwrite data that has been committed (no dirty writes)</li></ul><h3 id="4-1-2-No-dirty-reads-Explanation"><a href="#4-1-2-No-dirty-reads-Explanation" class="headerlink" title="4.1.2 No dirty reads Explanation"></a>4.1.2 No dirty reads Explanation</h3><p><img src="https://s2.loli.net/2022/03/25/IPV8vCO1M93YxZT.png" alt=""></p><ul><li><p>Prevent dirty read, only committed record could be seen</p></li><li><p>reasons for dirty read prevention</p><ul><li>See the database in a partially updated state is confusing to users and may cause other transactions to take incorrect decisions</li><li>If a transaction aborts, any writes it has made need to be rolled back (like in <a href="https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/ch07.html#fig_transactions_atomicity">Figure 7-3</a>). If the database allows dirty reads, that means a transaction may see data that is later rolled back—i.e., which is never actually committed to the database. Reasoning about the consequences quickly becomes mind-bending.</li></ul></li></ul><h3 id="4-1-3-No-Dirty-Write-Explanation"><a href="#4-1-3-No-Dirty-Write-Explanation" class="headerlink" title="4.1.3 No Dirty Write Explanation"></a>4.1.3 No Dirty Write Explanation</h3><ul><li>If two transactions concurrently try to update the same object in a db, we normally assume that the later write overwrites the earlier write</li><li>Dirty write happen when<ul><li>the earlier write is part of a transaction that has not yet committed, so the later write overwrites an uncommitted value</li></ul></li><li>how does committed isolation level work?<ul><li>by delaying the second write until the first write’s transaction has committed or aborted</li></ul></li></ul><p><img src="https://s2.loli.net/2022/03/25/3Z6NycPtjEGU9dB.png" alt=""></p><h3 id="4-1-4-Implementation"><a href="#4-1-4-Implementation" class="headerlink" title="4.1.4 Implementation"></a>4.1.4 Implementation</h3><ul><li><p>Dirty writes — Row level lock</p><ul><li>when a transaction wants to modify a particular object (row or document), it must first acquire a lock on that object.</li><li>It must then hold that lock until the transaction is committed or aborted.</li><li>Only one transaction can hold the lock for any given object;</li><li>if another transaction wants to write to the same object, it must wait until the first transaction is committed or aborted before it can acquire the lock and continue.</li><li>This locking is done automatically by databases in <strong>read committed mode (or stronger isolation levels)</strong>.</li></ul></li><li><p>Dirty Reads —</p><ul><li><p>still use row level lock</p><ul><li>the approach of requiring read locks does not work well in practice, because one long-running write transaction can force many other transactions to wait until the long-running transaction has completed, even if the other transactions only read and do not write anything to the database</li></ul></li><li><p>This harms the response time of read-only transactions and is bad for operability: a slowdown in one part of an application can have a knock-on effect in a completely different part of the application, due to waiting for locks.</p></li><li><p>most databases prevent dirty reads using the approach illustrated  here</p><p>  <img src="https://s2.loli.net/2022/03/25/IPV8vCO1M93YxZT.png" alt=""></p></li><li><p>for every object that is written, the database remembers <strong>both the old committed value and the new value</strong> set by the transaction that currently holds the write lock.</p></li><li><p>While the transaction is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions switch over to reading the new value.</p></li></ul></li></ul><h2 id="4-2-Snapshot-Isolation-and-Repeatable-Read"><a href="#4-2-Snapshot-Isolation-and-Repeatable-Read" class="headerlink" title="4.2 Snapshot Isolation and Repeatable Read"></a>4.2 Snapshot Isolation and Repeatable Read</h2><h3 id="4-2-1-Issue-with-Read-Committed"><a href="#4-2-1-Issue-with-Read-Committed" class="headerlink" title="4.2.1 Issue with Read Committed"></a>4.2.1 Issue with Read Committed</h3><p><img src="https://s2.loli.net/2022/03/25/1rRcBlD7Funjztg.png" alt=""></p><ul><li>Read committed can still have concurrency bug<ul><li>In the image above, there will be certain time the amount of 2 accounts are not equal to 1000</li><li>called read skew — nonrepeatable read<ul><li>there will be temporary inconsistency</li><li><strong>read the committed data by another transaction</strong></li></ul></li></ul></li><li>But there are certain situations, that cannot tolerate such temporary inconsistency<ul><li>Backup<ul><li>Taking a backup requires making a copy of the entire database, which may take hours on a large database. During the time that the backup process is running, writes will continue to be made to the database. Thus, you could end up with some parts of the backup containing an older version of the data, and other parts containing a newer version. If you need to restore from such a backup, the inconsistencies (such as disappearing money) become permanent.</li></ul></li><li>Analytic queries and integrity checks<ul><li>nonsensical results will be returned if the database is at different points in time</li></ul></li></ul></li></ul><h3 id="4-2-2-Solution-Snapshot-isolation"><a href="#4-2-2-Solution-Snapshot-isolation" class="headerlink" title="4.2.2 Solution: Snapshot isolation"></a>4.2.2 Solution: Snapshot isolation</h3><ul><li>Each transaction reads from a consistent snapshot of the database,<ul><li>the transaction sees all the data that was committed in the db at the start of the transaction</li></ul></li></ul><h3 id="4-2-3-Snapshot-Isolation-Implementation"><a href="#4-2-3-Snapshot-Isolation-Implementation" class="headerlink" title="4.2.3 Snapshot Isolation Implementation"></a>4.2.3 Snapshot Isolation Implementation</h3><ul><li><p>Use write locks to prevent dirty writes</p><ul><li>a transaction that makes a write can block the progress of another transaction that writes to the same object</li></ul></li><li><p>reads do not require any locks.</p><ul><li>From a performance point of view, a key principle of snapshot isolation is <em>readers never block writers, and writers never block readers</em></li><li>This allows a database to handle long-running read queries on a consistent snapshot at the same time as processing writes normally, without any lock contention between the two.</li></ul></li><li><p>To implement snapshot isolation, db uses a generalization of the mechanism</p><ul><li>db must potentially keep several different committed versions of an object, because various in progress transactions may need to see the state of the db at different points in time</li><li>— named as multi version concurrency control — MVCC</li></ul></li><li><p>If a database only needed to provide read committed isolation, but not snapshot isolation, it would be sufficient to keep two versions of an object:</p><ul><li>the committed version and the overwritten-but-not-yet-committed version.</li></ul></li><li><p>However, storage engines that support snapshot isolation typically use MVCC for their read committed isolation level as well.</p><ul><li><strong>A typical approach is that read committed uses a separate snapshot for each query, while snapshot isolation uses the same snapshot for an entire transaction.</strong></li></ul></li></ul><h3 id="4-2-4-Visibility-rules-for-observing-a-consistent-snapshot"><a href="#4-2-4-Visibility-rules-for-observing-a-consistent-snapshot" class="headerlink" title="4.2.4 Visibility rules for observing a consistent snapshot"></a>4.2.4 Visibility rules for observing a consistent snapshot</h3><ul><li>Transaction IDs are used to decide which objects it can see and which are invisible</li><li>Rules<ol><li>At the start of each transaction, the database makes a list of all the other transactions that are in progress (not yet committed or aborted) at that time. Any writes that those transactions have made are ignored, even if the transactions subsequently commit.</li><li>Any writes made by aborted transactions are ignored.</li><li>Any writes made by transactions with a later transaction ID (i.e., which started after the current transaction started) are ignored, regardless of whether those transactions have committed.</li><li>All other writes are visible to the application’s queries.</li></ol></li></ul><h2 id="4-3-Preventing-Lost-Updates"><a href="#4-3-Preventing-Lost-Updates" class="headerlink" title="4.3 Preventing Lost Updates"></a>4.3 Preventing Lost Updates</h2><h3 id="4-3-1-Issues"><a href="#4-3-1-Issues" class="headerlink" title="4.3.1 Issues"></a>4.3.1 Issues</h3><p>The lost update problem can occur if an application reads some value from the database, modifies it, and writes back the modified value (a <em>read-modify-write cycle</em>). <strong>If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification.</strong> (We sometimes say that the later write <em>clobbers</em> the earlier write.) This pattern occurs in various different scenarios: </p><ul><li>Incrementing a counter or updating an account balance (requires reading the current value, calculating the new value, and writing back the updated value)</li><li>Making a local change to a complex value, e.g., adding an element to a list within a JSON document (requires parsing the document, making the change, and writing back the modified document)</li><li>Two users editing a wiki page at the same time, where each user saves their changes by sending the entire page contents to the server, overwriting whatever is currently in the database</li></ul><h3 id="4-3-2-Solution-1-Atomic-Write-Operations"><a href="#4-3-2-Solution-1-Atomic-Write-Operations" class="headerlink" title="4.3.2 Solution 1: Atomic Write Operations"></a>4.3.2 Solution 1: Atomic Write Operations</h3><ul><li>Using atomic update provided by database,</li></ul><pre><code class="ruby">UPDATE counters SET value = value + 1 WHERE key = &#39;foo&#39;;</code></pre><ul><li>Atomic operations are usually implemented by taking an <strong>exclusive lock on the object when it is read</strong> so that no other transaction can read it until the update has been applied</li><li>Also, we could force all atomic operations to be executed on a single thread</li></ul><h3 id="4-3-3-Solution-2-Explicit-Locking"><a href="#4-3-3-Solution-2-Explicit-Locking" class="headerlink" title="4.3.3 Solution 2: Explicit Locking"></a>4.3.3 Solution 2: Explicit Locking</h3><ul><li>Explicitly lock objects that are going to be updated<ul><li>then the application can perform a read modify write cycle</li><li>if any other transaction tries to concurrently read the same object, it is forced to wait until the first read-modify-write cycle has completed</li></ul></li></ul><pre><code class="ruby">BEGIN TRANSACTION;SELECT * FROM figures  WHERE name = &#39;robot&#39; AND game_id = 222// For Update will let databse take a lock o  FOR UPDATE; 1-- Check whether move is valid, then update the position-- of the piece that was returned by the previous SELECT.UPDATE figures SET position = &#39;c4&#39; WHERE id = 1234;COMMIT;</code></pre><h3 id="4-3-4-Solution-3-Automatically-detecting-lost-updates"><a href="#4-3-4-Solution-3-Automatically-detecting-lost-updates" class="headerlink" title="4.3.4 Solution 3: Automatically detecting lost updates"></a>4.3.4 Solution 3: Automatically detecting lost updates</h3><ul><li>Atomic operations and locks are ways of preventing lost updates by forcing the read-modify-write cycles to happen sequentially.</li><li>An alternative is to allow them to <strong>execute in parallel</strong> and, if the transaction manager <strong>detects a lost update, abort the transaction</strong> and <strong>force it to retry its read-modify-write cycle</strong>.</li><li>Lost update detection is great because it doesn’t require application code to use any special database features, you could forget to use a lock or an atomic operation, but lost update detection happens automatically and thus less error prone</li></ul><h3 id="4-3-5-Solution-4-Compare-and-set"><a href="#4-3-5-Solution-4-Compare-and-set" class="headerlink" title="4.3.5 Solution 4: Compare and set"></a>4.3.5 Solution 4: Compare and set</h3><ul><li>Avoid lost updates by allowing an update to happen only if the value has not changed since you last read it</li></ul><pre><code class="ruby">UPDATE wiki_pages SET content = &#39;new content&#39;  WHERE id = 1234 AND content = &#39;old content&#39;;</code></pre><ul><li>But notice the compare and set operation is possible to be unsafe if db by default read from the old snapshot</li></ul><h3 id="4-3-6-Solution-5-Conflict-resolution-and-replication"><a href="#4-3-6-Solution-5-Conflict-resolution-and-replication" class="headerlink" title="4.3.6 Solution 5: Conflict resolution and replication"></a>4.3.6 Solution 5: Conflict resolution and replication</h3><p>In multi leader or leaderless replication system, a common approach in such replicated databases is to allow concurrent writes to create several conflicting versions of a value (also known as <em>siblings</em>), and to use application code or special data structures to resolve and merge these versions after the fact.</p><h2 id="4-4-Write-Skew-and-Phantoms"><a href="#4-4-Write-Skew-and-Phantoms" class="headerlink" title="4.4 Write Skew and Phantoms"></a>4.4 Write Skew and Phantoms</h2><p><img src="https://s2.loli.net/2022/03/25/pFhAQWIU8O4bEZY.png" alt=""></p><ul><li>In each transaction, we first check that two or more doctors are currently on call,<ul><li>since db is using snapshot isolation, both checks return 2, so both of them proceed to the next stage</li></ul></li><li>Write Skew<ul><li>it’s neither a dirty write nor a lost update, because the two transactions are updating two different objects</li><li>But it’s a race condition as the anomalous behavior was only possible because the transactions ran concurrently</li></ul></li><li>You can think of write skew as a generalization of the lost update problem.<ul><li>Write skew can occur if <strong>two transactions read the same objects</strong>, and <strong>then update some of those objects (different transactions may update different objects).</strong></li><li>In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing).</li></ul></li></ul><h3 id="4-4-2-Solution"><a href="#4-4-2-Solution" class="headerlink" title="4.4.2 Solution"></a>4.4.2 Solution</h3><ul><li>We need to explicitly lock the rows as solution in isolation snapshot works for one object, now the case come to be multiple objects, so that comes to be different</li></ul><pre><code class="ruby">BEGIN TRANSACTION;SELECT * FROM doctors  WHERE on_call = true  AND shift_id = 1234 FOR UPDATE; 1UPDATE doctors  SET on_call = false  WHERE name = &#39;Alice&#39;  AND shift_id = 1234;COMMIT;</code></pre><h3 id="4-4-3-Phantoms"><a href="#4-4-3-Phantoms" class="headerlink" title="4.4.3 Phantoms"></a>4.4.3 Phantoms</h3><p>All of these examples follow a similar pattern:</p><ol><li><p>A <code>SELECT</code> query checks whether some requirement is satisfied by searching for rows that match some search condition (there are at least two doctors on call, there are no existing bookings for that room at that time, the position on the board doesn’t already have another figure on it, the username isn’t already taken, there is still money in the account).</p></li><li><p>Depending on the result of the first query, the application code decides how to continue (perhaps to go ahead with the operation, or perhaps to report an error to the user and abort).</p></li><li><p>If the application decides to go ahead, it makes a write (<code>INSERT</code>, <code>UPDATE</code>, or <code>DELETE</code>) to the database and commits the transaction.</p><p> The effect of this write changes the precondition of the decision of step 2. In other words, if you were to repeat the <code>SELECT</code> query from step 1 after committing the write, you would get a different result, because the write changed the set of rows matching the search condition (there is now one fewer doctor on call, the meeting room is now booked for that time, the position on the board is now taken by the figure that was moved, the username is now taken, there is now less money in the account).</p></li></ol><p>The steps may occur in a different order. For example, you could first make the write, then the <code>SELECT</code> query, and finally decide whether to abort or commit based on the result of the query.</p><p>In the case of the doctor on call example, the row being modified in step 3 was one of the rows returned in step 1, so we could make the transaction safe and avoid write skew by locking the rows in step 1 (<code>SELECT FOR UPDATE</code>). However, the other four examples are different: they check for the <em>absence</em> of rows matching some search condition, and the write <em>adds</em> a row matching the same condition. If the query in step 1 doesn’t return any rows, <code>SELECT FOR UPDATE</code> can’t attach locks to anything.</p><p>This effect, where a write in one transaction changes the result of a search query in another transaction, is called a <em>phantom</em> [<a href="https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/ch07.html#Eswaran1976uu">3</a>]. Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions like the examples we discussed, phantoms can lead to particularly tricky cases of write skew.</p><h3 id="4-4-4-Materializing-conflicts-for-Phantoms"><a href="#4-4-4-Materializing-conflicts-for-Phantoms" class="headerlink" title="4.4.4 Materializing conflicts for Phantoms"></a>4.4.4 Materializing conflicts for Phantoms</h3><ul><li>Pre create all rows in the db, thus phantoms problems could be converted to the doctor appointment problem we discussed before</li></ul><h3 id="4-4-5-Predicate-Locks"><a href="#4-4-5-Predicate-Locks" class="headerlink" title="4.4.5 Predicate Locks"></a>4.4.5 Predicate Locks</h3><ul><li>We could use a predicate lock for booking case, works similarly to  the shared/ exclusive lock, but rather than belong to a particular object, it belongs to all objects that match some search condition</li></ul><pre><code class="ruby">SELECT * FROM bookings  WHERE room_id = 123 AND    end_time   &gt; &#39;2018-01-01 12:00&#39; AND    start_time &lt; &#39;2018-01-01 13:00&#39;;</code></pre><ul><li>If transaction A wants to read objects matching some condition, like in that <code>SELECT</code> query, it must acquire a shared-mode predicate lock on the conditions of the query. If another transaction B currently has an exclusive lock on any object matching those conditions, A must wait until B releases its lock before it is allowed to make its query.</li><li>If transaction A wants to insert, update, or delete any object, it must first check whether either the old or the new value matches any existing predicate lock. If there is a matching predicate lock held by transaction B, then A must wait until B has committed or aborted before it can continue.</li></ul><h2 id="4-5-Serializability"><a href="#4-5-Serializability" class="headerlink" title="4.5 Serializability"></a>4.5 Serializability</h2><ul><li>There are a lot of different isolation levels, and each db declare their isolation level slightly different,<ul><li>We could use serializable isolation to simplify it</li></ul></li><li>Serializable isolation<ul><li>Strongest isolation level</li><li>without any concurrency</li></ul></li><li>Serializable Mechanism<ul><li>Literally executing transactions in a serail order</li><li>Two Phase Locking</li><li>Optimistic concurrency control techniques such as serializable snapshot isolation</li></ul></li></ul><h3 id="4-5-1-Actual-Serial-Execution"><a href="#4-5-1-Actual-Serial-Execution" class="headerlink" title="4.5.1 Actual Serial Execution"></a>4.5.1 Actual Serial Execution</h3><ul><li><p>We could try to only execute one transaction at a time, in serial order, on a single thread</p></li><li><p>This comes to be realistic recently around 2007 because</p><ul><li>RAM became cheap enough, thus it’s now feasible to keep the entire active dataset in memory</li><li>OLTP transactions are usually short and only make a small number of reads and writes<ul><li>by contrast, long running analytic queries are typically read only, so they can be run on a consistent snapshot outside of the serial execution loop</li></ul></li></ul></li><li><p>This approach is implemented in VoltDB/ Hstore, Redis, and Datomic</p></li><li><p>Notice</p><ul><li>A system designed for a single threaded execution can sometimes perform better than a system that supports concurrency, because it can avoid the coordination overhead of locking</li></ul></li></ul><h3 id="4-5-2-Encapsulating-transactions-in-stored-procedures"><a href="#4-5-2-Encapsulating-transactions-in-stored-procedures" class="headerlink" title="4.5.2 Encapsulating transactions in stored procedures"></a>4.5.2 Encapsulating transactions in stored procedures</h3><ul><li><p>Philosophy</p><ul><li>Keep transactions short by avoiding interactively waiting for a user within a transaction</li><li>means a transaction is committed within the same HTTP request,</li></ul></li><li><p>IN the interactive style of transaction, network and db will take a lot time, we need to make sure we could handle enough throughput, we need to process multiple transactions concurrently in order to get reasonable performance.</p></li><li><p>Systems with single threaded serial transaction processing could receive the entier transaction code to db ahead of time , as a <strong>stored procedure</strong></p></li></ul><p><img src="https://s2.loli.net/2022/03/25/W1TFbpKA5JGhwXd.png" alt=""></p><ul><li>Pros and Cons<ul><li>Each db has its own language for stored procedures</li><li>Code running in db is difficult to manage, more awkward to keep in version control and deploy</li><li>db is often much more performance sensitive,because a single db instance is often shared by a lot of application servers. A bad written stored procedure can cause much more trouble than equivalent badly written code in an application server</li></ul></li></ul><h3 id="4-5-3-Partitioning"><a href="#4-5-3-Partitioning" class="headerlink" title="4.5.3 Partitioning"></a>4.5.3 Partitioning</h3><ul><li>Partition data to scale to multiple CPU cores, and multiple nodes.</li><li>However, for any transaction that needs to access multiple partitions, the database must coordinate the transaction across all the partitions that it touches. The stored procedure needs to be performed in lock-step across all partitions to ensure serializability across the whole system.</li></ul><h1 id="5-Two-Phase-Locking"><a href="#5-Two-Phase-Locking" class="headerlink" title="5. Two Phase Locking"></a>5. Two Phase Locking</h1><h2 id="5-1-Concepts"><a href="#5-1-Concepts" class="headerlink" title="5.1 Concepts"></a>5.1 Concepts</h2><ul><li>Several transactions are allowed to concurrently read the same object as long as nobody is writing to it</li><li>But as soon as anyone wants to write(modify or delete) an object, exclusive access is required<ul><li>Writes not only block other writers, it also block all readers</li></ul></li></ul><h2 id="5-2-Implementation"><a href="#5-2-Implementation" class="headerlink" title="5.2 Implementation"></a>5.2 Implementation</h2><ul><li>If a transaction wants to read an object, it must first acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simultaneously, but if another transaction already has an exclusive lock on the object, these transactions must wait.</li><li>If a transaction wants to write to an object, it must first acquire the lock in exclusive mode. No other transaction may hold the lock at the same time (either in shared or in exclusive mode), so if there is any existing lock on the object, the transaction must wait.</li><li>If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. The upgrade works the same as getting an exclusive lock directly.</li><li>After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name “two-phase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.</li></ul><h2 id="5-3-Performance"><a href="#5-3-Performance" class="headerlink" title="5.3 Performance"></a>5.3 Performance</h2><ul><li>transaction throughput and response times of queries are significantly worse under two-phase locking than under weak isolation.<ul><li>overhead of <strong>acquiring and releasing all those locks</strong>, but more importantly due to <strong>reduced concurrency</strong>.</li></ul></li></ul><h1 id="6-Serializable-Snapshot-Isolation"><a href="#6-Serializable-Snapshot-Isolation" class="headerlink" title="6. Serializable Snapshot Isolation"></a>6. Serializable Snapshot Isolation</h1><ul><li>It provides full serializability, but has only a small performance penalty compared to snapshot isolation</li><li>Snapshot + Serializable<ul><li>optimistic concurrency control mechanism</li><li>It performs badly if there is high contention (many transactions trying to access the same objects), as this leads to a high proportion of transactions needing to abort. If the system is already close to its maximum throughput, the additional transaction load from retried transactions can make performance worse.</li></ul></li><li>to achieve serialization, db need to know if the query result has been changed within the transaction<ul><li>Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)</li><li>Detecting writes that affect prior reads (the write occurs after the read)</li></ul></li></ul><h2 id="6-1-Detecting-stale-MVCC-reads"><a href="#6-1-Detecting-stale-MVCC-reads" class="headerlink" title="6.1 Detecting stale MVCC reads"></a>6.1 Detecting stale MVCC reads</h2><p>In order to prevent this anomaly, the database needs to track when a transaction ignores another transaction’s writes due to MVCC visibility rules. When the transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted.</p><p><img src="https://s2.loli.net/2022/03/25/kyCJZvs5QB93b1u.png" alt=""></p><h2 id="6-2-Detecting-writes-that-affect-prior-reads"><a href="#6-2-Detecting-writes-that-affect-prior-reads" class="headerlink" title="6.2 Detecting writes that affect prior reads"></a>6.2 Detecting writes that affect prior reads</h2><p>When a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. This process is similar to acquiring a write lock on the affected key range, but rather than blocking until the readers have committed, the lock acts as a tripwire: it simply notifies the transactions that the data they read may no longer be up to date.</p><h2 id="6-3-Performance"><a href="#6-3-Performance" class="headerlink" title="6.3 Performance"></a>6.3 Performance</h2><p>Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one transaction doesn’t need to block waiting for locks held by another transaction. Like under snapshot isolation, writers don’t block readers, and vice versa. This design principle makes query latency much more predictable and less variable. In particular, read-only queries can run on a consistent snapshot without requiring any locks, which is very appealing for read-heavy workloads.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Transaction-Understanding&quot;&gt;&lt;a href=&quot;#Transaction-Understanding&quot; class=&quot;headerlink&quot; title=&quot;Transaction Understanding&quot;&gt;&lt;/a&gt;Transaction
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="transaction" scheme="https://www.llchen60.com/tags/transaction/"/>
    
  </entry>
  
  <entry>
    <title>置身事内 笔记</title>
    <link href="https://www.llchen60.com/%E7%BD%AE%E8%BA%AB%E4%BA%8B%E5%86%85-%E7%AC%94%E8%AE%B0/"/>
    <id>https://www.llchen60.com/%E7%BD%AE%E8%BA%AB%E4%BA%8B%E5%86%85-%E7%AC%94%E8%AE%B0/</id>
    <published>2022-02-02T08:41:20.000Z</published>
    <updated>2022-02-05T09:21:58.786Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-整体框架"><a href="#1-整体框架" class="headerlink" title="1. 整体框架"></a>1. 整体框架</h1><ul><li>微观机制<ul><li>地方政府的权力和事务</li><li>财税与政府行为</li><li>政府投融资和债务</li><li>工业化当中的政府角色</li></ul></li><li>宏观现象<ul><li>城市化和不平衡</li><li>债务与风险</li><li>国内国际失衡</li><li>政府与经济发展</li></ul></li></ul><h1 id="2-地方政府的权力和事务"><a href="#2-地方政府的权力和事务" class="headerlink" title="2. 地方政府的权力和事务"></a>2. 地方政府的权力和事务</h1><aside>💡 地方政府不只提供了公共服务，也深度参与生产和分配。</aside><h2 id="2-1-体制特点"><a href="#2-1-体制特点" class="headerlink" title="2.1 体制特点"></a>2.1 体制特点</h2><ul><li>政府管理体系<ul><li>中央 - 省 - 市 - 县区 - 乡镇</li></ul></li><li>体制特点<ul><li>中央与地方政府<ul><li>日常运作以地方政府为主</li></ul></li><li>党与政府<ul><li>党负责重大决策和人事任免</li><li>政府负责执行</li></ul></li><li>条块分割，多重领导<ul><li>层层复制</li><li>党委政府人大政协四套班子</li><li>条条关系是业务关系，块块关系是领导关系—地方党委和政府可以决定人事任免</li></ul></li><li>上层领导与协调<ul><li>权力分散，决策容易向上集中</li><li>尽量在能达成共识的最低层级上解决问题</li></ul></li><li>官僚体系<ul><li>官员必须学习和贯彻统一的意识形态</li><li>官员由上级任命</li><li>地方主官需要在多地轮换任职</li></ul></li></ul></li><li>政府治理和运作的模式<ul><li>了解权力和资源在政府体系中的分布规则<ul><li>上下级政府间的纵向分布</li><li>同级政府间的横向分布</li></ul></li></ul></li><li>事权划分的原则<ul><li>公共服务的规模经济</li><li>信息的复杂性</li><li>激励相容</li></ul></li></ul><h2 id="2-2-外部性-规模经济"><a href="#2-2-外部性-规模经济" class="headerlink" title="2.2 外部性 规模经济"></a>2.2 外部性 规模经济</h2><ul><li>一个事物是只在划定的区域范围内有影响 还是对外也有影响</li><li>行政区域划分<ul><li>人口密度 — 提供公共物品和服务需要成本</li><li>山川河流</li><li>语言文化差异</li></ul></li><li>城市群的规划<ul><li>因为经济活动和人口集聚，需要打破现有的行政边界，在更大范围内提供无缝对接的标准化公共服务</li></ul></li><li>行政交界地区经济发展弱<ul><li>地理原因</li><li>亚文化区，距离主流文化相对远 — 文化，语言上</li><li>省政府也不会把有限资源优先配置到边界地区</li><li>环境污染谁来治理的问题</li></ul></li></ul><h2 id="2-3-复杂信息"><a href="#2-3-复杂信息" class="headerlink" title="2.3 复杂信息"></a>2.3 复杂信息</h2><ul><li>信息与权力<ul><li>原则上  上级形式权威</li><li>实际上给下级  下级有实际权威</li><li>信息优势  权威的平衡是核心问题</li></ul></li><li>信息获取+隐瞒<ul><li>信息传递很重要<ul><li>大量的会议和文件</li></ul></li><li>复杂的文件和会议制度<ul><li>文件类型，格式，以及报送都有严格的流程</li></ul></li></ul></li><li>上层的监督和审计</li></ul><aside>💡 所谓权力，实质上就是在说不清楚的情况下由谁来拍板决策的问题</aside><h2 id="2-4-激励相容"><a href="#2-4-激励相容" class="headerlink" title="2.4 激励相容"></a>2.4 激励相容</h2><ul><li>垂直管理<ul><li>专业性强，标准化程度高的部门</li><li>面对有双重领导的部门，都有一个根本的激励设计的问题<ul><li>能评价和奖惩工作业绩的上级，能决定工作内容的上级，受下级工作影响最大的上级，应该尽量是同一个上级</li></ul></li></ul></li><li>地方管理<ul><li>更加宏观，更加需要多部门合作的方面，比如经济发展</li><li>需要给地方放权，地方负责且地方能够分享发展成果</li></ul></li></ul><h2 id="2-5-招商引资"><a href="#2-5-招商引资" class="headerlink" title="2.5 招商引资"></a>2.5 招商引资</h2><aside>💡 地方政府不仅在为经济发展创造环境，它本身就是经济发展的深度参与者。</aside><ul><li>全民招商政策<ul><li>即招商不是单个部门要做的，是所有部门都需要熟悉政策，寻找招商机会的</li></ul></li><li>地方政府是城市土地所有者<ul><li>会将工业用地以非常优惠的价格转让给企业使用</li><li>负责对土地进行一系列初期开发 — 七通一平</li></ul></li><li>金融支持<ul><li>政府控制的投资平台入股</li><li>调动本地国企参与投资</li><li>通过各种方式协助企业获得银行贷款</li></ul></li><li>事务上支持<ul><li>各种许可</li></ul></li><li>补贴，税收优惠</li></ul><h1 id="3-财税与政府行为"><a href="#3-财税与政府行为" class="headerlink" title="3. 财税与政府行为"></a>3. 财税与政府行为</h1><ul><li><p>把握政府的真实意图，不能光读文件，还要看政府资金的流向和数量</p></li><li><p>事权与财力匹配 — 事权和支出责任匹配</p></li><li><p>实际情况</p><ul><li>事权与财权高度不匹配<ul><li>自1994年实行分税制改革依赖，地方财政预算支出ji</li></ul></li></ul></li></ul><h2 id="3-1-分税制改革"><a href="#3-1-分税制改革" class="headerlink" title="3.1 分税制改革"></a>3.1 分税制改革</h2><ul><li>1985年 — 1993年 财政包干<ul><li>承包制<ul><li>土地承包</li><li>企业承包</li><li>财政承包</li></ul></li><li>承包制原因，我国基本国策决定了不能对所有权做出根本性变革，只能对使用权和经营权实行承包制</li><li>实行方式<ul><li>1988年北京  以1987年财政收入为基数，设定一个固定的年收入增长率4%，超过4%的部分都归北京，没超过的部分则和中央五五分成</li></ul></li><li>地方实现方式<ul><li>大力发展乡镇企业</li></ul></li><li>影响<ul><li>中央财政预算收入占全国财政预算总收入的比重越来越低</li><li>全国财政预算总收入占GDP的比重也越来越低<ul><li>央地分成比例每过几年就要重新谈判</li><li>预算外收入可独享，因此地方政府疯狂给政府减税，藏富于企业，再通过行政收费，集资，摊派，赞助等方式收回一部分</li></ul></li></ul></li></ul></li><li>1994年分税制改革<ul><li>税务分类<ul><li>中央税</li><li>地方税</li><li>共享税<ul><li>增值税 — 占全国税收的1/4<ul><li>中央拿75% 地方留25%</li><li>转换过程<ul><li>为了防止地方财政收入的剧烈下跌，设立了税收返还的机制，保证改革后地方增值税收入和改革前一样，新增部分才和中央分</li></ul></li></ul></li></ul></li></ul></li><li>机构设置 — 与地方财政脱钩<ul><li>国税</li><li>地税</li></ul></li><li>行政方式<ul><li>省以下税务机关以垂直管理为主，由上级税务机构负责管理人员和工资</li></ul></li></ul></li></ul><aside>💡 公众所接触的信息和看到的现象，大都是已经博弈了的结果，而缺少社会阅历的学生容易把博弈结果错当成博弈过程。成功的政策背后是成功的协商和妥协，而不是机械的命令和执行，所以理解利益冲突，理解协调和解决机制，是理解政策的基础。</aside><ul><li>分税制改革中的权衡<ul><li>93年谈，想以92年作为基年，但是协商后改成93年，因此各个地方政府开始93年突击缴税</li></ul></li><li>分税制改革的影响<ul><li>中央占全国预算总收入的比重从改革前20% 到了55%</li><li>国家预算收入占GDP的比重从11%增加到了20%以上</li></ul></li></ul><h2 id="3-2-土地财政"><a href="#3-2-土地财政" class="headerlink" title="3.2 土地财政"></a>3.2 土地财政</h2><aside>💡 分税制改革减少了地方政府的可支配的财政资源，但是没有改变以经济建设为中心的任务；发展经济所需的诸多额外支出，比如招商引资，土地开发等，需要另筹资金了</aside><ul><li>解决方案<ul><li>努力提高税收规模</li><li>增加预算外收入<ul><li>围绕土地出让和土地开发所产生的的土地财政</li></ul></li></ul></li><li>地方政府青睐重资产的制造业<ul><li>投资规模大，对GDP的拉动作用明显</li><li>增值税在生产环节增收，跟生产规模直接挂钩</li><li>制造业不仅可以吸纳从农业部门转移出来的低技能劳动力，也可以带动第三产业发展，增加相关税收</li></ul></li><li>演变进程<ul><li>1998年 单位停止福利分房</li><li>1997 - 2002年 城镇住宅新开工面积年均增长26%</li><li>2001年 国家推行招标拍卖</li><li>2002年 国土部明确四类经营用地 商业，旅游，娱乐，房地产采用招拍挂的制度</li><li>国有土地转让收入占地方公共预算收入的比重在60%左右 从1999年低于10% 到现在的高度</li></ul></li><li>土地财政囊括<ul><li>土地使用权转让收入</li><li>和土地使用开发有关的各种税收收入<ul><li>大部分税基是价值而非面积</li></ul></li></ul></li><li>问题<ul><li>土地资本化运作，是在将未来的收益抵押到今天来借钱，因为地方官员任期有限，投资质量是难以保证的</li></ul></li></ul><h2 id="3-3-横向纵向的不平衡"><a href="#3-3-横向纵向的不平衡" class="headerlink" title="3.3 横向纵向的不平衡"></a>3.3 横向纵向的不平衡</h2><p>分税制改革以后，中央拿走了大头，但事情还是地方办，地方收支差距需要中央进行转移支付。全国总数上来看，能补得上。但总数不得上不代表每一级政府都能够补得上。</p><ul><li><p>问题</p><ul><li>财权层层上收，事权层层下压<ul><li>2000年 湖北监利县“农民真苦，农村真穷，农业真危险” — 三农问题<ul><li>农村税费改革，制止基层的乱摊派乱收费问题</li><li>废止农业税</li></ul></li></ul></li><li>财税体制的层级问题</li></ul></li><li><p>解决方案</p><ul><li>农村基本公共服务开始 纳入国家公共财政保障范围，中央地方政府共同承担</li><li>在转移支付中加入激励机制，鼓励基层政府达成特定目标，并给予奖励</li><li>将基层财政资源向上一级政府统筹<ul><li>乡财县管</li><li>扩权强县</li><li>财政省直管县</li></ul></li></ul></li><li><p>中央政府通过再分配  转移支付，支援中西部</p></li></ul><h1 id="4-政府投融资与债务"><a href="#4-政府投融资与债务" class="headerlink" title="4. 政府投融资与债务"></a>4. 政府投融资与债务</h1><aside>💡 土地不会移动也不会消失，天然适合作抵押，做各种资本交易的压仓标的，身价自然飙升。土地资本化的魔力，在于可以挣脱物理属性，在抽象的意义上交易承诺和希望，将过去的储蓄，现在的收入，未来的前途，统统汇聚和封存在一小片土地上，使其价值暴增。经济发展的奥秘之一，就是将有形资产转变成为这种抽象资本，从而聚合跨越空间和时间的资源。</aside><h2 id="4-1-城投公司和土地金融-—-土地开发和基础设施投资"><a href="#4-1-城投公司和土地金融-—-土地开发和基础设施投资" class="headerlink" title="4.1 城投公司和土地金融 — 土地开发和基础设施投资"></a>4.1 城投公司和土地金融 — 土地开发和基础设施投资</h2><ul><li>法律规定，地方政府不可以从银行贷款，2015年前也不允许发行债券 — 因此地方政府需要成立公司<ul><li>国有独资企业 — 地方政府融资平台 — 城投公司<ul><li>建设投资 + 投资开发 + 旅游发展</li></ul></li></ul></li><li>城投公司的一般特征<ul><li>持有从政府取得的大量土地使用权<ul><li>土地使用权可以用来撬动银行贷款，以及各种其他资金</li></ul></li><li>盈利状况依赖政府补贴<ul><li>因为承接的项目很多都有基础设施属性</li><li>项目本身盈利能力往往比较弱</li></ul></li><li>政府的隐性担保可以让企业大量借款</li></ul></li><li>土地一级开发<ul><li>平整整理土地</li><li>投入大，利润低，涉及拆迁等问题</li><li>一般由政府融资平台公司完成</li></ul></li><li>土地二级开发<ul><li>土地的建设运营</li><li>由房地产公司来做</li></ul></li><li>华夏幸福的方式—- 政府和社会资本合作 Public Private Partnership — PPP<ul><li>产城结合</li><li>政府委托华夏幸福做住宅用地的一级开发</li><li>这片熟地要还给政府的</li><li>再以招拍挂等公开方式出让给中标的房地产企业</li><li>从工业园区的发展当中，其可以和政府分享税收收益<ul><li>按照法律，政府不能和企业直接分享税收，但是可以购买企业服务，用产业发展服务费的名义来支付约定的分成</li></ul></li></ul></li></ul><h2 id="4-2-地方政府债务"><a href="#4-2-地方政府债务" class="headerlink" title="4.2 地方政府债务"></a>4.2 地方政府债务</h2><ul><li><p>政府依靠土地使用权转让收入支撑起土地财政，并将未来的土地收益资本化，从银行和其他渠道借入天量资金，利用土地金融，快速推动工业化和城市化，但也同时积累了大量债务</p></li><li><p>模式的关键</p><ul><li>土地价格<ul><li>只要不断地投资和建设能带来持续的经济增长，城市就会扩张，地价就会上涨</li><li>这样就能够偿还连本带利越来越多的债务</li><li>但是经济增速一旦放缓，地价下跌，土地出让收入减少，累积的债务就会成为沉重负担，可能压垮融资平台甚至地方政府</li></ul></li></ul></li><li><p>国家开发银行和城投债</p><ul><li>为什么需要上述二者<ul><li>因为地方政府穷，但是还要发展经济，做城市化</li><li>想要在城市建设开发当中引入银行资金，需要解决三个问题<ul><li>需要一个能借款的公司，政府不能直接从银行贷款</li><li>城建开发项目复杂，有些赚钱，有些赔钱，所以需要打包捆绑</li><li>依靠财政预算收入不够还债的，要能把跟土地有关的收益用起来</li></ul></li></ul></li><li>为了解决上述引入资金的问题，出现了<strong>城投公司</strong><ul><li>国家开发银行创立的这种模式<ul><li>贷款，然后可以用土地出让收益作为质押进行还款保证</li></ul></li></ul></li><li>城商行<ul><li>七成左右第一股东为地方政府</li><li>为了方便为融资平台公司和基础设施建设提供贷款</li><li>风险<ul><li>基础设施建设项目周期长，需要中长期贷款<ul><li>国开行是政策性银行，有稳定的长期资金来源</li><li>但是商业银行的存款大都来自短期存款，与中长期贷款期限不匹配，容易产生风险</li></ul></li><li>四大行存款来源庞大，可以承受一定程度期限错配，但是城商行不能，经常需要在资本市场融资，容易出现风险<ul><li>包商银行</li></ul></li></ul></li></ul></li><li>城投公司融资方式<ul><li>银行贷款</li><li>发行债券<ul><li>理论优势<ul><li>分散风险</li><li>债券可交易，价格和利率可变，配置效率高</li></ul></li><li>实际<ul><li>七八成商业银行所有</li><li>隐性背书，尽管有风险，但是大家有点不管不顾</li></ul></li></ul></li></ul></li><li>地方债务<ul><li>隐性负债很多</li><li>局部风险大</li><li>很多地方都靠着中央的补贴保证不违约，但是如果经济遇冷，地价下跌，政府也无法背起这沉重的债务了</li></ul></li></ul></li><li><p>地方债的治理</p><ul><li>债务置换<ul><li>用地方政府发行的公债，替换一部分融资平台公司的银行贷款和城投债<ul><li>好处<ul><li>利率从之前的7% - 8% 降低到4%左右<ul><li>原先因为有政府的隐性担保，所以银行都愿意贷给城投，这样还拉高了一般企业的贷款成本和难度</li></ul></li><li>政府公债期限要长很多<ul><li>降低了期限错配和流动性的风险</li></ul></li><li>信用级别升高</li></ul></li></ul></li><li>如何确定债务置换的规模<ul><li>国务院确定并报全国人大或者人大常委会批准</li></ul></li></ul></li><li>推动融资平台转型<ul><li>剥离为政府融资的功能，同时破除政府的隐性担保</li></ul></li><li>约束银行和各类金融机构，避免大量资金流入融资平台</li><li>问责官员，对于过度负债的行为终身追责</li></ul></li></ul><h2 id="4-3-招商引资当中的地方官员"><a href="#4-3-招商引资当中的地方官员" class="headerlink" title="4.3 招商引资当中的地方官员"></a>4.3 招商引资当中的地方官员</h2><ul><li>官员政绩与激励机制<ul><li>地方主官平均任期三四年，而基建项目一般都要两三年完成</li><li>故而刚上任会上马大量这种项目</li><li>各地的投资 政治投资周期比较频繁<ul><li>新官上任，土地出让数量会增加</li><li>新增的土地供应大多位于城市周边郊区</li><li>摊大饼的态势</li></ul></li></ul></li><li>偏重投资的增长模式带来的问题<ul><li>政府债务的不断攀升</li><li>重视看得见的东西，忽略看不见的</li></ul></li><li>2019考核官员标准发生变化<ul><li>看全面工作，看推动本地区经济建设，政治建设，文化建设，社会建设，生态文明建设</li><li>解决发展不平衡不充分的问题</li><li>满足人民日益增长的美好生活需要的情况和实际成效</li></ul></li></ul><h1 id="5-工业化中的政府角色"><a href="#5-工业化中的政府角色" class="headerlink" title="5. 工业化中的政府角色"></a>5. 工业化中的政府角色</h1><ul><li><p>政府与具体的工业企业的合作</p><ul><li>京东方<ul><li>与其说是一家公司的奋斗史，不如说是液晶屏产业在中国的奋斗史</li></ul></li></ul></li><li><p>现代经济的规模经济效应非常强，新企业的进入门槛非常高，不仅投资额度大，还要面对先进入者已经积累起来的巨大成本和技术的优势</p></li><li><p>东亚经济奇迹，一个很重要的特点</p><ul><li>政府帮助本土企业进入复杂度很高的行业</li><li>充分利用其中的学习效应，规模效应以及技术外溢效应</li><li>迅速提升本土制造业的技术能力和国际竞争力</li></ul></li><li><p>新兴制造业在地理上的集聚效应是很强的，因为扎堆生产可以节约原材料和中间投入的运输成本</p><ul><li>且同行聚集在一起有利于知识和技术交流，外溢效应很强</li><li>因此产业集群一旦形成，自身引力就会不断增强，很难被外力打破</li></ul></li><li><p>光伏产业</p><ul><li>新技术开始成本会非常高，其实都是靠政府补贴赚钱</li><li>政府通过补贴额度的降低来引导各个企业的发展</li></ul></li><li><p>政府产业引导基金与私募基金</p><ul><li>LP limited partner  给钱的</li><li>GP general partner 投资的</li><li>可以投一级市场以及二级市场</li><li>国内的最大一类LP就是政府引导基金</li></ul></li><li><p>产业引导基金的特点</p><ul><li>大多产业引导基金不直接投资企业，而是做LP，将钱交给市场化的私募基金的GP去投资企业<ul><li>因为一支私募不仅有政府资金，还会有很多社会资本，这样可以做产业引导</li></ul></li><li>政府引导基金本身就是一只基金，投资对象是各种私募基金，被称为母基金  funds of funds</li><li>借助市场力量去使用财政资金</li><li>大多数引导基金的最终投向都是战略新兴产业，比如芯片，新能源汽车，而不允许基础设施和房地产</li></ul></li><li><p>设置产业引导基金之后，也需要专门的公司来运营和管理这只基金</p><ul><li>运作模式分类<ul><li>政府独资公司</li><li>混合所有制公司</li><li>政府将钱委托给市场化的母基金管理人去运营</li></ul></li></ul></li><li><p>政府引导基金发展的外部条件</p><ul><li>制度条件<ul><li>制度和政策的指引</li><li>提供政策基础：设立引导基金发挥财政资金的杠杆放大效应，增加创业投资资本的供给，克服单纯通过市场配置创业投资资本的市场失灵问题</li></ul></li><li>产业条件<ul><li>经济需要发展到一定的水平，使得高技术，高风险的战略新兴行业得以出现</li></ul></li><li>资本市场成熟程度<ul><li>GP ，足够大的股权交易市场，退出渠道</li></ul></li></ul></li></ul><h1 id="6-地方政府推动经济发展的模式与特点"><a href="#6-地方政府推动经济发展的模式与特点" class="headerlink" title="6. 地方政府推动经济发展的模式与特点"></a>6. 地方政府推动经济发展的模式与特点</h1><h2 id="6-1-重土地，轻人"><a href="#6-1-重土地，轻人" class="headerlink" title="6.1 重土地，轻人"></a>6.1 重土地，轻人</h2><ul><li>优点<ul><li>可以快速推进城市化和基础设施建设</li></ul></li><li>缺点<ul><li>公共服务供给不足，提高了放假和居民债务负担</li><li>拉大了地区差距和贫富差距</li></ul></li></ul><h2 id="6-2-重规模，重扩张"><a href="#6-2-重规模，重扩张" class="headerlink" title="6.2 重规模，重扩张"></a>6.2 重规模，重扩张</h2><ul><li>优点<ul><li>推动了企业成长和快速工业化</li></ul></li><li>缺点<ul><li>加重了债务负担</li><li>企业，地方政府，居民三部门债务互相作用，加大了经济整体的债务和金融风险</li></ul></li></ul><h2 id="6-3-重投资，重生产，轻消费"><a href="#6-3-重投资，重生产，轻消费" class="headerlink" title="6.3 重投资，重生产，轻消费"></a>6.3 重投资，重生产，轻消费</h2><ul><li>优点<ul><li>拉动了经济的快速增长，扩大了对外贸易</li><li>使得我国迅速成为了制造业强国</li></ul></li><li>缺点<ul><li>经济结构的不平衡</li><li>对内<ul><li>资源向企业部门转移，居民收入和消费占比偏低，不利于经济长期稳定发展</li></ul></li><li>对外<ul><li>国内无法消纳的产能向国外输出，加剧了贸易冲突</li></ul></li></ul></li></ul><h1 id="7-城市化与不平衡"><a href="#7-城市化与不平衡" class="headerlink" title="7. 城市化与不平衡"></a>7. 城市化与不平衡</h1><aside>💡 中国特有的城市土地国有制度，为政府垄断土地一级市场创造了条件，将这笔隐匿的财富变成了启动城市化的巨大资本，但也让地方财源高度依赖土地价值，依赖房地产和房价。房价连着地价，地价连着财政，财政连着基础设施投资，于是经济增长，地方财政，银行，房地产之间形成了“一荣俱荣，一损俱损”的复杂关系。</aside><h2 id="7-1-房价与居民债务"><a href="#7-1-房价与居民债务" class="headerlink" title="7.1 房价与居民债务"></a>7.1 房价与居民债务</h2><ul><li><p>1994年分税制改革是很多重大经济现象的分水岭</p></li><li><p>地区房价差异的主要原因是供需失衡</p><ul><li>500万人和1000万人以上的大城市城区人口增量占全国城区人口增量的近四成，但居住用地增量才占亮程，房价于是就快速上涨了</li></ul></li><li><p>中国对建设用地指标实行严格的管理，每年的新增指标由中央分配到省，再由省分配到地方。</p><ul><li>这些指标无法跨省交易</li><li>但是这些土地倾斜政策无法改变人口流向，人还是不断向东部沿海和大城市聚集</li></ul></li><li><p>房地产</p><ul><li>被称为经济周期之母</li><li>根源在于其内在的供需矛盾<ul><li>银行可以通过按揭创造几乎无限的新购买力</li><li>不可再生的城市土地供给是有限的</li></ul></li><li>这种矛盾经常会导致资产泡沫和破裂的周期循环，是金融和房地产不稳定的核心矛盾</li></ul></li><li><p>中国情况</p><ul><li>中国城镇居民的主要财产是房子，房产占据家庭资产的将近七成<ul><li>六成是住房，一成是商铺</li><li>而美国72%是金融资产，28%是房产</li><li>即中美两国财富的压舱石不一样，也就能看出为什么中国看中房市，而美国看中股市了</li></ul></li><li>房价上涨的影响<ul><li>增加按揭债务负担</li><li>拉大贫富差距</li><li>刺激低收入人群举债消费</li></ul></li><li>书里对于房产，当前情况的描述我认为是浅尝辄止的，只稍微说了下，发出来不容易~</li></ul></li></ul><h2 id="7-2-不平衡与要素市场改革"><a href="#7-2-不平衡与要素市场改革" class="headerlink" title="7.2 不平衡与要素市场改革"></a>7.2 不平衡与要素市场改革</h2><ul><li>人口流动和收入平衡<ul><li>我国人口流动依旧受限<ul><li>重土地轻人，导致民生支出不足，不利于外来人口在城市中真正安家落户</li></ul></li></ul></li><li>要平衡的是人均差距，而不是地区差距</li><li>土地流转<ul><li>当前情况是大城市不仅土地面积有限，而且有对建设用地指标的管制</li><li>如果用地指标能够跟着人口流动，会很大程度环节土地供需矛盾的问题</li><li>还需要让农村集体用地参与流转 — 农村土地归集体所有</li></ul></li><li>改革的目标是为了让各种资源按照常住人口的规模进行配置<ul><li>里面涉及到<ul><li>建立健全城乡统一的建设用地市场</li><li>深化户籍制度的改革</li><li>以经常居住地登记户口制度</li><li>建立城镇教育，就业创业，医疗卫生等基本公共服务和常住人口挂钩的机制</li></ul></li></ul></li></ul><h2 id="7-3-经济发展与贫富差距"><a href="#7-3-经济发展与贫富差距" class="headerlink" title="7.3 经济发展与贫富差距"></a>7.3 经济发展与贫富差距</h2><ul><li>降低了全球的不平等<ul><li>改变了全球收入分布的格局</li></ul></li><li>大家都在变富<ul><li>2019年收入最高的20%人群占有全部收入的48%，而收入最低的20%只占有全部收入的8%</li><li>另外一个视角是30年间，实际收入涨了8 - 13倍 平均下来</li></ul></li><li>经济增长的过程<ul><li>尽管不一定减少收入差距，但是可以一定程度上遏制贫富差距在代际之间传递</li><li>当前情况是财产性收入占收入比重越来越大，人力资本无法在代际之间传承的，但是房产等有形资产是可以的</li><li>经济增速下滑，社会对不平等的容忍程度就会下降了，不安定因素会增加</li></ul></li><li>对收入差距的容忍度<ul><li>隧道效应<ul><li>一条在动，反向不动的时候的极度焦躁</li></ul></li><li>经济形势不好，富人动的慢，穷人可能完全无法有财富增长了</li></ul></li></ul><h1 id="8-债务与风险"><a href="#8-债务与风险" class="headerlink" title="8. 债务与风险"></a>8. 债务与风险</h1><ul><li>债务本身的风险<ul><li>人们在乐观的时候往往会低估负债的风险，过多借债。当风险出现的时候，又会因为债务负担沉重而缺乏腾挪的空间，没有办法应对。</li></ul></li></ul><h2 id="8-1-债务与经济衰退"><a href="#8-1-债务与经济衰退" class="headerlink" title="8.1 债务与经济衰退"></a>8.1 债务与经济衰退</h2><ul><li>负债率搞得经济中，资产价格的下跌往往非常迅猛。若债务太重，收入不够还本，甚至不够还息，就只能变卖资产，抛售的人多了，资产价格就跳水了</li><li>资产价格的下跌会引起信贷收缩，导致资金链的断裂</li><li>经济衰退会加剧不平等<ul><li>因为债务危机对穷人和富人的打击高度不对称</li><li>法律优先保护债权人的索赔权，即欠债的无论是公司还是个人，即使破产也要清算偿债</li></ul></li></ul><h2 id="8-2-债台为何高筑"><a href="#8-2-债台为何高筑" class="headerlink" title="8.2 债台为何高筑"></a>8.2 债台为何高筑</h2><ul><li>资金供给和银行管制<ul><li>资金供给的增加源于金融管制的放松</li><li>银行越做越大，创造的信贷越来越多</li><li>金融创新和衍生品不断增加，导致金融部门的规模和风险也在增加</li></ul></li><li>全球银行的危机的频率和国际资本流动规模高度相关，金融危机基本都伴随着银行的危机<ul><li>银行规模达，杠杆高</li><li>银行借来短期的钱，但是借出去的钱却大都是长期的<ul><li>这种负债和资产的期限不匹配会带来流动性风险</li></ul></li><li>银行信贷大都与房地产有关，常常和土地，以及房产价值一同起落，放大经济波动</li><li>银行风险会传导给其他金融部门<ul><li>比如银行将各种按揭贷款打包成证券组合，卖给其他金融机构</li></ul></li></ul></li><li>脱实向虚，实际上一直是富人的钱 贷给了穷人们</li><li>为什么投资变少了？<ul><li>制造业外迁 — 制造业本身是重资产，重投资的行业</li><li>当今技术提升，需要运用大量软件与服务，而服务业更依赖于人的集聚，也推升了对特定地段的住房和社交空间的需求</li></ul></li></ul><aside>💡 在缺乏增长点的情况下，央行给银行体系提供流动性，但商业银行资金贷不出去，容易流向资产市场。放松货币条件总体上有利于资产持有者，超宽松的货币政策可能加剧财富分化，固化结构扭曲，使危机调整的过程过长。</aside><h2 id="8-3-中国的债务与风险"><a href="#8-3-中国的债务与风险" class="headerlink" title="8.3 中国的债务与风险"></a>8.3 中国的债务与风险</h2><ul><li>始于2008年的财政刺激计划<ul><li>中央放松了对地方融资平台的限制，同时不断降准降息，放宽银行信贷</li><li>这些资金找到了基建和房地产两大载体</li></ul></li><li>2011 因为经济出现过热现象，货币政策收紧</li><li>但是2011年中，欧债危机爆发，国内制造业陷入困境</li><li>2012年又开始降准降息</li><li>2015年股灾<ul><li>美国退出量化宽松，人民币汇率迅速贬值</li><li>中央放松了对房地产的调控</li><li>全国棚户区改造从实物安置转变为货币化安置，带动房价进一步上涨</li></ul></li><li>我国企业债务负担非常重，应对风险能力很有限</li><li>影子银行<ul><li>类似银行的信贷业务，但是不在银行的资产负债表当中，不受到银行监管规则的约束</li><li>比如银行卖给老百姓一个理财产品，利息5%</li><li>然后将筹来的钱委托给信托公司</li><li>信托公司将钱借给公司</li><li>这样做的话，发行的理财产品不算银行储蓄，委托给信托公司的投资不算银行贷款，所以就绕开了银行的监管</li></ul></li></ul><h2 id="8-4-化解债务风险"><a href="#8-4-化解债务风险" class="headerlink" title="8.4 化解债务风险"></a>8.4 化解债务风险</h2><ul><li>解决方案<ul><li>偿还已有债务<ul><li>可能会陷入到经济衰退当中，因为都要勒紧裤腰带过日子</li><li>你的支出就是我的收入了呀</li><li>增发货币<ul><li>增发货币来降低利率<ul><li>可以刺激消费和投资，提振经济</li><li>计算实际收入不增加，随着物价上涨和时间推移，债务负担也会减轻</li></ul></li><li>量化宽松<ul><li>央行增发货币来买入各类资产，将货币注入经济</li><li>托住资产价格，为经济注入流动性</li><li>问题是难以将增发的货币发到穷人手里，因此难以刺激消费支出，还会拉大贫富差距</li><li>购买各种金融资产受益的是各种资产的所有者</li></ul></li><li>债务货币化<ul><li>政府加大财政支出去刺激经济</li><li>由财政部发债融资，央行直接印钱买过来，无需其他金融机构参与也无需支付利息</li></ul></li></ul></li></ul></li><li>遏制新增债务，改革滋生债务的政治经济环境<ul><li>核心点<ul><li>限制房价上涨</li><li>限制土地财政和土地金融</li><li>限制政府担保和国有企业过度借贷</li><li>资本市场改革<ul><li>改变以银行贷款为主的间接融资体系，扩展直接融资渠道<ul><li>能够降低债务负担</li><li>也能提高资金的使用效率</li><li>股权的约束力要更强<ul><li>风险共担</li><li>股权可以转让</li></ul></li></ul></li><li>改革很慢的原因其实是我国金融资产中72%的风险是由金融机构和政府承担的</li></ul></li></ul></li><li>债务问题<ul><li>以出口和投资驱动的经济体系的产物</li></ul></li></ul></li></ul></li></ul><h1 id="9-国内国际的失衡"><a href="#9-国内国际的失衡" class="headerlink" title="9. 国内国际的失衡"></a>9. 国内国际的失衡</h1><ul><li>中国成为制造业大国背后的两个问题<ul><li>内部经济结构失衡<ul><li>重生产，重投资，相对轻民生，轻消费</li><li>导致和巨大的产能对比，国内消费不足，消化不了的产品只能对外输出</li></ul></li><li>国外需求的不稳定和贸易冲突<ul><li>实际制造业中 中国占比从5% 到28%，而七国集团从72%到37%，其他国家基本没有变化</li></ul></li></ul></li></ul><h2 id="9-1-低消费和产能过剩"><a href="#9-1-低消费和产能过剩" class="headerlink" title="9.1 低消费和产能过剩"></a>9.1 低消费和产能过剩</h2><ul><li><p>面临消费不足的问题</p><ul><li>我国居民最终消费占GDP只有44%，美国有70%，欧盟和日本也有55%</li><li>GDP中可供老百姓支配的收入份额下降了，或者老百姓把更大一部分收入存了起来，储蓄类上升了— 我国这两种情况都出现了</li><li>孩子数量减少以后，养儿防老功效降低，父母开始增加储蓄来养老了</li></ul></li><li><p>当前的靠政府 靠投资拉动经济发展会存在几个方面的问题</p><ul><li>基础设施和工业体系已经完善，投资难度加大<ul><li>因此投资决策和调配资源的体制需要改变</li><li>地方政府主导投资的局面需要改变</li></ul></li><li>由于老百姓收入和消费都不足，无法消化投资形成的产能，很多投资不能变成有效的收入，所以债务负担越来越重，带来了一系列的风险</li><li>劳动收入份额下降，资本收入份额的上升，会扩大贫富差距</li><li>由于消费不足和投资过剩，过剩产能会向国外输出，因为我国体量很大，输出产能会家中全球贸易失衡，引发贸易冲突</li></ul></li><li><p>十九大提出了</p><ul><li>提高就业质量和人民收入水平<ul><li>破除妨碍劳动力，人才社会性流动的体制机制弊端</li><li>完善政府，工会，企业共同参与的协商协调机制</li><li>坚持按劳分配，完善按要素分配的体制机制</li><li>扩大中等收入群体，增加低收入者收入，调节过高收入，取缔非法收入</li><li>要努力使居民收入增长快于经济增长</li></ul></li></ul></li><li><p>出口占比高这种经济结构比较脆弱</p><ul><li>外国收到政治经济的影响很大，难以掌控</li></ul></li></ul><h2 id="9-2-中美贸易冲突"><a href="#9-2-中美贸易冲突" class="headerlink" title="9.2 中美贸易冲突"></a>9.2 中美贸易冲突</h2><ul><li><p>反全球化，民粹主义，</p></li><li><p>中美之间的技术冲击，技术竞争才是真正的博弈</p></li><li><p>对于一个站在科技前沿的国家来说，新技术的发明和应用一般从科学研究和实验室开始，再到技术应用和专利阶段，然后再到大规模的工业量产</p></li><li><p>但是对于一个后起的发展中国家来说，很多时候顺序是反的</p><ul><li>先从制造环节开始，边干边学，积累技术和经验</li><li>根据自身需要改进技术，创造一些专利</li><li>产品销量逐步扩大，技术逐步向前沿靠拢以后，拿出更多资源投入研发，推进更为基础，应用范围更广的科研项目</li></ul></li></ul><h2 id="9-3-再平衡与国内大循环"><a href="#9-3-再平衡与国内大循环" class="headerlink" title="9.3 再平衡与国内大循环"></a>9.3 再平衡与国内大循环</h2><ul><li>加快构建以国内大循环为主体，国内国际双循环相互促进的新发展格局<ul><li>需要提高居民的收入和消费</li><li>需要靠服务业的大力发展了，而这些只能发展在人口密度非常高的城市当中</li><li>需要户籍制度，还有土地制度的改革</li><li>需要把更多的西苑从政府和企业手中转移出来，分配给居民</li><li>需要改变地方政府在经济中扮演的角色，遏制其投资冲动，降低其生产性支出，加大民生支出</li></ul></li></ul><h1 id="10-政府与经济发展的总结"><a href="#10-政府与经济发展的总结" class="headerlink" title="10. 政府与经济发展的总结"></a>10. 政府与经济发展的总结</h1><ul><li>一套严格的概念框架无疑有助于厘清问题，但也经常让人错把问题当成答案</li><li>需要将竞争机制引入政府<ul><li>以中央政府为主，部委为单位，</li></ul></li><li>瓦格纳法则<ul><li>国家越富裕，政府在国民经济中所占的比重也往往会越大，而不是越小</li><li>因为随着愈发富裕，民众对政府服务的需求会越来越多，政府在公立教育，医疗，退休金，失业保险等方面的支出也会随之增加</li></ul></li><li>经济发展的核心是提高生产率<ul><li>对于处于技术前沿的发达国家来说，提高生产率的关键是不断探索和创新</li><li>但是对于发展中国家来说，提高生产率的关键不是探索未知和创新，而是学习已知的技术和管理模式，将更多的资源尽快组织和投入到学习过程当中，来提高学习效率</li><li>后进国家虽然有模仿和学习先进国家技术的后发优势，但是其组织学习模式不可能一直持续下去</li><li>当技术和生产率提高到一定的水平以后，需要成功转型成探索创新模式。</li></ul></li><li>城市化 — 是个农民转化为工人的过程</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-整体框架&quot;&gt;&lt;a href=&quot;#1-整体框架&quot; class=&quot;headerlink&quot; title=&quot;1. 整体框架&quot;&gt;&lt;/a&gt;1. 整体框架&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;微观机制&lt;ul&gt;
&lt;li&gt;地方政府的权力和事务&lt;/li&gt;
&lt;li&gt;财税与政府行为&lt;/li&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>人月神话笔记</title>
    <link href="https://www.llchen60.com/%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D%E7%AC%94%E8%AE%B0/"/>
    <id>https://www.llchen60.com/%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D%E7%AC%94%E8%AE%B0/</id>
    <published>2022-01-31T06:34:42.000Z</published>
    <updated>2022-01-31T06:35:57.437Z</updated>
    
    <content type="html"><![CDATA[<ul><li>软件活动的根本任务<ul><li>打造构成抽象软件实体的复杂概念结构</li></ul></li><li>软件活动的次要任务1<ul><li>使用编程语言来表达这些抽象实体，在空间和时间的限制下将他们映射成机器语言</li></ul></li></ul><blockquote><p>一个相互牵制的概念结构是软件实体必不可少的部分，它包括：数据集合，数据条码之间的关系，算法以及功能调用等。这些要素本身是抽象的，体现在不同的表现形式下的概念构造是相同的。</p></blockquote><ul><li>软件系统无法规避的内在特性<ul><li>复杂度<ul><li>元素之间的交互，使得软件的复杂度比非线性增长要高得多</li><li>软件的复杂度是根本属性，抽掉复杂度的软件实体描述实际上也去掉了一些本质属性。所以我们无法做太多的简化的</li></ul></li><li>一致性<ul><li>软件工程当中没有太多的一致性，更多的核心开发工程师本身的理念</li></ul></li><li>可变性<ul><li>软件中的功能，功能属性很容易感受到变更的压力</li></ul></li><li>不可见性<ul><li>软件是不可见以及无法可视化的<ul><li>例如几何抽象是强大的工具，包括机械制图，以及化学分子模型，但是软体的客观存在不具有空间的形体特征</li></ul></li></ul></li></ul></li></ul><ul><li><p>编程系统产品(Programming Systems Product) 开发的工作量是供个人使用的，独立开发的构件程序的9倍。估计软件构件产品化引起了3倍工作量，将软件构件整合成完整系统所需要的设计、集成和测试又加强了3倍的工作量，这些高成本的构件在根本上是相互独立的</p></li><li><p>编程行业满足了我们内心深处的创造渴望和愉悦所有人的共有情感，提供了5种乐趣</p><ul><li>创建事物的快乐</li><li>开发对其他人有用的东西的乐趣</li><li>将可以活动，相互齿合的零部件组装成类似迷宫的东西，这个过程体现出的令人神魂颠倒的美丽</li><li>面对不重复的任务，不断学习的乐趣</li><li>工作在如此易于驾驭的介质上的乐趣—— 纯粹的思维活动 —— 其存在，移动和运转方式完全不同于实际物体</li></ul></li><li><p>同样，这个行业有一些内在固有的烦恼</p><ul><li>将做事方式调整到追求完美是学习编程的最困难部分</li><li>权威不等同于责任；真正的权威来自于每次任务的完成</li><li>任何创造性活动都伴随着枯燥艰苦的劳动，编程也不例外</li><li>人们通常希望项目在接近结束的时候，软件项目能收敛得快一些，然而，情况却是越接近完成，收敛得越慢</li><li>产品在完成前总面临着陈旧过时的威胁，只有实际需要的时候，才会用到最新的设想</li></ul></li><li><p>缺乏合理的时间进度是造成项目滞后的最主要原因，它比其他所有因素的总和影响还大</p></li><li><p>所有编程人员都是乐观主义者，一切都将运作良好</p></li><li><p>由于编程人员通过纯粹的思维活动来开发，我们期待在实现过程当中不会碰到困难。但是我们本身的构思是会有缺陷的，因此总会有bug</p></li><li><p>围绕着成本核算的估计技术，混淆了工作量和项目进展。人月是危险的，因为它暗示着人员数量和时间是可以相互替换的。</p></li><li><p>在若干人员中分解任务会引发额外的沟通工作量 — 培训和相互沟通。</p></li><li><p>Brooks法则： 为进度落后的项目增加人手，只会使得进度更加落后</p><ul><li>增派人手可能增加的工作量<ul><li>任务重新分配本身和所造成的工作中断</li><li>培训新人员</li><li>额外的相互沟通</li></ul></li></ul></li><li><p>同样有两年经验而且在受到同样培训的情况下，优秀的专业程序员的生产力是较差的程序员的10倍。</p></li><li><p>一位首席程序员，类似于外科手术队伍的团队架构提供了一种方法—— 既能获得由少数头脑产生的产品完整性，又能够得到多位协助人员的总体生产率，还彻底减少了沟通的工作量。</p></li><li><p>概念完整性是系统设计中最重要的考虑因素</p><ul><li>为了获得概念的完整性，设计必须由一个人或者具有共识的小型团队来完成</li><li>为了获得概念的完整性，就必须有人控制这些概念，这实际上是一种无需任何歉意的贵族专制统治</li></ul></li><li><p>功能和理解上的复杂程度的比值才是系统设计的最终测试标准，而不仅仅是丰富的功能</p></li><li><p>尽早交流和持续沟通能够使得结构师有较好的成本意识，使开发人员获得对设计的信心，并且不会混淆各自的责任分工</p></li><li><p>交流</p><ul><li>巴比伦项目的失败是因为缺乏交流以及交流的结果 —— 组织</li><li>因为左手不知道右手在做什么，从而进度灾难，功能的不合理，以及系统的缺陷纷纷出现。由于存在对其他人的各种假设，团队成员之间的理解开始出现偏差了。</li></ul></li><li><p>项目工作手册</p><ul><li>项目工作手册是对项目必须产生的一些列文档进行组织的一种结构</li><li>每一个团队成员应该了解所有的材料</li><li>实时更新很重要</li><li>工作手册的使用者应该将注意力集中在上次阅读之后的变更以及关于这些变更重要性的评述上</li><li>每个部分应该被封装，从而没有人需要或者被允许看到其他部分的内部结构，只需要了解接口</li></ul></li><li><p>组织架构</p><ul><li>团队组织的目标是为了减少必要的交流和协作量</li><li>为了减少交流，组织结构包括了人力划分(Division of labor) 和限定职责范围(Specialization of function)</li><li>组织内的交流是网状的，而不是树状结构，因此所有的特殊组织机制都是为了进行调整，来克服树状组织结构中交流缺乏的困难</li></ul></li><li><p>在大型团队当中，各个小组倾向于不断的去做局部优化，来满足自己的目标，而比较少的考虑对用户的整体影响。这种方向性的问题是大型项目的主要危险</p></li><li><p>从系统整体出发和面向用户的态度是软体编程管理人员最重要的职能</p></li><li><p>文档的规范，目标，用户手册，内部文档，进度，预算，组织机构图，和工作空间分配</p><ul><li>每个文档本身就可以作为检查列表或者数据库</li></ul></li><li><p>项目经理的基本职责是使每个人都向着相同的方向前进</p></li><li><p>项目经理的主要日常工作是沟通，而不是做出决定；文档使得各项计划和决策在整个团队范围内得到交流</p></li><li><p>用户的实际需要和用户感觉会随着程序构建，测试和使用而发生变化。</p></li><li><p>对于文档，需要采用定义良好的数字化版本将变更量子化</p></li><li><p>程序员不愿意为设计书写文档，不仅仅是因为惰性，更多的是源于设计人员的踌躇 —— 要为自己尝试性的设计决策进行辩解。</p></li><li><p>只要管理人员和技术人员的天赋允许，老板必须对他们的能力培养给予极大的关注，使得管理人员和技术人员具有互换性；特别是希望在技术和管理角色之间自由的分配人手的时候</p></li><li><p>具有两条晋升线的高效组织机构存在着一些社会性的障碍，人们必须警惕并积极的同它做持续的斗争</p></li><li><p>程序维护基本上不同于硬件的维护：主要由各种变更组成，入修复设计缺陷，新增功能，或者是使用环境或者配置变换引起的调整</p></li><li><p>对于一个广泛使用的程序，其维护总成本通常是开发成本的40%或者更多</p></li><li><p>Campbell指出了一个显示产品生命期中每月bug数的有趣曲线，其先是下降，后面是上升</p></li><li><p>每次修复之后，必须重新运行先前所有的测试用例，确保系统不会以更隐蔽的方式被破坏</p></li><li><p>所有的修改都倾向于破坏系统的架构，增加了系统的混乱程度(熵)。即使是最熟练的软件维护工作，也只是延缓了系统退化到不可修复的混乱状态的进程，以致必须重新进行设计。</p></li><li><p>项目经理应该制定一套策略，并为通用工具的开发分配资源，与此同时，还必须意识到专业工具的需求</p></li><li><p>调试是系统编程中较慢和较困难的部分，而漫长的调试周转时间是调试的祸根</p></li><li><p>在编写任何代码之前，规格说明必须提交给外部的测试人员，来详细的检查说明的完整性和明确性。开发人员自己无法完成这项工作。</p></li><li><p>开发大量的辅助测试平台和测试代码是很值得的，代码量甚至可能有测试对象的一半</p></li><li><p>项目是怎么样被延迟了整整一年的时间的….. 一次一天。一次一天的进度落后比重大灾难更难以识别，更不容易防范和更加难以弥补。</p></li><li><p>根据一个严格的进度表来控制大型项目的第一个步骤是制定进度表，进度表由里程碑和日期组成</p><ul><li>里程碑必须是具体的，特定的，可度量的事件，需要能够进行清晰的定义</li></ul></li><li><p>慢性进度偏离是士气杀手。</p></li><li><p>状态的获取是困难的，因为下属经理有充分的理由不提供信息共享</p></li><li><p>老板的不良反应肯定会对信息的完全公开造成压制；相反，仔细区分状态报告，毫无惊慌地接收报告，决不越俎代庖，将能够鼓励诚实的汇报。</p></li><li><p>必须有评审机制，使得所有成员可以通过它了解真正的状态。出于这个目的，里程碑的进度和完成文档是关键。</p></li><li><p>程序修改人员所使用的文档中，除了描述事情如何，还应当阐述它为什么那样。对于加深理解，目的是非常关键的，即使是高级语言的语法，也不能表达目的</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;软件活动的根本任务&lt;ul&gt;
&lt;li&gt;打造构成抽象软件实体的复杂概念结构&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;软件活动的次要任务1&lt;ul&gt;
&lt;li&gt;使用编程语言来表达这些抽象实体，在空间和时间的限制下将他们映射成机器语言&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="人月神话" scheme="https://www.llchen60.com/tags/%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D/"/>
    
      <category term="Project Management" scheme="https://www.llchen60.com/tags/Project-Management/"/>
    
  </entry>
  
  <entry>
    <title>Real time data’s unifying abstraction</title>
    <link href="https://www.llchen60.com/Real-time-data%E2%80%99s-unifying-abstraction/"/>
    <id>https://www.llchen60.com/Real-time-data%E2%80%99s-unifying-abstraction/</id>
    <published>2022-01-29T02:14:20.000Z</published>
    <updated>2022-01-29T02:20:03.851Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Real-time-data’s-unifying-abstraction"><a href="#Real-time-data’s-unifying-abstraction" class="headerlink" title="Real time data’s unifying abstraction"></a>Real time data’s unifying abstraction</h1><h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>Logs play a key role in distributed data systems and real time application architectures.<ul><li>write ahead log</li><li>commit log</li><li>transaction logs</li></ul></li></ul><h1 id="2-What-is-a-log"><a href="#2-What-is-a-log" class="headerlink" title="2. What is a log?"></a>2. What is a log?</h1><h2 id="2-1-Some-Concepts"><a href="#2-1-Some-Concepts" class="headerlink" title="2.1 Some Concepts"></a>2.1 Some Concepts</h2><ul><li>Storage abstraction<ul><li>append only</li><li>totally ordered sequence of records ordered by time</li></ul></li><li>Records<ul><li>appended to the end of the log</li><li>read from left to right</li><li>each entry has a unique sequential log entry number</li></ul></li><li>Log<ul><li>record what happened and when</li><li>for distributed system, that’s the very heart of the problem</li><li>types<ul><li>text logs<ul><li>meant primarily for humans to read</li></ul></li><li>journal/ data logs<ul><li>built for programmatic access</li></ul></li></ul></li></ul></li></ul><h2 id="2-2-Log-in-different-scenario"><a href="#2-2-Log-in-different-scenario" class="headerlink" title="2.2 Log in different scenario"></a>2.2 Log in different scenario</h2><h3 id="2-2-1-Logs-in-DB"><a href="#2-2-1-Logs-in-DB" class="headerlink" title="2.2.1 Logs in DB"></a>2.2.1 Logs in DB</h3><ul><li>Function 1: Authoritative source for restoring data<ul><li>DB need to keep in sync the variety of data structures and indexes in the presense of crashes</li><li>To make this atomic and durable, a db uses a log to <strong>write out information about the records</strong> they will be modifying, <strong>before applying the changes</strong> to all the various data structures it maintains</li><li>Since the log is <strong>immediately persisted</strong> it is used as the <strong>authoritative source</strong> in restoring all other persistent structures in the event of a crash.</li></ul></li><li>Function 2: Replicating data between DBs<ul><li>Oracle, MySQL and Postgres SQL include <strong>log shipping protocols</strong> to <strong>transmit portions of log to replica databases</strong> which act as slaves</li></ul></li></ul><h3 id="2-2-2-Logs-in-distributed-systems"><a href="#2-2-2-Logs-in-distributed-systems" class="headerlink" title="2.2.2 Logs in distributed systems"></a>2.2.2 Logs in distributed systems</h3><ul><li>Log Centric Approach</li></ul><aside>💡 State Machine Replication Principle: If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.</aside><ul><li>we could reduce the problem of making multiple machines all do the same thing to the problem of <strong>implementing a distributed consistent log to feed these processes input</strong><ul><li>squeeze all the non-determinism out of the input stream to ensure that each replica processing this input stays in sync.</li><li>time stamps that index the log now act as <strong>the clock for the state of the replicas</strong>—you can describe each replica by a single number, the timestamp for the maximum log entry it has processed.</li></ul></li></ul><h1 id="3-Log-Types"><a href="#3-Log-Types" class="headerlink" title="3. Log Types"></a>3. Log Types</h1><ul><li>What we could put in log<ul><li>log the incoming requests to a service</li><li>the state changes the service undergoes in response to request</li><li>the transformation commands it executes.</li></ul></li><li>For DB usage<ul><li>physical logging<ul><li>log the contents of each row that is changed</li></ul></li><li>logical logging<ul><li>log not the changed rows but the SQL commands that lead to the row chagnes</li></ul></li></ul></li><li>For Distributed Systems to process and replicate<ul><li>Primary Backup<ul><li>elect one replica as the leader</li><li>allow this leader to process requests in the order they arrive</li><li>log out the changes to its state from processing the requests.</li><li>The other replicas apply in order the state changes the leader makes so that they will be in sync and ready to take over as leader should the leader fail.</li><li><strong>backup will copy the result from the primary, no logical action to walk through all action primary did</strong></li></ul></li></ul></li></ul><pre><code>- State Machine Replication    - active-active model where we keep a log of the incoming requests and each replica processes each request    - **each machine will do real execution, do the logical stuff**    ![State Machine Replication](https://s2.loli.net/2022/01/29/6JtLFNpmXBYW9wO.png)</code></pre><aside>💡 Below 3 sections has a centric idea:  Log as a stand-alone service. The usefulness of the log comes from simple function that the log provides: **producing a persistent, re-playable record of history**. At the core of these problems is the ability to **have many machines playback history at their own rate in a deterministic manner**</aside><h1 id="4-Changelog-in-Database"><a href="#4-Changelog-in-Database" class="headerlink" title="4. Changelog in Database"></a>4. Changelog in Database</h1><ul><li>duality between a log of changes and a table<ul><li>The log is similar to the list of all credits and debits and bank processes;</li><li>a table is all the current account balances.</li><li>If you have a log of changes, you can apply these changes in order to create the table capturing the current state.</li></ul></li><li>if you have a table taking updates, you can record these changes and publish a “changelog” of all the updates to the state of the table. This changelog is exactly what you need to <strong>support near-real-time replicas</strong>.</li><li>Table support data at rest and logs capture changes</li></ul><aside>💡 The magic of the log is that if it’s complete log of changes, it holds not only the contents of the final version of the table, but also allows recreating all other versions that might have existed. That’s a backup of every previous state of the table</aside><h1 id="5-Data-Integration"><a href="#5-Data-Integration" class="headerlink" title="5. Data Integration"></a>5. Data Integration</h1><blockquote><p>Make all of an organization’s data easily available in all its storage and processing systems</p></blockquote><h2 id="5-1-Expected-workflow-for-data-integration"><a href="#5-1-Expected-workflow-for-data-integration" class="headerlink" title="5.1 Expected workflow for data integration"></a>5.1 Expected workflow for data integration</h2><ul><li>Definition in author’s scope: Making all the data an organization has available in all its services and systems.</li><li>Effective Use of Data<ul><li>Capture all relevant data</li><li>Put it together in an applicable processing env<ul><li>real time query system</li><li>text files</li><li>python scripts, etc.</li></ul></li><li>Infra to process data<ul><li>mapReduce</li><li>Real time query systems</li></ul></li><li>Good data models and consistent well understood semantics</li><li>Sophisticated processing<ul><li>visualization</li><li>reporting</li><li>algorithmic processing and prediction</li></ul></li></ul></li></ul><h2 id="5-2-Problem1-The-event-data-firehose"><a href="#5-2-Problem1-The-event-data-firehose" class="headerlink" title="5.2 Problem1: The event data firehose"></a>5.2 Problem1: The event data firehose</h2><ul><li>Event data rising</li><li>google’s fortune is actually generated by a relevance pipeline built on clicks and impressions — events</li><li>that would be huge amount of data,</li></ul><h2 id="5-3-Problem-2-The-explosion-of-specialized-data-systems"><a href="#5-3-Problem-2-The-explosion-of-specialized-data-systems" class="headerlink" title="5.3 Problem 2: The explosion of specialized data systems"></a>5.3 Problem 2: The explosion of specialized data systems</h2><ul><li>Explosion of specialized data systems</li><li>The combination of more data of more varieties and a desire to get this data into more systems leads to a huge data integration problem.</li></ul><h2 id="5-4-Log-Structured-Data-Flow"><a href="#5-4-Log-Structured-Data-Flow" class="headerlink" title="5.4 Log Structured Data Flow"></a>5.4 Log Structured Data Flow</h2><h3 id="5-4-1-How-the-flow-work"><a href="#5-4-1-How-the-flow-work" class="headerlink" title="5.4.1 How the flow work"></a>5.4.1 How the flow work</h3><ul><li>Recipe: Take all the organization’s data and put it into a central log for <strong>real time subscription</strong></li><li>How the flow works<ul><li>Each logical data source can be modeled as its own log</li><li>A data source could be an application that logs out events, or a db table that accepts modifications</li><li>each subscribing system reads from this log <strong>as quickly as it can</strong>, <strong>applied each new record to its own store</strong>, and <strong>advances its position</strong> in the log</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/29/ykIEs4XwnBD3LUF.png" alt="How the flow work"></p><ul><li><p>Log gives a <strong>logical clock</strong> for each change against which all subscriber can be measured</p><ul><li>Consider a case where there is a database and a collection of caching servers</li><li>log provides a way to synchronize the updates to all these systems and reason about the point of time of each of these systems</li><li>Let’s say we <strong>write a record with log entry X</strong> and then need to do a read from the cache. If we want to guarantee we don’t see stale data, we just need to ensure we <strong>don’t read from any cache which has not replicated up to X.</strong></li></ul></li><li><p>Log also acts as a <strong>buffer</strong> that makes <strong>data production asynchronous from data consumption</strong></p><ul><li>satisfy different requirements like<ul><li>A batch system such as Hadoop or a data warehouse may consume only hourly or daily,</li><li>A real-time query system may need to be up-to-the-second.</li></ul></li></ul></li><li><p>Consumer only need to know about the log and not any details of the system of origin</p></li><li><p>What values most from author perspective</p><ul><li>The pipeline they built for process data, though a bit of a mess, were actually extremely valuable . Just the process of makeing data available in a new processing system (Hadoop) unblocked a lot of possibilities<ul><li>Many new products and analysis just came from putting together multiple pieces of data that had previously been locked up in specialized systems</li></ul></li></ul></li><li><p>LinkedIn Went Through from O(N^2) to O(2N)</p></li></ul><p><img src="https://s2.loli.net/2022/01/29/fXnmrpYFLSVGoR9.png" alt="Pre Architecture LinkedIn"><br><img src="https://s2.loli.net/2022/01/29/9dZa67tYEl1yc8e.png" alt="Cur Architecture LinkedIn"></p><ul><li>Actions for the migration<ul><li>Isolate each consumer from the source of the data</li><li>Create a new data system to be both a data source and a data destination</li><li>Here LinkedIn create Kafka</li><li>Kinesis is similar to Kafka as AWS use it to connects all different distributed systems as a piping</li></ul></li></ul><h3 id="5-4-2-Relationship-to-ETL-and-the-Data-Warehouse"><a href="#5-4-2-Relationship-to-ETL-and-the-Data-Warehouse" class="headerlink" title="5.4.2  Relationship to ETL and the Data Warehouse"></a>5.4.2  Relationship to ETL and the Data Warehouse</h3><ul><li><p>Data Warehouse</p><ul><li>target<ul><li>A repository of the clean, integrated data structured to support analysis</li></ul></li><li>what be involved<ul><li>periodically extracting data from source databases</li><li>munging it into some kind of understandable form</li><li>loading it into a central data warehouse</li></ul></li><li>Problems<ul><li>coupling the clean integrated data to the data warehouse.<ul><li>cannot get real time feed</li></ul></li><li>organization perspective<ul><li>The incentives are not aligned: data producers are often not very aware of the use of the data in the data warehouse and end up creating data that is hard to extract or requires heavy, hard to scale transformation to get into usable form.</li><li>the central team never quite manages to scale to match the pace of the rest of the organization, so data coverage is always spotty, data flow is fragile, and changes are slow.</li></ul></li></ul></li></ul></li><li><p>ETL</p><ul><li>tasks<ul><li>extraction and data cleanup process, liberating data locked up in a variety of systems in the organization and removing an system-specific non-sense</li><li>data is restructured for data warehousing queries (i.e. made to fit the type system of a relational DB, forced into a star or snowflake schema, perhaps broken up into a high performance <a href="http://parquet.io/">column</a> <a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html">format</a>,</li></ul></li><li>problems<ul><li>still, we need such data in real time as well for low latency processing as well as indexing in real time storage systems</li></ul></li></ul></li><li><p>A better approach as ETL and Data Warehouse substitution</p><ul><li><p>Have a central pipeline, the log, with a well defined API for adding data</p></li><li><p>Responsibility Classification</p><ul><li><p>Producer of the data feed: integrating with this pipeline and providing a clean, well-structured data feed</p></li><li><p>Datawarehouse team now <strong>only care about loading structured feeds</strong> of data from the <strong>central log</strong> and <strong>carrying out transformation specific to their system</strong></p><p>  <img src="https://s2.loli.net/2022/01/29/fMl9IQwDGk8TL4o.png" alt="Workflow and ownership classification">            </p></li></ul></li></ul></li></ul><h3 id="5-4-3-Log-Files-and-Events"><a href="#5-4-3-Log-Files-and-Events" class="headerlink" title="5.4.3 Log Files and Events"></a>5.4.3 Log Files and Events</h3><ul><li>Current structure also enables decoupled and event driven systems</li></ul><h3 id="5-4-4-How-to-build-scalable-logs"><a href="#5-4-4-How-to-build-scalable-logs" class="headerlink" title="5.4.4 How to build scalable logs"></a>5.4.4 How to build scalable logs</h3><ul><li>Need a log system that’s fast, cheap, scalable enough to make this practical at scale</li><li>LinkedIn in 2013 actually has already support 60 billion unique message writes through Kafka per day</li><li>Kafka achieve such high throughput via<ul><li>Partitioning the log<ul><li>each partition is a <strong>totally ordered log</strong>, but there is <strong>no global ordering between partitions</strong></li><li>Assignment of the messages to a particular partition is controllable by the writer, with most users choosing to partition by some kind of key</li><li>Replication<ul><li>Each partition is replicated across a configurable number of replicas</li><li>At any time, a single one of them will act as the leader, if the leader fails, one of the replicas will take over as leader</li></ul></li><li>Order Guarantee<ul><li>each partition is order preserving, and Kafka guarantees that appends to a particular partition from a single sender will be delivered in the order they are sent.</li></ul></li></ul></li><li>Optimizing throughput by batching reads and writes<ul><li>occurs when<ul><li>sending data</li><li>writes to disk</li><li>replication between servers</li><li>data transfer to consumers</li><li>acknowledging committed data</li></ul></li></ul></li><li>Avoiding needless data copies<ul><li>Use a simple binary format that is maintained between in memory log, on disk log and in network data transfers</li><li>Thus we could make use of numerous optimizations including zero copy data transfer <a href="https://en.wikipedia.org/wiki/Zero-copy">https://en.wikipedia.org/wiki/Zero-copy</a></li></ul></li></ul></li></ul><h1 id="6-Real-Time-Data-Processing"><a href="#6-Real-Time-Data-Processing" class="headerlink" title="6. Real Time Data Processing"></a>6. Real Time Data Processing</h1><blockquote><p>Computing derived data streams</p></blockquote><h2 id="6-1-Definition-of-Stream-Processing"><a href="#6-1-Definition-of-Stream-Processing" class="headerlink" title="6.1 Definition of Stream Processing"></a>6.1 Definition of Stream Processing</h2><ul><li>Infrastructure for continuous data processing<ul><li>computational model can be general like MapReduce or other distributed processing frameworks,</li><li>need the ability to produce low latency results</li></ul></li><li>Instead of batch get and process, we could do continuous changes</li><li>it is just processing which includes a notion of time in the underlying data being processed and does not require a static snapshot of the data so it can produce output at a user-controlled frequency instead of waiting for the “end” of the data set to be reached. In this sense, stream processing is a generalization of batch processing, and, given the prevalence of real-time data, a very important generalization</li><li>Log role<ul><li>making data available in real-time multi-subscriber data feeds.</li></ul></li></ul><h2 id="6-2-Stateful-Real-Time-Processing"><a href="#6-2-Stateful-Real-Time-Processing" class="headerlink" title="6.2 Stateful Real Time Processing"></a>6.2 Stateful Real Time Processing</h2><ul><li>Stateful real time processing means some more sophisticated operations, like counts, aggregations, or joins over windows in the stream</li><li>We need to maintain certain state in such case</li><li>Strategies for that<ul><li>Keep state in memory<ul><li>cons<ul><li>if the process crash, it would lose its intermediate state</li><li>if the state is only maintained over a window, the process could fall back to the point where the window began</li></ul></li></ul></li><li>Store all state in a remote storage system, and join over the network to that store<ul><li>cons<ul><li>no locality of data and lots of network round trips</li></ul></li></ul></li><li>Duality of tables and logs<ul><li>a stream processor can keep its state in a local table or index — a bdb, leveldb</li><li>the contents of this store is fed from its input streams</li><li>it could journal out a changelog for this local index it keeps to allow it to restore its state in the event of a crash and restart</li><li>This mechanism allows a generic mechanism for keeping co-partitioned state in arbitrary index types local with the incoming stream data.</li><li>when facing process fails<ul><li>recover its index from the changelog</li><li>changelog itself is the transformation of the local state into a sort of incremental record at a time backup</li></ul></li></ul></li></ul></li></ul><h2 id="6-3-Log-Compaction"><a href="#6-3-Log-Compaction" class="headerlink" title="6.3 Log Compaction"></a>6.3 Log Compaction</h2><ul><li>Log need to be cleaned up someway to save the space</li><li>In Kafka, clean up has two options depending on whether the data contains keyed updates or event data<ul><li>for event data, supports just retain a window of data<ul><li>configured to be few days</li><li>also could be configured as space</li></ul></li><li>for keyed data<ul><li>as the complete log give you ability to replay it to recreate the state of the source system</li><li>but we could do log compaction by removing obsolete records — records whose primary key has a more recent update</li></ul></li></ul></li></ul><h1 id="7-Distributed-System-Design"><a href="#7-Distributed-System-Design" class="headerlink" title="7. Distributed System Design"></a>7. Distributed System Design</h1><blockquote><p>How practical systems can be simplified with a log centric design</p></blockquote><h2 id="7-1-Distributed-system-design-thought"><a href="#7-1-Distributed-system-design-thought" class="headerlink" title="7.1 Distributed system design thought"></a>7.1 Distributed system design thought</h2><p>Log here is responsible for data flow, consistency and recovery </p><ul><li><p>Directions</p><ul><li>Coalescing lots of little instances of each system into a few big clusters</li></ul></li><li><p>Possibility 1</p><ul><li>separation of systems remains more or less as it is for a good deal longer.</li><li>an external log that integrates data will be very important.</li></ul></li><li><p>Possibility 2</p><ul><li>re-consolidation in which a single system with enough generality starts to merge back in all the different functions into a single uber-system.</li><li>extremely hard</li></ul></li><li><p>Possibility 3</p><ul><li>data infrastructure could be unbundled into a collection of services and application-facing system apis</li><li>use open source, like in Java stacks<ul><li>zookeeper<ul><li>handle system coordination</li></ul></li><li>mesos and yarn<ul><li>process virtualization</li><li>resource management</li></ul></li><li>netty, jetty<ul><li>handle remote communication</li></ul></li><li>protobuf<ul><li>handle serialization</li></ul></li><li>kafka and bookeeper<ul><li>provide a backing log</li></ul></li></ul></li><li>path towards getting the simplicity of the single system in a more diverse and modular world that continues to evolve. If the implementation time for a distributed system goes from years to weeks because reliable, flexible building blocks emerge, then the pressure to coalesce into a single monolithic system disappears.</li></ul></li></ul><h2 id="7-2-Usage-of-log-in-system-architecture"><a href="#7-2-Usage-of-log-in-system-architecture" class="headerlink" title="7.2 Usage of log in system architecture"></a>7.2 Usage of log in system architecture</h2><ul><li><p>Usage of log in system architecture</p><ul><li>Handle data consistency (whether eventual or immediate) by sequencing concurrent updates to nodes</li><li>Provide data replication between nodes</li><li>Provide “commit” semantics to the writer (i.e. acknowledging only when your write guaranteed not to be lost)</li><li>Provide the external data subscription feed from the system</li><li>Provide the capability to restore failed replicas that lost their data or bootstrap new replicas</li><li>Handle rebalancing of data between nodes.</li></ul></li><li><p>What mentioned above is actually a large portion of what a distributed data system does. left over is mainly related with client facing query API and indexing strategy</p></li><li><p>System Look</p><ul><li>System is divided into two logical pieces<ul><li>log<ul><li>capture the state changes in sequential order</li></ul></li><li>serving layer<ul><li>store whatever index is required to serve queries</li></ul></li></ul></li><li>writes could go directly to the log or may be proxied by the serving layer</li><li>writes to the log yields a logical timestamp, if the system is partitioned, then the log and serving nodes will have the same number of partitions, though they may have very different numbers of machines</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/29/EXfRmnKxbGs8kHT.png" alt="Log and serving layer">    </p><ul><li><p>The client can get <strong>read-your-write semantics</strong> from any node by providing the <strong>timestamp of a write</strong> as part of its query—a serving node receiving such a query will <strong>compare the desired timestamp</strong> to <strong>its own index point</strong> and if necessary delay the request until it has indexed up to at least that time to avoid serving stale data.</p></li><li><p>For handling restoring failed nodes or moving partitions from node to node</p><ul><li>have the log retain only a fixed window of data and combine this with a snapshot of the data stored in the partition</li><li>it’s possible for the log to retain a complete copy of data and garbage collect the log itself</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying</a> </li><li><a href="https://kafka.apache.org/documentation.html#design">https://kafka.apache.org/documentation.html#design</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Real-time-data’s-unifying-abstraction&quot;&gt;&lt;a href=&quot;#Real-time-data’s-unifying-abstraction&quot; class=&quot;headerlink&quot; title=&quot;Real time data’s u
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="log" scheme="https://www.llchen60.com/tags/log/"/>
    
      <category term="distributed system" scheme="https://www.llchen60.com/tags/distributed-system/"/>
    
  </entry>
  
  <entry>
    <title>Flyway</title>
    <link href="https://www.llchen60.com/Flyway/"/>
    <id>https://www.llchen60.com/Flyway/</id>
    <published>2022-01-01T13:57:09.000Z</published>
    <updated>2022-01-01T13:57:52.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>An open source database migration tool</li><li>Favors simplicity and convention over configuration</li><li>has 7 basic commands<ul><li>migrate</li><li>clean</li><li>info</li><li>validate</li><li>undo</li><li>baseline</li><li>repair</li></ul></li></ul><h2 id="1-1-Why-DB-migrations"><a href="#1-1-Why-DB-migrations" class="headerlink" title="1.1 Why DB migrations?"></a>1.1 Why DB migrations?</h2><ul><li>we need a way to version the table</li><li>we need to know what state is the db on this machine</li><li>database migration help us<ul><li>recreate a database from scratch</li><li>make it clear at all times what state a database is in</li><li>migrate in a deterministic way from your current version of the database to a newer one</li></ul></li></ul><h2 id="1-2-How-Flyway-works"><a href="#1-2-How-Flyway-works" class="headerlink" title="1.2 How Flyway works?"></a>1.2 How Flyway works?</h2><ul><li><p>Flyway first try to locate its schema history table</p><ul><li>if db empty, then flyway will create it instead</li><li>this default db named as <code>flyway_schema_history</code></li></ul></li><li><p>Then flyway will begin scanning the filesystem or the classpath of the application for migrations</p></li><li><p>The migrations are then sorted based on the version number and applied in order</p></li><li><p>The schema history table will be updated accordingly as each migration gets applied</p></li><li><p>we use <code>flyway migrate</code> to execute the migration</p></li></ul><h1 id="2-Flyway-Commands"><a href="#2-Flyway-Commands" class="headerlink" title="2. Flyway Commands"></a>2. Flyway Commands</h1><h2 id="2-1-migrate"><a href="#2-1-migrate" class="headerlink" title="2.1 migrate"></a>2.1 <code>migrate</code></h2><ul><li>help migrate the db</li></ul><h2 id="2-2-clean"><a href="#2-2-clean" class="headerlink" title="2.2 clean"></a>2.2 <code>clean</code></h2><ul><li>drop all objects in the confgured schemas</li></ul><h2 id="2-3-info"><a href="#2-3-info" class="headerlink" title="2.3 info"></a>2.3 <code>info</code></h2><ul><li>print the details and status information about all migrations</li></ul><h2 id="2-4-validate"><a href="#2-4-validate" class="headerlink" title="2.4 validate"></a>2.4 <code>validate</code></h2><ul><li>validates the applied migrations against the ones available on the classpath</li></ul><h2 id="2-5-undo"><a href="#2-5-undo" class="headerlink" title="2.5 undo"></a>2.5 <code>undo</code></h2><ul><li>undoes the most recently applied versioned migration</li></ul><h2 id="2-6-baseline"><a href="#2-6-baseline" class="headerlink" title="2.6 baseline"></a>2.6 <code>baseline</code></h2><ul><li>baselines an existing database, excluding all migrations up and including baseline version</li></ul><h2 id="2-7-repair"><a href="#2-7-repair" class="headerlink" title="2.7 repair"></a>2.7 <code>repair</code></h2><ul><li>repair the schema history table</li></ul><h1 id="3-Concepts"><a href="#3-Concepts" class="headerlink" title="3. Concepts"></a>3. Concepts</h1><h2 id="3-1-Migrations"><a href="#3-1-Migrations" class="headerlink" title="3.1 Migrations"></a>3.1 Migrations</h2><ul><li>all changes to the db are called migrations</li><li>migrations can be<ul><li>versioned<ul><li>types<ul><li>regular</li><li>undo<ul><li>the effect can be undone by supplying an undo migration</li></ul></li></ul></li><li>contains<ul><li><strong>version</strong><ul><li>must be unique</li></ul></li><li><strong>description</strong><ul><li>purely informative for you to be able to remember what each migration does</li></ul></li><li><strong>checksum</strong><ul><li>detect accidental changes</li></ul></li></ul></li></ul></li><li>repeatable<ul><li>contains<ul><li><strong>description</strong></li><li><strong>checksum</strong></li></ul></li><li>instead of being run just once, they are re-applied every time their checksum changes</li><li>Within a single migration run, repeatable migrations are always <strong>applied last</strong>, after all pending versioned migrations have been executed. Repeatable migrations are applied in the order of their description</li></ul></li></ul></li></ul><h3 id="3-1-1-Versioned-Migrations"><a href="#3-1-1-Versioned-Migrations" class="headerlink" title="3.1.1 Versioned Migrations"></a>3.1.1 Versioned Migrations</h3><ul><li>contains<ul><li>version<ul><li><strong>must be unique</strong></li><li>applied in order exactly once</li></ul></li><li>description<ul><li>purely informative for you to be able to remember what each migration does</li></ul></li><li>checksum<ul><li>detect accidental changes</li></ul></li></ul></li><li>used for<ul><li>creating/ altering/ dropping tables/ indexes/ foreign keys/ enums</li><li>reference data updates</li><li>user data corrections</li></ul></li></ul><h3 id="3-1-2-Undo-Migrations"><a href="#3-1-2-Undo-Migrations" class="headerlink" title="3.1.2 Undo Migrations"></a>3.1.2 Undo Migrations</h3><ul><li>A migration can fail at any point. If you have 10 statements, it is possible for the 1st, the 5th, the 7th or the 10th to fail. There is simply no way to know in advance. In contrast, undo migrations are written to undo an entire versioned migration and will not help under such conditions.</li><li>we should <strong>maintain backwards compatibility</strong> between the <strong>DB and all versions of the code</strong> currently deployed in production</li></ul><h3 id="3-1-3-Repeatable-Migrations"><a href="#3-1-3-Repeatable-Migrations" class="headerlink" title="3.1.3 Repeatable Migrations"></a>3.1.3 Repeatable Migrations</h3><ul><li><p>contains</p><ul><li>description and a checksum, but no version</li></ul></li><li><p>repeatable migrations are re-applied every time their checksum changes</p></li><li><p>Very useful for managing database objects whose definition can then simply be maintained in a single file in version control</p></li><li><p>Repeatable migrations are always applied last, after all pending versioned migrations have been executed; always applied in the order of their description</p></li></ul><h3 id="3-1-4-SQL-Based-Migrations"><a href="#3-1-4-SQL-Based-Migrations" class="headerlink" title="3.1.4 SQL Based Migrations"></a>3.1.4 SQL Based Migrations</h3><ul><li>used for<ul><li>DDL change — CREATE/ALTER/DROP statements for TABLES,VIEWS,TRIGGERS,SEQUENCES,…</li><li>Simple reference data changes</li><li>simple bulk data changes</li></ul></li><li>Naming  Patterns<ul><li>Prefix<ul><li>v for versioned</li><li>u for undo</li><li>r for repeatable migrations</li></ul></li><li>version<ul><li>with dots or underscores separate as many parts as you like</li></ul></li><li>Separator<ul><li>__ two underscores</li></ul></li><li>Suffix<ul><li><code>.sql</code></li></ul></li></ul></li><li>Discovery<ul><li>Flyway discover sql migrations from directories <strong>referenced by the location property</strong></li></ul></li></ul><h3 id="3-1-5-Script-Based-Migrations"><a href="#3-1-5-Script-Based-Migrations" class="headerlink" title="3.1.5 Script Based Migrations"></a>3.1.5 Script Based Migrations</h3><ul><li>name patten<ul><li>``V1__execute_batch_tool.sh`</li></ul></li><li>could be used for<ul><li>triggering execution of a 3rd party application as part of the migrations</li><li>cleaning up local files</li></ul></li></ul><h3 id="3-1-6-Transactions"><a href="#3-1-6-Transactions" class="headerlink" title="3.1.6 Transactions"></a>3.1.6 Transactions</h3><ul><li>By default, Flyway <strong>wraps the execution of an entire migration within a single transaction</strong></li></ul><h2 id="3-2-Callbacks"><a href="#3-2-Callbacks" class="headerlink" title="3.2 Callbacks"></a>3.2 Callbacks</h2><ul><li><p>For the case we need to execute same action over and over again</p></li><li><p>we could hook into its lifecycle</p></li><li><p>there are certain keywords we could use, and invoke them during the process</p></li></ul><p><a href="https://flywaydb.org/documentation/concepts/callbacks">https://flywaydb.org/documentation/concepts/callbacks</a> </p><h2 id="3-3-Error-Overrides"><a href="#3-3-Error-Overrides" class="headerlink" title="3.3 Error Overrides"></a>3.3 Error Overrides</h2><ul><li>By default, in case an error is returned, flyway displays it with all necessary details, marks the migration as failed and automatically rolls it back if possible</li><li>But we could change the behavior like<ul><li>treat an error as a waring</li><li>treat a waring as an error</li><li>perform an additional action</li></ul></li></ul><h2 id="3-4-Dry-Runs"><a href="#3-4-Dry-Runs" class="headerlink" title="3.4 Dry Runs"></a>3.4 Dry Runs</h2><ul><li>Used for<ul><li>preview changes Flyway will make to the db</li><li>submit the SQL statements for review</li><li>use Flyway to determine what needs updating,</li></ul></li><li>how it works<ul><li>flyway sets up a read only connection to the db,</li><li>assesses what migrations need to run and generates a single SQL file containing all statements it would have executed in case of a regular migration run</li></ul></li></ul><h2 id="3-5-Baseline-Migrations"><a href="#3-5-Baseline-Migrations" class="headerlink" title="3.5 Baseline Migrations"></a>3.5 Baseline Migrations</h2><ul><li><p>Over the lifetime of a project, there would be tons of db objects be created/ destroyed across many migrations</p><ul><li>we want to simplify with a single, cumulative migration that represents the state of db after all of those migrations have been applied without disrupting existing env</li></ul></li><li><p>How it works?</p><ul><li>Prefixed with B followed by the version of your db they represent</li><li>Only used when deploying to new env</li><li>If used in an env where some Flyway migrations have already been applied, <strong>baseline migrations will be ignored,</strong> <strong>new env will choose the latest baseline migration as the starting point</strong><ul><li>every migration with a version below the latest baseline migration’s version is marked as ignored</li></ul></li><li>baseline migration are executed during the migrate process</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://flywaydb.org/documentation/">https://flywaydb.org/documentation/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;An open source database migrat
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="database" scheme="https://www.llchen60.com/tags/database/"/>
    
      <category term="migration" scheme="https://www.llchen60.com/tags/migration/"/>
    
  </entry>
  
  <entry>
    <title>Protobuf Rampup</title>
    <link href="https://www.llchen60.com/Protobuf-Rampup/"/>
    <id>https://www.llchen60.com/Protobuf-Rampup/</id>
    <published>2021-12-25T02:09:49.000Z</published>
    <updated>2021-12-25T02:11:09.003Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Protobuf-Learning"><a href="#Protobuf-Learning" class="headerlink" title="Protobuf Learning"></a>Protobuf Learning</h1><h1 id="1-What-are-Protocol-Buffers"><a href="#1-What-are-Protocol-Buffers" class="headerlink" title="1. What are Protocol Buffers?"></a>1. What are Protocol Buffers?</h1><ul><li>Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler.</li><li>You <strong>define how you want your data to be structured</strong> once, then you <strong>use special generated source code to easily write and read your structured data</strong></li></ul><h1 id="2-Why-use-Protocol-Buffers"><a href="#2-Why-use-Protocol-Buffers" class="headerlink" title="2. Why use Protocol Buffers?"></a>2. Why use Protocol Buffers?</h1><ul><li>XML is human readable and wide language supports<ul><li>but is notoriously space intensive</li><li>encoding/ decoding can impose a huge performance penalty on applications</li></ul></li><li>With protocol buffers<ul><li>write a <code>.proto</code> description of the data structure</li><li>the <strong>protocol buffer compiler</strong> then <strong>creates a class</strong> that implements <strong>automatic encoding and parsing</strong> of the protocol buffer data with an <strong>efficient binary format</strong></li><li>the generated class <strong>provides getters and setters</strong> for the fields</li><li>take care of the details of reading and writing the protocol buffer <strong>as a unit</strong></li></ul></li></ul><h1 id="3-Java-Tutorial-In-Proto2"><a href="#3-Java-Tutorial-In-Proto2" class="headerlink" title="3. Java Tutorial (In Proto2)"></a>3. Java Tutorial (In Proto2)</h1><h2 id="3-1-Define-Protocol-Format"><a href="#3-1-Define-Protocol-Format" class="headerlink" title="3.1 Define Protocol Format"></a>3.1 Define Protocol Format</h2><pre><code class="protobuf">syntax = &quot;proto2&quot;;// starts with package delcaration // we should define this to get rid of name conflict package tutorial;// enable generating a separate .java file for each generated class option java_multiple_files = true;// specify in what java package name your generated classes should live// if not set here, it will simply match the pkg name given by the package declaration option java_package = &quot;com.example.tutorial.protos&quot;;// define the class name of the wrapper class which will represent this file // if not given, it will be auto generated by converting the file name to upper camel case option java_outer_classname = &quot;AddressBookProtos&quot;;/**Message Definition: An aggregate containing a set of typed fields Contain certain standard types    + boo1    + int32     + float     + double     + string we could also add further structure to msgs by using other msg types as field types + marker     + identify the unique tag field use in binary encoding     + try to use 1 - 15 as it neeeds one less byte+ modifier     + optional         + field may or may not be set         + if not, a default value will be used             + we could set our own default values             + or system will provide defaults                 + numeric types -- zero                 + strings -- empty string                 + bools -- false                 + embedded messages -- default instance or prototype of the message, which has none of its fields set     + repeated         + the field may be repeated any number of times [0, xxx)         + order will be preserved in the protocol buffer         + act like a dynamic sized array     + required         + a value for the field must be provided        + try to build an uninitialized msg will throw runtime exception         + parse an uninitialzied msg will throw IOException         + required is not favored as it cannot be backward compatible */message Person &#123;    // =1 marker identify the unique tag that field uses in the binary encoding   optional string name = 1;  optional int32 id = 2;  optional string email = 3;  enum PhoneType &#123;    MOBILE = 0;    HOME = 1;    WORK = 2;  &#125;  message PhoneNumber &#123;    optional string number = 1;    optional PhoneType type = 2 [default = HOME];  &#125;  repeated PhoneNumber phones = 4;&#125;message AddressBook &#123;  repeated Person people = 1;&#125;</code></pre><h2 id="3-2-Compiling-Protocol-Buffers"><a href="#3-2-Compiling-Protocol-Buffers" class="headerlink" title="3.2 Compiling Protocol Buffers"></a>3.2 Compiling Protocol Buffers</h2><ul><li>To generate the classes, we need to run the protocol buffer compiler</li><li>specify the source directory, the destination directory and the path to our <code>.proto</code></li></ul><pre><code class="protobuf">protoc -I=$SRC_DIR --java_out=$DST_DIR $SRC_DIR/addressbook.proto </code></pre><h2 id="3-3-Protocol-Buffer-API"><a href="#3-3-Protocol-Buffer-API" class="headerlink" title="3.3 Protocol Buffer API"></a>3.3 Protocol Buffer API</h2><ul><li>compiler helps auto generate source file<ul><li>getters and setters</li><li>each field also has <code>clear</code> method to set the field back to its empty state</li></ul></li><li>Builders vs Messages<ul><li>message classes are immutable</li><li>builder is used when you first construct a builder, then we could call the builder’s build() method</li></ul></li><li>standard message methods<ul><li><code>isInitialized</code> check if all the required fields have been set</li><li><code>toString</code> returns a human readable representation of the msg</li><li><code>mergeFrom(Message other)</code> merge the contents of other into this msg, overwrite singular scalar fields</li><li><code>clear</code> clear all the fields back to the empty state</li></ul></li><li>Parsing and Serialization<ul><li><code>byte[] toByteArray();</code><ul><li>serializes the msg and returns a byte array containing its raw bytes</li></ul></li><li><code>static xxx parseFrom(byte[] data);</code><ul><li>parse a msg from the given byte array</li></ul></li><li><code>void writeTo(OutputStream output);</code><ul><li>serialize the msg and writes to an OutputStream</li></ul></li><li><code>static xxx parseFrom(InputStream input);</code><ul><li>reads and parses a msg from an InputStream</li></ul></li></ul></li></ul><h2 id="3-4-How-to-extend-a-Protocol-Buffer"><a href="#3-4-How-to-extend-a-Protocol-Buffer" class="headerlink" title="3.4 How to extend a Protocol Buffer"></a>3.4 How to extend a Protocol Buffer</h2><ul><li>In the new version of the protocol buffer<ul><li>must not change the tag numbers of any existing fields</li><li>must not add or delete any required fields</li><li>may delete optional or repeated fields</li><li>may add new optional or repeated fields but must use fresh tag numbers</li></ul></li></ul><h1 id="4-Overall-Guide-In-Proto3"><a href="#4-Overall-Guide-In-Proto3" class="headerlink" title="4. Overall Guide (In Proto3)"></a>4. Overall Guide (In Proto3)</h1><h2 id="4-1-Define-message-type"><a href="#4-1-Define-message-type" class="headerlink" title="4.1 Define message type"></a>4.1 Define message type</h2><ul><li>Each field in the msg definition need to have a <strong>unique number</strong><ul><li>those numbers are used to identify fields in the message binary format</li><li>the number should never be changed</li></ul></li><li>specify field rules<ul><li>singular<ul><li>default field rule for proto3 syntax</li><li>can have <strong>zero or one of this field</strong></li></ul></li><li>repeated<ul><li>can be repeated any number of times (including zero)</li></ul></li></ul></li><li>reserved fields<ul><li>if you update a msg type by entirely removing a field or commenting it out, future users can reuse the field number but it would bring severe issues,</li><li>thus we could reserved the number for deleted fields and tag number</li></ul></li></ul><pre><code class="protobuf">message Foo &#123;  reserved 2, 15, 9 to 11;  reserved &quot;foo&quot;, &quot;bar&quot;;&#125;</code></pre><ul><li>Post compiler running<ul><li>Compiler generates a <code>.java</code> file with a class for each message type, as well as Builder classes for creating message class instances</li></ul></li><li>For enum values<ul><li>every enum definition must contain a constant that maps to zero as its first element</li><li>we can allow alias thus we could assign the same value to different enum constants</li></ul></li><li>import<ul><li>we could do import thus we could use definitions from other <code>.proto</code> file</li></ul></li></ul><h2 id="4-2-Scalar-Value-Types"><a href="#4-2-Scalar-Value-Types" class="headerlink" title="4.2 Scalar Value Types"></a>4.2 Scalar Value Types</h2><p><a href="https://developers.google.com/protocol-buffers/docs/proto3#scalar">Language Guide (proto3) | Protocol Buffers | Google Developers</a></p><h2 id="4-3-Nested-Types"><a href="#4-3-Nested-Types" class="headerlink" title="4.3 Nested Types"></a>4.3 Nested Types</h2><ul><li>we could define and use msg types inside other msg types</li></ul><pre><code class="protobuf">message SearchResponse &#123;  message Result &#123;    string url = 1;    string title = 2;    repeated string snippets = 3;  &#125;  repeated Result results = 1;&#125;// to use the msg type outside its parent message type message SomeOtherMessage &#123;  SearchResponse.Result result = 1;&#125;</code></pre><h2 id="4-4-Updating-a-Message-Type"><a href="#4-4-Updating-a-Message-Type" class="headerlink" title="4.4 Updating a Message Type"></a>4.4 Updating a Message Type</h2><ul><li>don’t change the field numbers for any existing fields</li><li>if you add new fields, any msg serialized by code using your old msg format can still be parsed by your new generated code<ul><li>keep in mind the default values for these elements so that new code can properly interact with msgs generated by old code</li></ul></li><li>to remove a field<ul><li>rename the field with prefix like <code>OBSOLETE_</code></li><li>or make the filed number reserved,</li></ul></li><li>int32, uint32, int64, uint64 and bool are all compatible</li><li>string and bytes are compatible as long as the bytes are valid UTF-8</li></ul><h2 id="4-5-Special-Keywords"><a href="#4-5-Special-Keywords" class="headerlink" title="4.5 Special Keywords"></a>4.5 Special Keywords</h2><h3 id="4-5-1-Any"><a href="#4-5-1-Any" class="headerlink" title="4.5.1 Any"></a>4.5.1 <code>Any</code></h3><ul><li>let you use messages as embedded types without having their .proto definition</li><li>it contains an aribitrary serialized messages as bytes</li></ul><h3 id="4-5-2-Oneof"><a href="#4-5-2-Oneof" class="headerlink" title="4.5.2 Oneof"></a>4.5.2 Oneof</h3><ul><li>if we have a msg with many fields and where at most one field will be set at the same time, we can enforce the behavior and save memory by using the oneof feature</li><li>at most one field can be set at the same time</li><li>setting any member of the oneof automatically clears all the other members</li></ul><h2 id="4-6-Maps"><a href="#4-6-Maps" class="headerlink" title="4.6 Maps"></a>4.6 Maps</h2><ul><li><code>map&lt;key_type, value_type&gt; map_field = N;</code></li></ul><h2 id="4-7-Define-Service"><a href="#4-7-Define-Service" class="headerlink" title="4.7 Define Service"></a>4.7 Define Service</h2><ul><li>If you want to use message types with an RPC system, we can define an RPC service interface in a <code>.proto</code> file</li><li>then the protocol buffer compiler will <strong>generate service interface code and stubs</strong> in chosen language</li></ul><h2 id="4-8-Options"><a href="#4-8-Options" class="headerlink" title="4.8 Options"></a>4.8 Options</h2><ul><li><p>Options do not change the overall meaning of a declaration, but may affect the way it is handled in a particular context.</p></li><li><p>java_package</p><ul><li>pkg you want to use for your generated Java classes</li></ul></li><li><p>java_outer_classname</p><ul><li>class name for the wrapper java class you want to generate</li></ul></li><li><p>java_multiple_files</p></li><li><p><code>optimize_for</code></p><ul><li><code>SPEED</code><ul><li>Compiler will generate code for serializing, parsing and performing other common operations on your msg types.</li><li>Code is highly optimized</li></ul></li><li><code>CODE_SIZE</code><ul><li>generate minimal classes</li><li>operations will be slower</li></ul></li><li><code>LITE_RUNTIME</code><ul><li>only depend on the lite runtime library</li><li>usefyl for apps running on constrained platform like mobile phones</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Overview <a href="https://developers.google.com/protocol-buffers">https://developers.google.com/protocol-buffers</a> </li><li>Language Guide <a href="https://developers.google.com/protocol-buffers/docs/overview">https://developers.google.com/protocol-buffers/docs/overview</a> </li><li>Java Tutorial <a href="https://developers.google.com/protocol-buffers/docs/javatutorial">https://developers.google.com/protocol-buffers/docs/javatutorial</a> </li><li>Java Generated Code <a href="https://developers.google.com/protocol-buffers/docs/reference/java-generated">https://developers.google.com/protocol-buffers/docs/reference/java-generated</a> </li><li>Java Encoding <a href="https://developers.google.com/protocol-buffers/docs/encoding">https://developers.google.com/protocol-buffers/docs/encoding</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Protobuf-Learning&quot;&gt;&lt;a href=&quot;#Protobuf-Learning&quot; class=&quot;headerlink&quot; title=&quot;Protobuf Learning&quot;&gt;&lt;/a&gt;Protobuf Learning&lt;/h1&gt;&lt;h1 id=&quot;1-Wha
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Protobuf" scheme="https://www.llchen60.com/tags/Protobuf/"/>
    
  </entry>
  
  <entry>
    <title>Bazel Intro</title>
    <link href="https://www.llchen60.com/Bazel-Intro/"/>
    <id>https://www.llchen60.com/Bazel-Intro/</id>
    <published>2021-12-22T02:05:15.000Z</published>
    <updated>2021-12-22T02:07:14.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-What-is-Bazel"><a href="#1-What-is-Bazel" class="headerlink" title="1. What is Bazel?"></a>1. What is Bazel?</h1><ul><li><p>Bazel is a build and test tool built that supports building and testing multiple projects for multiple languages and build outputs</p></li><li><p>What</p><ul><li>Build and Test tool similar to Make, Maven, Gradle</li><li>Caches all the previously done work, tests or builds faster everytime</li><li>Support multi languages, multi platforms,</li><li>Support large code base across multi repos</li><li>build, test, and query to trace dependencies in the code</li></ul></li><li><p>Why</p><ul><li>scales</li><li>multi platform</li></ul></li><li><p>How</p><ul><li>Need a BUILD file</li></ul></li></ul><h1 id="2-Concepts"><a href="#2-Concepts" class="headerlink" title="2. Concepts"></a>2. Concepts</h1><ul><li>Workspace - With WORKSPACE<ul><li>dir contains the source file</li><li>considered as root</li></ul></li><li>WORKSPACE<ul><li>a blank text file, which identifies the directory and its content as a Bazel workspace</li><li>at the root of the project’s directory structure</li></ul></li><li>Repos - With WORKSPACE<ul><li>External repos are defined in the WORKSPACE file using workspace rules</li></ul></li><li>Packages - With BUILD<ul><li>A package is defined as a directory containing a file named BUILD or BUILD.bazel</li><li>which reside beneath top level directory in the ws</li><li>This file has instructions on how to <strong>run or build or test</strong> the project</li></ul></li><li>Rules<ul><li>written using a DSL named Starlark</li><li>thus are built for certain language already like rules_java, etc.</li></ul></li><li>Targets<ul><li>Pkg is container, element of pkg —- target</li><li>Most targets are files or rules<ul><li>File<ul><li>source files - written by people</li><li>generated files — generated by build tool</li></ul></li><li>rule<ul><li>specify relationship between set of inputs and output</li><li>output are always generated files</li></ul></li></ul></li></ul></li></ul><h1 id="3-Best-Practices"><a href="#3-Best-Practices" class="headerlink" title="3. Best Practices"></a>3. Best Practices</h1><ul><li>A project should always be able to run <code>bazel build //...</code> and <code>bazel test //...</code></li><li>You may declare third party dependencies<ul><li>either declare them as remote repositories in the WORKSPACE file</li><li>or put them in a directory called third_party under workspace directory</li></ul></li><li>everything should be built from source whenever possible, instead of depending on a library so file, we should create a BUILD file and build so from its sources, then depend on that target</li><li>for project specific options, use the configuration file under <code>workspace/.bazelrc</code></li><li>every directory that contains buildable files should be a package</li></ul><h1 id="4-Build-a-Java-Project"><a href="#4-Build-a-Java-Project" class="headerlink" title="4. Build a Java Project"></a>4. Build a Java Project</h1><h2 id="4-1-Bazel-Jave-Basic"><a href="#4-1-Bazel-Jave-Basic" class="headerlink" title="4.1 Bazel Jave Basic"></a>4.1 Bazel Jave Basic</h2><ul><li><p>Refer <a href="https://docs.bazel.build/versions/1.2.0/tutorial/java.html">https://docs.bazel.build/versions/1.2.0/tutorial/java.html</a></p></li><li><p>build rule tells bazel how to build the desired outputs, executable binaries or libraries</p><ul><li>the java binary rule tells bazel to build a jar file and a wrapper shell script</li></ul></li><li><p><code>bazel build //:ProjectRunner</code></p><ul><li>the <code>//</code> part is the location of our BUILD file relative to the root of the workspace</li><li><code>ProjectRunner</code> is the target name we define in the BUILD file</li></ul></li><li><p>we could review our dependency graph by using</p><ul><li><code>bazel query --notool_deps --noimplicit_deps &quot;deps(//:ProjectRunner)&quot; --output graph</code></li></ul></li></ul><pre><code class="ruby">// generate graph for class in use, and output as a svg file bazel query  --notool_deps --noimplicit_deps &quot;deps(//booking)&quot; --output graph &gt; /Users/lchen1/Documents/bookingGraph.in dot -Tsvg &lt; bookingGraph.in &gt; graph.svg</code></pre><h2 id="4-2-Specify-multiple-build-targets"><a href="#4-2-Specify-multiple-build-targets" class="headerlink" title="4.2 Specify multiple build targets"></a>4.2 Specify multiple build targets</h2><ul><li><p>Package Splits</p><ul><li><p>for larger project, we may want to split into multiple targets and packages to allow for fast incremental builds, this could also speed up builds by building multiple parts of a project at once</p><pre><code class="json">java_binary(  name = &quot;ProjectRunner&quot;,  srcs = [&quot;src/main/java/com/example/ProjectRunner.java&quot;],  main_class = &quot;com.example.ProjectRunner&quot;,  deps = [&quot;:greeter&quot;],)java_library(  name = &quot;greeter&quot;,  srcs = [&quot;src/main/java/com/example/Greeting.java&quot;],)</code></pre></li></ul></li></ul><ul><li>with this configuration, bazel will first build greeter library, then the projectRunner binary<ul><li>deps attribute tells bazel the greeter library is required to build the projectRunner binary</li></ul></li></ul><h2 id="4-3-Use-multiple-packages"><a href="#4-3-Use-multiple-packages" class="headerlink" title="4.3 Use multiple packages"></a>4.3 Use multiple packages</h2><pre><code class="json">java_binary(    name = &quot;runner&quot;,    srcs = [&quot;Runner.java&quot;],    main_class = &quot;com.example.cmdline.Runner&quot;,    deps = [&quot;//:greeter&quot;])</code></pre><ul><li>To make sure above works, we need to let greeter be visible to cmdline.Runner<ul><li>Let the resource owner set the visibility attribute</li><li>we need to do this cause Bazel by default makes target only visible to other targets in the same BUILD file</li><li>bazel uses target visibility to prevent issues such as libraries containing implementation details leaking into public APIs</li></ul></li></ul><pre><code class="json">java_library(    name = &quot;greeter&quot;,    srcs = [&quot;src/main/java/com/example/Greeting.java&quot;],    visibility = [&quot;//src/main/java/com/example/cmdline:__pkg__&quot;],    )</code></pre><h2 id="4-4-Use-labels-to-reference-targets"><a href="#4-4-Use-labels-to-reference-targets" class="headerlink" title="4.4 Use labels to reference targets"></a>4.4 Use labels to reference targets</h2><ul><li>Bazel uses target labels to reference targets<ul><li><code>//:ProjectRunner</code></li><li>sync as follow:<ul><li><code>//path/to/package:target-name</code></li></ul></li></ul></li><li>when referencing targets within the same BUILD file, we can skip the <code>//</code> workspace root identifier and just use <code>:target_name</code></li></ul><h1 id="5-E-G"><a href="#5-E-G" class="headerlink" title="5. E.G"></a>5. E.G</h1><ul><li>java_binary<ul><li>pre defined rule telling bazel to create a binary when a target is invoked</li></ul></li></ul><pre><code class="json">java_binary(        // target name     name = &quot;mymain&quot;,        // all source files, passed as glob, inside the fully qualified directory names on classpath     srcs = glob([&quot;src/main/java/com/abhi/*.java&quot;]),        // main runner class     main_class = &quot;com.abhi.MyMain&quot;,        // dependent classes/ interfaces to be included, not part of srcs     deps = [&quot;//another-dir:animal&quot;])</code></pre><ul><li>java_library<ul><li>pre-defined to create library as the name suggests</li></ul></li></ul><pre><code class="json">java_library(    name = &quot;animal&quot;,    srcs = [&quot;src/main/java/com/abhi/Animal.java&quot;],        // if other class is implemented in a different pkg, it has to be visible to main-dir     visibility = [&quot;//main-dir:__pkg__&quot;])</code></pre><ul><li>CLI Reference<ul><li><code>bazel build //main-dir:mymain</code><ul><li>// means a valid package name</li><li>mymain is the target name</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Bazel best practice <a href="https://docs.bazel.build/versions/main/best-practices.html">https://docs.bazel.build/versions/main/best-practices.html</a> </li><li>Bazel Overview  <a href="https://docs.bazel.build/versions/1.2.0/bazel-overview.html">https://docs.bazel.build/versions/1.2.0/bazel-overview.html</a> </li><li>Java Tutorial <a href="https://docs.bazel.build/versions/1.2.0/tutorial/java.html">https://docs.bazel.build/versions/1.2.0/tutorial/java.html</a> </li><li>How to specify targets to build <a href="https://docs.bazel.build/versions/main/guide.html#target-patterns">https://docs.bazel.build/versions/main/guide.html#target-patterns</a> </li></ol><p><a href="https://docs.bazel.build/versions/4.2.1/command-line-reference.html">Command-Line Reference</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-What-is-Bazel&quot;&gt;&lt;a href=&quot;#1-What-is-Bazel&quot; class=&quot;headerlink&quot; title=&quot;1. What is Bazel?&quot;&gt;&lt;/a&gt;1. What is Bazel?&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bazel
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Bazel" scheme="https://www.llchen60.com/tags/Bazel/"/>
    
      <category term="Package Management" scheme="https://www.llchen60.com/tags/Package-Management/"/>
    
  </entry>
  
  <entry>
    <title>GraphQL Read</title>
    <link href="https://www.llchen60.com/GraphQL-Read/"/>
    <id>https://www.llchen60.com/GraphQL-Read/</id>
    <published>2021-12-04T09:29:50.000Z</published>
    <updated>2021-12-04T09:31:23.613Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GraphQL-Read"><a href="#GraphQL-Read" class="headerlink" title="GraphQL Read"></a>GraphQL Read</h1><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li>QraphQL is<ul><li>a query language</li><li>a server side runtime for executing queries using a type system you define</li></ul></li></ul><h1 id="2-Queries-and-Mutations"><a href="#2-Queries-and-Mutations" class="headerlink" title="2. Queries and Mutations"></a>2. Queries and Mutations</h1><ul><li>Fields<ul><li>GraphQL is about asking specific fields on objects</li><li>Query has the same shape as result<ul><li>server knows exactly what fields the client is asking for</li></ul></li></ul></li></ul><pre><code class="ruby">&#123;    hero &#123;        name    &#125;&#125;&#123;    &quot;data&quot;: &#123;        &quot;hero&quot;: &#123;            &quot;name&quot;: &quot;12test&quot;        &#125;    &#125;&#125;</code></pre><ul><li><p>Arguments</p><ul><li>we could pass arguments to fields</li><li>comparing with Restful, in GraphQL every field and nested object can get its own set of arguments, making GraphQL a complete replacement for making multiple APU fetches</li></ul></li><li><p>Fragments</p><ul><li>That’s the reusable units in GraphQL</li><li>Fragments let you construct sets of fields, and then include them in queries where you need to</li><li>It’s commonly used to split complicated application data requirements into smaller chunks</li></ul></li><li><p>Operation Name</p><ul><li>Operation Type<ul><li>Query</li><li>Mutation</li><li>Subscription</li></ul></li><li>Operation Name</li></ul></li><li><p>Variables</p><ul><li>It want to give dynamic power to graphql, as in most applications, the arguments to fields will be dynamic</li><li>Graphql supports this use case via variables</li><li>we could do:<ul><li>replace the static value in the query with <code>$variable</code></li><li>declare <code>$variable</code> as one of the variables accepted by the query</li><li>pass <code>variable: value</code> in the separate transport specific variables dictionary</li></ul></li><li>using variable could help us denote which arguments are expected to be dynamic</li><li>we should never do string interpolation to construct queries from user supplied values</li></ul></li></ul><pre><code class="ruby">query HeroNameAndFriends($episode: Episode) &#123;  hero(episode: $episode) &#123;    name    friends &#123;      name    &#125;  &#125;&#125;&#123;  &quot;episode&quot;: &quot;JEDI&quot;&#125; </code></pre><ul><li>Default variables</li></ul><pre><code class="ruby">query HeroNameAndFriends($episode: Episode = &quot;defaultOne&quot;) &#123;  hero(episode: $episode) &#123;    name    friends &#123;      name    &#125;  &#125;&#125;&#123;  &quot;episode&quot;: &quot;JEDI&quot;&#125; </code></pre><ul><li><p>Directives</p><ul><li>use this variable to dynamically change the structure and shape of our queries using variables</li><li><code>@include(if: Boolean)</code> only includes this field in the result if the argument is true</li><li><code>@skip(if: Boolean)</code> skip this field if the argument is true</li></ul></li><li><p>Mutations</p><ul><li>A way to modify server side data</li><li>A convention that any operations that cause writes should be sent explicitly via a mutation</li><li>!!! While query fields are executed in parallel, mutation fields run in series, one after the other<ul><li>means if we send two incrementCredits mutations in one request, the first is guranteed to finish before the second begins, ensuring that we don’t end up with a race condition with ourselves</li></ul></li></ul></li><li><p>Inline Fragments</p><ul><li><p>GraphQL schemas include the ability to define interfaces and union types</p></li><li><p>EG below, we need to return different attributes based on hero character</p><pre><code class="ruby">query HeroForEpisode($ep: Episode!) &#123;hero(episode: $ep) &#123;  name  ... on Droid &#123;    primaryFunction  &#125;  ... on Human &#123;    height  &#125;&#125;&#125;</code></pre></li></ul></li><li><p>Meta fields</p><ul><li>there are situations where you don’t know what type you’ll get back from the service</li><li>we need to determine how to handle that data on the client</li><li>we could use <code>__typename</code></li></ul></li></ul><h1 id="3-Schemas-and-Types"><a href="#3-Schemas-and-Types" class="headerlink" title="3. Schemas and Types"></a>3. Schemas and Types</h1><h2 id="3-1-how-the-schema-work"><a href="#3-1-how-the-schema-work" class="headerlink" title="3.1 how the schema work"></a>3.1 how the schema work</h2><ul><li><p>How does GraphQL work</p><ul><li><p>start with a <code>root</code> object</p></li><li><p>select the hero field on that</p></li><li><p>for the object returned by hero, we select the name and appearsIn fields</p><pre><code class="ruby">&#123;hero &#123;  name  appearsIn&#125;&#125;</code></pre></li></ul></li><li><p>we should know what we could query for</p><ul><li>an exact description of the data we can ask for</li><li>what kind of objects might they return</li><li>what fields are available on those sub objects</li></ul></li><li><p>Schema</p><ul><li>Each graphQL services defines a set of types which completely describe the set of possible data you can query on the service</li></ul></li></ul><h2 id="3-2-Type-Language"><a href="#3-2-Type-Language" class="headerlink" title="3.2 Type Language"></a>3.2 Type Language</h2><ul><li>GraphQL use its won Schema Language</li></ul><h3 id="3-2-1-Object-Types-and-Fields"><a href="#3-2-1-Object-Types-and-Fields" class="headerlink" title="3.2.1 Object Types and Fields"></a>3.2.1 Object Types and Fields</h3><ul><li>Object types<ul><li>represent a kind of object you can fetch from your service, and what fields it has</li></ul></li></ul><pre><code class="ruby">type Character &#123;  name: String!// means an array of Episode objects, notnull, 0 or more items, and each item would be an episode object   appearsIn: [Episode!]!&#125;</code></pre><h3 id="3-2-2-Query-and-Mutation-types"><a href="#3-2-2-Query-and-Mutation-types" class="headerlink" title="3.2.2 Query and Mutation types"></a>3.2.2 Query and Mutation types</h3><ul><li>Entry points into the schema</li></ul><h3 id="3-2-3-Scalar-Types"><a href="#3-2-3-Scalar-Types" class="headerlink" title="3.2.3 Scalar Types"></a>3.2.3 Scalar Types</h3><ul><li>Scalar types represent the leaves of the query</li><li>default scalar types<ul><li>Int</li><li>Float</li><li>String</li><li>Boolean</li><li>ID<ul><li>it represents a unique identifier</li><li>The ID type is serialized in the same way as a String, but it means it’s not intended to be human readable</li></ul></li></ul></li><li>We could also define our own Scalar type in this way<ul><li><code>scalar Date</code></li></ul></li></ul><h3 id="3-2-4-Enumeration-Types"><a href="#3-2-4-Enumeration-Types" class="headerlink" title="3.2.4 Enumeration Types"></a>3.2.4 Enumeration Types</h3><ul><li>Restricted to a particular set of allowed values</li><li>Allow you to<ul><li>validate that any arguments of this type are one of the allowed values</li><li>communicate through the type system that a field will always be one of a finite set of values</li></ul></li></ul><h3 id="3-2-5-Lists-and-Non-Null"><a href="#3-2-5-Lists-and-Non-Null" class="headerlink" title="3.2.5 Lists and Non-Null"></a>3.2.5 Lists and Non-Null</h3><pre><code>type Character &#123;  name: String!  appearsIn: [Episode]!&#125;</code></pre><ul><li>We could use <code>!</code> to indicate it should never return null</li><li>We could use <code>[]</code> to indicate that should be an array</li></ul><h3 id="3-2-6-Interfaces"><a href="#3-2-6-Interfaces" class="headerlink" title="3.2.6 Interfaces"></a>3.2.6 Interfaces</h3><ul><li>An abstract type that includes a certain set of fields that a type must include to implement the interface</li></ul><pre><code class="ruby">interface Character &#123;    id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!&#125;type Human implements Character &#123;  id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!  starships: [Starship]  totalCredits: Int&#125;type Droid implements Character &#123;  id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!  primaryFunction: String&#125;</code></pre><ul><li>Type implement the interface need to have all those fields, but they could also have their own fields</li></ul><h3 id="3-2-7-Union-Types"><a href="#3-2-7-Union-Types" class="headerlink" title="3.2.7 Union Types"></a>3.2.7 Union Types</h3><p><code>union SearchResult = Human | Droid | Starship</code></p><h3 id="3-2-8-Input-Types"><a href="#3-2-8-Input-Types" class="headerlink" title="3.2.8 Input Types"></a>3.2.8 Input Types</h3><ul><li>we need to pass complex objects especially when we are using mutations, where we want to pass in a whole object to be created</li></ul><pre><code class="ruby">input ReviewInput &#123;  stars: Int!  commentary: String&#125;mutation CreateReviewForEpisode($ep: Episode!, $review: ReviewInput!) &#123;  createReview(episode: $ep, review: $review) &#123;    stars    commentary  &#125;&#125;</code></pre><h1 id="4-Validation"><a href="#4-Validation" class="headerlink" title="4. Validation"></a>4. Validation</h1><ul><li>Graph ql has validation module to fulfill the validation phase of fulfilling a graphQL result</li></ul><h1 id="5-Execution"><a href="#5-Execution" class="headerlink" title="5. Execution"></a>5. Execution</h1><h2 id="5-1-Resolvers"><a href="#5-1-Resolvers" class="headerlink" title="5.1 Resolvers"></a>5.1 Resolvers</h2><ul><li>Each field in a GraphQL query is a function or method of the previous type which returns the next type</li><li>Each field is backed by a function called the resolver. When a field is executed, the corresponding resolver is called to produce the next value</li><li>The resolver continue to work until reach scalar values</li></ul><h2 id="5-2-Root-fields"><a href="#5-2-Root-fields" class="headerlink" title="5.2 Root fields"></a>5.2 Root fields</h2><pre><code class="ruby">Query: &#123;  human(obj, args, context, info) &#123;    return context.db.loadHumanByID(args.id).then(      userData =&gt; new Human(userData)    )  &#125;&#125;</code></pre><ul><li>obj<ul><li>previous object</li></ul></li><li>args<ul><li>arguments provided to the field in the graphQL query</li></ul></li><li>context<ul><li>a value which is provided to every resolver and holds important contextual information like the currently logged in user, or access to a database</li></ul></li><li>info<ul><li>a value which holds field specific information relevant to the current query as well as the schema details</li></ul></li></ul><h1 id="6-Best-Practices"><a href="#6-Best-Practices" class="headerlink" title="6. Best Practices"></a>6. Best Practices</h1><h2 id="6-1-HTTP"><a href="#6-1-HTTP" class="headerlink" title="6.1 HTTP"></a>6.1 HTTP</h2><ul><li><p>Mostly use HTTP with graphQL</p></li><li><p>Normally web frameworks use a pipeline model where requests are passed through a stack of middle ware</p></li><li><p>Requests could be inspected, transformed, modified or terminated with a response</p></li><li><p>GraphQL should be placed after all authentication middleware— thus you have access to the same session and user info you would in your HTTP endpoints handler</p></li><li><p>GraphQL server operates on a single URL/ endpoint, usually <code>graphql</code> , and all graphql requests for a given service should be directed at this endpoint</p></li></ul><h2 id="6-2-JSON-with-GZIP"><a href="#6-2-JSON-with-GZIP" class="headerlink" title="6.2 JSON with GZIP"></a>6.2 JSON with GZIP</h2><ul><li>typically respond using JSON, and we compress it with GZIP</li></ul><h2 id="6-3-Versioning"><a href="#6-3-Versioning" class="headerlink" title="6.3 Versioning"></a>6.3 Versioning</h2><ul><li>No need to do versioning for graphql api</li><li>Why do most APIs version? When there’s limited control over the data that’s returned from an API endpoint, <em>any change</em> can be considered a breaking change, and breaking changes require a new version. If adding new features to an API requires a new version, then a tradeoff emerges between releasing often and having many incremental versions versus the understandability and maintainability of the API.</li><li>In contrast, GraphQL only returns the data that’s explicitly requested, so new capabilities can be added via new types and new fields on those types without creating a breaking change. This has led to a common practice of always avoiding breaking changes and serving a versionless API.</li></ul><h2 id="6-4-Nullability"><a href="#6-4-Nullability" class="headerlink" title="6.4 Nullability"></a>6.4 Nullability</h2><ul><li>GraphQL default to nullable unless you specifically declare nonnull</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.graphql-java-kickstart.com/">https://www.graphql-java-kickstart.com/</a>  </li><li><a href="https://graphql.org/learn/">https://graphql.org/learn/</a>  </li><li><a href="https://www.apollographql.com/docs/federation/">https://www.apollographql.com/docs/federation/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;GraphQL-Read&quot;&gt;&lt;a href=&quot;#GraphQL-Read&quot; class=&quot;headerlink&quot; title=&quot;GraphQL Read&quot;&gt;&lt;/a&gt;GraphQL Read&lt;/h1&gt;&lt;h1 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="Web" scheme="https://www.llchen60.com/categories/Web/"/>
    
    
  </entry>
  
  <entry>
    <title>如何缓解疲劳</title>
    <link href="https://www.llchen60.com/%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3%E7%96%B2%E5%8A%B3/"/>
    <id>https://www.llchen60.com/%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3%E7%96%B2%E5%8A%B3/</id>
    <published>2021-11-02T13:31:04.000Z</published>
    <updated>2021-11-02T13:34:58.934Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/11/02/zEvGV8uDnJxoITr.png" alt="如何抵抗缓解疲劳.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/11/02/zEvGV8uDnJxoITr.png&quot; alt=&quot;如何抵抗缓解疲劳.png&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>碳达峰与碳中和</title>
    <link href="https://www.llchen60.com/%E7%A2%B3%E8%BE%BE%E5%B3%B0%E4%B8%8E%E7%A2%B3%E4%B8%AD%E5%92%8C/"/>
    <id>https://www.llchen60.com/%E7%A2%B3%E8%BE%BE%E5%B3%B0%E4%B8%8E%E7%A2%B3%E4%B8%AD%E5%92%8C/</id>
    <published>2021-09-30T02:21:14.000Z</published>
    <updated>2021-09-30T02:22:44.858Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/09/30/XGS6Qey4qHki2FC.png" alt="碳达峰与碳中和"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/09/30/XGS6Qey4qHki2FC.png&quot; alt=&quot;碳达峰与碳中和&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Cassandra</title>
    <link href="https://www.llchen60.com/Cassandra/"/>
    <id>https://www.llchen60.com/Cassandra/</id>
    <published>2021-09-17T13:05:55.000Z</published>
    <updated>2021-09-17T13:41:43.051Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/09/17/IGdzKBJwD9TbZ6f.png" alt="Cassandra MindMap.png"></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><h2 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h2><ul><li>We wanna have a distributed and scalable system that can store a <strong>huge amount of structured data</strong>, which is indexed by a row key where each row can have an <strong>unbounded</strong> number of columns.</li><li>Cassandra was originally developed at Facebook in 2007 for index search feature. It’s designed to provide scalability, availability, and reliability to store large amounts of data.</li><li>It combines nature of Dynamo which is a <strong>key value store</strong> and the data model of Bigtable which is a <strong>column based</strong> data store</li><li>Cassandra is in favor of availability and partition tolerance, it could be tuned with <strong>replication factor</strong> and <strong>consistency levels</strong> to meet <strong>strong consistency</strong> requirements, and of course with a performance cost.</li><li>It uses peer to peer architecture, with each node connected to all other nodes</li><li>Each Cassandra node performs all database operations and can serve client requests without the need for any leader node.</li></ul><h2 id="1-2-Use-cases"><a href="#1-2-Use-cases" class="headerlink" title="1.2 Use cases"></a>1.2 Use cases</h2><ul><li>Store key value data with high availability</li><li>Time series data model<ul><li>Due to its data model and log structured storage engine, cassandra benefits from high performing write operations, This also make it well suited for storing and analyzing sequentially captured metrics</li></ul></li><li>Write Heavy Applications<ul><li>Suited for write intensive applications such as time series streaming services, sensor logs, and IoT applications</li></ul></li></ul><h1 id="2-High-Level-Architecture"><a href="#2-High-Level-Architecture" class="headerlink" title="2. High Level Architecture"></a>2. High Level Architecture</h1><h2 id="2-1-Common-Terms"><a href="#2-1-Common-Terms" class="headerlink" title="2.1 Common Terms"></a>2.1 Common Terms</h2><p><img src="https://i.loli.net/2021/09/17/IrfBD5HFqAX76NJ.png" alt="Primary and Clustering Keys"></p><ul><li>Column<ul><li>A key value pair and is the most basic unit of data structure</li><li>Column Key: Uniquely identifies a column in a row</li><li>Column Value: Store a value or a collection of values</li></ul></li><li>Row<ul><li>A container for columns referenced by primary key. Cassandra does not store a column that has a null value, this saves a lot of space</li></ul></li><li>Table<ul><li>A container of rows</li></ul></li><li>Keyspace<ul><li>A container for tables that span over one or more Cassandra nodes</li></ul></li><li>Cluster<ul><li>Container of Keyspace</li></ul></li><li>Node<ul><li>A computer system running an instance of Cassandra,</li><li>Can be a physical host, a machine instance in the cloud or even a docker container</li></ul></li></ul><h2 id="2-2-Data-Partitioning"><a href="#2-2-Data-Partitioning" class="headerlink" title="2.2 Data Partitioning"></a>2.2 Data Partitioning</h2><ul><li>Cassandra use consistent hashing as DynamoDB does</li></ul><h2 id="2-3-Primary-Key"><a href="#2-3-Primary-Key" class="headerlink" title="2.3 Primary Key"></a>2.3 Primary Key</h2><ul><li>The primary key consists of two parts:  E.G Primary Key as (city_id, employee_id)<ul><li>Partition Key<ul><li>Decides how data is distributed across nodes</li><li>city_id is the primary key, means the data will be partitioned by the city_id field, all rows with the same city_id will reside on the same node</li></ul></li><li>Clustering Key<ul><li>Decides how data is stored within a node</li><li>We could have multiple clustering keys, clustering columns specify the order that the data is arranged on a node.</li><li>employee_id is the clustering key. Within each node, the data is stored in sorted order according to the employee_id column.</li></ul></li></ul></li></ul><h2 id="2-4-Partitioner"><a href="#2-4-Partitioner" class="headerlink" title="2.4 Partitioner"></a>2.4 Partitioner</h2><p><img src="https://i.loli.net/2021/09/17/3NdkOaXUpbgnWq9.png" alt="Partitioner Flow"></p><ul><li>Responsible for determining how data is distributed on the consistent hash ring.</li><li>Cassandra use <strong>Murmur3 hashing function</strong> — which will always produce the same hash for a given partition key</li><li>All Cassandra nodes learn about the <strong>token assignments of other nodes</strong> through gossip. This means any node can handle a request for any other node’s range. The node receiving the request is called the <strong>coordinator</strong>, and any node can act in this role. If a key does not belong to the coordinator’s range, it <strong>forwards the request</strong> to the replicas responsible for that range.</li></ul><h2 id="2-5-Coordinator-Node"><a href="#2-5-Coordinator-Node" class="headerlink" title="2.5 Coordinator Node"></a>2.5 Coordinator Node</h2><ul><li>A client may connect to any node in the cluster to initiate a read or write query. This node is known as the coordinator node, the coordinator identifies the nodes responsible for the data that is being written or read    and forwards the queries to them</li></ul><h1 id="3-Low-Level-Architecture"><a href="#3-Low-Level-Architecture" class="headerlink" title="3. Low Level Architecture"></a>3. Low Level Architecture</h1><h2 id="3-1-Replication-Strategy"><a href="#3-1-Replication-Strategy" class="headerlink" title="3.1 Replication Strategy"></a>3.1 Replication Strategy</h2><ul><li><p>Each node in Cassandra serves as a replica for a different range of data.</p></li><li><p>It stores <strong>multiple copies of data</strong> and <strong>spreads them across various replicas</strong>.</p></li><li><p>The replication behavior is controlled by two factors</p><ul><li><p>Replication Factor</p><ul><li>Decides how many replicas the system will have</li><li>This represents the <strong>number of nodes that will receive the copy of the same data</strong></li><li>Each keyspace in cassandra can have a different replication factor</li></ul></li><li><p>Replication Strategy</p><ul><li><p>Decides which nodes will be responsible for the replicas</p></li><li><p>The node that owns the range in which the hash of the partition key falls will be the first replica</p></li><li><p>All the additional replicas are placed on the <strong>consecutive nodes</strong></p></li><li><p>Cassandra places the subsequent replicas on the next nodes in a clockwise manner</p></li><li><p>Two kinds of replication strategies</p><ul><li><p>Simple Replication Strategy</p><p>  <img src="https://i.loli.net/2021/09/17/cnz12lGFWEPw4fS.png" alt="Simple Replication Strategy"></p><ul><li>Used for a <strong>single data center cluster</strong></li><li>Cassandra places the first replica on a node determined by the partitioner and the subsequent replicas on the next node in a clockwise manner</li></ul></li><li><p>Network Topology Strategy</p><p>  <img src="https://i.loli.net/2021/09/17/TSAZbXKCYf9IsoN.png" alt="Network Topology Strategy"></p><ul><li>Used for multiple data centers</li><li>We can specify different replication factors for different data centers. We could then specify how many replicas will be placed in each data center</li><li>Additional replicas, in the same data center, are placed by <strong>walking the ring clockwise until reaching the 1st node in another rack</strong>. This is done to guard against a complete rack failure, as nodes in the same rack(or similar physical grouping) tend to fail together due to power, cooling or network issues.</li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="3-2-Consistency-Levels"><a href="#3-2-Consistency-Levels" class="headerlink" title="3.2 Consistency Levels"></a>3.2 Consistency Levels</h2><ul><li>Definition<ul><li><strong>Minimum number of nodes</strong> that must fulfill a read or write operation before the operation can be considered successful</li><li>It allows use to <strong>specify different consistency levels</strong> for read and write</li><li>It also has <strong>tunable consistency level</strong></li><li>Tradeoff between consistency and response time<ul><li>As a higher consistency level means more nodes need to respond to a read or write query, giving user more assurance that the values present on each replica are the same</li></ul></li></ul></li></ul><h3 id="3-2-1-Write-Consistency-Levels"><a href="#3-2-1-Write-Consistency-Levels" class="headerlink" title="3.2.1 Write Consistency Levels"></a>3.2.1 Write Consistency Levels</h3><ul><li>Consistency Levels specify how many replica nodes must respond for the write to be reported as successful to the client</li><li>Level is specified <strong>per query by the client</strong></li><li>Cassandra is eventually consistent, updates to other replica nodes may continue in the background</li><li>How does Cassandra perform a write operation?<ul><li>Coordinator node contacts all replicas, as determined by the <strong>replication factor</strong> , and consider the write successful when a number of replicas equal to the consistency level acknowledge the write</li></ul></li><li>Write Consistency Levels List:<ul><li>One/ Two/ Three<ul><li>The data must be written to at least the specified number of replica nodes before a write is considered successful</li></ul></li><li>Quorum<ul><li>Data must be written to at least a quorum of replica nodes</li><li>Quorum is defined as <code>floor(RF/2 + 1)</code>  RF represents replication factor</li></ul></li><li>All<ul><li>ensures the data is written to all replica nodes</li><li>provides the highest consistency but lowest availability as writes will fail if any replica is down</li></ul></li><li>Local Quorum<ul><li>Ensure that data is written to a quorum of nodes in the same datacenter as the coordinator</li><li>Does not wait for the response from the other data centers</li></ul></li><li>Each Quorum<ul><li>Ensures that the data is written to a quorum of nodes in each datacenter</li></ul></li><li>Any<ul><li>The data must be written to at least one node</li><li>In the extreme case, when all replica nodes for the given partition key are down, the write can still succeed after a hinted handoff (see 3.2.4 section) has been written.<ul><li>In this case, an any write could succeed with hinted handoff, but it will not be readable until the replica nodes for that partition has recovered and the latest data is written on them</li></ul></li></ul></li></ul></li></ul><h3 id="3-2-2-Read-Consistency-Levels"><a href="#3-2-2-Read-Consistency-Levels" class="headerlink" title="3.2.2 Read Consistency Levels"></a>3.2.2 Read Consistency Levels</h3><ul><li><p>Read Query Consistency Level specify how many replica nodes must respond to a read request before returning the data</p></li><li><p>It has the same consistency levels for read operations as that of write operations exception Each_Quorum cause it’s too expensive</p></li><li><p>To achieve strong consistency, we need to do <code>R + W &gt; RF</code> R represents read replica count, W represents write replication count, RF represents replication factor</p><ul><li>All client reads will see the most recent write in this scenario, and we will have strong consistency</li></ul></li><li><p>How does Cassandra perform a read operation?</p><ul><li><p>Coordinator always sends the read request to the fastest node</p><ul><li>E.G  for quorum=2, the coordinator sends the requests to the fastest node and the <strong>digest of the data</strong> from the second fastest node<ul><li>digest is the checksum of the data, we use this to save network bandwidth</li></ul></li></ul></li><li><p>if the digest doesn’t match, means some replica do not have the latest version of data</p><ul><li><p>Coordinator then <strong>reads the data from all the replicas</strong> to determine the latest data</p></li><li><p>Then coordinator <strong>returns the latest data to the client and initiates a read repair request</strong></p></li><li><p>The read repair request will help push the newer version of data to nodes with the older version</p><p>  <img src="https://i.loli.net/2021/09/17/sPM6HnKthBpT945.png" alt="Read Operation with Snitch"></p></li></ul></li><li><p>latest write timestamp is used as a mark for the correct version of data, read repair operation is performed only in a portion of the total reads to avoid performance degradation</p></li></ul></li></ul><h3 id="3-2-3-Snitch"><a href="#3-2-3-Snitch" class="headerlink" title="3.2.3 Snitch"></a>3.2.3 Snitch</h3><ul><li><p>Functions</p><ul><li>Application that determines the proximity of nodes within the ring, also tells which nodes are faster — monitor the read latencies</li><li>It keeps track of the network topology of Cassandra nodes, determines which <strong>data centers and racks</strong> nodes belong to</li><li>Replication strategy use this information provided by the Snitch to spread the replicas across the cluster intelligently. It could do its best by not having more than one replica on the same rack</li></ul></li><li><p>Cassandra nodes use this info to route read/ write requests efficiently</p><p>  <img src="https://i.loli.net/2021/09/17/ZnaOJqIvgdcMmAW.png" alt="Request when set consistency to one"></p></li></ul><h3 id="3-2-4-Hinted-Handoff"><a href="#3-2-4-Hinted-Handoff" class="headerlink" title="3.2.4 Hinted Handoff"></a>3.2.4 Hinted Handoff</h3><p><img src="https://i.loli.net/2021/09/17/QvSmb5wntE2AHJd.png" alt="Hinted Handoff"></p><ul><li>To let Cassandra still serve write requests even when nodes are down</li><li>When a node is down, the coordinator nodes <strong>writes a hint in a text file on local disk</strong><ul><li>Hint contains the data itself along with information about which node the data belongs to</li><li>Recover from gossiper — When the coordinator node discovers from the gossiper that a node for which it holds hints has recovered, it forwards the write request for each hint to the target</li><li>Recover from routine call — each node every ten minutes checks to see if the failing node, for which it is holding any hints, has recovered</li></ul></li><li>With consistency level ‘Any,’<ul><li>if all the replica nodes are down, the coordinator node will <strong>write the hints for all the nodes and report success to the client.</strong></li><li>However, this data will <strong>not reappear in any subsequent reads</strong> until one of the replica nodes comes back online, and the coordinator node successfully forwards the write requests to it.</li><li>This is assuming that the coordinator node is up when the replica node comes back.</li><li>This also means that we can lose our data if the coordinator node dies and never comes back. For this reason, we should avoid using the ‘Any’ consistency level</li></ul></li><li>For node offline for quite long<ul><li>Hints can build up considerably on other nodes</li><li>When it back online, other nodes tend to flood that node with write requests</li><li>It would cause issues on the node, as it is already trying to come back after a failure</li><li>To address this, Cassandra <strong>limits the storage of hints to a configurable time window</strong></li><li>By default, set the time window to 3 hours. Post that, older hints will be removed  — now the recovered nodes will have stale data<ul><li>The stale data would be fixed during the read path, it will issue a read repair when it sees the stale data</li></ul></li></ul></li><li>When the cluster cannot meet the consistency level specified by the client, Cassandra fails the write request and does not store a hint .</li></ul><h2 id="3-3-Gossiper"><a href="#3-3-Gossiper" class="headerlink" title="3.3 Gossiper"></a>3.3 Gossiper</h2><h3 id="3-3-1-How-does-Cassandra-use-Gossip-Protocol"><a href="#3-3-1-How-does-Cassandra-use-Gossip-Protocol" class="headerlink" title="3.3.1 How does Cassandra use Gossip Protocol?"></a>3.3.1 How does Cassandra use Gossip Protocol?</h3><ul><li>What’s for?<ul><li>Cassandra uses gossip protocol that allows each node to keep track of state information about the other nodes in the cluster.</li><li>It’s a Peer to Peer communication mechanism in which nodes <strong>periodically exchange state information about themselves and other nodes they know about</strong></li></ul></li><li>How it works?<ul><li>Each node initiates a gossip round every second to exchange state info about themselves with one to three other random nodes</li><li>Each gossip message has a version associated with it, so during a gossip exchange, older info is overwritten with the most current state for a particular node</li></ul></li><li>Generation number<ul><li>Each node stores a generation number which will be incremented every time a node restart</li><li>Node receiving the gossip message can compare the generation number it knows and the gossip message’s generation number</li><li>If the generation number in the gossip message is higher, it knows the node was restarted</li></ul></li><li>Seed nodes<ul><li>For node starting up for the first time</li><li>Assist in gossip convergence, thus guarantee schema/ state changes propagate regularly</li></ul></li></ul><h3 id="3-3-2-Node-Failure-Detection"><a href="#3-3-2-Node-Failure-Detection" class="headerlink" title="3.3.2 Node Failure Detection"></a>3.3.2 Node Failure Detection</h3><ul><li>Disadvantages for heartbeat<ul><li>outputs a boolean value telling us if the system is alive or not;</li><li>there is no middle ground.</li><li>Heartbeating uses a fixed timeout, and if there is no heartbeat from a server, the system, after the timeout, assumes that the server has crashed.</li><li>If we keep the timeout short, the system will be able to detect failures quickly but with many false positives due to slow machines or faulty networks.</li><li>On the other hand, if we keep the timeout long, the false positives will be reduced, but the system will not perform efficiently for being slow in detecting failures.</li></ul></li><li>Use adaptive failure detection mechanism  —— Phi Accrual Failure Detector<ul><li>Use historical heartbeat information to make the threshold adaptive</li><li>It outputs the suspicion level about a server</li><li>As a node’s suspicion level increases, the system can gradually decide to stop sending new requests to it</li><li>It makes the distributed system efficient as it takes into account fluctuations in the network env and other intermittent server issues before declaring a system completely dead</li></ul></li></ul><h2 id="3-4-Anatomy-of-Cassandra’s-Write-Operation"><a href="#3-4-Anatomy-of-Cassandra’s-Write-Operation" class="headerlink" title="3.4 Anatomy of Cassandra’s Write Operation"></a>3.4 Anatomy of Cassandra’s Write Operation</h2><p>Cassandra stores data both <strong>in memory and on disk</strong> to provide both high performance and durability. Every write includes a timestamp, write path involves a lot of components: </p><p><img src="https://i.loli.net/2021/09/17/LrMK7ckIS2zEsU1.png" alt="Write Path"></p><ul><li>Each write is appended to a commit log, which is stored on disk</li><li>It is then written to Memtable in memory</li><li>Periodically, MemTables are flushed to SSTables on the disk</li><li>Periodically, compaction runs to merge SSTables</li></ul><h3 id="3-4-1-Commit-Log"><a href="#3-4-1-Commit-Log" class="headerlink" title="3.4.1 Commit Log"></a>3.4.1 Commit Log</h3><ul><li>When a node receives a write request, it immediately writes data to a commit log</li><li>Commit log is a <strong>write ahead log</strong> stored on disk</li><li>Used as a crash recovery mechanism to support Cassandra’s durability goals</li><li>A write will not be considered successful on the node until it’s <strong>written to the commit log</strong><ul><li>This ensures if a write operation does not make it to the in-memory store, it will still be possible to recover the data</li></ul></li><li>If we shut down the node or it crashes unexpectedly, the commit log can ensure that data is not lost; that’s because if the node restart, the commit log gets replayed</li></ul><h3 id="3-4-2-MemTable"><a href="#3-4-2-MemTable" class="headerlink" title="3.4.2 MemTable"></a>3.4.2 MemTable</h3><ul><li>After written to the commit log, the data is written to a memory resident data structure called memTable<ul><li>Each node has a MemTable in memory for each Cassandra table</li><li>Each MemTable contains data for a specific Cassandra table, and it resembles that table in memory</li><li>Each MemTable accrues writes and <strong>provides reads for data not yet flushed to disk</strong></li><li>Commit log stores all the writes in sequential order, with each new write appended to the end; whereas MemTable stores data in the sorted order of partition key and clustering columns</li><li>After writing data to the commit log and MemTable, the node <strong>sends an acknowledgement to the coordinator</strong> that the data has been successfully written</li></ul></li></ul><h3 id="3-4-3-SStable"><a href="#3-4-3-SStable" class="headerlink" title="3.4.3 SStable"></a>3.4.3 SStable</h3><ul><li>When the number of objects stored in the MemTable reaches a threshold, the contents of the MemTable are <strong>flushed to disk</strong> in a file called <strong>SSTable</strong><ul><li>At this point, a new MemTable is created to store subsequent data</li><li>The flush is non blocking operation</li><li>Multiple Memtables may exist for a single table<ul><li>One current, and the rest waiting to be flushed</li></ul></li><li>When the MemTable is flushed to SStables, <strong>corresponding entries in the commit log</strong> are removed</li></ul></li><li>SStable —Sorted String Table<ul><li>Once a MemTable is flushed to disk as an SStable, it is immutable and cannot be changed later</li><li>Each delete or update is considered as a new write operation</li></ul></li><li>The current data state of a Cassandra table consists of its MemTables in memory and SSTables on the disk.<ul><li>Therefore, on reads, Cassandra will read both SSTables and MemTables to find data values, as the MemTable may contain values that have not yet been flushed to the disk.</li><li>The MemTable works like a write-back cache that Cassandra looks up by key</li></ul></li></ul><p><img src="https://i.loli.net/2021/09/17/Qd7x4M6HRrtuAoZ.png" alt="Whole Write Path"></p><h2 id="3-5-Anatomy-of-Cassandra’s-Read-Operation"><a href="#3-5-Anatomy-of-Cassandra’s-Read-Operation" class="headerlink" title="3.5 Anatomy of Cassandra’s Read Operation"></a>3.5 Anatomy of Cassandra’s Read Operation</h2><p><img src="https://i.loli.net/2021/09/17/wIZKE97YqVNAsrP.png" alt="Whole Read Path"></p><h3 id="3-5-1-Caching"><a href="#3-5-1-Caching" class="headerlink" title="3.5.1 Caching"></a>3.5.1 Caching</h3><ul><li>Row Cache<ul><li>Cache frequently read/ hot rows</li><li>Stores a complete data row, which can be returned directly to the client if requested by a read operation</li><li>Could significantly speed up read access for frequently accessed rows, at the cost of more memory usage</li></ul></li><li>Key Cache<ul><li>Stores a map of recently read partition keys to their <strong>SSTable offsets</strong></li><li>This facilitates faster read access into SSTables and improves the read performance</li><li>Use less memory comparing with row cache and provides a considerable improvement for read operations</li></ul></li><li>Chunk Cache<ul><li>Chunk Cache is used to store umcompressed chunks of data read from SSTable files that are accessed frequently</li></ul></li></ul><h3 id="3-5-2-Read-From-MemTable"><a href="#3-5-2-Read-From-MemTable" class="headerlink" title="3.5.2 Read From MemTable"></a>3.5.2 Read From MemTable</h3><ul><li>When a read request come in, node performs a binary search on the partition key to find the required partition and then return the row</li></ul><h3 id="3-5-3-Read-From-SSTable"><a href="#3-5-3-Read-From-SSTable" class="headerlink" title="3.5.3 Read From SSTable"></a>3.5.3 Read From SSTable</h3><ul><li><p>Bloom Filters</p><ul><li>Each SSTable has a Bloom Filter associated with it, which tells if a particular key is present in it or not</li><li>Used to boost performance of read operations</li><li>It’s a very fast, non deterministic algorithms for testing whether an element is a member of a set</li><li>It’s possible to get a false positive but never a false negative</li><li>Theory<ul><li>It works by <strong>mapping the values in a data set into a bit array</strong> and <strong>condensing a larger data set into a digest string</strong> with a hash function</li><li>Filters are stored in memory and are used to improve performance by reducing the need for disk access on key lookups</li></ul></li></ul></li><li><p>How are SSTables stored on the disk?</p><ul><li><p>Consists of two files</p><ul><li><p>Data File</p><ul><li>Actual data is stored here</li><li>It has partitions and rows associated with those partitions</li><li>Partitions are in sorted order</li></ul></li><li><p>Partition Index File</p><ul><li><p>Stored on disk, partition index file stores the sorted partition keys mapped to their SSTable offsets</p></li><li><p>Enable locating a partition exactly in an SSTable rather than scanning data</p><p><img src="https://i.loli.net/2021/09/17/9gUpTXZyLSksDdK.png" alt="Read via Partition Index File"></p></li></ul></li></ul></li></ul></li><li><p>Partition Index Summary File</p><ul><li><p>It’s stored in memory, stores the summary of the partition index file for performance improvement</p><ul><li><p>Two level index, e.g, search for key=19</p></li><li><p>in partition index summary file, it lays to key range 10 - 21</p></li><li><p>then we could go to byte offset 32,</p></li><li><p>in partition index file , we start from 32, to find partition key 19, and then we could go to 5450</p><p><img src="https://i.loli.net/2021/09/17/efsVEmvGAkIldF6.png" alt="Read via Partition Index Summary File"></p></li></ul></li></ul></li><li><p>Read from KeyCache</p><ul><li><p>As the Key Cache stores a map of recently read partition keys to their SSTable offset, it’s the fastest way to find the required row in the SSTable</p><p>  <img src="https://i.loli.net/2021/09/17/5KPTohGmWpecr1a.png" alt="Read From KeyCache"></p></li></ul></li><li><p>Overall workflow</p><p>  <img src="https://i.loli.net/2021/09/17/2zKlRtS48NQYkud.png" alt="Overall Workflow"></p></li></ul><h2 id="3-6-Compaction"><a href="#3-6-Compaction" class="headerlink" title="3.6 Compaction"></a>3.6 Compaction</h2><h3 id="3-6-1-Why-we-need-compaction-And-How-it-Works"><a href="#3-6-1-Why-we-need-compaction-And-How-it-Works" class="headerlink" title="3.6.1 Why we need compaction? And How it Works?"></a>3.6.1 Why we need compaction? And How it Works?</h3><p><img src="https://i.loli.net/2021/09/17/2DgirVjkeq6AI4T.png" alt="Compaction"></p><ul><li>SSTables are immutable, which helps Cassandra achieve high write speeds</li><li>And flushing from MemTable to SSTable is a continuous process, which means we could have a large number of SSTables lying on the disk</li><li>It’s tedious to scan all these SSTables while reading</li><li>We need compaction thus we could merge multiple related SSTables into a single one to improve reading speed</li><li>During compaction, the data in SSTables is merged, keys are merged, columns are combined, obsolete values are discarded, and a new index is created</li></ul><h3 id="3-6-2-Compaction-Strategies"><a href="#3-6-2-Compaction-Strategies" class="headerlink" title="3.6.2 Compaction Strategies"></a>3.6.2 Compaction Strategies</h3><ul><li>SizeTiered Compaction Strategy<ul><li>Suitable for insert-heavy and general workloads</li><li>Triggered when multiple SSTables of a similar size are present</li></ul></li><li>Leveled Compaction Strategy<ul><li>Optimize read performance</li><li>Groups SSTables into levels, each of which has a fixed size limit which is ten times larger than the previous level</li></ul></li><li>Time Window Compaction Strategy<ul><li>Work on time series data</li><li>Compact SSTables within a configured time window</li><li>Ideal for time series data which is immutable after a fixed time interval</li></ul></li></ul><h3 id="3-6-3-Sequential-Writes"><a href="#3-6-3-Sequential-Writes" class="headerlink" title="3.6.3 Sequential Writes"></a>3.6.3 Sequential Writes</h3><ul><li>Main reason that writes perform so well in Cassandra</li><li>No reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations</li><li>Compaction is intended to amortize the reorganization of data, but it uses sequential I/O to do so, which makes it efficient</li></ul><h2 id="3-7-Tombstones"><a href="#3-7-Tombstones" class="headerlink" title="3.7 Tombstones"></a>3.7 Tombstones</h2><h3 id="3-7-1-What-are-Tombstones"><a href="#3-7-1-What-are-Tombstones" class="headerlink" title="3.7.1 What are Tombstones?"></a>3.7.1 What are Tombstones?</h3><ul><li>Scenario<ul><li>We delete some data for a node that is down or unreachable, it would miss a delete</li><li>When the node com back online later and a repair occurs, the node could resurrect the data due to re-sharing it with other nodes</li><li>To prevent deleted data from being reintroduced, Cassandra used a concept of a Tombstone</li></ul></li><li>Tombstone<ul><li>Similar to the idea of soft delete from the relational database</li><li>When we delete, Cassandra does not delete it right away, instead, it associated a tombstone with it, with Time to Expiry</li><li>It’s a marker to indicate data that has been deleted</li><li>When we execute a delete operation, data is not immediately deleted</li><li>Instead, it’s treated as an update operation that places a tombstone on the value</li><li>Default Time to Expiry is set to 10 days<ul><li>If the node is down longer than this value, it should be treated as failed and replaced</li></ul></li><li>Tombstones are removed as part of compaction</li></ul></li></ul><h3 id="3-7-2-Common-problems-associated-with-Tombstones"><a href="#3-7-2-Common-problems-associated-with-Tombstones" class="headerlink" title="3.7.2 Common problems associated with Tombstones"></a>3.7.2 Common problems associated with Tombstones</h3><ul><li>Takes storage space</li><li>When a table accumulates many tombstones, read queries on that table could become slow and can cause serious performance problems like timeouts.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/09/17/IGdzKBJwD9TbZ6f.png&quot; alt=&quot;Cassandra MindMap.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introdu
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
      <category term="Cassandra" scheme="https://www.llchen60.com/tags/Cassandra/"/>
    
  </entry>
  
  <entry>
    <title>Logical Fallacies</title>
    <link href="https://www.llchen60.com/Logical-Fallacies/"/>
    <id>https://www.llchen60.com/Logical-Fallacies/</id>
    <published>2021-09-12T01:23:39.000Z</published>
    <updated>2021-09-12T01:50:35.560Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Logical-Fallacies"><a href="#Logical-Fallacies" class="headerlink" title="Logical Fallacies"></a>Logical Fallacies</h1><h2 id="Overconfidence"><a href="#Overconfidence" class="headerlink" title="Overconfidence"></a>Overconfidence</h2><ul><li>Overconfidence — wishful thinking bias<ul><li>most people think they are above avg</li><li>overestimate possibilities that they want to happen</li><li><strong>this could explain the trade in financial market</strong></li><li>overconfidence in friends and leaders</li></ul></li></ul><h2 id="Cognitive-Dissonance"><a href="#Cognitive-Dissonance" class="headerlink" title="Cognitive Dissonance"></a>Cognitive Dissonance</h2><ul><li>Cognitive Dissonance  认知失调<ul><li>this concept used to describe the mental discomfort that results from holding two conflicting beliefs, values or attitudes</li><li>People tend to seek consistency in their attitudes and perceptions, so this conflict causes feelings of unease or discomfort</li><li>This inconsistency between <strong>what people believe and how they behave</strong> motivates people to <strong>engage in actions</strong> that will help minimize feelings of discomfort</li><li>when we made decision, most people will still look for info about it, to self prove hisself right… in a lot different aspects… to make themselves happy, and to prove they are make right decision</li><li>disposition effect — gonna avoid that</li><li>what’s the causes for that?<ul><li>Forced Compliance<ul><li>Engaging in behaviors that are opposed to your own beliefs due to external expectations, often for work, school, or a social situation</li></ul></li><li>New Information</li><li>Decisions<ul><li>People make decisions both large and small, on a daily basis</li><li>When faced with two similar choices, people often are left with feelings of dissonance because both options are equally appealing</li><li>Once they make decisions, people need to find a way to <strong>reduce feelings of discomfort</strong></li><li>Accomplish by justifying why their choice was the best option so that they can believe they made the right decision</li></ul></li></ul></li></ul></li></ul><h2 id="Mental-Compartments"><a href="#Mental-Compartments" class="headerlink" title="Mental Compartments"></a>Mental Compartments</h2><ul><li>Mental compartments<ul><li>people don’t look at whole portfolio, in fact, people has two or more portfolio<ul><li>usually they have a safe part and a risky part</li></ul></li></ul></li></ul><h2 id="Attention-Anomalies"><a href="#Attention-Anomalies" class="headerlink" title="Attention Anomalies"></a>Attention Anomalies</h2><ul><li>Attention Anomalies<ul><li>We cannot pay attention to anything</li><li>Attention is fundamental aspect of human intelligence and its limits</li><li>Social Basis for attention<ul><li>We incline to pay more attention to what other s pay attention to</li></ul></li></ul></li></ul><h2 id="Anchoring"><a href="#Anchoring" class="headerlink" title="Anchoring"></a>Anchoring</h2><ul><li>Anchoring<ul><li>A tendency in ambiguous situations to allow one’s decisions to be affected by some anchor</li><li>Our subconscious will do anchoring for us, lol</li><li>subjects unaware of their own anchoring behavior</li><li>stock prices anchored to past values, or to other stock in same market</li></ul></li></ul><h2 id="Representativeness-Heuristic"><a href="#Representativeness-Heuristic" class="headerlink" title="Representativeness Heuristic"></a>Representativeness Heuristic</h2><ul><li>Representativeness Heuristic<ul><li>People judge by similarity to familiar types, without regard to <strong>base rate probabilities</strong><ul><li>For example, we describe a person as artist, and skeptical, then what’s the highest possible occupation of him/ her?<ul><li>two choice: banker, and sculptress</li><li>should be banker, cause there are so many more bank tellers than sculptresses</li></ul></li></ul></li><li>Tendency to see patterns in what is really random walk</li><li>Stock price manipulators try to create patterns to fool investors</li></ul></li></ul><h2 id="Disjunction-Effect"><a href="#Disjunction-Effect" class="headerlink" title="Disjunction Effect"></a>Disjunction Effect</h2><ul><li>inability to make decisions in advance in anticipation of future information</li></ul><h2 id="Magical-Thinking-amp-Quasi-Magical-Thinking"><a href="#Magical-Thinking-amp-Quasi-Magical-Thinking" class="headerlink" title="Magical Thinking  &amp; Quasi Magical Thinking"></a>Magical Thinking  &amp; Quasi Magical Thinking</h2><ul><li>Some coincidence lead you to build superstitious, but there are actually no karma (cause and effect)</li><li>Belief that unrelated events are causally connected despite the absence of any plausible causal link between them, particularly as a result of supernatural effects.</li><li>E.G<ul><li>For voting, though our vote actually has basically 0 possibility to influence president election, but a lot people do it</li><li>For lottery, we somehow put more money if we select the number</li></ul></li></ul><h2 id="Personality-Disorders"><a href="#Personality-Disorders" class="headerlink" title="Personality Disorders"></a>Personality Disorders</h2><ul><li>culture and social contagion — collective memory<ul><li>same effect, same memory, then similar decisions</li></ul></li><li>Antisocial Personality Disorder</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Logical-Fallacies&quot;&gt;&lt;a href=&quot;#Logical-Fallacies&quot; class=&quot;headerlink&quot; title=&quot;Logical Fallacies&quot;&gt;&lt;/a&gt;Logical Fallacies&lt;/h1&gt;&lt;h2 id=&quot;Overc
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>抗逆力</title>
    <link href="https://www.llchen60.com/%E6%8A%97%E9%80%86%E5%8A%9B/"/>
    <id>https://www.llchen60.com/%E6%8A%97%E9%80%86%E5%8A%9B/</id>
    <published>2021-08-28T04:28:59.000Z</published>
    <updated>2022-07-01T14:07:08.744Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>很庆幸听了这场讲座，回想起原先做的一些决定，会囿于固定性思维，因为害怕失败，害怕被证实不够聪明，失去了很多很多成长的机会。感觉无论是工作还是生活，在度过了刚刚开始的熟悉了解阶段以后，后期除了专业技能上的提升之外，更加重要的是mindset的提升 — 关于你是怎么看待自己的，怎么看待你经历的事情。不知道最终答案是什么，但是这篇里面说的东西至少告诉了我想要达到理想的状态，你需要每天做些什么 :)  值得过一段时间回来再看看各种action items 呀</p></blockquote><h1 id="1-心理韧性"><a href="#1-心理韧性" class="headerlink" title="1. 心理韧性"></a>1. 心理韧性</h1><ul><li>高心理韧性是成功者的共性<ul><li>因为对于任何一个成功者来说，磨难是必不可少的一部分</li><li>成功者 坚韧不拔的精神</li><li>心理学研究<ul><li>预测成功的概率 和 坚韧不拔的特质有很明显的正相关，和智商的关系反倒并不是很强</li><li>生存下来的不是最强大的生物，也不是最聪明的生物，而是最能够适应变化的生物</li></ul></li></ul></li></ul><h2 id="1-1-心理学定义"><a href="#1-1-心理学定义" class="headerlink" title="1.1 心理学定义"></a>1.1 心理学定义</h2><ul><li>复原力 resilence<ul><li>人从逆境，冲突，痛楚，失败，压力当中迅速恢复的心理能力</li></ul></li><li>坚毅力 grip</li><li>创伤后的成长 PTG — Post Traumatic Growth<ul><li>不消沉，奋进</li></ul></li></ul><h2 id="1-2-高心理韧性人的特质"><a href="#1-2-高心理韧性人的特质" class="headerlink" title="1.2 高心理韧性人的特质"></a>1.2 高心理韧性人的特质</h2><ul><li>能力<ul><li>适应力</li><li>成长力</li><li>抗挫力</li><li>积极力 — 情绪的调节的方法</li><li>关系力 — 如何建立关系</li><li>控制力 — 淡定从容，自我控制</li></ul></li><li>特质<ul><li>有积极的认知方式  — the power of positive thinking<ul><li>决定我们的幸福指数的不是事情本身，而是我们如何看待这个事情</li></ul></li><li>乐观的情绪调节</li><li>健康的身心状态</li><li>强大的自我效能感  — 感觉自己能成，感觉自己有用  lol<ul><li>结婚能让男性长寿7年 lol</li></ul></li><li>解决问题的行动精神</li><li>良好的人际关系</li></ul></li></ul><h1 id="2-如何提升心理韧性"><a href="#2-如何提升心理韧性" class="headerlink" title="2. 如何提升心理韧性"></a>2. 如何提升心理韧性</h1><h2 id="2-1-自我效能感的提升"><a href="#2-1-自我效能感的提升" class="headerlink" title="2.1 自我效能感的提升"></a>2.1 自我效能感的提升</h2><ul><li>自我效能感的提升 Self Efficacy<ul><li>定义 — 是个人对自己完成某方面工作能力的主观评估，通过两条路径体现出来</li><li>高自我效能感的人，甚至会把压力 挫折 打击当做一种证明自己的能力的机遇</li><li>体现路径<ul><li>结果预期<ul><li>相信自己，认为我可以做到，是一种自我实现的预言</li></ul></li><li>效能预期<ul><li>我认为我能做到不是因为运气好或者环境好，而是因为我的能力</li><li>因此我要施展我的能力，为结果做足准备</li></ul></li></ul></li></ul></li><li>如何去做<ul><li>做出成功的模样<ul><li>装积极，是会变成真积极的</li><li>步伐更快</li><li>说话更多</li><li>做事主动</li><li>穿衣更正式些</li><li>锻炼更频繁些</li></ul></li><li>被成功者接纳<ul><li>与积极的人同行</li><li>替代性强化<ul><li>观察者看到榜样或者他人收到强化，成功了; 从而使得自己也倾向于做出榜样的行为</li></ul></li></ul></li><li>社会支持<ul><li>进化选择的是合作者</li><li>社会网络面积越大，更容易产生优势效应</li><li><strong>弱联系的强势效应</strong><ul><li>弱联系有着很快的低成本和高效能的传播效率</li><li>在六度分隔试验当中，正是层层叠加的弱联系将世界上原本毫不相关的人联系到了一起</li></ul></li></ul></li><li>模拟实战<ul><li>预见</li><li>大脑休闲的时候处于默认模式状态  hh<ul><li>会畅想未来，是一种竞争优势的~</li><li>对于事情进行遇见，是对我们帮助很大的</li></ul></li><li>Visualization  预见想象<ul><li>将自己将要做的事情去提前想象一下</li><li>过一遍自己需要怎么做</li><li>训练越多，意向越清晰</li><li>设想遇到打击，困难的时候你要怎么做</li></ul></li></ul></li><li>不断积累成功<ul><li>人最可怕的是发现自己一成不变</li><li>要去做</li></ul></li></ul></li></ul><h2 id="2-2-培养成长性思维"><a href="#2-2-培养成长性思维" class="headerlink" title="2.2 培养成长性思维"></a>2.2 培养成长性思维</h2><ul><li><p>人的思维模式</p><ul><li>成长性思维  Growth Mindset<ul><li>天赋只是起点</li><li>态度和努力可以决定一切</li><li>可以学会任何我想学会的东西</li><li>喜欢自我挑战</li><li>当我失败的时候，我学到了很多东西</li><li>我希望你表扬我很努力</li><li>如果别人成功了，我会收到别人的启发</li></ul></li><li>固定性思维 — 卓越的包袱<ul><li>我的聪明才智决定了一切</li><li>我擅长某些事，不擅长另外一些事</li><li>我不想尝试我可能不擅长的东西</li><li>如果我失败了，我就无地自容了</li><li>我希望你表扬我很聪明</li><li>如果别人成功了，他会威胁到我</li></ul></li></ul></li><li><p>固定性思维对于人的影响很大</p><ul><li>你会因为认为自己聪明，不敢做更大的挑战，因为一旦失败，你会害怕别人认为你不聪明了 会越来越难达到别人的预期的</li><li>被表扬努力的往往会选择更加困难的任务，也会更愿意通过学习，去尝试解决方案</li><li>卓越的包袱<ul><li>装酷的孩子的包袱</li><li>不愿意去冒险，不愿意去奋斗</li><li>努力愚蠢，装聪明</li><li>精英父母的过高的期望造成的心理压力和心理阴影</li><li>优秀女孩的诅咒，这种包袱往往对女孩的打击更大，她们往往更在意外在的评价，不敢冒险和努力</li><li>we are supposed to be dumb all the way, hhh</li></ul></li></ul></li><li><p>如何培养成长性思维</p><ul><li>改变考核的标准<ul><li>关注进步，而不是结果</li></ul></li><li>改变沟通的方式<ul><li>在评价表现的时候，用暂时不行代替就是不行</li><li>短暂 局部 可以改的</li><li>不要把事情说成稳定的长期的不可改变的</li><li>not yet instead of failed</li></ul></li><li>改变认知的习惯 — Albert Ellis 的认知治疗ABC</li><li>发挥辩证思维的优势 — 从负面体验中吸取成功的经验<ul><li>当一个人出于自我保护而抗拒内心的地狱的时候，他一并切断了通往内在天堂的道路。<ul><li>不承认自己内心的阴暗龌龊，那么就无从改进了</li></ul></li></ul></li></ul></li></ul><ul><li>认知治疗ABC<ul><li>构成<ul><li>A — Activating Events  诱发刺激</li><li>B — Beliefs  信念反应</li><li>C — Consequences  行为后果</li></ul></li><li>原理<ul><li>我们是改不了A的，但是我们可以改B，然后C就会发生变化！！</li><li>关键是你怎么看待A的 ！ 改变认知</li><li>真正困扰我们的并不是发生在我们身上的事情，而是我们围绕这个事情对它编织的故事，和由此引起的身心反应</li></ul></li></ul></li><li>情绪的ABCD理论 — 对于孩子而言<ul><li>出现了ABC以后，给一个机会让其反驳</li><li>让孩子去反驳他当时的念头</li><li>干预B  从而干预C</li></ul></li></ul><h2 id="2-3-提高自我调控的能力"><a href="#2-3-提高自我调控的能力" class="headerlink" title="2.3 提高自我调控的能力"></a>2.3 提高自我调控的能力</h2><ul><li>延迟满足， 自我控制</li><li>自我调控能力是可以锻炼从而获得提升的</li><li>如何进行训练<ul><li>体育锻炼</li><li>正念冥想 — 做事情沉浸其中就好啊！！</li><li>自我挑战</li><li>目标想象</li><li>有效休息</li><li>积极心态</li></ul></li></ul><h1 id="3-组织韧性"><a href="#3-组织韧性" class="headerlink" title="3. 组织韧性"></a>3. 组织韧性</h1><ul><li><p>复原力</p><ul><li>企业遇到困难后，如何回归正常</li></ul></li><li><p>复原后的发展能力</p></li><li><p>影响组织韧性的维度</p><ul><li>组织资本<ul><li>人力资源的保障<ul><li>什么政策</li></ul></li></ul></li><li>组织承诺<ul><li>员工对于组织的感情</li><li>信任</li></ul></li><li>组织领导<ul><li>leader本身的态度，思考，韧性</li></ul></li><li>组织学习</li><li>组织文化<ul><li>组织的传统和信仰</li></ul></li><li>社会网络</li></ul></li><li><p>提升组织韧性的方式</p><ul><li>Staff  选择心理韧性高的人才，锻炼心理韧性<ul><li>积极的自我认识</li><li>提倡积极的思维</li><li>加强关系建设</li><li>未来导向</li><li>乐观主义精神<ul><li>对于路径的乐观</li><li>对于结果的乐观</li></ul></li></ul></li><li>System 创造积极的心理健康环境</li><li>Skill</li></ul></li></ul><h1 id="4-压力的应对技巧"><a href="#4-压力的应对技巧" class="headerlink" title="4. 压力的应对技巧"></a>4. 压力的应对技巧</h1><h2 id="4-1-压力的应激反应"><a href="#4-1-压力的应激反应" class="headerlink" title="4.1 压力的应激反应"></a>4.1 压力的应激反应</h2><ul><li>应激反应的三轴心<ul><li>下丘脑</li><li>垂体</li><li>肾上腺</li></ul></li><li>三个器官会释放压力激素，使得我们的反应是fight or flight lol</li><li>而后激素水平下降</li><li>各种情绪<ul><li>焦虑  为未来的恐慌</li><li>抑郁  为过去伤心</li><li>自残</li></ul></li></ul><h2 id="4-2-与情绪有关的脑区"><a href="#4-2-与情绪有关的脑区" class="headerlink" title="4.2 与情绪有关的脑区"></a>4.2 与情绪有关的脑区</h2><ul><li><p>杏仁核  Amygdala</p><ul><li>会影响我们的情绪</li><li>情绪不好的时候会让杏仁核充血，然后杏仁核温度升高</li></ul></li><li><p>大脑皮层 Cerebral Cortex</p></li><li><p>如何应对压力 — <strong>抑制</strong>杏仁核的活动</p><ul><li>吸入凉气，降低杏仁核的温度  hhh</li><li>香气  让我们产生愉悦的感觉</li><li>写写日记</li></ul></li><li><p>激活大脑的奖励中枢</p><ul><li><p>神经元之间的间隙 靠神经递质连接</p></li><li><p>当奖赏中枢释放神经递质的时候，会释放积极的情绪</p></li><li><p>！！心理活动不是一个一个点，而是一片一片的产生的</p></li><li><p>多巴胺</p><ul><li>庆祝自己的成功 — 让自己的成功和快乐的体验延续一段时间<ul><li>将快乐的体验延续4分钟，就可以在大脑中形成记忆，从而形成一个快乐的神经网络</li></ul></li><li>做自己喜欢做的事情</li><li>享受艺术的美妙</li></ul></li><li><p>血清素</p><ul><li>能够振奋人的心情</li><li>什么时候会分泌<ul><li>体验到自我的价值</li><li>帮助别人的时候</li><li>自尊心的呵护<ul><li>保护自尊心  体现其价值</li></ul></li></ul></li><li>一些行为<ul><li>晒太阳~</li></ul></li></ul></li><li><p>内啡肽</p><ul><li>只有我们身心痛苦的时候，才会释放</li><li>行为<ul><li>有规律的运动</li><li>先苦后甜的体验</li><li>看喜剧，相声~ 烧脑的幽默</li></ul></li></ul></li><li><p>催产素</p><ul><li>男人也有催产素</li><li>主要作用是增加人的爱的感受，而不是为了怀孕</li><li>行为<ul><li>夸奖，赞美</li><li>陪伴</li></ul></li></ul><h2 id="4-3-应对压力的长期策略"><a href="#4-3-应对压力的长期策略" class="headerlink" title="4.3 应对压力的长期策略"></a>4.3 应对压力的长期策略</h2></li><li><p>压力容易让人失控失常</p></li><li><p>培养应对压力的积极习惯</p><ul><li>strength based approach  发挥自己的长处优势  找到自己的优势，然后充分在工作生活当中使用  使用自己的优势！！！</li><li>找到自己的流  find your flow<ul><li>喜欢做的事情，进入心流状态</li></ul></li><li>借助一些科学方法，自修，同修，专修<ul><li>reading &amp; learning</li></ul></li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>彭凯平 演讲  — 抗逆力— 重压下的心理韧性与成功</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;很庆幸听了这场讲座，回想起原先做的一些决定，会囿于固定性思维，因为害怕失败，害怕被证实不够聪明，失去了很多很多成长的机会。感觉无论是工作还是生活，在度过了刚刚开始的熟悉了解阶段以后，后期除了专业技能上的提升之外，更加重要的是mindset的提升 —
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="积极心理学" scheme="https://www.llchen60.com/tags/%E7%A7%AF%E6%9E%81%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Make Body Language Your Super Power</title>
    <link href="https://www.llchen60.com/Make-Body-Language-Your-Super-Power/"/>
    <id>https://www.llchen60.com/Make-Body-Language-Your-Super-Power/</id>
    <published>2021-08-28T02:24:14.000Z</published>
    <updated>2021-08-28T02:25:19.176Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Stand Strong</li><li>Gesture Effectively</li><li>Mind Your Audience</li></ul><h1 id="1-Posture"><a href="#1-Posture" class="headerlink" title="1. Posture"></a>1. Posture</h1><ul><li>Communication begins before you open your mouth</li><li>How to stand<ul><li>no<ul><li>hands in the pocket  — cannot convey strong msg</li><li>hands in the hip — tend to look overwhelming and powerful</li><li>hands in front of family jewels lol</li></ul></li><li>yes<ul><li>base posture — feet should be shoulder width apart<ul><li>that’s the first impression</li></ul></li><li>movement<ul><li>give</li><li>show</li><li>chop — strong msg</li></ul></li><li>palms up has better impact! comparing with palm down and pointing</li></ul></li></ul></li><li>Where to stand<ul><li>face your audience</li><li>move around in the center box</li><li>get rid of potential distraction<ul><li>like window, by nature we are attracted by moving thing, will break the concentration</li></ul></li></ul></li></ul><h1 id="2-Audience"><a href="#2-Audience" class="headerlink" title="2. Audience"></a>2. Audience</h1><ul><li>Speaker need to understand what audience is doing , make sure we are all in the journey</li><li>How to engage with audience more<ul><li>gesture</li><li>notice<ul><li>how your audience sitting</li><li>eye contact</li></ul></li><li>surprise<ul><li>cold call, lol</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.youtube.com/watch?v=cFLjudWTuGQ&ab_channel=StanfordGraduateSchoolofBusiness">https://www.youtube.com/watch?v=cFLjudWTuGQ&amp;ab_channel=StanfordGraduateSchoolofBusiness</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Stand Strong&lt;/li&gt;
&lt;li&gt;Gesture Effectively&lt;/li&gt;
&lt;li&gt;Mind Your Audience&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;1-Posture&quot;&gt;&lt;a href=&quot;#1-Posture&quot; class=&quot;hea
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Distributed Messaging System: Kafka</title>
    <link href="https://www.llchen60.com/Distributed-Messaging-System-Kafka/"/>
    <id>https://www.llchen60.com/Distributed-Messaging-System-Kafka/</id>
    <published>2021-08-24T17:07:33.000Z</published>
    <updated>2021-08-24T17:10:35.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview-of-Messaging-Systems"><a href="#1-Overview-of-Messaging-Systems" class="headerlink" title="1. Overview of Messaging Systems"></a>1. Overview of Messaging Systems</h1><h2 id="1-1-Why-we-need-a-messaging-system"><a href="#1-1-Why-we-need-a-messaging-system" class="headerlink" title="1.1 Why we need a messaging system"></a>1.1 Why we need a messaging system</h2><ul><li><p>Aim:</p><ul><li>Reliably transfer a high throughput of messages between different entities</li></ul></li><li><p>Challenges</p><ul><li>how we handle a spike of messages</li><li>how we divide the work among a set of instances</li><li>how could we receive messages from different types of sources</li><li>what will happen if the service is down?</li></ul></li><li><p>We need messaging systems in distributed architecture due to challenges above</p></li></ul><h2 id="1-2-What-is-a-messaging-system"><a href="#1-2-What-is-a-messaging-system" class="headerlink" title="1.2 What is a messaging system?"></a>1.2 What is a messaging system?</h2><ul><li><p>responsible for transferring data among services /applications/ processes/ servers</p></li><li><p>help decouple different parts of a distributed system by providing an asynchronous way of transferring messaging between the sender and the receiver</p></li><li><p>Two common ways to handle messages</p><ul><li>Queuing<ul><li>msgs are stored sequentially in a queue</li><li>producers push msg to the rear of the queue</li><li>consumers extract the msgs from the front of the queue</li><li>a particular msg can be consumed by a <strong>max of one consumer</strong> only</li></ul></li><li>Publish - Subscribe<ul><li>messages are divided into topics</li><li>a publisher sends a message to a topic</li><li>subscribers subscribe to a topic to receive every message published to that topic</li><li>msg system that stores and maintains the msg named as <strong>message broker</strong></li></ul></li></ul></li></ul><h1 id="2-Kafka"><a href="#2-Kafka" class="headerlink" title="2. Kafka"></a>2. Kafka</h1><h2 id="2-1-General"><a href="#2-1-General" class="headerlink" title="2.1 General"></a>2.1 General</h2><ul><li><strong>publish subscribe based</strong> messaging system</li><li>takes streams of messages from applications known as producers, stores them reliably on a central cluster, and allows those messages to be received by applications that process the messages</li><li>kafka is mainly used for<ul><li>reliably storing a huge amount of data</li><li>enabling high throughput of message transfer between different entities</li><li>streaming real time data</li></ul></li><li>kafka is a distributed commit log — write ahead log<ul><li>append-only data structure that can <strong>persistently store a sequence of records</strong></li><li>all messages are stored <strong>on disk</strong></li><li>since all reads and writes happen <strong>in sequence</strong>, Kafka takes advantage of <strong>sequential disk reads</strong></li></ul></li></ul><h2 id="2-2-Use-Cases"><a href="#2-2-Use-Cases" class="headerlink" title="2.2 Use Cases"></a>2.2 Use Cases</h2><ul><li>Metrics<ul><li>collect and aggregate monitoring data</li></ul></li><li>Log Aggregation<ul><li>collect logs from multiple sources and make them available in a standard format to multiple consumers</li></ul></li><li>Stream Processing<ul><li>the raw data consumed from a topic is transformed, enriched, or aggregated and pushed to a <strong>new topic</strong> for further consumption. This way of data processing is known as stream processing.</li></ul></li><li>Commit Log<ul><li>can be used as an external commit log for any distributed system</li><li>Distributed services can log their transactions to Kafka to keep track of what is happening. This transaction data can be used for replication between nodes and also becomes very useful for disaster recovery, for example, to help failed nodes to recover their states.</li></ul></li><li>Website activity tracking<ul><li>Build a user activity tracking pipeline</li><li>User activities like page clicks, searches, etc., are published to Kafka into separate topics. These topics are available for subscription for a range of use cases, including real-time processing, real-time monitoring, or loading into Hadoop or data warehousing systems for offline processing and reporting</li></ul></li><li>Product Suggestion</li></ul><h1 id="3-High-Level-Architecture"><a href="#3-High-Level-Architecture" class="headerlink" title="3. High Level Architecture"></a>3. High Level Architecture</h1><h2 id="3-1-Common-Terms"><a href="#3-1-Common-Terms" class="headerlink" title="3.1 Common Terms"></a>3.1 Common Terms</h2><ul><li>Brokers<ul><li>A Kafka server</li><li>responsible for reliably storing data provided by the producers and making it available to the consumers</li></ul></li><li>Records<ul><li>A message or an event that get stored in Kafka</li><li>A record contains<ul><li>key</li><li>value</li><li>timestamp</li><li>optional metadata headers</li></ul></li></ul></li><li>Topics<ul><li>Messages are divided into categories called topics</li><li>Each msg that Kafka receives from a producer is associated with a topic</li><li>consumers can subscribe to a topic to get notified when new messages are added to the topic</li><li>a topic can have multiple subscribers that read messages from it</li><li>a topic is identified by its name and must be unique</li><li>mes in a topic can be read as often as needed — message are not deleted after consumption, instead, Kafka <strong>retains messages for a configurable amount of time or until a storage size is exceeded</strong></li></ul></li><li>Producers<ul><li>Applications that publish or write records to Kafka</li></ul></li><li>Consumers<ul><li>Applications that subscribe to read and process data from Kafka topics</li><li>Consumers subscribe to one or more topics and consume published messages by pulling data from the brokers</li><li>In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for</li></ul></li></ul><h2 id="3-2-Architecture"><a href="#3-2-Architecture" class="headerlink" title="3.2 Architecture"></a>3.2 Architecture</h2><p><img src="https://i.loli.net/2021/08/25/Chuwtvg4mNfFU58.png" alt="Overall Architecture"></p><ul><li>Kafka cluster<ul><li>Kafka is run as a cluster of one or more servers, where each server is responsible for running one Kafka broker</li></ul></li><li>ZooKeeper<ul><li>Distributed key value store</li><li>Used for coordination and storing configurations</li><li>Kafka uses ZooKeeper to coordinate between Kafka brokers; ZooKeeper maintains metadata information about the Kafka cluster</li></ul></li></ul><h2 id="3-3-Performance-concern"><a href="#3-3-Performance-concern" class="headerlink" title="3.3 Performance concern"></a>3.3 Performance concern</h2><h3 id="3-3-1-Storing-messages-to-disks"><a href="#3-3-1-Storing-messages-to-disks" class="headerlink" title="3.3.1 Storing messages to disks"></a>3.3.1 Storing messages to disks</h3><ul><li>there is a huge difference in disk performance between <strong>random block access and sequential access</strong>. Random block access is slower because of <strong>numerous disk seeks</strong>, whereas the sequential nature of writing or reading, enables disk operations to be <strong>thousands of times faster</strong> than random access.</li><li>OS level optimization<ul><li>Read Ahead — prefetch large block multiples</li><li>Write Behind — group small logical writes into big physical writes</li><li>PageCache — cache the disk in free RAM</li></ul></li><li>Zero Copy optimization<ul><li>OS copy data from the pageCache directly to a socket, effectively bypassing the kafka broker application entirely</li></ul></li><li>Kafka protocol to group msg together<ul><li>reduce network overhead</li></ul></li></ul><h1 id="4-Dive-Deep-in-Kafka-Cluster"><a href="#4-Dive-Deep-in-Kafka-Cluster" class="headerlink" title="4. Dive Deep in Kafka Cluster"></a>4. Dive Deep in Kafka Cluster</h1><h2 id="4-1-Topic-Partitions"><a href="#4-1-Topic-Partitions" class="headerlink" title="4.1 Topic Partitions"></a>4.1 Topic Partitions</h2><ul><li><p>Topics are partitioned, spread over a number of fragments</p></li><li><p>Each partition can be placed on a separate Kafka broker</p></li><li><p>A new message get appended to one of the topic’s partition</p><ul><li>producer controls which partition it publishes to based on the data</li></ul></li><li><p>One partition is an <strong>ordered sequence</strong> of messages</p><ul><li>producers continually append new messages to partition</li><li>ordering of messages is <strong>maintained at the partition level, not across the topic</strong></li></ul></li><li><p>Unique sequence ID — offset</p><ul><li>It will get assigned to every message that enters a partition</li><li>used to identify every message’s sequential position within a topic’s partition</li><li>offset sequences are unique only to each partition</li><li>to locate a specific message<ul><li>topic</li><li>partition</li><li>offset number</li></ul></li><li>producers can choose to publish a message to any partition<ul><li>if ordering within a partition is not needed, a round robin partition strategy can be used</li><li>Placing each partition on separate Kafka brokers enables multiple consumers to read from a topic in parallel. That means, different consumers can concurrently read different partitions present on separate brokers</li></ul></li></ul></li><li><p>Messages once written to partitions are immutable and cannot be updated.</p></li><li><p>Kafka guarantees that messages with the same key are written to the same partition.</p></li></ul><h2 id="4-2-Dumb-Broker-and-Smart-Consumer"><a href="#4-2-Dumb-Broker-and-Smart-Consumer" class="headerlink" title="4.2 Dumb Broker and Smart Consumer"></a>4.2 Dumb Broker and Smart Consumer</h2><ul><li>Kafka does not keep track of what records are read by the consumer</li><li>Consumers themselves poll kafka for new messages and say what records they want to read<ul><li>this allow them to increment/ decrement the offset they are as they wish</li></ul></li></ul><h2 id="4-3-Leader-and-Follower"><a href="#4-3-Leader-and-Follower" class="headerlink" title="4.3 Leader and Follower"></a>4.3 Leader and Follower</h2><p>Every topic can be replicated to multiple Kafka brokers to make the data fault-tolerant and highly available. Each topic partition has one leader broker and multiple replica (follower) brokers. </p><ul><li>Structure<ul><li>the broker cluster could have multiple brokers, each broker could have multiple partitions which belong to different topics</li><li>Each topic partition would have one lead broker and multiple replica brokers</li></ul></li></ul><h3 id="4-3-1-Leader"><a href="#4-3-1-Leader" class="headerlink" title="4.3.1 Leader"></a>4.3.1 Leader</h3><ul><li>A leader is the node responsible for all reads and writes for the given partition</li><li>Each partition has one kafka broker acting as a leader</li></ul><h3 id="4-3-2-Follower"><a href="#4-3-2-Follower" class="headerlink" title="4.3.2 Follower"></a>4.3.2 Follower</h3><ul><li><p>To handle single point of failure, Kafka replicate partitions and distribute them across multiple broker servers called followers.</p></li><li><p>Each follower’s responsibility is to replicate the leader’s data to serve as a backup partition</p><ul><li><p>any follower can take over the leadership if the leader goes down</p><ul><li><p>from the image below, you could see only the leader take read and write requests, follower acts as replica but not take any read and write reqeusts</p><p><img src="https://i.loli.net/2021/08/25/4t7kVJfv3jliZyK.png" alt="Leader and Follower"></p></li></ul></li></ul></li></ul><h3 id="4-3-3-In-Sync-Replicas"><a href="#4-3-3-In-Sync-Replicas" class="headerlink" title="4.3.3 In Sync Replicas"></a>4.3.3 In Sync Replicas</h3><ul><li>In Sync Replicas means the broker has the latest data for a given partition</li><li>A leader is always an in sync replica</li><li>A follower is an in sync replica only if it has fully caught up to the partition it is following</li><li>Only ISRs are eligible to become partition leaders.</li><li>Kafka can choose the minimum number of ISRs required before the data becomes available for consumers to read</li></ul><h3 id="4-3-4-High-Water-mark"><a href="#4-3-4-High-Water-mark" class="headerlink" title="4.3.4 High Water mark"></a>4.3.4 High Water mark</h3><ul><li><p>To ensure data consistency, the leader broker never returns (or exposes) messages which have not been replicated to a minimum set of ISRs</p></li><li><p>For this, brokers keep track of the high-water mark, which is the highest offset that all ISRs of a particular partition share</p></li><li><p>The leader exposes data only up to the high-water mark offset and propagates the high-water mark offset to all followers</p><p>  <img src="https://i.loli.net/2021/08/25/pyDKzGCRow6O2Wu.png" alt="High Water Mark"></p></li></ul><h1 id="5-Consumer-Group"><a href="#5-Consumer-Group" class="headerlink" title="5. Consumer Group"></a>5. Consumer Group</h1><ul><li>A set of one or more consumers working together in parallel to consume messages from topic partitions, messages are equally divided among all the consumers of a group. with no two consumers receiving the same message</li></ul><h2 id="5-1-How-to-distribute-a-specific-message-to-only-a-single-consumer"><a href="#5-1-How-to-distribute-a-specific-message-to-only-a-single-consumer" class="headerlink" title="5.1 How to distribute a specific message to only a single consumer"></a>5.1 How to distribute a specific message to only a single consumer</h2><ul><li><p>only a single consumer reads messages from any partition within a consumer group</p><ul><li>means only one consumer can work on a partition in a consumer group at a time</li><li>every time a consumer is added to or removed from a group, the consumption is rebalanced within the group</li></ul></li><li><p>with consumer groups, consumers can be parallelized so that multiple consumers can read from multiple partitions on a topic, allowing a very high message processing throughput</p></li><li><p>number of partitions impacts consumers’ maximum parallelism. as there cannot be more consumers than partitions</p></li><li><p>Kafka stores the <strong>current offset per consumer group per topic per partition</strong>, as it would for a single consumer. This means that unique messages are only sent to a single consumer in a consumer group, and the load is balanced across consumers as equally as possible</p></li><li><p><strong>Number of consumers in a group = number of partitions:</strong> each consumer consumes one partition.</p></li><li><p><strong>Number of consumers in a group &gt; number of partitions:</strong> some consumers will be idle.</p></li><li><p><strong>Number of consumers in a group &lt; number of partitions:</strong> some consumers will consume more partitions than others.</p></li></ul><h1 id="6-Kafka-Workflow"><a href="#6-Kafka-Workflow" class="headerlink" title="6. Kafka Workflow"></a>6. Kafka Workflow</h1><h2 id="6-1-Pub-sub-messaging"><a href="#6-1-Pub-sub-messaging" class="headerlink" title="6.1 Pub sub messaging"></a>6.1 Pub sub messaging</h2><ul><li>Producer publish messages on a topic</li><li>Kafka broker stores messages in the partitions configured for that particular topic.<ul><li>If the producer did not specify the partition in which the msg should be stored, the broker ensures that the msg are equally shared between partitions</li><li>If the producer sends two msgs and there are two partitions, Kafka will store those two in two partitions separately.</li></ul></li><li>consumer subscribe to a specific topic</li><li>Kafka will provide the current offset of the topic to the consumer and also saves that offset in the zookeeper</li><li>consumer request kafka at regular intervals for new msgs</li><li>once kafka receives msg from producers, it forward these messages to the consumer</li><li>consumer will receive msg and process it</li><li>once processed, consumer will send an acknowledgement to the kafka broker</li><li>upon receiving the acknowledgement, kafka <strong>increments the offset and updates it in the zooKeeper</strong><ul><li>this info is stored in zooKeeper, thus consumer could read the next msg correctly even during broker outages</li></ul></li><li>consumers can rewind/ skip to the desired offset of a topic at any time and read all the subsequent messages</li></ul><h2 id="6-2-Kafka-workflow-for-consumer-group"><a href="#6-2-Kafka-workflow-for-consumer-group" class="headerlink" title="6.2 Kafka workflow for consumer group"></a>6.2 Kafka workflow for consumer group</h2><ul><li>Producers publish messages on a topic.</li><li>Kafka stores all messages in the partitions configured for that particular topic, similar to the earlier scenario.</li><li>A single consumer subscribes to a specific topic, assume <code>Topic-01</code> with Group ID as <code>Group-1</code>.</li><li>Kafka interacts with the consumer in the same way as pub-sub messaging until a new consumer subscribes to the same topic, <code>Topic-01</code>, with the same Group ID as <code>Group-1</code>.</li><li>Once the new consumer arrives, Kafka switches its operation to share mode, such that each message is passed to only one of the subscribers<br>of the consumer group <code>Group-1</code>. This message transfer is<br>similar to queue-based messaging, as only one consumer of the group<br>consumes a message. Contrary to queue-based messaging, messages are not<br>removed after consumption.</li><li>This message transfer can go on until the number of consumers<br>reaches the number of partitions configured for that particular topic.</li><li>Once the number of consumers exceeds the number of partitions, the<br>new consumer will not receive any message until an existing consumer<br>unsubscribes. This scenario arises because each consumer in Kafka will<br>be assigned a minimum of one partition. Once all the partitions are<br>assigned to the existing consumers, the new consumers will have to wait.</li></ul><h1 id="7-ZooKeeper"><a href="#7-ZooKeeper" class="headerlink" title="7. ZooKeeper"></a>7. ZooKeeper</h1><h2 id="7-1-What-is-ZooKeeper"><a href="#7-1-What-is-ZooKeeper" class="headerlink" title="7.1 What is ZooKeeper"></a>7.1 What is ZooKeeper</h2><ul><li>A distributed configuration and synchronization service</li><li>In Kafka case, help to store basic metadata<ul><li>information about brokers</li><li>topics</li><li>partitions</li><li>partition leader/ followers</li><li>consumer offsets</li></ul></li></ul><h2 id="7-2-Act-as-central-coordinator"><a href="#7-2-Act-as-central-coordinator" class="headerlink" title="7.2 Act as central coordinator"></a>7.2 Act as central coordinator</h2><p>ZooKeeper is used for storing all sorts of metadata about the Kafka cluster:</p><ul><li>It maintains the <strong>last offset position</strong> of each consumer group per partition, so that consumers can quickly recover from the last position in case of a failure (although modern clients store offsets in a<br>separate Kafka topic).</li><li>It tracks the topics, number of partitions assigned to those topics, and leaders’/followers’ location in each partition.</li><li>It also manages the access control lists (ACLs) to different topics in the cluster. ACLs are used to enforce access or authorization.</li></ul><h2 id="7-3-How-to-find-leaders"><a href="#7-3-How-to-find-leaders" class="headerlink" title="7.3 How to find leaders"></a>7.3 How to find leaders</h2><ul><li>The producer connects to any broker and asks for the leader of Partition 1<ul><li>each broker contains metadata</li><li>each brokers will talk to zooKeeper to get the latest metadata</li></ul></li><li>The broker responds with the identification of the leader broker responsible for partition 1</li><li>The producer connects to the leader broker to publish the message</li></ul><h1 id="8-Controller-Broker"><a href="#8-Controller-Broker" class="headerlink" title="8. Controller Broker"></a>8. Controller Broker</h1><ul><li>Within the Kafka cluster, one broker will be elected as the Controller</li><li>Responsibility<ul><li>admin operations<ul><li>creating/ deleting a topic</li><li>adding partitions</li><li>assigning leaders to partitions</li><li>monitoring broker failures</li></ul></li><li>check the health of other brokers in the system periodically</li><li>communicates the result of the partition leader election to other brokers in the system</li></ul></li></ul><h2 id="8-1-Split-brain-issue"><a href="#8-1-Split-brain-issue" class="headerlink" title="8.1 Split brain issue"></a>8.1 Split brain issue</h2><ul><li>some controller has temporary issue, during the period, we assign a new controller, but the previous one auto recover, so we have two controllers and it could bring inconsistency easily.</li><li>Solution:<ul><li>Generation Clock<ul><li>simply a monotonically increasing number to indicate a server’s generation</li><li>If the old leader had an epoch number of ‘1’, the new one would have ‘2’.</li><li>This epoch is included in every request that is sent from the Controller to other brokers.</li><li>This way, brokers can now easily differentiate the real Controller by simply trusting the Controller with the highest number.</li><li>The Controller with the highest number is undoubtedly the latest one, since the epoch number is always increasing.</li><li>This epoch number is stored in ZooKeeper.</li></ul></li></ul></li></ul><h1 id="9-Delivery-Semantics"><a href="#9-Delivery-Semantics" class="headerlink" title="9. Delivery Semantics"></a>9. Delivery Semantics</h1><h2 id="9-1-Producer-Delivery-Semantics"><a href="#9-1-Producer-Delivery-Semantics" class="headerlink" title="9.1 Producer Delivery Semantics"></a>9.1 Producer Delivery Semantics</h2><ul><li>How can a producer know that the data is successfully stored at the leader or that the followers are keeping up with the leader</li><li>Kafka offers three options to denote the <strong>number of brokers</strong> that <strong>must receive the record</strong> before the <strong>producer considers the write as successful</strong><ul><li>Async<ul><li>Producer sends a msg to kafka and does not wait for acknowledgement from the server</li><li>fire-and-forget approach gives the best performance as we can write data to Kafka at network speed, but <strong>no guarantee can be made</strong> that the server has received the record in this case.</li></ul></li><li>Committed to Leader<ul><li>Producer waits for an acknowledgment from the leader.</li><li>This ensures that the data is committed at the leader; it will be slower than the ‘Async’ option, as the data has to be written on disk on the leader.</li><li>Under this scenario, the leader will respond without waiting for acknowledgments from the followers.</li><li>In this case, the record <strong>will be lost if the leader crashes immediately after acknowledging the producer but before the followers have replicated it</strong>.</li></ul></li><li>Committed to Leader and Quorum<ul><li>Producer waits for an acknowledgment from the leader and the quorum. This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This will be the slowest write but guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee.</li></ul></li></ul></li></ul><h2 id="9-2-Consumer-Delivery-Semantics"><a href="#9-2-Consumer-Delivery-Semantics" class="headerlink" title="9.2 Consumer Delivery Semantics"></a>9.2 Consumer Delivery Semantics</h2><ul><li>Ways to provide consistency to the consumer<ul><li>At most once<ul><li>Message may be lost but are never redelivered</li><li>Under this option, the consumer upon receiving a message, commit (or increment) the offset to the broker. Now, if the consumer crashes before fully consuming the message, that message will be lost, as when the consumer restarts, it will receive the next message from the last committed offset.</li></ul></li><li>At least once<ul><li>Messages are never lost but maybe redelivered</li><li>This scenario occurs when the consumer receives a message from Kafka, and it does not immediately commit the offset.</li><li>Instead, it waits till it completes the processing.</li><li>So, if the consumer crashes after processing the message but before committing the offset, it has to reread the message upon restart.</li><li>Since, in this case, the consumer never committed the offset to the broker, the broker will redeliver the same message. Thus, duplicate message delivery could happen in such a scenario.</li></ul></li><li>Exactly once<ul><li>It is very hard to achieve this unless the consumer is working with a transactional system.</li><li>Under this option, the consumer puts the message processing and the offset increment in one transaction.</li><li>This will ensure that the offset increment will happen only if the whole transaction is complete.</li><li>If the consumer crashes while processing, the transaction will be rolled back, and the offset will not be incremented. When the consumer restarts, it can reread the message as it failed to process it last time. This option leads to no data duplication and no data loss but can lead to decreased throughput.</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview-of-Messaging-Systems&quot;&gt;&lt;a href=&quot;#1-Overview-of-Messaging-Systems&quot; class=&quot;headerlink&quot; title=&quot;1. Overview of Messaging Syste
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>货运代理法律风险</title>
    <link href="https://www.llchen60.com/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E6%B3%95%E5%BE%8B%E9%A3%8E%E9%99%A9/"/>
    <id>https://www.llchen60.com/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E6%B3%95%E5%BE%8B%E9%A3%8E%E9%99%A9/</id>
    <published>2021-08-17T04:29:17.000Z</published>
    <updated>2021-08-19T23:01:27.883Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-货运代理法律风险基本内容"><a href="#1-货运代理法律风险基本内容" class="headerlink" title="1. 货运代理法律风险基本内容"></a>1. 货运代理法律风险基本内容</h1><h2 id="1-1-国际货运代理人定义"><a href="#1-1-国际货运代理人定义" class="headerlink" title="1.1 国际货运代理人定义"></a>1.1 国际货运代理人定义</h2><ul><li>历史视角上来看<ul><li>国际货运代理经历了由显明代理人到隐名代理人再到<strong>运输合同当事人</strong>的过程  Fright Agency —→ Freight Forwarder</li><li>研究内容从单纯的代理法律关系向运输法律关系扩展</li><li>现代货运代理的定义 1992 《货运代理》<ul><li>提供并安排货物运输以取得报酬，</li><li>或者为货物合并拼箱并承担将这些货物由收货地运至目的地的<strong>运输责任</strong></li></ul></li><li>2002 《货运代理法》  Legal Classification of Fright Forwarders<ul><li>奠定了国际货运代理制度的FIATA的立法模式下，以货物运输合同的当事人 (as principal) 和非当事人 (except as principal) 来区分国际货物代理企业作为<strong>契约承运人和纯粹代理人</strong>的情况</li></ul></li><li>FIATA (International Federation of Freight Forwarders Association)《国际货运代理业示范法》<ul><li>国际货运代理人是指与客户达成货运代理协定，为其提供各类运输相关服务及其他辅助和咨询服务，或者在前述服务之外还以使用自有运输工具或者签发自己的运输单据的方式为客户承运货物的人</li></ul></li></ul></li><li>无船承运人<ul><li>对于实际货主而言，作为公共承运人与之订立海上货物运输合同</li><li>对于实际承运人而言，又承担着托运人的义务</li></ul></li><li>国际货运代理人定义<ul><li>International Freight Forwarder</li></ul></li><li>对货代的两类界定<ul><li>代理人说<ul><li>将国际货运代理人规定为受委托人的指示为其货物在国际间的运输及其他有关事务提供合理审慎服务的自然人或经济组织，本身业务不涉及货物的承运</li><li>与货主或委托人之间是纯粹的代理关系</li></ul></li><li>双重身份说<ul><li>规定了国际货运代理人是为委托方代办国际货运事务的代理人</li><li>规定了在一定条件下可以<strong>成为运输合同的当事人并对外承担承运人的责任</strong></li></ul></li></ul></li></ul><h2 id="1-2-货运代理法律关系辨析"><a href="#1-2-货运代理法律关系辨析" class="headerlink" title="1.2 货运代理法律关系辨析"></a>1.2 货运代理法律关系辨析</h2><ul><li>法律关系<ul><li>指相关海运国际公约，各国国内法以及行业规范等在调整国际海上货运代理行为的过程中形成的各有关主体间的权利和义务关系</li><li>国际海上货运代理法律关系包括<ul><li>国际货运代理企业作为海运代理人的法律关系</li><li>作为无船承运人的法律关系</li></ul></li></ul></li><li>作为海运代理人的法律关系<ul><li>在提供海上货物运输的相关代理业务的时候，呈现出的关系</li><li>内部委托法律关系<ul><li>国际货运代理企业与托运人之间的委托代理合同法律关系</li></ul></li><li>外部代理法律关系<ul><li>国际货运代理企业为托运人的利益和承运人签订海上货物运输合同而产生的运输合同法律关系</li><li>货代企业往往会以自己的名义代替多笔散货托运人与承运人签订一个总的运输合同，自己再分别同各托运人签订货运代理协议</li></ul></li><li>我国合同法对于包括货运代理合同在内的委托合同采用过错责任原则<ul><li>货代仅在自身确实受托事项存在过错并造成托运人损失的情况下承担违约损害赔偿责任</li><li>且只要在第三方选任上能够证明已经履行了合理和谨慎的义务，对由于第三方造成的托运人损失，可以免于承担责任</li></ul></li></ul></li><li>作为无船承运人的法律关系<ul><li>契约承运人的一种，即不拥有或者不经营船舶，不进行实际的货物运输活动，以签发无船承运人提单(Non Vessel Operating Common Carrier Bill of Loading)的方式明示或者默示对运输负有责任的人，承运责任来源于和托运人签订的货物运输契约，而不是实际的运输行为</li><li>法律关系<ul><li>无船承运人和托运人之间的海上货物运输合同关系</li><li>无船承运人和船公司之间的海上货物运输合同关系<ul><li>货代以托运人的身份和船公司 — 实际承运人签订运输合同并获得海运提单的 (Master Bill of Loading MBL)</li><li><strong>根据海商法， 无船承运人所需要承担的法律责任和实际承运人的责任是一样的</strong></li><li>无船承运人不享有不完全责任制<ul><li>当免责事由发生的时候，船公司可以免于对无船承运人承担责任，而无船承运人无法以相同的事由对托运人免责</li></ul></li><li>当承运人和实际承运人需要对货损货差需要进行赔偿的时候，二者在责任范围内承担连带责任</li><li>对于除了海上运输之外实际托运人提供的其他运输服务，无船承运人均承担严格责任</li></ul></li><li>无船承运人和船公司代理人之间的海上货物运输合同关系</li></ul></li></ul></li></ul><h1 id="2-风险状况分析"><a href="#2-风险状况分析" class="headerlink" title="2. 风险状况分析"></a>2. 风险状况分析</h1><ul><li><p>风险的定义</p><ul><li>由于国际货运代理企业在开展海上业务过程中受到客观法律环境，包括自身在内的各海洋运输相关主体所实施的法律行为的影响，导致其权利义务状态发生改变，从而产生的可能由该企业承担的法律上的不利后果</li><li>法律风险由环境诱因 — 客观法律环境，行为诱因 — 实施的法律行为及二者所引发的不利后果构成<ul><li>法律的不完善和不确定性<ul><li>不完善 — 制定法因为无法避免地滞后于社会的发展而必然存在的漏洞和空白</li><li>不确定性 — 由于法律条文意义晦涩之处需要法官来阐释说明，法官对于法律的理解会有所不同，从而导致了裁判结果不总是一致的</li></ul></li><li>行为诱因<ul><li>货代的一些不规范的法律行为，</li></ul></li></ul></li></ul></li><li><p>货代纠纷案件研究 争议焦点主要在：</p><ul><li>涉案主体间货运代理合同关系的认定<ul><li>因为转委托而引发的对双方间是否存在直接法律关系的异议</li><li>因为货运代理人和无船承运人身份识别而引发的对合同关系性质是货物运输合同法律关系还是货物运输代理合同法律关系的异议</li></ul></li></ul></li><li><p>主要风险分类</p><ul><li>身份认定上的风险<ul><li>指对国际货运代理企业是涉案货物的海运代理人还是无船承运人的认定结果和企业自身对此的认识不同造成的问题</li></ul></li><li>转委托和双方代理上的风险</li><li>涉外法律适用上的风险<ul><li>譬如美国统一商法典，允许以记名提单的方式放货，如果双方在法律适用条款当中约定适用美国法，那么海外代理提单无单放货的行为就不属于过错行为</li></ul></li></ul></li></ul><h1 id="3-风险成因分析"><a href="#3-风险成因分析" class="headerlink" title="3. 风险成因分析"></a>3. 风险成因分析</h1><h2 id="3-1-身份认定上的法律风险成因"><a href="#3-1-身份认定上的法律风险成因" class="headerlink" title="3.1 身份认定上的法律风险成因"></a>3.1 身份认定上的法律风险成因</h2><ul><li>国际货运代理企业是涉案货物的海运代理人还是无船承运人的认定结果的不同，会导致企业需要承担的法律责任和风险完全不同<ul><li>立法上的小混乱<ul><li>因为很有可能货运代理企业同时担当着无船承运人还有代理人的角色</li></ul></li><li>司法裁判的不确定<ul><li>《海上货代纠纷规定》的一些判断逻辑<ul><li>双方之间是否订立了海上货运代理合同，反映出来的是代理协议还是运输协议</li><li>受托人向委托人是否签发了提单，是承运提单还是代理人的分提单</li><li>受托人收取费用的名义是佣金还是运费</li><li>双方以往的交易历史和交易习惯</li></ul></li></ul></li><li>从业者自身行为的不规范<ul><li>是否保留了重要的往来文件，发票和提单</li></ul></li></ul></li></ul><h2 id="3-2-转委托与双方代理上的法律风险成因"><a href="#3-2-转委托与双方代理上的法律风险成因" class="headerlink" title="3.2 转委托与双方代理上的法律风险成因"></a>3.2 转委托与双方代理上的法律风险成因</h2><h3 id="3-2-1-转委托行为的法律风险成因"><a href="#3-2-1-转委托行为的法律风险成因" class="headerlink" title="3.2.1 转委托行为的法律风险成因"></a>3.2.1 转委托行为的法律风险成因</h3><ul><li>什么是转委托<ul><li>受托人将委托人委托其代为处理的事务转交给第三人处理的行为</li></ul></li><li>转委托行为的法律风险成因<ul><li>合同法规定，对于委托事务，除了经过委托人同意或者出现紧急状况可以转委托之外，受托人均应当亲自处理，否则就要为第三人的行为承担责任</li><li>但是对于国际货运代理行业来说，转委托是一个常规方式<ul><li>原因在于海运货代委托人更为看重成本和效率，让货代企业去完全处理每一件委托事务是不经济也不现实的</li></ul></li><li>造成货代企业转委托风险的是是否取得了委托人的同意</li><li>当前《海上货代纠纷规定》明确排除了推定托运人默示同意货运代理人转委托的可能<ul><li>转委托具体权限约定不明的时候，委托人将负有就不明权限想委托人报告的义务</li><li>委托人在受托人指示下与第三人的通常接触行为不能认定为委托人以该行为对转委托的明确同意</li></ul></li></ul></li></ul><h3 id="3-2-2-双方代理行为上的法律风险成因"><a href="#3-2-2-双方代理行为上的法律风险成因" class="headerlink" title="3.2.2 双方代理行为上的法律风险成因"></a>3.2.2 双方代理行为上的法律风险成因</h3><ul><li>什么是双方代理？<ul><li>指在同一法律关系内，一方当事人的代理人同时又接受另一方当事人委托，并为其代理的行为</li><li>由于合同关系中双方是相对的，双方代理会使得本是冲突的合同双方意思表示被代理的个人意志予以替代，偏离了合同的本质属性</li></ul></li><li>海运货代行业需要这样做，因为效率上的提升。但会有法律上的风险</li></ul><h2 id="3-3-涉外法律适用上的风险成因"><a href="#3-3-涉外法律适用上的风险成因" class="headerlink" title="3.3 涉外法律适用上的风险成因"></a>3.3 涉外法律适用上的风险成因</h2><ul><li>作为法院裁判依据的国外法律规定可能会让国际货运代理企业在诉讼中处于不利地位</li><li>涉外商事代理法律关系，会受到途经国家的法律管辖</li><li>如果双方未约定准据法，或涉诉的国际货代企业没有准确理解和把握已约定的准据法，就会大大增加诉讼中的不稳定因素</li><li>英美法系，判例法； 成文法国家，有专门的货运代理法律</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>《国际海上货运代理法律风险研究》  邓大鸣</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-货运代理法律风险基本内容&quot;&gt;&lt;a href=&quot;#1-货运代理法律风险基本内容&quot; class=&quot;headerlink&quot; title=&quot;1. 货运代理法律风险基本内容&quot;&gt;&lt;/a&gt;1. 货运代理法律风险基本内容&lt;/h1&gt;&lt;h2 id=&quot;1-1-国际货运代理人定义&quot;&gt;
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="货运代理" scheme="https://www.llchen60.com/tags/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86/"/>
    
  </entry>
  
</feed>
