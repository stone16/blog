<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leilei&#39;s Blog | 磊磊的博客</title>
  
  <subtitle>Because it&#39;s there</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.llchen60.com/"/>
  <updated>2022-02-04T02:41:50.858Z</updated>
  <id>https://www.llchen60.com/</id>
  
  <author>
    <name>Leilei Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>置身事内 笔记</title>
    <link href="https://www.llchen60.com/%E7%BD%AE%E8%BA%AB%E4%BA%8B%E5%86%85-%E7%AC%94%E8%AE%B0/"/>
    <id>https://www.llchen60.com/%E7%BD%AE%E8%BA%AB%E4%BA%8B%E5%86%85-%E7%AC%94%E8%AE%B0/</id>
    <published>2022-02-02T08:41:20.000Z</published>
    <updated>2022-02-04T02:41:50.858Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-整体框架"><a href="#1-整体框架" class="headerlink" title="1. 整体框架"></a>1. 整体框架</h1><ul><li>微观机制<ul><li>地方政府的权力和事务</li><li>财税与政府行为</li><li>政府投融资和债务</li><li>工业化当中的政府角色</li></ul></li><li>宏观现象<ul><li>城市化和不平衡</li><li>债务与风险</li><li>国内国际失衡</li><li>政府与经济发展</li></ul></li></ul><h1 id="2-地方政府的权力和事务"><a href="#2-地方政府的权力和事务" class="headerlink" title="2. 地方政府的权力和事务"></a>2. 地方政府的权力和事务</h1><aside>💡 地方政府不只提供了公共服务，也深度参与生产和分配。</aside><h2 id="2-1-体制特点"><a href="#2-1-体制特点" class="headerlink" title="2.1 体制特点"></a>2.1 体制特点</h2><ul><li>政府管理体系<ul><li>中央 - 省 - 市 - 县区 - 乡镇</li></ul></li><li>体制特点<ul><li>中央与地方政府<ul><li>日常运作以地方政府为主</li></ul></li><li>党与政府<ul><li>党负责重大决策和人事任免</li><li>政府负责执行</li></ul></li><li>条块分割，多重领导<ul><li>层层复制</li><li>党委政府人大政协四套班子</li><li>条条关系是业务关系，块块关系是领导关系—地方党委和政府可以决定人事任免</li></ul></li><li>上层领导与协调<ul><li>权力分散，决策容易向上集中</li><li>尽量在能达成共识的最低层级上解决问题</li></ul></li><li>官僚体系<ul><li>官员必须学习和贯彻统一的意识形态</li><li>官员由上级任命</li><li>地方主官需要在多地轮换任职</li></ul></li></ul></li><li>政府治理和运作的模式<ul><li>了解权力和资源在政府体系中的分布规则<ul><li>上下级政府间的纵向分布</li><li>同级政府间的横向分布</li></ul></li></ul></li><li>事权划分的原则<ul><li>公共服务的规模经济</li><li>信息的复杂性</li><li>激励相容</li></ul></li></ul><h2 id="2-2-外部性-规模经济"><a href="#2-2-外部性-规模经济" class="headerlink" title="2.2 外部性 规模经济"></a>2.2 外部性 规模经济</h2><ul><li>一个事物是只在划定的区域范围内有影响 还是对外也有影响</li><li>行政区域划分<ul><li>人口密度 — 提供公共物品和服务需要成本</li><li>山川河流</li><li>语言文化差异</li></ul></li><li>城市群的规划<ul><li>因为经济活动和人口集聚，需要打破现有的行政边界，在更大范围内提供无缝对接的标准化公共服务</li></ul></li><li>行政交界地区经济发展弱<ul><li>地理原因</li><li>亚文化区，距离主流文化相对远 — 文化，语言上</li><li>省政府也不会把有限资源优先配置到边界地区</li><li>环境污染谁来治理的问题</li></ul></li></ul><h2 id="2-3-复杂信息"><a href="#2-3-复杂信息" class="headerlink" title="2.3 复杂信息"></a>2.3 复杂信息</h2><ul><li>信息与权力<ul><li>原则上  上级形式权威</li><li>实际上给下级  下级有实际权威</li><li>信息优势  权威的平衡是核心问题</li></ul></li><li>信息获取+隐瞒<ul><li>信息传递很重要<ul><li>大量的会议和文件</li></ul></li><li>复杂的文件和会议制度<ul><li>文件类型，格式，以及报送都有严格的流程</li></ul></li></ul></li><li>上层的监督和审计</li></ul><aside>💡 所谓权力，实质上就是在说不清楚的情况下由谁来拍板决策的问题</aside><h2 id="2-4-激励相容"><a href="#2-4-激励相容" class="headerlink" title="2.4 激励相容"></a>2.4 激励相容</h2><ul><li>垂直管理<ul><li>专业性强，标准化程度高的部门</li><li>面对有双重领导的部门，都有一个根本的激励设计的问题<ul><li>能评价和奖惩工作业绩的上级，能决定工作内容的上级，受下级工作影响最大的上级，应该尽量是同一个上级</li></ul></li></ul></li><li>地方管理<ul><li>更加宏观，更加需要多部门合作的方面，比如经济发展</li><li>需要给地方放权，地方负责且地方能够分享发展成果</li></ul></li></ul><h2 id="2-5-招商引资"><a href="#2-5-招商引资" class="headerlink" title="2.5 招商引资"></a>2.5 招商引资</h2><aside>💡 地方政府不仅在为经济发展创造环境，它本身就是经济发展的深度参与者。</aside><ul><li>全民招商政策<ul><li>即招商不是单个部门要做的，是所有部门都需要熟悉政策，寻找招商机会的</li></ul></li><li>地方政府是城市土地所有者<ul><li>会将工业用地以非常优惠的价格转让给企业使用</li><li>负责对土地进行一系列初期开发 — 七通一平</li></ul></li><li>金融支持<ul><li>政府控制的投资平台入股</li><li>调动本地国企参与投资</li><li>通过各种方式协助企业获得银行贷款</li></ul></li><li>事务上支持<ul><li>各种许可</li></ul></li><li>补贴，税收优惠</li></ul><h1 id="3-财税与政府行为"><a href="#3-财税与政府行为" class="headerlink" title="3. 财税与政府行为"></a>3. 财税与政府行为</h1><ul><li><p>把握政府的真实意图，不能光读文件，还要看政府资金的流向和数量</p></li><li><p>事权与财力匹配 — 事权和支出责任匹配</p></li><li><p>实际情况</p><ul><li>事权与财权高度不匹配<ul><li>自1994年实行分税制改革依赖，地方财政预算支出ji</li></ul></li></ul></li></ul><h2 id="3-1-分税制改革"><a href="#3-1-分税制改革" class="headerlink" title="3.1 分税制改革"></a>3.1 分税制改革</h2><ul><li>1985年 — 1993年 财政包干<ul><li>承包制<ul><li>土地承包</li><li>企业承包</li><li>财政承包</li></ul></li><li>承包制原因，我国基本国策决定了不能对所有权做出根本性变革，只能对使用权和经营权实行承包制</li><li>实行方式<ul><li>1988年北京  以1987年财政收入为基数，设定一个固定的年收入增长率4%，超过4%的部分都归北京，没超过的部分则和中央五五分成</li></ul></li><li>地方实现方式<ul><li>大力发展乡镇企业</li></ul></li><li>影响<ul><li>中央财政预算收入占全国财政预算总收入的比重越来越低</li><li>全国财政预算总收入占GDP的比重也越来越低<ul><li>央地分成比例每过几年就要重新谈判</li><li>预算外收入可独享，因此地方政府疯狂给政府减税，藏富于企业，再通过行政收费，集资，摊派，赞助等方式收回一部分</li></ul></li></ul></li></ul></li><li>1994年分税制改革<ul><li>税务分类<ul><li>中央税</li><li>地方税</li><li>共享税<ul><li>增值税 — 占全国税收的1/4<ul><li>中央拿75% 地方留25%</li><li>转换过程<ul><li>为了防止地方财政收入的剧烈下跌，设立了税收返还的机制，保证改革后地方增值税收入和改革前一样，新增部分才和中央分</li></ul></li></ul></li></ul></li></ul></li><li>机构设置 — 与地方财政脱钩<ul><li>国税</li><li>地税</li></ul></li><li>行政方式<ul><li>省以下税务机关以垂直管理为主，由上级税务机构负责管理人员和工资</li></ul></li></ul></li></ul><aside>💡 公众所接触的信息和看到的现象，大都是已经博弈了的结果，而缺少社会阅历的学生容易把博弈结果错当成博弈过程。成功的政策背后是成功的协商和妥协，而不是机械的命令和执行，所以理解利益冲突，理解协调和解决机制，是理解政策的基础。</aside><ul><li>分税制改革中的权衡<ul><li>93年谈，想以92年作为基年，但是协商后改成93年，因此各个地方政府开始93年突击缴税</li></ul></li><li>分税制改革的影响<ul><li>中央占全国预算总收入的比重从改革前20% 到了55%</li><li>国家预算收入占GDP的比重从11%增加到了20%以上</li></ul></li></ul><h2 id="3-2-土地财政"><a href="#3-2-土地财政" class="headerlink" title="3.2 土地财政"></a>3.2 土地财政</h2><aside>💡 分税制改革减少了地方政府的可支配的财政资源，但是没有改变以经济建设为中心的任务；发展经济所需的诸多额外支出，比如招商引资，土地开发等，需要另筹资金了</aside><ul><li>解决方案<ul><li>努力提高税收规模</li><li>增加预算外收入<ul><li>围绕土地出让和土地开发所产生的的土地财政</li></ul></li></ul></li><li>地方政府青睐重资产的制造业<ul><li>投资规模大，对GDP的拉动作用明显</li><li>增值税在生产环节增收，跟生产规模直接挂钩</li><li>制造业不仅可以吸纳从农业部门转移出来的低技能劳动力，也可以带动第三产业发展，增加相关税收</li></ul></li><li>演变进程<ul><li>1998年 单位停止福利分房</li><li>1997 - 2002年 城镇住宅新开工面积年均增长26%</li><li>2001年 国家推行招标拍卖</li><li>2002年 国土部明确四类经营用地 商业，旅游，娱乐，房地产采用招拍挂的制度</li><li>国有土地转让收入占地方公共预算收入的比重在60%左右 从1999年低于10% 到现在的高度</li></ul></li><li>土地财政囊括<ul><li>土地使用权转让收入</li><li>和土地使用开发有关的各种税收收入<ul><li>大部分税基是价值而非面积</li></ul></li></ul></li><li>问题<ul><li>土地资本化运作，是在将未来的收益抵押到今天来借钱，因为地方官员任期有限，投资质量是难以保证的</li></ul></li></ul><h2 id="3-3-横向纵向的不平衡"><a href="#3-3-横向纵向的不平衡" class="headerlink" title="3.3 横向纵向的不平衡"></a>3.3 横向纵向的不平衡</h2><p>分税制改革以后，中央拿走了大头，但事情还是地方办，地方收支差距需要中央进行转移支付。全国总数上来看，能补得上。但总数不得上不代表每一级政府都能够补得上。</p><ul><li><p>问题</p><ul><li>财权层层上收，事权层层下压<ul><li>2000年 湖北监利县“农民真苦，农村真穷，农业真危险” — 三农问题<ul><li>农村税费改革，制止基层的乱摊派乱收费问题</li><li>废止农业税</li></ul></li></ul></li><li>财税体制的层级问题</li></ul></li><li><p>解决方案</p><ul><li>农村基本公共服务开始 纳入国家公共财政保障范围，中央地方政府共同承担</li><li>在转移支付中加入激励机制，鼓励基层政府达成特定目标，并给予奖励</li><li>将基层财政资源向上一级政府统筹<ul><li>乡财县管</li><li>扩权强县</li><li>财政省直管县</li></ul></li></ul></li><li><p>中央政府通过再分配  转移支付，支援中西部</p></li></ul><h1 id="4-政府投融资与债务"><a href="#4-政府投融资与债务" class="headerlink" title="4. 政府投融资与债务"></a>4. 政府投融资与债务</h1><aside>💡 土地不会移动也不会消失，天然适合作抵押，做各种资本交易的压仓标的，身价自然飙升。土地资本化的魔力，在于可以挣脱物理属性，在抽象的意义上交易承诺和希望，将过去的储蓄，现在的收入，未来的前途，统统汇聚和封存在一小片土地上，使其价值暴增。经济发展的奥秘之一，就是将有形资产转变成为这种抽象资本，从而聚合跨越空间和时间的资源。</aside><h2 id="4-1-城投公司和土地金融-—-土地开发和基础设施投资"><a href="#4-1-城投公司和土地金融-—-土地开发和基础设施投资" class="headerlink" title="4.1 城投公司和土地金融 — 土地开发和基础设施投资"></a>4.1 城投公司和土地金融 — 土地开发和基础设施投资</h2><ul><li>法律规定，地方政府不可以从银行贷款，2015年前也不允许发行债券 — 因此地方政府需要成立公司<ul><li>国有独资企业 — 地方政府融资平台 — 城投公司<ul><li>建设投资 + 投资开发 + 旅游发展</li></ul></li></ul></li><li>城投公司的一般特征<ul><li>持有从政府取得的大量土地使用权<ul><li>土地使用权可以用来撬动银行贷款，以及各种其他资金</li></ul></li><li>盈利状况依赖政府补贴<ul><li>因为承接的项目很多都有基础设施属性</li><li>项目本身盈利能力往往比较弱</li></ul></li><li>政府的隐性担保可以让企业大量借款</li></ul></li><li>土地一级开发<ul><li>平整整理土地</li><li>投入大，利润低，涉及拆迁等问题</li><li>一般由政府融资平台公司完成</li></ul></li><li>土地二级开发<ul><li>土地的建设运营</li><li>由房地产公司来做</li></ul></li><li>华夏幸福的方式—- 政府和社会资本合作 Public Private Partnership — PPP<ul><li>产城结合</li><li>政府委托华夏幸福做住宅用地的一级开发</li><li>这片熟地要还给政府的</li><li>再以招拍挂等公开方式出让给中标的房地产企业</li><li>从工业园区的发展当中，其可以和政府分享税收收益<ul><li>按照法律，政府不能和企业直接分享税收，但是可以购买企业服务，用产业发展服务费的名义来支付约定的分成</li></ul></li></ul></li></ul><h2 id="4-2-地方政府债务"><a href="#4-2-地方政府债务" class="headerlink" title="4.2 地方政府债务"></a>4.2 地方政府债务</h2><ul><li><p>政府依靠土地使用权转让收入支撑起土地财政，并将未来的土地收益资本化，从银行和其他渠道借入天量资金，利用土地金融，快速推动工业化和城市化，但也同时积累了大量债务</p></li><li><p>模式的关键</p><ul><li>土地价格<ul><li>只要不断地投资和建设能带来持续的经济增长，城市就会扩张，地价就会上涨</li><li>这样就能够偿还连本带利越来越多的债务</li><li>但是经济增速一旦放缓，地价下跌，土地出让收入减少，累积的债务就会成为沉重负担，可能压垮融资平台甚至地方政府</li></ul></li></ul></li><li><p>国家开发银行和城投债</p><ul><li>为什么需要上述二者<ul><li>因为地方政府穷，但是还要发展经济，做城市化</li><li>想要在城市建设开发当中引入银行资金，需要解决三个问题<ul><li>需要一个能借款的公司，政府不能直接从银行贷款</li><li>城建开发项目复杂，有些赚钱，有些赔钱，所以需要打包捆绑</li><li>依靠财政预算收入不够还债的，要能把跟土地有关的收益用起来</li></ul></li></ul></li><li>为了解决上述引入资金的问题，出现了<strong>城投公司</strong><ul><li>国家开发银行创立的这种模式<ul><li>贷款，然后可以用土地出让收益作为质押进行还款保证</li></ul></li></ul></li><li>城商行<ul><li>七成左右第一股东为地方政府</li><li>为了方便为融资平台公司和基础设施建设提供贷款</li><li>风险<ul><li>基础设施建设项目周期长，需要中长期贷款<ul><li>国开行是政策性银行，有稳定的长期资金来源</li><li>但是商业银行的存款大都来自短期存款，与中长期贷款期限不匹配，容易产生风险</li></ul></li><li>四大行存款来源庞大，可以承受一定程度期限错配，但是城商行不能，经常需要在资本市场融资，容易出现风险<ul><li>包商银行</li></ul></li></ul></li></ul></li><li>城投公司融资方式<ul><li>银行贷款</li><li>发行债券<ul><li>理论优势<ul><li>分散风险</li><li>债券可交易，价格和利率可变，配置效率高</li></ul></li><li>实际<ul><li>七八成商业银行所有</li><li>隐性背书，尽管有风险，但是大家有点不管不顾</li></ul></li></ul></li></ul></li><li>地方债务<ul><li>隐性负债很多</li><li>局部风险大</li><li>很多地方都靠着中央的补贴保证不违约，但是如果经济遇冷，地价下跌，政府也无法背起这沉重的债务了</li></ul></li></ul></li><li><p>地方债的治理</p><ul><li>债务置换<ul><li>用地方政府发行的公债，替换一部分融资平台公司的银行贷款和城投债<ul><li>好处<ul><li>利率从之前的7% - 8% 降低到4%左右<ul><li>原先因为有政府的隐性担保，所以银行都愿意贷给城投，这样还拉高了一般企业的贷款成本和难度</li></ul></li><li>政府公债期限要长很多<ul><li>降低了期限错配和流动性的风险</li></ul></li><li>信用级别升高</li></ul></li></ul></li><li>如何确定债务置换的规模<ul><li>国务院确定并报全国人大或者人大常委会批准</li></ul></li></ul></li><li>推动融资平台转型<ul><li>剥离为政府融资的功能，同时破除政府的隐性担保</li></ul></li><li>约束银行和各类金融机构，避免大量资金流入融资平台</li><li>问责官员，对于过度负债的行为终身追责</li></ul></li></ul><h2 id="4-3-招商引资当中的地方官员"><a href="#4-3-招商引资当中的地方官员" class="headerlink" title="4.3 招商引资当中的地方官员"></a>4.3 招商引资当中的地方官员</h2><ul><li>官员政绩与激励机制<ul><li>地方主官平均任期三四年，而基建项目一般都要两三年完成</li><li>故而刚上任会上马大量这种项目</li><li>各地的投资 政治投资周期比较频繁<ul><li>新官上任，土地出让数量会增加</li><li>新增的土地供应大多位于城市周边郊区</li><li>摊大饼的态势</li></ul></li></ul></li><li>偏重投资的增长模式带来的问题<ul><li>政府债务的不断攀升</li><li>重视看得见的东西，忽略看不见的</li></ul></li><li>2019考核官员标准发生变化<ul><li>看全面工作，看推动本地区经济建设，政治建设，文化建设，社会建设，生态文明建设</li><li>解决发展不平衡不充分的问题</li><li>满足人民日益增长的美好生活需要的情况和实际成效</li></ul></li></ul><h1 id="5-工业化中的政府角色"><a href="#5-工业化中的政府角色" class="headerlink" title="5. 工业化中的政府角色"></a>5. 工业化中的政府角色</h1><ul><li><p>政府与具体的工业企业的合作</p><ul><li>京东方<ul><li>与其说是一家公司的奋斗史，不如说是液晶屏产业在中国的奋斗史</li></ul></li></ul></li><li><p>现代经济的规模经济效应非常强，新企业的进入门槛非常高，不仅投资额度大，还要面对先进入者已经积累起来的巨大成本和技术的优势</p></li><li><p>东亚经济奇迹，一个很重要的特点</p><ul><li>政府帮助本土企业进入复杂度很高的行业</li><li>充分利用其中的学习效应，规模效应以及技术外溢效应</li><li>迅速提升本土制造业的技术能力和国际竞争力</li></ul></li><li><p>新兴制造业在地理上的集聚效应是很强的，因为扎堆生产可以节约原材料和中间投入的运输成本</p><ul><li>且同行聚集在一起有利于知识和技术交流，外溢效应很强</li><li>因此产业集群一旦形成，自身引力就会不断增强，很难被外力打破</li></ul></li><li><p>光伏产业</p><ul><li>新技术开始成本会非常高，其实都是靠政府补贴赚钱</li><li>政府通过补贴额度的降低来引导各个企业的发展</li></ul></li><li><p>政府产业引导基金与私募基金</p><ul><li>LP limited partner  给钱的</li><li>GP general partner 投资的</li><li>可以投一级市场以及二级市场</li><li>国内的最大一类LP就是政府引导基金</li></ul></li><li><p>产业引导基金的特点</p><ul><li>大多产业引导基金不直接投资企业，而是做LP，将钱交给市场化的私募基金的GP去投资企业<ul><li>因为一支私募不仅有政府资金，还会有很多社会资本，这样可以做产业引导</li></ul></li><li>政府引导基金本身就是一只基金，投资对象是各种私募基金，被称为母基金  funds of funds</li><li>借助市场力量去使用财政资金</li><li>大多数引导基金的最终投向都是战略新兴产业，比如芯片，新能源汽车，而不允许基础设施和房地产</li></ul></li><li><p>设置产业引导基金之后，也需要专门的公司来运营和管理这只基金</p><ul><li>运作模式分类<ul><li>政府独资公司</li><li>混合所有制公司</li><li>政府将钱委托给市场化的母基金管理人去运营</li></ul></li></ul></li><li><p>政府引导基金发展的外部条件</p><ul><li>制度条件<ul><li>制度和政策的指引</li><li>提供政策基础：设立引导基金发挥财政资金的杠杆放大效应，增加创业投资资本的供给，克服单纯通过市场配置创业投资资本的市场失灵问题</li></ul></li><li>产业条件<ul><li>经济需要发展到一定的水平，使得高技术，高风险的战略新兴行业得以出现</li></ul></li><li>资本市场成熟程度<ul><li>GP ，足够大的股权交易市场，退出渠道</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-整体框架&quot;&gt;&lt;a href=&quot;#1-整体框架&quot; class=&quot;headerlink&quot; title=&quot;1. 整体框架&quot;&gt;&lt;/a&gt;1. 整体框架&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;微观机制&lt;ul&gt;
&lt;li&gt;地方政府的权力和事务&lt;/li&gt;
&lt;li&gt;财税与政府行为&lt;/li&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>人月神话笔记</title>
    <link href="https://www.llchen60.com/%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D%E7%AC%94%E8%AE%B0/"/>
    <id>https://www.llchen60.com/%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D%E7%AC%94%E8%AE%B0/</id>
    <published>2022-01-31T06:34:42.000Z</published>
    <updated>2022-01-31T06:35:57.437Z</updated>
    
    <content type="html"><![CDATA[<ul><li>软件活动的根本任务<ul><li>打造构成抽象软件实体的复杂概念结构</li></ul></li><li>软件活动的次要任务1<ul><li>使用编程语言来表达这些抽象实体，在空间和时间的限制下将他们映射成机器语言</li></ul></li></ul><blockquote><p>一个相互牵制的概念结构是软件实体必不可少的部分，它包括：数据集合，数据条码之间的关系，算法以及功能调用等。这些要素本身是抽象的，体现在不同的表现形式下的概念构造是相同的。</p></blockquote><ul><li>软件系统无法规避的内在特性<ul><li>复杂度<ul><li>元素之间的交互，使得软件的复杂度比非线性增长要高得多</li><li>软件的复杂度是根本属性，抽掉复杂度的软件实体描述实际上也去掉了一些本质属性。所以我们无法做太多的简化的</li></ul></li><li>一致性<ul><li>软件工程当中没有太多的一致性，更多的核心开发工程师本身的理念</li></ul></li><li>可变性<ul><li>软件中的功能，功能属性很容易感受到变更的压力</li></ul></li><li>不可见性<ul><li>软件是不可见以及无法可视化的<ul><li>例如几何抽象是强大的工具，包括机械制图，以及化学分子模型，但是软体的客观存在不具有空间的形体特征</li></ul></li></ul></li></ul></li></ul><ul><li><p>编程系统产品(Programming Systems Product) 开发的工作量是供个人使用的，独立开发的构件程序的9倍。估计软件构件产品化引起了3倍工作量，将软件构件整合成完整系统所需要的设计、集成和测试又加强了3倍的工作量，这些高成本的构件在根本上是相互独立的</p></li><li><p>编程行业满足了我们内心深处的创造渴望和愉悦所有人的共有情感，提供了5种乐趣</p><ul><li>创建事物的快乐</li><li>开发对其他人有用的东西的乐趣</li><li>将可以活动，相互齿合的零部件组装成类似迷宫的东西，这个过程体现出的令人神魂颠倒的美丽</li><li>面对不重复的任务，不断学习的乐趣</li><li>工作在如此易于驾驭的介质上的乐趣—— 纯粹的思维活动 —— 其存在，移动和运转方式完全不同于实际物体</li></ul></li><li><p>同样，这个行业有一些内在固有的烦恼</p><ul><li>将做事方式调整到追求完美是学习编程的最困难部分</li><li>权威不等同于责任；真正的权威来自于每次任务的完成</li><li>任何创造性活动都伴随着枯燥艰苦的劳动，编程也不例外</li><li>人们通常希望项目在接近结束的时候，软件项目能收敛得快一些，然而，情况却是越接近完成，收敛得越慢</li><li>产品在完成前总面临着陈旧过时的威胁，只有实际需要的时候，才会用到最新的设想</li></ul></li><li><p>缺乏合理的时间进度是造成项目滞后的最主要原因，它比其他所有因素的总和影响还大</p></li><li><p>所有编程人员都是乐观主义者，一切都将运作良好</p></li><li><p>由于编程人员通过纯粹的思维活动来开发，我们期待在实现过程当中不会碰到困难。但是我们本身的构思是会有缺陷的，因此总会有bug</p></li><li><p>围绕着成本核算的估计技术，混淆了工作量和项目进展。人月是危险的，因为它暗示着人员数量和时间是可以相互替换的。</p></li><li><p>在若干人员中分解任务会引发额外的沟通工作量 — 培训和相互沟通。</p></li><li><p>Brooks法则： 为进度落后的项目增加人手，只会使得进度更加落后</p><ul><li>增派人手可能增加的工作量<ul><li>任务重新分配本身和所造成的工作中断</li><li>培训新人员</li><li>额外的相互沟通</li></ul></li></ul></li><li><p>同样有两年经验而且在受到同样培训的情况下，优秀的专业程序员的生产力是较差的程序员的10倍。</p></li><li><p>一位首席程序员，类似于外科手术队伍的团队架构提供了一种方法—— 既能获得由少数头脑产生的产品完整性，又能够得到多位协助人员的总体生产率，还彻底减少了沟通的工作量。</p></li><li><p>概念完整性是系统设计中最重要的考虑因素</p><ul><li>为了获得概念的完整性，设计必须由一个人或者具有共识的小型团队来完成</li><li>为了获得概念的完整性，就必须有人控制这些概念，这实际上是一种无需任何歉意的贵族专制统治</li></ul></li><li><p>功能和理解上的复杂程度的比值才是系统设计的最终测试标准，而不仅仅是丰富的功能</p></li><li><p>尽早交流和持续沟通能够使得结构师有较好的成本意识，使开发人员获得对设计的信心，并且不会混淆各自的责任分工</p></li><li><p>交流</p><ul><li>巴比伦项目的失败是因为缺乏交流以及交流的结果 —— 组织</li><li>因为左手不知道右手在做什么，从而进度灾难，功能的不合理，以及系统的缺陷纷纷出现。由于存在对其他人的各种假设，团队成员之间的理解开始出现偏差了。</li></ul></li><li><p>项目工作手册</p><ul><li>项目工作手册是对项目必须产生的一些列文档进行组织的一种结构</li><li>每一个团队成员应该了解所有的材料</li><li>实时更新很重要</li><li>工作手册的使用者应该将注意力集中在上次阅读之后的变更以及关于这些变更重要性的评述上</li><li>每个部分应该被封装，从而没有人需要或者被允许看到其他部分的内部结构，只需要了解接口</li></ul></li><li><p>组织架构</p><ul><li>团队组织的目标是为了减少必要的交流和协作量</li><li>为了减少交流，组织结构包括了人力划分(Division of labor) 和限定职责范围(Specialization of function)</li><li>组织内的交流是网状的，而不是树状结构，因此所有的特殊组织机制都是为了进行调整，来克服树状组织结构中交流缺乏的困难</li></ul></li><li><p>在大型团队当中，各个小组倾向于不断的去做局部优化，来满足自己的目标，而比较少的考虑对用户的整体影响。这种方向性的问题是大型项目的主要危险</p></li><li><p>从系统整体出发和面向用户的态度是软体编程管理人员最重要的职能</p></li><li><p>文档的规范，目标，用户手册，内部文档，进度，预算，组织机构图，和工作空间分配</p><ul><li>每个文档本身就可以作为检查列表或者数据库</li></ul></li><li><p>项目经理的基本职责是使每个人都向着相同的方向前进</p></li><li><p>项目经理的主要日常工作是沟通，而不是做出决定；文档使得各项计划和决策在整个团队范围内得到交流</p></li><li><p>用户的实际需要和用户感觉会随着程序构建，测试和使用而发生变化。</p></li><li><p>对于文档，需要采用定义良好的数字化版本将变更量子化</p></li><li><p>程序员不愿意为设计书写文档，不仅仅是因为惰性，更多的是源于设计人员的踌躇 —— 要为自己尝试性的设计决策进行辩解。</p></li><li><p>只要管理人员和技术人员的天赋允许，老板必须对他们的能力培养给予极大的关注，使得管理人员和技术人员具有互换性；特别是希望在技术和管理角色之间自由的分配人手的时候</p></li><li><p>具有两条晋升线的高效组织机构存在着一些社会性的障碍，人们必须警惕并积极的同它做持续的斗争</p></li><li><p>程序维护基本上不同于硬件的维护：主要由各种变更组成，入修复设计缺陷，新增功能，或者是使用环境或者配置变换引起的调整</p></li><li><p>对于一个广泛使用的程序，其维护总成本通常是开发成本的40%或者更多</p></li><li><p>Campbell指出了一个显示产品生命期中每月bug数的有趣曲线，其先是下降，后面是上升</p></li><li><p>每次修复之后，必须重新运行先前所有的测试用例，确保系统不会以更隐蔽的方式被破坏</p></li><li><p>所有的修改都倾向于破坏系统的架构，增加了系统的混乱程度(熵)。即使是最熟练的软件维护工作，也只是延缓了系统退化到不可修复的混乱状态的进程，以致必须重新进行设计。</p></li><li><p>项目经理应该制定一套策略，并为通用工具的开发分配资源，与此同时，还必须意识到专业工具的需求</p></li><li><p>调试是系统编程中较慢和较困难的部分，而漫长的调试周转时间是调试的祸根</p></li><li><p>在编写任何代码之前，规格说明必须提交给外部的测试人员，来详细的检查说明的完整性和明确性。开发人员自己无法完成这项工作。</p></li><li><p>开发大量的辅助测试平台和测试代码是很值得的，代码量甚至可能有测试对象的一半</p></li><li><p>项目是怎么样被延迟了整整一年的时间的….. 一次一天。一次一天的进度落后比重大灾难更难以识别，更不容易防范和更加难以弥补。</p></li><li><p>根据一个严格的进度表来控制大型项目的第一个步骤是制定进度表，进度表由里程碑和日期组成</p><ul><li>里程碑必须是具体的，特定的，可度量的事件，需要能够进行清晰的定义</li></ul></li><li><p>慢性进度偏离是士气杀手。</p></li><li><p>状态的获取是困难的，因为下属经理有充分的理由不提供信息共享</p></li><li><p>老板的不良反应肯定会对信息的完全公开造成压制；相反，仔细区分状态报告，毫无惊慌地接收报告，决不越俎代庖，将能够鼓励诚实的汇报。</p></li><li><p>必须有评审机制，使得所有成员可以通过它了解真正的状态。出于这个目的，里程碑的进度和完成文档是关键。</p></li><li><p>程序修改人员所使用的文档中，除了描述事情如何，还应当阐述它为什么那样。对于加深理解，目的是非常关键的，即使是高级语言的语法，也不能表达目的</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;软件活动的根本任务&lt;ul&gt;
&lt;li&gt;打造构成抽象软件实体的复杂概念结构&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;软件活动的次要任务1&lt;ul&gt;
&lt;li&gt;使用编程语言来表达这些抽象实体，在空间和时间的限制下将他们映射成机器语言&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="人月神话" scheme="https://www.llchen60.com/tags/%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D/"/>
    
      <category term="Project Management" scheme="https://www.llchen60.com/tags/Project-Management/"/>
    
  </entry>
  
  <entry>
    <title>Real time data’s unifying abstraction</title>
    <link href="https://www.llchen60.com/Real-time-data%E2%80%99s-unifying-abstraction/"/>
    <id>https://www.llchen60.com/Real-time-data%E2%80%99s-unifying-abstraction/</id>
    <published>2022-01-29T02:14:20.000Z</published>
    <updated>2022-01-29T02:20:03.851Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Real-time-data’s-unifying-abstraction"><a href="#Real-time-data’s-unifying-abstraction" class="headerlink" title="Real time data’s unifying abstraction"></a>Real time data’s unifying abstraction</h1><h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>Logs play a key role in distributed data systems and real time application architectures.<ul><li>write ahead log</li><li>commit log</li><li>transaction logs</li></ul></li></ul><h1 id="2-What-is-a-log"><a href="#2-What-is-a-log" class="headerlink" title="2. What is a log?"></a>2. What is a log?</h1><h2 id="2-1-Some-Concepts"><a href="#2-1-Some-Concepts" class="headerlink" title="2.1 Some Concepts"></a>2.1 Some Concepts</h2><ul><li>Storage abstraction<ul><li>append only</li><li>totally ordered sequence of records ordered by time</li></ul></li><li>Records<ul><li>appended to the end of the log</li><li>read from left to right</li><li>each entry has a unique sequential log entry number</li></ul></li><li>Log<ul><li>record what happened and when</li><li>for distributed system, that’s the very heart of the problem</li><li>types<ul><li>text logs<ul><li>meant primarily for humans to read</li></ul></li><li>journal/ data logs<ul><li>built for programmatic access</li></ul></li></ul></li></ul></li></ul><h2 id="2-2-Log-in-different-scenario"><a href="#2-2-Log-in-different-scenario" class="headerlink" title="2.2 Log in different scenario"></a>2.2 Log in different scenario</h2><h3 id="2-2-1-Logs-in-DB"><a href="#2-2-1-Logs-in-DB" class="headerlink" title="2.2.1 Logs in DB"></a>2.2.1 Logs in DB</h3><ul><li>Function 1: Authoritative source for restoring data<ul><li>DB need to keep in sync the variety of data structures and indexes in the presense of crashes</li><li>To make this atomic and durable, a db uses a log to <strong>write out information about the records</strong> they will be modifying, <strong>before applying the changes</strong> to all the various data structures it maintains</li><li>Since the log is <strong>immediately persisted</strong> it is used as the <strong>authoritative source</strong> in restoring all other persistent structures in the event of a crash.</li></ul></li><li>Function 2: Replicating data between DBs<ul><li>Oracle, MySQL and Postgres SQL include <strong>log shipping protocols</strong> to <strong>transmit portions of log to replica databases</strong> which act as slaves</li></ul></li></ul><h3 id="2-2-2-Logs-in-distributed-systems"><a href="#2-2-2-Logs-in-distributed-systems" class="headerlink" title="2.2.2 Logs in distributed systems"></a>2.2.2 Logs in distributed systems</h3><ul><li>Log Centric Approach</li></ul><aside>💡 State Machine Replication Principle: If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.</aside><ul><li>we could reduce the problem of making multiple machines all do the same thing to the problem of <strong>implementing a distributed consistent log to feed these processes input</strong><ul><li>squeeze all the non-determinism out of the input stream to ensure that each replica processing this input stays in sync.</li><li>time stamps that index the log now act as <strong>the clock for the state of the replicas</strong>—you can describe each replica by a single number, the timestamp for the maximum log entry it has processed.</li></ul></li></ul><h1 id="3-Log-Types"><a href="#3-Log-Types" class="headerlink" title="3. Log Types"></a>3. Log Types</h1><ul><li>What we could put in log<ul><li>log the incoming requests to a service</li><li>the state changes the service undergoes in response to request</li><li>the transformation commands it executes.</li></ul></li><li>For DB usage<ul><li>physical logging<ul><li>log the contents of each row that is changed</li></ul></li><li>logical logging<ul><li>log not the changed rows but the SQL commands that lead to the row chagnes</li></ul></li></ul></li><li>For Distributed Systems to process and replicate<ul><li>Primary Backup<ul><li>elect one replica as the leader</li><li>allow this leader to process requests in the order they arrive</li><li>log out the changes to its state from processing the requests.</li><li>The other replicas apply in order the state changes the leader makes so that they will be in sync and ready to take over as leader should the leader fail.</li><li><strong>backup will copy the result from the primary, no logical action to walk through all action primary did</strong></li></ul></li></ul></li></ul><pre><code>- State Machine Replication    - active-active model where we keep a log of the incoming requests and each replica processes each request    - **each machine will do real execution, do the logical stuff**    ![State Machine Replication](https://s2.loli.net/2022/01/29/6JtLFNpmXBYW9wO.png)</code></pre><aside>💡 Below 3 sections has a centric idea:  Log as a stand-alone service. The usefulness of the log comes from simple function that the log provides: **producing a persistent, re-playable record of history**. At the core of these problems is the ability to **have many machines playback history at their own rate in a deterministic manner**</aside><h1 id="4-Changelog-in-Database"><a href="#4-Changelog-in-Database" class="headerlink" title="4. Changelog in Database"></a>4. Changelog in Database</h1><ul><li>duality between a log of changes and a table<ul><li>The log is similar to the list of all credits and debits and bank processes;</li><li>a table is all the current account balances.</li><li>If you have a log of changes, you can apply these changes in order to create the table capturing the current state.</li></ul></li><li>if you have a table taking updates, you can record these changes and publish a “changelog” of all the updates to the state of the table. This changelog is exactly what you need to <strong>support near-real-time replicas</strong>.</li><li>Table support data at rest and logs capture changes</li></ul><aside>💡 The magic of the log is that if it’s complete log of changes, it holds not only the contents of the final version of the table, but also allows recreating all other versions that might have existed. That’s a backup of every previous state of the table</aside><h1 id="5-Data-Integration"><a href="#5-Data-Integration" class="headerlink" title="5. Data Integration"></a>5. Data Integration</h1><blockquote><p>Make all of an organization’s data easily available in all its storage and processing systems</p></blockquote><h2 id="5-1-Expected-workflow-for-data-integration"><a href="#5-1-Expected-workflow-for-data-integration" class="headerlink" title="5.1 Expected workflow for data integration"></a>5.1 Expected workflow for data integration</h2><ul><li>Definition in author’s scope: Making all the data an organization has available in all its services and systems.</li><li>Effective Use of Data<ul><li>Capture all relevant data</li><li>Put it together in an applicable processing env<ul><li>real time query system</li><li>text files</li><li>python scripts, etc.</li></ul></li><li>Infra to process data<ul><li>mapReduce</li><li>Real time query systems</li></ul></li><li>Good data models and consistent well understood semantics</li><li>Sophisticated processing<ul><li>visualization</li><li>reporting</li><li>algorithmic processing and prediction</li></ul></li></ul></li></ul><h2 id="5-2-Problem1-The-event-data-firehose"><a href="#5-2-Problem1-The-event-data-firehose" class="headerlink" title="5.2 Problem1: The event data firehose"></a>5.2 Problem1: The event data firehose</h2><ul><li>Event data rising</li><li>google’s fortune is actually generated by a relevance pipeline built on clicks and impressions — events</li><li>that would be huge amount of data,</li></ul><h2 id="5-3-Problem-2-The-explosion-of-specialized-data-systems"><a href="#5-3-Problem-2-The-explosion-of-specialized-data-systems" class="headerlink" title="5.3 Problem 2: The explosion of specialized data systems"></a>5.3 Problem 2: The explosion of specialized data systems</h2><ul><li>Explosion of specialized data systems</li><li>The combination of more data of more varieties and a desire to get this data into more systems leads to a huge data integration problem.</li></ul><h2 id="5-4-Log-Structured-Data-Flow"><a href="#5-4-Log-Structured-Data-Flow" class="headerlink" title="5.4 Log Structured Data Flow"></a>5.4 Log Structured Data Flow</h2><h3 id="5-4-1-How-the-flow-work"><a href="#5-4-1-How-the-flow-work" class="headerlink" title="5.4.1 How the flow work"></a>5.4.1 How the flow work</h3><ul><li>Recipe: Take all the organization’s data and put it into a central log for <strong>real time subscription</strong></li><li>How the flow works<ul><li>Each logical data source can be modeled as its own log</li><li>A data source could be an application that logs out events, or a db table that accepts modifications</li><li>each subscribing system reads from this log <strong>as quickly as it can</strong>, <strong>applied each new record to its own store</strong>, and <strong>advances its position</strong> in the log</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/29/ykIEs4XwnBD3LUF.png" alt="How the flow work"></p><ul><li><p>Log gives a <strong>logical clock</strong> for each change against which all subscriber can be measured</p><ul><li>Consider a case where there is a database and a collection of caching servers</li><li>log provides a way to synchronize the updates to all these systems and reason about the point of time of each of these systems</li><li>Let’s say we <strong>write a record with log entry X</strong> and then need to do a read from the cache. If we want to guarantee we don’t see stale data, we just need to ensure we <strong>don’t read from any cache which has not replicated up to X.</strong></li></ul></li><li><p>Log also acts as a <strong>buffer</strong> that makes <strong>data production asynchronous from data consumption</strong></p><ul><li>satisfy different requirements like<ul><li>A batch system such as Hadoop or a data warehouse may consume only hourly or daily,</li><li>A real-time query system may need to be up-to-the-second.</li></ul></li></ul></li><li><p>Consumer only need to know about the log and not any details of the system of origin</p></li><li><p>What values most from author perspective</p><ul><li>The pipeline they built for process data, though a bit of a mess, were actually extremely valuable . Just the process of makeing data available in a new processing system (Hadoop) unblocked a lot of possibilities<ul><li>Many new products and analysis just came from putting together multiple pieces of data that had previously been locked up in specialized systems</li></ul></li></ul></li><li><p>LinkedIn Went Through from O(N^2) to O(2N)</p></li></ul><p><img src="https://s2.loli.net/2022/01/29/fXnmrpYFLSVGoR9.png" alt="Pre Architecture LinkedIn"><br><img src="https://s2.loli.net/2022/01/29/9dZa67tYEl1yc8e.png" alt="Cur Architecture LinkedIn"></p><ul><li>Actions for the migration<ul><li>Isolate each consumer from the source of the data</li><li>Create a new data system to be both a data source and a data destination</li><li>Here LinkedIn create Kafka</li><li>Kinesis is similar to Kafka as AWS use it to connects all different distributed systems as a piping</li></ul></li></ul><h3 id="5-4-2-Relationship-to-ETL-and-the-Data-Warehouse"><a href="#5-4-2-Relationship-to-ETL-and-the-Data-Warehouse" class="headerlink" title="5.4.2  Relationship to ETL and the Data Warehouse"></a>5.4.2  Relationship to ETL and the Data Warehouse</h3><ul><li><p>Data Warehouse</p><ul><li>target<ul><li>A repository of the clean, integrated data structured to support analysis</li></ul></li><li>what be involved<ul><li>periodically extracting data from source databases</li><li>munging it into some kind of understandable form</li><li>loading it into a central data warehouse</li></ul></li><li>Problems<ul><li>coupling the clean integrated data to the data warehouse.<ul><li>cannot get real time feed</li></ul></li><li>organization perspective<ul><li>The incentives are not aligned: data producers are often not very aware of the use of the data in the data warehouse and end up creating data that is hard to extract or requires heavy, hard to scale transformation to get into usable form.</li><li>the central team never quite manages to scale to match the pace of the rest of the organization, so data coverage is always spotty, data flow is fragile, and changes are slow.</li></ul></li></ul></li></ul></li><li><p>ETL</p><ul><li>tasks<ul><li>extraction and data cleanup process, liberating data locked up in a variety of systems in the organization and removing an system-specific non-sense</li><li>data is restructured for data warehousing queries (i.e. made to fit the type system of a relational DB, forced into a star or snowflake schema, perhaps broken up into a high performance <a href="http://parquet.io/">column</a> <a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html">format</a>,</li></ul></li><li>problems<ul><li>still, we need such data in real time as well for low latency processing as well as indexing in real time storage systems</li></ul></li></ul></li><li><p>A better approach as ETL and Data Warehouse substitution</p><ul><li><p>Have a central pipeline, the log, with a well defined API for adding data</p></li><li><p>Responsibility Classification</p><ul><li><p>Producer of the data feed: integrating with this pipeline and providing a clean, well-structured data feed</p></li><li><p>Datawarehouse team now <strong>only care about loading structured feeds</strong> of data from the <strong>central log</strong> and <strong>carrying out transformation specific to their system</strong></p><p>  <img src="https://s2.loli.net/2022/01/29/fMl9IQwDGk8TL4o.png" alt="Workflow and ownership classification">            </p></li></ul></li></ul></li></ul><h3 id="5-4-3-Log-Files-and-Events"><a href="#5-4-3-Log-Files-and-Events" class="headerlink" title="5.4.3 Log Files and Events"></a>5.4.3 Log Files and Events</h3><ul><li>Current structure also enables decoupled and event driven systems</li></ul><h3 id="5-4-4-How-to-build-scalable-logs"><a href="#5-4-4-How-to-build-scalable-logs" class="headerlink" title="5.4.4 How to build scalable logs"></a>5.4.4 How to build scalable logs</h3><ul><li>Need a log system that’s fast, cheap, scalable enough to make this practical at scale</li><li>LinkedIn in 2013 actually has already support 60 billion unique message writes through Kafka per day</li><li>Kafka achieve such high throughput via<ul><li>Partitioning the log<ul><li>each partition is a <strong>totally ordered log</strong>, but there is <strong>no global ordering between partitions</strong></li><li>Assignment of the messages to a particular partition is controllable by the writer, with most users choosing to partition by some kind of key</li><li>Replication<ul><li>Each partition is replicated across a configurable number of replicas</li><li>At any time, a single one of them will act as the leader, if the leader fails, one of the replicas will take over as leader</li></ul></li><li>Order Guarantee<ul><li>each partition is order preserving, and Kafka guarantees that appends to a particular partition from a single sender will be delivered in the order they are sent.</li></ul></li></ul></li><li>Optimizing throughput by batching reads and writes<ul><li>occurs when<ul><li>sending data</li><li>writes to disk</li><li>replication between servers</li><li>data transfer to consumers</li><li>acknowledging committed data</li></ul></li></ul></li><li>Avoiding needless data copies<ul><li>Use a simple binary format that is maintained between in memory log, on disk log and in network data transfers</li><li>Thus we could make use of numerous optimizations including zero copy data transfer <a href="https://en.wikipedia.org/wiki/Zero-copy">https://en.wikipedia.org/wiki/Zero-copy</a></li></ul></li></ul></li></ul><h1 id="6-Real-Time-Data-Processing"><a href="#6-Real-Time-Data-Processing" class="headerlink" title="6. Real Time Data Processing"></a>6. Real Time Data Processing</h1><blockquote><p>Computing derived data streams</p></blockquote><h2 id="6-1-Definition-of-Stream-Processing"><a href="#6-1-Definition-of-Stream-Processing" class="headerlink" title="6.1 Definition of Stream Processing"></a>6.1 Definition of Stream Processing</h2><ul><li>Infrastructure for continuous data processing<ul><li>computational model can be general like MapReduce or other distributed processing frameworks,</li><li>need the ability to produce low latency results</li></ul></li><li>Instead of batch get and process, we could do continuous changes</li><li>it is just processing which includes a notion of time in the underlying data being processed and does not require a static snapshot of the data so it can produce output at a user-controlled frequency instead of waiting for the “end” of the data set to be reached. In this sense, stream processing is a generalization of batch processing, and, given the prevalence of real-time data, a very important generalization</li><li>Log role<ul><li>making data available in real-time multi-subscriber data feeds.</li></ul></li></ul><h2 id="6-2-Stateful-Real-Time-Processing"><a href="#6-2-Stateful-Real-Time-Processing" class="headerlink" title="6.2 Stateful Real Time Processing"></a>6.2 Stateful Real Time Processing</h2><ul><li>Stateful real time processing means some more sophisticated operations, like counts, aggregations, or joins over windows in the stream</li><li>We need to maintain certain state in such case</li><li>Strategies for that<ul><li>Keep state in memory<ul><li>cons<ul><li>if the process crash, it would lose its intermediate state</li><li>if the state is only maintained over a window, the process could fall back to the point where the window began</li></ul></li></ul></li><li>Store all state in a remote storage system, and join over the network to that store<ul><li>cons<ul><li>no locality of data and lots of network round trips</li></ul></li></ul></li><li>Duality of tables and logs<ul><li>a stream processor can keep its state in a local table or index — a bdb, leveldb</li><li>the contents of this store is fed from its input streams</li><li>it could journal out a changelog for this local index it keeps to allow it to restore its state in the event of a crash and restart</li><li>This mechanism allows a generic mechanism for keeping co-partitioned state in arbitrary index types local with the incoming stream data.</li><li>when facing process fails<ul><li>recover its index from the changelog</li><li>changelog itself is the transformation of the local state into a sort of incremental record at a time backup</li></ul></li></ul></li></ul></li></ul><h2 id="6-3-Log-Compaction"><a href="#6-3-Log-Compaction" class="headerlink" title="6.3 Log Compaction"></a>6.3 Log Compaction</h2><ul><li>Log need to be cleaned up someway to save the space</li><li>In Kafka, clean up has two options depending on whether the data contains keyed updates or event data<ul><li>for event data, supports just retain a window of data<ul><li>configured to be few days</li><li>also could be configured as space</li></ul></li><li>for keyed data<ul><li>as the complete log give you ability to replay it to recreate the state of the source system</li><li>but we could do log compaction by removing obsolete records — records whose primary key has a more recent update</li></ul></li></ul></li></ul><h1 id="7-Distributed-System-Design"><a href="#7-Distributed-System-Design" class="headerlink" title="7. Distributed System Design"></a>7. Distributed System Design</h1><blockquote><p>How practical systems can be simplified with a log centric design</p></blockquote><h2 id="7-1-Distributed-system-design-thought"><a href="#7-1-Distributed-system-design-thought" class="headerlink" title="7.1 Distributed system design thought"></a>7.1 Distributed system design thought</h2><p>Log here is responsible for data flow, consistency and recovery </p><ul><li><p>Directions</p><ul><li>Coalescing lots of little instances of each system into a few big clusters</li></ul></li><li><p>Possibility 1</p><ul><li>separation of systems remains more or less as it is for a good deal longer.</li><li>an external log that integrates data will be very important.</li></ul></li><li><p>Possibility 2</p><ul><li>re-consolidation in which a single system with enough generality starts to merge back in all the different functions into a single uber-system.</li><li>extremely hard</li></ul></li><li><p>Possibility 3</p><ul><li>data infrastructure could be unbundled into a collection of services and application-facing system apis</li><li>use open source, like in Java stacks<ul><li>zookeeper<ul><li>handle system coordination</li></ul></li><li>mesos and yarn<ul><li>process virtualization</li><li>resource management</li></ul></li><li>netty, jetty<ul><li>handle remote communication</li></ul></li><li>protobuf<ul><li>handle serialization</li></ul></li><li>kafka and bookeeper<ul><li>provide a backing log</li></ul></li></ul></li><li>path towards getting the simplicity of the single system in a more diverse and modular world that continues to evolve. If the implementation time for a distributed system goes from years to weeks because reliable, flexible building blocks emerge, then the pressure to coalesce into a single monolithic system disappears.</li></ul></li></ul><h2 id="7-2-Usage-of-log-in-system-architecture"><a href="#7-2-Usage-of-log-in-system-architecture" class="headerlink" title="7.2 Usage of log in system architecture"></a>7.2 Usage of log in system architecture</h2><ul><li><p>Usage of log in system architecture</p><ul><li>Handle data consistency (whether eventual or immediate) by sequencing concurrent updates to nodes</li><li>Provide data replication between nodes</li><li>Provide “commit” semantics to the writer (i.e. acknowledging only when your write guaranteed not to be lost)</li><li>Provide the external data subscription feed from the system</li><li>Provide the capability to restore failed replicas that lost their data or bootstrap new replicas</li><li>Handle rebalancing of data between nodes.</li></ul></li><li><p>What mentioned above is actually a large portion of what a distributed data system does. left over is mainly related with client facing query API and indexing strategy</p></li><li><p>System Look</p><ul><li>System is divided into two logical pieces<ul><li>log<ul><li>capture the state changes in sequential order</li></ul></li><li>serving layer<ul><li>store whatever index is required to serve queries</li></ul></li></ul></li><li>writes could go directly to the log or may be proxied by the serving layer</li><li>writes to the log yields a logical timestamp, if the system is partitioned, then the log and serving nodes will have the same number of partitions, though they may have very different numbers of machines</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/29/EXfRmnKxbGs8kHT.png" alt="Log and serving layer">    </p><ul><li><p>The client can get <strong>read-your-write semantics</strong> from any node by providing the <strong>timestamp of a write</strong> as part of its query—a serving node receiving such a query will <strong>compare the desired timestamp</strong> to <strong>its own index point</strong> and if necessary delay the request until it has indexed up to at least that time to avoid serving stale data.</p></li><li><p>For handling restoring failed nodes or moving partitions from node to node</p><ul><li>have the log retain only a fixed window of data and combine this with a snapshot of the data stored in the partition</li><li>it’s possible for the log to retain a complete copy of data and garbage collect the log itself</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying</a> </li><li><a href="https://kafka.apache.org/documentation.html#design">https://kafka.apache.org/documentation.html#design</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Real-time-data’s-unifying-abstraction&quot;&gt;&lt;a href=&quot;#Real-time-data’s-unifying-abstraction&quot; class=&quot;headerlink&quot; title=&quot;Real time data’s u
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="log" scheme="https://www.llchen60.com/tags/log/"/>
    
      <category term="distributed system" scheme="https://www.llchen60.com/tags/distributed-system/"/>
    
  </entry>
  
  <entry>
    <title>Flyway</title>
    <link href="https://www.llchen60.com/Flyway/"/>
    <id>https://www.llchen60.com/Flyway/</id>
    <published>2022-01-01T13:57:09.000Z</published>
    <updated>2022-01-01T13:57:52.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>An open source database migration tool</li><li>Favors simplicity and convention over configuration</li><li>has 7 basic commands<ul><li>migrate</li><li>clean</li><li>info</li><li>validate</li><li>undo</li><li>baseline</li><li>repair</li></ul></li></ul><h2 id="1-1-Why-DB-migrations"><a href="#1-1-Why-DB-migrations" class="headerlink" title="1.1 Why DB migrations?"></a>1.1 Why DB migrations?</h2><ul><li>we need a way to version the table</li><li>we need to know what state is the db on this machine</li><li>database migration help us<ul><li>recreate a database from scratch</li><li>make it clear at all times what state a database is in</li><li>migrate in a deterministic way from your current version of the database to a newer one</li></ul></li></ul><h2 id="1-2-How-Flyway-works"><a href="#1-2-How-Flyway-works" class="headerlink" title="1.2 How Flyway works?"></a>1.2 How Flyway works?</h2><ul><li><p>Flyway first try to locate its schema history table</p><ul><li>if db empty, then flyway will create it instead</li><li>this default db named as <code>flyway_schema_history</code></li></ul></li><li><p>Then flyway will begin scanning the filesystem or the classpath of the application for migrations</p></li><li><p>The migrations are then sorted based on the version number and applied in order</p></li><li><p>The schema history table will be updated accordingly as each migration gets applied</p></li><li><p>we use <code>flyway migrate</code> to execute the migration</p></li></ul><h1 id="2-Flyway-Commands"><a href="#2-Flyway-Commands" class="headerlink" title="2. Flyway Commands"></a>2. Flyway Commands</h1><h2 id="2-1-migrate"><a href="#2-1-migrate" class="headerlink" title="2.1 migrate"></a>2.1 <code>migrate</code></h2><ul><li>help migrate the db</li></ul><h2 id="2-2-clean"><a href="#2-2-clean" class="headerlink" title="2.2 clean"></a>2.2 <code>clean</code></h2><ul><li>drop all objects in the confgured schemas</li></ul><h2 id="2-3-info"><a href="#2-3-info" class="headerlink" title="2.3 info"></a>2.3 <code>info</code></h2><ul><li>print the details and status information about all migrations</li></ul><h2 id="2-4-validate"><a href="#2-4-validate" class="headerlink" title="2.4 validate"></a>2.4 <code>validate</code></h2><ul><li>validates the applied migrations against the ones available on the classpath</li></ul><h2 id="2-5-undo"><a href="#2-5-undo" class="headerlink" title="2.5 undo"></a>2.5 <code>undo</code></h2><ul><li>undoes the most recently applied versioned migration</li></ul><h2 id="2-6-baseline"><a href="#2-6-baseline" class="headerlink" title="2.6 baseline"></a>2.6 <code>baseline</code></h2><ul><li>baselines an existing database, excluding all migrations up and including baseline version</li></ul><h2 id="2-7-repair"><a href="#2-7-repair" class="headerlink" title="2.7 repair"></a>2.7 <code>repair</code></h2><ul><li>repair the schema history table</li></ul><h1 id="3-Concepts"><a href="#3-Concepts" class="headerlink" title="3. Concepts"></a>3. Concepts</h1><h2 id="3-1-Migrations"><a href="#3-1-Migrations" class="headerlink" title="3.1 Migrations"></a>3.1 Migrations</h2><ul><li>all changes to the db are called migrations</li><li>migrations can be<ul><li>versioned<ul><li>types<ul><li>regular</li><li>undo<ul><li>the effect can be undone by supplying an undo migration</li></ul></li></ul></li><li>contains<ul><li><strong>version</strong><ul><li>must be unique</li></ul></li><li><strong>description</strong><ul><li>purely informative for you to be able to remember what each migration does</li></ul></li><li><strong>checksum</strong><ul><li>detect accidental changes</li></ul></li></ul></li></ul></li><li>repeatable<ul><li>contains<ul><li><strong>description</strong></li><li><strong>checksum</strong></li></ul></li><li>instead of being run just once, they are re-applied every time their checksum changes</li><li>Within a single migration run, repeatable migrations are always <strong>applied last</strong>, after all pending versioned migrations have been executed. Repeatable migrations are applied in the order of their description</li></ul></li></ul></li></ul><h3 id="3-1-1-Versioned-Migrations"><a href="#3-1-1-Versioned-Migrations" class="headerlink" title="3.1.1 Versioned Migrations"></a>3.1.1 Versioned Migrations</h3><ul><li>contains<ul><li>version<ul><li><strong>must be unique</strong></li><li>applied in order exactly once</li></ul></li><li>description<ul><li>purely informative for you to be able to remember what each migration does</li></ul></li><li>checksum<ul><li>detect accidental changes</li></ul></li></ul></li><li>used for<ul><li>creating/ altering/ dropping tables/ indexes/ foreign keys/ enums</li><li>reference data updates</li><li>user data corrections</li></ul></li></ul><h3 id="3-1-2-Undo-Migrations"><a href="#3-1-2-Undo-Migrations" class="headerlink" title="3.1.2 Undo Migrations"></a>3.1.2 Undo Migrations</h3><ul><li>A migration can fail at any point. If you have 10 statements, it is possible for the 1st, the 5th, the 7th or the 10th to fail. There is simply no way to know in advance. In contrast, undo migrations are written to undo an entire versioned migration and will not help under such conditions.</li><li>we should <strong>maintain backwards compatibility</strong> between the <strong>DB and all versions of the code</strong> currently deployed in production</li></ul><h3 id="3-1-3-Repeatable-Migrations"><a href="#3-1-3-Repeatable-Migrations" class="headerlink" title="3.1.3 Repeatable Migrations"></a>3.1.3 Repeatable Migrations</h3><ul><li><p>contains</p><ul><li>description and a checksum, but no version</li></ul></li><li><p>repeatable migrations are re-applied every time their checksum changes</p></li><li><p>Very useful for managing database objects whose definition can then simply be maintained in a single file in version control</p></li><li><p>Repeatable migrations are always applied last, after all pending versioned migrations have been executed; always applied in the order of their description</p></li></ul><h3 id="3-1-4-SQL-Based-Migrations"><a href="#3-1-4-SQL-Based-Migrations" class="headerlink" title="3.1.4 SQL Based Migrations"></a>3.1.4 SQL Based Migrations</h3><ul><li>used for<ul><li>DDL change — CREATE/ALTER/DROP statements for TABLES,VIEWS,TRIGGERS,SEQUENCES,…</li><li>Simple reference data changes</li><li>simple bulk data changes</li></ul></li><li>Naming  Patterns<ul><li>Prefix<ul><li>v for versioned</li><li>u for undo</li><li>r for repeatable migrations</li></ul></li><li>version<ul><li>with dots or underscores separate as many parts as you like</li></ul></li><li>Separator<ul><li>__ two underscores</li></ul></li><li>Suffix<ul><li><code>.sql</code></li></ul></li></ul></li><li>Discovery<ul><li>Flyway discover sql migrations from directories <strong>referenced by the location property</strong></li></ul></li></ul><h3 id="3-1-5-Script-Based-Migrations"><a href="#3-1-5-Script-Based-Migrations" class="headerlink" title="3.1.5 Script Based Migrations"></a>3.1.5 Script Based Migrations</h3><ul><li>name patten<ul><li>``V1__execute_batch_tool.sh`</li></ul></li><li>could be used for<ul><li>triggering execution of a 3rd party application as part of the migrations</li><li>cleaning up local files</li></ul></li></ul><h3 id="3-1-6-Transactions"><a href="#3-1-6-Transactions" class="headerlink" title="3.1.6 Transactions"></a>3.1.6 Transactions</h3><ul><li>By default, Flyway <strong>wraps the execution of an entire migration within a single transaction</strong></li></ul><h2 id="3-2-Callbacks"><a href="#3-2-Callbacks" class="headerlink" title="3.2 Callbacks"></a>3.2 Callbacks</h2><ul><li><p>For the case we need to execute same action over and over again</p></li><li><p>we could hook into its lifecycle</p></li><li><p>there are certain keywords we could use, and invoke them during the process</p></li></ul><p><a href="https://flywaydb.org/documentation/concepts/callbacks">https://flywaydb.org/documentation/concepts/callbacks</a> </p><h2 id="3-3-Error-Overrides"><a href="#3-3-Error-Overrides" class="headerlink" title="3.3 Error Overrides"></a>3.3 Error Overrides</h2><ul><li>By default, in case an error is returned, flyway displays it with all necessary details, marks the migration as failed and automatically rolls it back if possible</li><li>But we could change the behavior like<ul><li>treat an error as a waring</li><li>treat a waring as an error</li><li>perform an additional action</li></ul></li></ul><h2 id="3-4-Dry-Runs"><a href="#3-4-Dry-Runs" class="headerlink" title="3.4 Dry Runs"></a>3.4 Dry Runs</h2><ul><li>Used for<ul><li>preview changes Flyway will make to the db</li><li>submit the SQL statements for review</li><li>use Flyway to determine what needs updating,</li></ul></li><li>how it works<ul><li>flyway sets up a read only connection to the db,</li><li>assesses what migrations need to run and generates a single SQL file containing all statements it would have executed in case of a regular migration run</li></ul></li></ul><h2 id="3-5-Baseline-Migrations"><a href="#3-5-Baseline-Migrations" class="headerlink" title="3.5 Baseline Migrations"></a>3.5 Baseline Migrations</h2><ul><li><p>Over the lifetime of a project, there would be tons of db objects be created/ destroyed across many migrations</p><ul><li>we want to simplify with a single, cumulative migration that represents the state of db after all of those migrations have been applied without disrupting existing env</li></ul></li><li><p>How it works?</p><ul><li>Prefixed with B followed by the version of your db they represent</li><li>Only used when deploying to new env</li><li>If used in an env where some Flyway migrations have already been applied, <strong>baseline migrations will be ignored,</strong> <strong>new env will choose the latest baseline migration as the starting point</strong><ul><li>every migration with a version below the latest baseline migration’s version is marked as ignored</li></ul></li><li>baseline migration are executed during the migrate process</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://flywaydb.org/documentation/">https://flywaydb.org/documentation/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;An open source database migrat
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="database" scheme="https://www.llchen60.com/tags/database/"/>
    
      <category term="migration" scheme="https://www.llchen60.com/tags/migration/"/>
    
  </entry>
  
  <entry>
    <title>Protobuf Rampup</title>
    <link href="https://www.llchen60.com/Protobuf-Rampup/"/>
    <id>https://www.llchen60.com/Protobuf-Rampup/</id>
    <published>2021-12-25T02:09:49.000Z</published>
    <updated>2021-12-25T02:11:09.003Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Protobuf-Learning"><a href="#Protobuf-Learning" class="headerlink" title="Protobuf Learning"></a>Protobuf Learning</h1><h1 id="1-What-are-Protocol-Buffers"><a href="#1-What-are-Protocol-Buffers" class="headerlink" title="1. What are Protocol Buffers?"></a>1. What are Protocol Buffers?</h1><ul><li>Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler.</li><li>You <strong>define how you want your data to be structured</strong> once, then you <strong>use special generated source code to easily write and read your structured data</strong></li></ul><h1 id="2-Why-use-Protocol-Buffers"><a href="#2-Why-use-Protocol-Buffers" class="headerlink" title="2. Why use Protocol Buffers?"></a>2. Why use Protocol Buffers?</h1><ul><li>XML is human readable and wide language supports<ul><li>but is notoriously space intensive</li><li>encoding/ decoding can impose a huge performance penalty on applications</li></ul></li><li>With protocol buffers<ul><li>write a <code>.proto</code> description of the data structure</li><li>the <strong>protocol buffer compiler</strong> then <strong>creates a class</strong> that implements <strong>automatic encoding and parsing</strong> of the protocol buffer data with an <strong>efficient binary format</strong></li><li>the generated class <strong>provides getters and setters</strong> for the fields</li><li>take care of the details of reading and writing the protocol buffer <strong>as a unit</strong></li></ul></li></ul><h1 id="3-Java-Tutorial-In-Proto2"><a href="#3-Java-Tutorial-In-Proto2" class="headerlink" title="3. Java Tutorial (In Proto2)"></a>3. Java Tutorial (In Proto2)</h1><h2 id="3-1-Define-Protocol-Format"><a href="#3-1-Define-Protocol-Format" class="headerlink" title="3.1 Define Protocol Format"></a>3.1 Define Protocol Format</h2><pre><code class="protobuf">syntax = &quot;proto2&quot;;// starts with package delcaration // we should define this to get rid of name conflict package tutorial;// enable generating a separate .java file for each generated class option java_multiple_files = true;// specify in what java package name your generated classes should live// if not set here, it will simply match the pkg name given by the package declaration option java_package = &quot;com.example.tutorial.protos&quot;;// define the class name of the wrapper class which will represent this file // if not given, it will be auto generated by converting the file name to upper camel case option java_outer_classname = &quot;AddressBookProtos&quot;;/**Message Definition: An aggregate containing a set of typed fields Contain certain standard types    + boo1    + int32     + float     + double     + string we could also add further structure to msgs by using other msg types as field types + marker     + identify the unique tag field use in binary encoding     + try to use 1 - 15 as it neeeds one less byte+ modifier     + optional         + field may or may not be set         + if not, a default value will be used             + we could set our own default values             + or system will provide defaults                 + numeric types -- zero                 + strings -- empty string                 + bools -- false                 + embedded messages -- default instance or prototype of the message, which has none of its fields set     + repeated         + the field may be repeated any number of times [0, xxx)         + order will be preserved in the protocol buffer         + act like a dynamic sized array     + required         + a value for the field must be provided        + try to build an uninitialized msg will throw runtime exception         + parse an uninitialzied msg will throw IOException         + required is not favored as it cannot be backward compatible */message Person &#123;    // =1 marker identify the unique tag that field uses in the binary encoding   optional string name = 1;  optional int32 id = 2;  optional string email = 3;  enum PhoneType &#123;    MOBILE = 0;    HOME = 1;    WORK = 2;  &#125;  message PhoneNumber &#123;    optional string number = 1;    optional PhoneType type = 2 [default = HOME];  &#125;  repeated PhoneNumber phones = 4;&#125;message AddressBook &#123;  repeated Person people = 1;&#125;</code></pre><h2 id="3-2-Compiling-Protocol-Buffers"><a href="#3-2-Compiling-Protocol-Buffers" class="headerlink" title="3.2 Compiling Protocol Buffers"></a>3.2 Compiling Protocol Buffers</h2><ul><li>To generate the classes, we need to run the protocol buffer compiler</li><li>specify the source directory, the destination directory and the path to our <code>.proto</code></li></ul><pre><code class="protobuf">protoc -I=$SRC_DIR --java_out=$DST_DIR $SRC_DIR/addressbook.proto </code></pre><h2 id="3-3-Protocol-Buffer-API"><a href="#3-3-Protocol-Buffer-API" class="headerlink" title="3.3 Protocol Buffer API"></a>3.3 Protocol Buffer API</h2><ul><li>compiler helps auto generate source file<ul><li>getters and setters</li><li>each field also has <code>clear</code> method to set the field back to its empty state</li></ul></li><li>Builders vs Messages<ul><li>message classes are immutable</li><li>builder is used when you first construct a builder, then we could call the builder’s build() method</li></ul></li><li>standard message methods<ul><li><code>isInitialized</code> check if all the required fields have been set</li><li><code>toString</code> returns a human readable representation of the msg</li><li><code>mergeFrom(Message other)</code> merge the contents of other into this msg, overwrite singular scalar fields</li><li><code>clear</code> clear all the fields back to the empty state</li></ul></li><li>Parsing and Serialization<ul><li><code>byte[] toByteArray();</code><ul><li>serializes the msg and returns a byte array containing its raw bytes</li></ul></li><li><code>static xxx parseFrom(byte[] data);</code><ul><li>parse a msg from the given byte array</li></ul></li><li><code>void writeTo(OutputStream output);</code><ul><li>serialize the msg and writes to an OutputStream</li></ul></li><li><code>static xxx parseFrom(InputStream input);</code><ul><li>reads and parses a msg from an InputStream</li></ul></li></ul></li></ul><h2 id="3-4-How-to-extend-a-Protocol-Buffer"><a href="#3-4-How-to-extend-a-Protocol-Buffer" class="headerlink" title="3.4 How to extend a Protocol Buffer"></a>3.4 How to extend a Protocol Buffer</h2><ul><li>In the new version of the protocol buffer<ul><li>must not change the tag numbers of any existing fields</li><li>must not add or delete any required fields</li><li>may delete optional or repeated fields</li><li>may add new optional or repeated fields but must use fresh tag numbers</li></ul></li></ul><h1 id="4-Overall-Guide-In-Proto3"><a href="#4-Overall-Guide-In-Proto3" class="headerlink" title="4. Overall Guide (In Proto3)"></a>4. Overall Guide (In Proto3)</h1><h2 id="4-1-Define-message-type"><a href="#4-1-Define-message-type" class="headerlink" title="4.1 Define message type"></a>4.1 Define message type</h2><ul><li>Each field in the msg definition need to have a <strong>unique number</strong><ul><li>those numbers are used to identify fields in the message binary format</li><li>the number should never be changed</li></ul></li><li>specify field rules<ul><li>singular<ul><li>default field rule for proto3 syntax</li><li>can have <strong>zero or one of this field</strong></li></ul></li><li>repeated<ul><li>can be repeated any number of times (including zero)</li></ul></li></ul></li><li>reserved fields<ul><li>if you update a msg type by entirely removing a field or commenting it out, future users can reuse the field number but it would bring severe issues,</li><li>thus we could reserved the number for deleted fields and tag number</li></ul></li></ul><pre><code class="protobuf">message Foo &#123;  reserved 2, 15, 9 to 11;  reserved &quot;foo&quot;, &quot;bar&quot;;&#125;</code></pre><ul><li>Post compiler running<ul><li>Compiler generates a <code>.java</code> file with a class for each message type, as well as Builder classes for creating message class instances</li></ul></li><li>For enum values<ul><li>every enum definition must contain a constant that maps to zero as its first element</li><li>we can allow alias thus we could assign the same value to different enum constants</li></ul></li><li>import<ul><li>we could do import thus we could use definitions from other <code>.proto</code> file</li></ul></li></ul><h2 id="4-2-Scalar-Value-Types"><a href="#4-2-Scalar-Value-Types" class="headerlink" title="4.2 Scalar Value Types"></a>4.2 Scalar Value Types</h2><p><a href="https://developers.google.com/protocol-buffers/docs/proto3#scalar">Language Guide (proto3) | Protocol Buffers | Google Developers</a></p><h2 id="4-3-Nested-Types"><a href="#4-3-Nested-Types" class="headerlink" title="4.3 Nested Types"></a>4.3 Nested Types</h2><ul><li>we could define and use msg types inside other msg types</li></ul><pre><code class="protobuf">message SearchResponse &#123;  message Result &#123;    string url = 1;    string title = 2;    repeated string snippets = 3;  &#125;  repeated Result results = 1;&#125;// to use the msg type outside its parent message type message SomeOtherMessage &#123;  SearchResponse.Result result = 1;&#125;</code></pre><h2 id="4-4-Updating-a-Message-Type"><a href="#4-4-Updating-a-Message-Type" class="headerlink" title="4.4 Updating a Message Type"></a>4.4 Updating a Message Type</h2><ul><li>don’t change the field numbers for any existing fields</li><li>if you add new fields, any msg serialized by code using your old msg format can still be parsed by your new generated code<ul><li>keep in mind the default values for these elements so that new code can properly interact with msgs generated by old code</li></ul></li><li>to remove a field<ul><li>rename the field with prefix like <code>OBSOLETE_</code></li><li>or make the filed number reserved,</li></ul></li><li>int32, uint32, int64, uint64 and bool are all compatible</li><li>string and bytes are compatible as long as the bytes are valid UTF-8</li></ul><h2 id="4-5-Special-Keywords"><a href="#4-5-Special-Keywords" class="headerlink" title="4.5 Special Keywords"></a>4.5 Special Keywords</h2><h3 id="4-5-1-Any"><a href="#4-5-1-Any" class="headerlink" title="4.5.1 Any"></a>4.5.1 <code>Any</code></h3><ul><li>let you use messages as embedded types without having their .proto definition</li><li>it contains an aribitrary serialized messages as bytes</li></ul><h3 id="4-5-2-Oneof"><a href="#4-5-2-Oneof" class="headerlink" title="4.5.2 Oneof"></a>4.5.2 Oneof</h3><ul><li>if we have a msg with many fields and where at most one field will be set at the same time, we can enforce the behavior and save memory by using the oneof feature</li><li>at most one field can be set at the same time</li><li>setting any member of the oneof automatically clears all the other members</li></ul><h2 id="4-6-Maps"><a href="#4-6-Maps" class="headerlink" title="4.6 Maps"></a>4.6 Maps</h2><ul><li><code>map&lt;key_type, value_type&gt; map_field = N;</code></li></ul><h2 id="4-7-Define-Service"><a href="#4-7-Define-Service" class="headerlink" title="4.7 Define Service"></a>4.7 Define Service</h2><ul><li>If you want to use message types with an RPC system, we can define an RPC service interface in a <code>.proto</code> file</li><li>then the protocol buffer compiler will <strong>generate service interface code and stubs</strong> in chosen language</li></ul><h2 id="4-8-Options"><a href="#4-8-Options" class="headerlink" title="4.8 Options"></a>4.8 Options</h2><ul><li><p>Options do not change the overall meaning of a declaration, but may affect the way it is handled in a particular context.</p></li><li><p>java_package</p><ul><li>pkg you want to use for your generated Java classes</li></ul></li><li><p>java_outer_classname</p><ul><li>class name for the wrapper java class you want to generate</li></ul></li><li><p>java_multiple_files</p></li><li><p><code>optimize_for</code></p><ul><li><code>SPEED</code><ul><li>Compiler will generate code for serializing, parsing and performing other common operations on your msg types.</li><li>Code is highly optimized</li></ul></li><li><code>CODE_SIZE</code><ul><li>generate minimal classes</li><li>operations will be slower</li></ul></li><li><code>LITE_RUNTIME</code><ul><li>only depend on the lite runtime library</li><li>usefyl for apps running on constrained platform like mobile phones</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Overview <a href="https://developers.google.com/protocol-buffers">https://developers.google.com/protocol-buffers</a> </li><li>Language Guide <a href="https://developers.google.com/protocol-buffers/docs/overview">https://developers.google.com/protocol-buffers/docs/overview</a> </li><li>Java Tutorial <a href="https://developers.google.com/protocol-buffers/docs/javatutorial">https://developers.google.com/protocol-buffers/docs/javatutorial</a> </li><li>Java Generated Code <a href="https://developers.google.com/protocol-buffers/docs/reference/java-generated">https://developers.google.com/protocol-buffers/docs/reference/java-generated</a> </li><li>Java Encoding <a href="https://developers.google.com/protocol-buffers/docs/encoding">https://developers.google.com/protocol-buffers/docs/encoding</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Protobuf-Learning&quot;&gt;&lt;a href=&quot;#Protobuf-Learning&quot; class=&quot;headerlink&quot; title=&quot;Protobuf Learning&quot;&gt;&lt;/a&gt;Protobuf Learning&lt;/h1&gt;&lt;h1 id=&quot;1-Wha
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Protobuf" scheme="https://www.llchen60.com/tags/Protobuf/"/>
    
  </entry>
  
  <entry>
    <title>Bazel Intro</title>
    <link href="https://www.llchen60.com/Bazel-Intro/"/>
    <id>https://www.llchen60.com/Bazel-Intro/</id>
    <published>2021-12-22T02:05:15.000Z</published>
    <updated>2021-12-22T02:07:14.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-What-is-Bazel"><a href="#1-What-is-Bazel" class="headerlink" title="1. What is Bazel?"></a>1. What is Bazel?</h1><ul><li><p>Bazel is a build and test tool built that supports building and testing multiple projects for multiple languages and build outputs</p></li><li><p>What</p><ul><li>Build and Test tool similar to Make, Maven, Gradle</li><li>Caches all the previously done work, tests or builds faster everytime</li><li>Support multi languages, multi platforms,</li><li>Support large code base across multi repos</li><li>build, test, and query to trace dependencies in the code</li></ul></li><li><p>Why</p><ul><li>scales</li><li>multi platform</li></ul></li><li><p>How</p><ul><li>Need a BUILD file</li></ul></li></ul><h1 id="2-Concepts"><a href="#2-Concepts" class="headerlink" title="2. Concepts"></a>2. Concepts</h1><ul><li>Workspace - With WORKSPACE<ul><li>dir contains the source file</li><li>considered as root</li></ul></li><li>WORKSPACE<ul><li>a blank text file, which identifies the directory and its content as a Bazel workspace</li><li>at the root of the project’s directory structure</li></ul></li><li>Repos - With WORKSPACE<ul><li>External repos are defined in the WORKSPACE file using workspace rules</li></ul></li><li>Packages - With BUILD<ul><li>A package is defined as a directory containing a file named BUILD or BUILD.bazel</li><li>which reside beneath top level directory in the ws</li><li>This file has instructions on how to <strong>run or build or test</strong> the project</li></ul></li><li>Rules<ul><li>written using a DSL named Starlark</li><li>thus are built for certain language already like rules_java, etc.</li></ul></li><li>Targets<ul><li>Pkg is container, element of pkg —- target</li><li>Most targets are files or rules<ul><li>File<ul><li>source files - written by people</li><li>generated files — generated by build tool</li></ul></li><li>rule<ul><li>specify relationship between set of inputs and output</li><li>output are always generated files</li></ul></li></ul></li></ul></li></ul><h1 id="3-Best-Practices"><a href="#3-Best-Practices" class="headerlink" title="3. Best Practices"></a>3. Best Practices</h1><ul><li>A project should always be able to run <code>bazel build //...</code> and <code>bazel test //...</code></li><li>You may declare third party dependencies<ul><li>either declare them as remote repositories in the WORKSPACE file</li><li>or put them in a directory called third_party under workspace directory</li></ul></li><li>everything should be built from source whenever possible, instead of depending on a library so file, we should create a BUILD file and build so from its sources, then depend on that target</li><li>for project specific options, use the configuration file under <code>workspace/.bazelrc</code></li><li>every directory that contains buildable files should be a package</li></ul><h1 id="4-Build-a-Java-Project"><a href="#4-Build-a-Java-Project" class="headerlink" title="4. Build a Java Project"></a>4. Build a Java Project</h1><h2 id="4-1-Bazel-Jave-Basic"><a href="#4-1-Bazel-Jave-Basic" class="headerlink" title="4.1 Bazel Jave Basic"></a>4.1 Bazel Jave Basic</h2><ul><li><p>Refer <a href="https://docs.bazel.build/versions/1.2.0/tutorial/java.html">https://docs.bazel.build/versions/1.2.0/tutorial/java.html</a></p></li><li><p>build rule tells bazel how to build the desired outputs, executable binaries or libraries</p><ul><li>the java binary rule tells bazel to build a jar file and a wrapper shell script</li></ul></li><li><p><code>bazel build //:ProjectRunner</code></p><ul><li>the <code>//</code> part is the location of our BUILD file relative to the root of the workspace</li><li><code>ProjectRunner</code> is the target name we define in the BUILD file</li></ul></li><li><p>we could review our dependency graph by using</p><ul><li><code>bazel query --notool_deps --noimplicit_deps &quot;deps(//:ProjectRunner)&quot; --output graph</code></li></ul></li></ul><pre><code class="ruby">// generate graph for class in use, and output as a svg file bazel query  --notool_deps --noimplicit_deps &quot;deps(//booking)&quot; --output graph &gt; /Users/lchen1/Documents/bookingGraph.in dot -Tsvg &lt; bookingGraph.in &gt; graph.svg</code></pre><h2 id="4-2-Specify-multiple-build-targets"><a href="#4-2-Specify-multiple-build-targets" class="headerlink" title="4.2 Specify multiple build targets"></a>4.2 Specify multiple build targets</h2><ul><li><p>Package Splits</p><ul><li><p>for larger project, we may want to split into multiple targets and packages to allow for fast incremental builds, this could also speed up builds by building multiple parts of a project at once</p><pre><code class="json">java_binary(  name = &quot;ProjectRunner&quot;,  srcs = [&quot;src/main/java/com/example/ProjectRunner.java&quot;],  main_class = &quot;com.example.ProjectRunner&quot;,  deps = [&quot;:greeter&quot;],)java_library(  name = &quot;greeter&quot;,  srcs = [&quot;src/main/java/com/example/Greeting.java&quot;],)</code></pre></li></ul></li></ul><ul><li>with this configuration, bazel will first build greeter library, then the projectRunner binary<ul><li>deps attribute tells bazel the greeter library is required to build the projectRunner binary</li></ul></li></ul><h2 id="4-3-Use-multiple-packages"><a href="#4-3-Use-multiple-packages" class="headerlink" title="4.3 Use multiple packages"></a>4.3 Use multiple packages</h2><pre><code class="json">java_binary(    name = &quot;runner&quot;,    srcs = [&quot;Runner.java&quot;],    main_class = &quot;com.example.cmdline.Runner&quot;,    deps = [&quot;//:greeter&quot;])</code></pre><ul><li>To make sure above works, we need to let greeter be visible to cmdline.Runner<ul><li>Let the resource owner set the visibility attribute</li><li>we need to do this cause Bazel by default makes target only visible to other targets in the same BUILD file</li><li>bazel uses target visibility to prevent issues such as libraries containing implementation details leaking into public APIs</li></ul></li></ul><pre><code class="json">java_library(    name = &quot;greeter&quot;,    srcs = [&quot;src/main/java/com/example/Greeting.java&quot;],    visibility = [&quot;//src/main/java/com/example/cmdline:__pkg__&quot;],    )</code></pre><h2 id="4-4-Use-labels-to-reference-targets"><a href="#4-4-Use-labels-to-reference-targets" class="headerlink" title="4.4 Use labels to reference targets"></a>4.4 Use labels to reference targets</h2><ul><li>Bazel uses target labels to reference targets<ul><li><code>//:ProjectRunner</code></li><li>sync as follow:<ul><li><code>//path/to/package:target-name</code></li></ul></li></ul></li><li>when referencing targets within the same BUILD file, we can skip the <code>//</code> workspace root identifier and just use <code>:target_name</code></li></ul><h1 id="5-E-G"><a href="#5-E-G" class="headerlink" title="5. E.G"></a>5. E.G</h1><ul><li>java_binary<ul><li>pre defined rule telling bazel to create a binary when a target is invoked</li></ul></li></ul><pre><code class="json">java_binary(        // target name     name = &quot;mymain&quot;,        // all source files, passed as glob, inside the fully qualified directory names on classpath     srcs = glob([&quot;src/main/java/com/abhi/*.java&quot;]),        // main runner class     main_class = &quot;com.abhi.MyMain&quot;,        // dependent classes/ interfaces to be included, not part of srcs     deps = [&quot;//another-dir:animal&quot;])</code></pre><ul><li>java_library<ul><li>pre-defined to create library as the name suggests</li></ul></li></ul><pre><code class="json">java_library(    name = &quot;animal&quot;,    srcs = [&quot;src/main/java/com/abhi/Animal.java&quot;],        // if other class is implemented in a different pkg, it has to be visible to main-dir     visibility = [&quot;//main-dir:__pkg__&quot;])</code></pre><ul><li>CLI Reference<ul><li><code>bazel build //main-dir:mymain</code><ul><li>// means a valid package name</li><li>mymain is the target name</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Bazel best practice <a href="https://docs.bazel.build/versions/main/best-practices.html">https://docs.bazel.build/versions/main/best-practices.html</a> </li><li>Bazel Overview  <a href="https://docs.bazel.build/versions/1.2.0/bazel-overview.html">https://docs.bazel.build/versions/1.2.0/bazel-overview.html</a> </li><li>Java Tutorial <a href="https://docs.bazel.build/versions/1.2.0/tutorial/java.html">https://docs.bazel.build/versions/1.2.0/tutorial/java.html</a> </li><li>How to specify targets to build <a href="https://docs.bazel.build/versions/main/guide.html#target-patterns">https://docs.bazel.build/versions/main/guide.html#target-patterns</a> </li></ol><p><a href="https://docs.bazel.build/versions/4.2.1/command-line-reference.html">Command-Line Reference</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-What-is-Bazel&quot;&gt;&lt;a href=&quot;#1-What-is-Bazel&quot; class=&quot;headerlink&quot; title=&quot;1. What is Bazel?&quot;&gt;&lt;/a&gt;1. What is Bazel?&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bazel
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Bazel" scheme="https://www.llchen60.com/tags/Bazel/"/>
    
      <category term="Package Management" scheme="https://www.llchen60.com/tags/Package-Management/"/>
    
  </entry>
  
  <entry>
    <title>GraphQL Read</title>
    <link href="https://www.llchen60.com/GraphQL-Read/"/>
    <id>https://www.llchen60.com/GraphQL-Read/</id>
    <published>2021-12-04T09:29:50.000Z</published>
    <updated>2021-12-04T09:31:23.613Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GraphQL-Read"><a href="#GraphQL-Read" class="headerlink" title="GraphQL Read"></a>GraphQL Read</h1><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li>QraphQL is<ul><li>a query language</li><li>a server side runtime for executing queries using a type system you define</li></ul></li></ul><h1 id="2-Queries-and-Mutations"><a href="#2-Queries-and-Mutations" class="headerlink" title="2. Queries and Mutations"></a>2. Queries and Mutations</h1><ul><li>Fields<ul><li>GraphQL is about asking specific fields on objects</li><li>Query has the same shape as result<ul><li>server knows exactly what fields the client is asking for</li></ul></li></ul></li></ul><pre><code class="ruby">&#123;    hero &#123;        name    &#125;&#125;&#123;    &quot;data&quot;: &#123;        &quot;hero&quot;: &#123;            &quot;name&quot;: &quot;12test&quot;        &#125;    &#125;&#125;</code></pre><ul><li><p>Arguments</p><ul><li>we could pass arguments to fields</li><li>comparing with Restful, in GraphQL every field and nested object can get its own set of arguments, making GraphQL a complete replacement for making multiple APU fetches</li></ul></li><li><p>Fragments</p><ul><li>That’s the reusable units in GraphQL</li><li>Fragments let you construct sets of fields, and then include them in queries where you need to</li><li>It’s commonly used to split complicated application data requirements into smaller chunks</li></ul></li><li><p>Operation Name</p><ul><li>Operation Type<ul><li>Query</li><li>Mutation</li><li>Subscription</li></ul></li><li>Operation Name</li></ul></li><li><p>Variables</p><ul><li>It want to give dynamic power to graphql, as in most applications, the arguments to fields will be dynamic</li><li>Graphql supports this use case via variables</li><li>we could do:<ul><li>replace the static value in the query with <code>$variable</code></li><li>declare <code>$variable</code> as one of the variables accepted by the query</li><li>pass <code>variable: value</code> in the separate transport specific variables dictionary</li></ul></li><li>using variable could help us denote which arguments are expected to be dynamic</li><li>we should never do string interpolation to construct queries from user supplied values</li></ul></li></ul><pre><code class="ruby">query HeroNameAndFriends($episode: Episode) &#123;  hero(episode: $episode) &#123;    name    friends &#123;      name    &#125;  &#125;&#125;&#123;  &quot;episode&quot;: &quot;JEDI&quot;&#125; </code></pre><ul><li>Default variables</li></ul><pre><code class="ruby">query HeroNameAndFriends($episode: Episode = &quot;defaultOne&quot;) &#123;  hero(episode: $episode) &#123;    name    friends &#123;      name    &#125;  &#125;&#125;&#123;  &quot;episode&quot;: &quot;JEDI&quot;&#125; </code></pre><ul><li><p>Directives</p><ul><li>use this variable to dynamically change the structure and shape of our queries using variables</li><li><code>@include(if: Boolean)</code> only includes this field in the result if the argument is true</li><li><code>@skip(if: Boolean)</code> skip this field if the argument is true</li></ul></li><li><p>Mutations</p><ul><li>A way to modify server side data</li><li>A convention that any operations that cause writes should be sent explicitly via a mutation</li><li>!!! While query fields are executed in parallel, mutation fields run in series, one after the other<ul><li>means if we send two incrementCredits mutations in one request, the first is guranteed to finish before the second begins, ensuring that we don’t end up with a race condition with ourselves</li></ul></li></ul></li><li><p>Inline Fragments</p><ul><li><p>GraphQL schemas include the ability to define interfaces and union types</p></li><li><p>EG below, we need to return different attributes based on hero character</p><pre><code class="ruby">query HeroForEpisode($ep: Episode!) &#123;hero(episode: $ep) &#123;  name  ... on Droid &#123;    primaryFunction  &#125;  ... on Human &#123;    height  &#125;&#125;&#125;</code></pre></li></ul></li><li><p>Meta fields</p><ul><li>there are situations where you don’t know what type you’ll get back from the service</li><li>we need to determine how to handle that data on the client</li><li>we could use <code>__typename</code></li></ul></li></ul><h1 id="3-Schemas-and-Types"><a href="#3-Schemas-and-Types" class="headerlink" title="3. Schemas and Types"></a>3. Schemas and Types</h1><h2 id="3-1-how-the-schema-work"><a href="#3-1-how-the-schema-work" class="headerlink" title="3.1 how the schema work"></a>3.1 how the schema work</h2><ul><li><p>How does GraphQL work</p><ul><li><p>start with a <code>root</code> object</p></li><li><p>select the hero field on that</p></li><li><p>for the object returned by hero, we select the name and appearsIn fields</p><pre><code class="ruby">&#123;hero &#123;  name  appearsIn&#125;&#125;</code></pre></li></ul></li><li><p>we should know what we could query for</p><ul><li>an exact description of the data we can ask for</li><li>what kind of objects might they return</li><li>what fields are available on those sub objects</li></ul></li><li><p>Schema</p><ul><li>Each graphQL services defines a set of types which completely describe the set of possible data you can query on the service</li></ul></li></ul><h2 id="3-2-Type-Language"><a href="#3-2-Type-Language" class="headerlink" title="3.2 Type Language"></a>3.2 Type Language</h2><ul><li>GraphQL use its won Schema Language</li></ul><h3 id="3-2-1-Object-Types-and-Fields"><a href="#3-2-1-Object-Types-and-Fields" class="headerlink" title="3.2.1 Object Types and Fields"></a>3.2.1 Object Types and Fields</h3><ul><li>Object types<ul><li>represent a kind of object you can fetch from your service, and what fields it has</li></ul></li></ul><pre><code class="ruby">type Character &#123;  name: String!// means an array of Episode objects, notnull, 0 or more items, and each item would be an episode object   appearsIn: [Episode!]!&#125;</code></pre><h3 id="3-2-2-Query-and-Mutation-types"><a href="#3-2-2-Query-and-Mutation-types" class="headerlink" title="3.2.2 Query and Mutation types"></a>3.2.2 Query and Mutation types</h3><ul><li>Entry points into the schema</li></ul><h3 id="3-2-3-Scalar-Types"><a href="#3-2-3-Scalar-Types" class="headerlink" title="3.2.3 Scalar Types"></a>3.2.3 Scalar Types</h3><ul><li>Scalar types represent the leaves of the query</li><li>default scalar types<ul><li>Int</li><li>Float</li><li>String</li><li>Boolean</li><li>ID<ul><li>it represents a unique identifier</li><li>The ID type is serialized in the same way as a String, but it means it’s not intended to be human readable</li></ul></li></ul></li><li>We could also define our own Scalar type in this way<ul><li><code>scalar Date</code></li></ul></li></ul><h3 id="3-2-4-Enumeration-Types"><a href="#3-2-4-Enumeration-Types" class="headerlink" title="3.2.4 Enumeration Types"></a>3.2.4 Enumeration Types</h3><ul><li>Restricted to a particular set of allowed values</li><li>Allow you to<ul><li>validate that any arguments of this type are one of the allowed values</li><li>communicate through the type system that a field will always be one of a finite set of values</li></ul></li></ul><h3 id="3-2-5-Lists-and-Non-Null"><a href="#3-2-5-Lists-and-Non-Null" class="headerlink" title="3.2.5 Lists and Non-Null"></a>3.2.5 Lists and Non-Null</h3><pre><code>type Character &#123;  name: String!  appearsIn: [Episode]!&#125;</code></pre><ul><li>We could use <code>!</code> to indicate it should never return null</li><li>We could use <code>[]</code> to indicate that should be an array</li></ul><h3 id="3-2-6-Interfaces"><a href="#3-2-6-Interfaces" class="headerlink" title="3.2.6 Interfaces"></a>3.2.6 Interfaces</h3><ul><li>An abstract type that includes a certain set of fields that a type must include to implement the interface</li></ul><pre><code class="ruby">interface Character &#123;    id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!&#125;type Human implements Character &#123;  id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!  starships: [Starship]  totalCredits: Int&#125;type Droid implements Character &#123;  id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!  primaryFunction: String&#125;</code></pre><ul><li>Type implement the interface need to have all those fields, but they could also have their own fields</li></ul><h3 id="3-2-7-Union-Types"><a href="#3-2-7-Union-Types" class="headerlink" title="3.2.7 Union Types"></a>3.2.7 Union Types</h3><p><code>union SearchResult = Human | Droid | Starship</code></p><h3 id="3-2-8-Input-Types"><a href="#3-2-8-Input-Types" class="headerlink" title="3.2.8 Input Types"></a>3.2.8 Input Types</h3><ul><li>we need to pass complex objects especially when we are using mutations, where we want to pass in a whole object to be created</li></ul><pre><code class="ruby">input ReviewInput &#123;  stars: Int!  commentary: String&#125;mutation CreateReviewForEpisode($ep: Episode!, $review: ReviewInput!) &#123;  createReview(episode: $ep, review: $review) &#123;    stars    commentary  &#125;&#125;</code></pre><h1 id="4-Validation"><a href="#4-Validation" class="headerlink" title="4. Validation"></a>4. Validation</h1><ul><li>Graph ql has validation module to fulfill the validation phase of fulfilling a graphQL result</li></ul><h1 id="5-Execution"><a href="#5-Execution" class="headerlink" title="5. Execution"></a>5. Execution</h1><h2 id="5-1-Resolvers"><a href="#5-1-Resolvers" class="headerlink" title="5.1 Resolvers"></a>5.1 Resolvers</h2><ul><li>Each field in a GraphQL query is a function or method of the previous type which returns the next type</li><li>Each field is backed by a function called the resolver. When a field is executed, the corresponding resolver is called to produce the next value</li><li>The resolver continue to work until reach scalar values</li></ul><h2 id="5-2-Root-fields"><a href="#5-2-Root-fields" class="headerlink" title="5.2 Root fields"></a>5.2 Root fields</h2><pre><code class="ruby">Query: &#123;  human(obj, args, context, info) &#123;    return context.db.loadHumanByID(args.id).then(      userData =&gt; new Human(userData)    )  &#125;&#125;</code></pre><ul><li>obj<ul><li>previous object</li></ul></li><li>args<ul><li>arguments provided to the field in the graphQL query</li></ul></li><li>context<ul><li>a value which is provided to every resolver and holds important contextual information like the currently logged in user, or access to a database</li></ul></li><li>info<ul><li>a value which holds field specific information relevant to the current query as well as the schema details</li></ul></li></ul><h1 id="6-Best-Practices"><a href="#6-Best-Practices" class="headerlink" title="6. Best Practices"></a>6. Best Practices</h1><h2 id="6-1-HTTP"><a href="#6-1-HTTP" class="headerlink" title="6.1 HTTP"></a>6.1 HTTP</h2><ul><li><p>Mostly use HTTP with graphQL</p></li><li><p>Normally web frameworks use a pipeline model where requests are passed through a stack of middle ware</p></li><li><p>Requests could be inspected, transformed, modified or terminated with a response</p></li><li><p>GraphQL should be placed after all authentication middleware— thus you have access to the same session and user info you would in your HTTP endpoints handler</p></li><li><p>GraphQL server operates on a single URL/ endpoint, usually <code>graphql</code> , and all graphql requests for a given service should be directed at this endpoint</p></li></ul><h2 id="6-2-JSON-with-GZIP"><a href="#6-2-JSON-with-GZIP" class="headerlink" title="6.2 JSON with GZIP"></a>6.2 JSON with GZIP</h2><ul><li>typically respond using JSON, and we compress it with GZIP</li></ul><h2 id="6-3-Versioning"><a href="#6-3-Versioning" class="headerlink" title="6.3 Versioning"></a>6.3 Versioning</h2><ul><li>No need to do versioning for graphql api</li><li>Why do most APIs version? When there’s limited control over the data that’s returned from an API endpoint, <em>any change</em> can be considered a breaking change, and breaking changes require a new version. If adding new features to an API requires a new version, then a tradeoff emerges between releasing often and having many incremental versions versus the understandability and maintainability of the API.</li><li>In contrast, GraphQL only returns the data that’s explicitly requested, so new capabilities can be added via new types and new fields on those types without creating a breaking change. This has led to a common practice of always avoiding breaking changes and serving a versionless API.</li></ul><h2 id="6-4-Nullability"><a href="#6-4-Nullability" class="headerlink" title="6.4 Nullability"></a>6.4 Nullability</h2><ul><li>GraphQL default to nullable unless you specifically declare nonnull</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.graphql-java-kickstart.com/">https://www.graphql-java-kickstart.com/</a>  </li><li><a href="https://graphql.org/learn/">https://graphql.org/learn/</a>  </li><li><a href="https://www.apollographql.com/docs/federation/">https://www.apollographql.com/docs/federation/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;GraphQL-Read&quot;&gt;&lt;a href=&quot;#GraphQL-Read&quot; class=&quot;headerlink&quot; title=&quot;GraphQL Read&quot;&gt;&lt;/a&gt;GraphQL Read&lt;/h1&gt;&lt;h1 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="Web" scheme="https://www.llchen60.com/categories/Web/"/>
    
    
  </entry>
  
  <entry>
    <title>如何缓解疲劳</title>
    <link href="https://www.llchen60.com/%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3%E7%96%B2%E5%8A%B3/"/>
    <id>https://www.llchen60.com/%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3%E7%96%B2%E5%8A%B3/</id>
    <published>2021-11-02T13:31:04.000Z</published>
    <updated>2021-11-02T13:34:58.934Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/11/02/zEvGV8uDnJxoITr.png" alt="如何抵抗缓解疲劳.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/11/02/zEvGV8uDnJxoITr.png&quot; alt=&quot;如何抵抗缓解疲劳.png&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>碳达峰与碳中和</title>
    <link href="https://www.llchen60.com/%E7%A2%B3%E8%BE%BE%E5%B3%B0%E4%B8%8E%E7%A2%B3%E4%B8%AD%E5%92%8C/"/>
    <id>https://www.llchen60.com/%E7%A2%B3%E8%BE%BE%E5%B3%B0%E4%B8%8E%E7%A2%B3%E4%B8%AD%E5%92%8C/</id>
    <published>2021-09-30T02:21:14.000Z</published>
    <updated>2021-09-30T02:22:44.858Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/09/30/XGS6Qey4qHki2FC.png" alt="碳达峰与碳中和"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/09/30/XGS6Qey4qHki2FC.png&quot; alt=&quot;碳达峰与碳中和&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Cassandra</title>
    <link href="https://www.llchen60.com/Cassandra/"/>
    <id>https://www.llchen60.com/Cassandra/</id>
    <published>2021-09-17T13:05:55.000Z</published>
    <updated>2021-09-17T13:41:43.051Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/09/17/IGdzKBJwD9TbZ6f.png" alt="Cassandra MindMap.png"></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><h2 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h2><ul><li>We wanna have a distributed and scalable system that can store a <strong>huge amount of structured data</strong>, which is indexed by a row key where each row can have an <strong>unbounded</strong> number of columns.</li><li>Cassandra was originally developed at Facebook in 2007 for index search feature. It’s designed to provide scalability, availability, and reliability to store large amounts of data.</li><li>It combines nature of Dynamo which is a <strong>key value store</strong> and the data model of Bigtable which is a <strong>column based</strong> data store</li><li>Cassandra is in favor of availability and partition tolerance, it could be tuned with <strong>replication factor</strong> and <strong>consistency levels</strong> to meet <strong>strong consistency</strong> requirements, and of course with a performance cost.</li><li>It uses peer to peer architecture, with each node connected to all other nodes</li><li>Each Cassandra node performs all database operations and can serve client requests without the need for any leader node.</li></ul><h2 id="1-2-Use-cases"><a href="#1-2-Use-cases" class="headerlink" title="1.2 Use cases"></a>1.2 Use cases</h2><ul><li>Store key value data with high availability</li><li>Time series data model<ul><li>Due to its data model and log structured storage engine, cassandra benefits from high performing write operations, This also make it well suited for storing and analyzing sequentially captured metrics</li></ul></li><li>Write Heavy Applications<ul><li>Suited for write intensive applications such as time series streaming services, sensor logs, and IoT applications</li></ul></li></ul><h1 id="2-High-Level-Architecture"><a href="#2-High-Level-Architecture" class="headerlink" title="2. High Level Architecture"></a>2. High Level Architecture</h1><h2 id="2-1-Common-Terms"><a href="#2-1-Common-Terms" class="headerlink" title="2.1 Common Terms"></a>2.1 Common Terms</h2><p><img src="https://i.loli.net/2021/09/17/IrfBD5HFqAX76NJ.png" alt="Primary and Clustering Keys"></p><ul><li>Column<ul><li>A key value pair and is the most basic unit of data structure</li><li>Column Key: Uniquely identifies a column in a row</li><li>Column Value: Store a value or a collection of values</li></ul></li><li>Row<ul><li>A container for columns referenced by primary key. Cassandra does not store a column that has a null value, this saves a lot of space</li></ul></li><li>Table<ul><li>A container of rows</li></ul></li><li>Keyspace<ul><li>A container for tables that span over one or more Cassandra nodes</li></ul></li><li>Cluster<ul><li>Container of Keyspace</li></ul></li><li>Node<ul><li>A computer system running an instance of Cassandra,</li><li>Can be a physical host, a machine instance in the cloud or even a docker container</li></ul></li></ul><h2 id="2-2-Data-Partitioning"><a href="#2-2-Data-Partitioning" class="headerlink" title="2.2 Data Partitioning"></a>2.2 Data Partitioning</h2><ul><li>Cassandra use consistent hashing as DynamoDB does</li></ul><h2 id="2-3-Primary-Key"><a href="#2-3-Primary-Key" class="headerlink" title="2.3 Primary Key"></a>2.3 Primary Key</h2><ul><li>The primary key consists of two parts:  E.G Primary Key as (city_id, employee_id)<ul><li>Partition Key<ul><li>Decides how data is distributed across nodes</li><li>city_id is the primary key, means the data will be partitioned by the city_id field, all rows with the same city_id will reside on the same node</li></ul></li><li>Clustering Key<ul><li>Decides how data is stored within a node</li><li>We could have multiple clustering keys, clustering columns specify the order that the data is arranged on a node.</li><li>employee_id is the clustering key. Within each node, the data is stored in sorted order according to the employee_id column.</li></ul></li></ul></li></ul><h2 id="2-4-Partitioner"><a href="#2-4-Partitioner" class="headerlink" title="2.4 Partitioner"></a>2.4 Partitioner</h2><p><img src="https://i.loli.net/2021/09/17/3NdkOaXUpbgnWq9.png" alt="Partitioner Flow"></p><ul><li>Responsible for determining how data is distributed on the consistent hash ring.</li><li>Cassandra use <strong>Murmur3 hashing function</strong> — which will always produce the same hash for a given partition key</li><li>All Cassandra nodes learn about the <strong>token assignments of other nodes</strong> through gossip. This means any node can handle a request for any other node’s range. The node receiving the request is called the <strong>coordinator</strong>, and any node can act in this role. If a key does not belong to the coordinator’s range, it <strong>forwards the request</strong> to the replicas responsible for that range.</li></ul><h2 id="2-5-Coordinator-Node"><a href="#2-5-Coordinator-Node" class="headerlink" title="2.5 Coordinator Node"></a>2.5 Coordinator Node</h2><ul><li>A client may connect to any node in the cluster to initiate a read or write query. This node is known as the coordinator node, the coordinator identifies the nodes responsible for the data that is being written or read    and forwards the queries to them</li></ul><h1 id="3-Low-Level-Architecture"><a href="#3-Low-Level-Architecture" class="headerlink" title="3. Low Level Architecture"></a>3. Low Level Architecture</h1><h2 id="3-1-Replication-Strategy"><a href="#3-1-Replication-Strategy" class="headerlink" title="3.1 Replication Strategy"></a>3.1 Replication Strategy</h2><ul><li><p>Each node in Cassandra serves as a replica for a different range of data.</p></li><li><p>It stores <strong>multiple copies of data</strong> and <strong>spreads them across various replicas</strong>.</p></li><li><p>The replication behavior is controlled by two factors</p><ul><li><p>Replication Factor</p><ul><li>Decides how many replicas the system will have</li><li>This represents the <strong>number of nodes that will receive the copy of the same data</strong></li><li>Each keyspace in cassandra can have a different replication factor</li></ul></li><li><p>Replication Strategy</p><ul><li><p>Decides which nodes will be responsible for the replicas</p></li><li><p>The node that owns the range in which the hash of the partition key falls will be the first replica</p></li><li><p>All the additional replicas are placed on the <strong>consecutive nodes</strong></p></li><li><p>Cassandra places the subsequent replicas on the next nodes in a clockwise manner</p></li><li><p>Two kinds of replication strategies</p><ul><li><p>Simple Replication Strategy</p><p>  <img src="https://i.loli.net/2021/09/17/cnz12lGFWEPw4fS.png" alt="Simple Replication Strategy"></p><ul><li>Used for a <strong>single data center cluster</strong></li><li>Cassandra places the first replica on a node determined by the partitioner and the subsequent replicas on the next node in a clockwise manner</li></ul></li><li><p>Network Topology Strategy</p><p>  <img src="https://i.loli.net/2021/09/17/TSAZbXKCYf9IsoN.png" alt="Network Topology Strategy"></p><ul><li>Used for multiple data centers</li><li>We can specify different replication factors for different data centers. We could then specify how many replicas will be placed in each data center</li><li>Additional replicas, in the same data center, are placed by <strong>walking the ring clockwise until reaching the 1st node in another rack</strong>. This is done to guard against a complete rack failure, as nodes in the same rack(or similar physical grouping) tend to fail together due to power, cooling or network issues.</li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="3-2-Consistency-Levels"><a href="#3-2-Consistency-Levels" class="headerlink" title="3.2 Consistency Levels"></a>3.2 Consistency Levels</h2><ul><li>Definition<ul><li><strong>Minimum number of nodes</strong> that must fulfill a read or write operation before the operation can be considered successful</li><li>It allows use to <strong>specify different consistency levels</strong> for read and write</li><li>It also has <strong>tunable consistency level</strong></li><li>Tradeoff between consistency and response time<ul><li>As a higher consistency level means more nodes need to respond to a read or write query, giving user more assurance that the values present on each replica are the same</li></ul></li></ul></li></ul><h3 id="3-2-1-Write-Consistency-Levels"><a href="#3-2-1-Write-Consistency-Levels" class="headerlink" title="3.2.1 Write Consistency Levels"></a>3.2.1 Write Consistency Levels</h3><ul><li>Consistency Levels specify how many replica nodes must respond for the write to be reported as successful to the client</li><li>Level is specified <strong>per query by the client</strong></li><li>Cassandra is eventually consistent, updates to other replica nodes may continue in the background</li><li>How does Cassandra perform a write operation?<ul><li>Coordinator node contacts all replicas, as determined by the <strong>replication factor</strong> , and consider the write successful when a number of replicas equal to the consistency level acknowledge the write</li></ul></li><li>Write Consistency Levels List:<ul><li>One/ Two/ Three<ul><li>The data must be written to at least the specified number of replica nodes before a write is considered successful</li></ul></li><li>Quorum<ul><li>Data must be written to at least a quorum of replica nodes</li><li>Quorum is defined as <code>floor(RF/2 + 1)</code>  RF represents replication factor</li></ul></li><li>All<ul><li>ensures the data is written to all replica nodes</li><li>provides the highest consistency but lowest availability as writes will fail if any replica is down</li></ul></li><li>Local Quorum<ul><li>Ensure that data is written to a quorum of nodes in the same datacenter as the coordinator</li><li>Does not wait for the response from the other data centers</li></ul></li><li>Each Quorum<ul><li>Ensures that the data is written to a quorum of nodes in each datacenter</li></ul></li><li>Any<ul><li>The data must be written to at least one node</li><li>In the extreme case, when all replica nodes for the given partition key are down, the write can still succeed after a hinted handoff (see 3.2.4 section) has been written.<ul><li>In this case, an any write could succeed with hinted handoff, but it will not be readable until the replica nodes for that partition has recovered and the latest data is written on them</li></ul></li></ul></li></ul></li></ul><h3 id="3-2-2-Read-Consistency-Levels"><a href="#3-2-2-Read-Consistency-Levels" class="headerlink" title="3.2.2 Read Consistency Levels"></a>3.2.2 Read Consistency Levels</h3><ul><li><p>Read Query Consistency Level specify how many replica nodes must respond to a read request before returning the data</p></li><li><p>It has the same consistency levels for read operations as that of write operations exception Each_Quorum cause it’s too expensive</p></li><li><p>To achieve strong consistency, we need to do <code>R + W &gt; RF</code> R represents read replica count, W represents write replication count, RF represents replication factor</p><ul><li>All client reads will see the most recent write in this scenario, and we will have strong consistency</li></ul></li><li><p>How does Cassandra perform a read operation?</p><ul><li><p>Coordinator always sends the read request to the fastest node</p><ul><li>E.G  for quorum=2, the coordinator sends the requests to the fastest node and the <strong>digest of the data</strong> from the second fastest node<ul><li>digest is the checksum of the data, we use this to save network bandwidth</li></ul></li></ul></li><li><p>if the digest doesn’t match, means some replica do not have the latest version of data</p><ul><li><p>Coordinator then <strong>reads the data from all the replicas</strong> to determine the latest data</p></li><li><p>Then coordinator <strong>returns the latest data to the client and initiates a read repair request</strong></p></li><li><p>The read repair request will help push the newer version of data to nodes with the older version</p><p>  <img src="https://i.loli.net/2021/09/17/sPM6HnKthBpT945.png" alt="Read Operation with Snitch"></p></li></ul></li><li><p>latest write timestamp is used as a mark for the correct version of data, read repair operation is performed only in a portion of the total reads to avoid performance degradation</p></li></ul></li></ul><h3 id="3-2-3-Snitch"><a href="#3-2-3-Snitch" class="headerlink" title="3.2.3 Snitch"></a>3.2.3 Snitch</h3><ul><li><p>Functions</p><ul><li>Application that determines the proximity of nodes within the ring, also tells which nodes are faster — monitor the read latencies</li><li>It keeps track of the network topology of Cassandra nodes, determines which <strong>data centers and racks</strong> nodes belong to</li><li>Replication strategy use this information provided by the Snitch to spread the replicas across the cluster intelligently. It could do its best by not having more than one replica on the same rack</li></ul></li><li><p>Cassandra nodes use this info to route read/ write requests efficiently</p><p>  <img src="https://i.loli.net/2021/09/17/ZnaOJqIvgdcMmAW.png" alt="Request when set consistency to one"></p></li></ul><h3 id="3-2-4-Hinted-Handoff"><a href="#3-2-4-Hinted-Handoff" class="headerlink" title="3.2.4 Hinted Handoff"></a>3.2.4 Hinted Handoff</h3><p><img src="https://i.loli.net/2021/09/17/QvSmb5wntE2AHJd.png" alt="Hinted Handoff"></p><ul><li>To let Cassandra still serve write requests even when nodes are down</li><li>When a node is down, the coordinator nodes <strong>writes a hint in a text file on local disk</strong><ul><li>Hint contains the data itself along with information about which node the data belongs to</li><li>Recover from gossiper — When the coordinator node discovers from the gossiper that a node for which it holds hints has recovered, it forwards the write request for each hint to the target</li><li>Recover from routine call — each node every ten minutes checks to see if the failing node, for which it is holding any hints, has recovered</li></ul></li><li>With consistency level ‘Any,’<ul><li>if all the replica nodes are down, the coordinator node will <strong>write the hints for all the nodes and report success to the client.</strong></li><li>However, this data will <strong>not reappear in any subsequent reads</strong> until one of the replica nodes comes back online, and the coordinator node successfully forwards the write requests to it.</li><li>This is assuming that the coordinator node is up when the replica node comes back.</li><li>This also means that we can lose our data if the coordinator node dies and never comes back. For this reason, we should avoid using the ‘Any’ consistency level</li></ul></li><li>For node offline for quite long<ul><li>Hints can build up considerably on other nodes</li><li>When it back online, other nodes tend to flood that node with write requests</li><li>It would cause issues on the node, as it is already trying to come back after a failure</li><li>To address this, Cassandra <strong>limits the storage of hints to a configurable time window</strong></li><li>By default, set the time window to 3 hours. Post that, older hints will be removed  — now the recovered nodes will have stale data<ul><li>The stale data would be fixed during the read path, it will issue a read repair when it sees the stale data</li></ul></li></ul></li><li>When the cluster cannot meet the consistency level specified by the client, Cassandra fails the write request and does not store a hint .</li></ul><h2 id="3-3-Gossiper"><a href="#3-3-Gossiper" class="headerlink" title="3.3 Gossiper"></a>3.3 Gossiper</h2><h3 id="3-3-1-How-does-Cassandra-use-Gossip-Protocol"><a href="#3-3-1-How-does-Cassandra-use-Gossip-Protocol" class="headerlink" title="3.3.1 How does Cassandra use Gossip Protocol?"></a>3.3.1 How does Cassandra use Gossip Protocol?</h3><ul><li>What’s for?<ul><li>Cassandra uses gossip protocol that allows each node to keep track of state information about the other nodes in the cluster.</li><li>It’s a Peer to Peer communication mechanism in which nodes <strong>periodically exchange state information about themselves and other nodes they know about</strong></li></ul></li><li>How it works?<ul><li>Each node initiates a gossip round every second to exchange state info about themselves with one to three other random nodes</li><li>Each gossip message has a version associated with it, so during a gossip exchange, older info is overwritten with the most current state for a particular node</li></ul></li><li>Generation number<ul><li>Each node stores a generation number which will be incremented every time a node restart</li><li>Node receiving the gossip message can compare the generation number it knows and the gossip message’s generation number</li><li>If the generation number in the gossip message is higher, it knows the node was restarted</li></ul></li><li>Seed nodes<ul><li>For node starting up for the first time</li><li>Assist in gossip convergence, thus guarantee schema/ state changes propagate regularly</li></ul></li></ul><h3 id="3-3-2-Node-Failure-Detection"><a href="#3-3-2-Node-Failure-Detection" class="headerlink" title="3.3.2 Node Failure Detection"></a>3.3.2 Node Failure Detection</h3><ul><li>Disadvantages for heartbeat<ul><li>outputs a boolean value telling us if the system is alive or not;</li><li>there is no middle ground.</li><li>Heartbeating uses a fixed timeout, and if there is no heartbeat from a server, the system, after the timeout, assumes that the server has crashed.</li><li>If we keep the timeout short, the system will be able to detect failures quickly but with many false positives due to slow machines or faulty networks.</li><li>On the other hand, if we keep the timeout long, the false positives will be reduced, but the system will not perform efficiently for being slow in detecting failures.</li></ul></li><li>Use adaptive failure detection mechanism  —— Phi Accrual Failure Detector<ul><li>Use historical heartbeat information to make the threshold adaptive</li><li>It outputs the suspicion level about a server</li><li>As a node’s suspicion level increases, the system can gradually decide to stop sending new requests to it</li><li>It makes the distributed system efficient as it takes into account fluctuations in the network env and other intermittent server issues before declaring a system completely dead</li></ul></li></ul><h2 id="3-4-Anatomy-of-Cassandra’s-Write-Operation"><a href="#3-4-Anatomy-of-Cassandra’s-Write-Operation" class="headerlink" title="3.4 Anatomy of Cassandra’s Write Operation"></a>3.4 Anatomy of Cassandra’s Write Operation</h2><p>Cassandra stores data both <strong>in memory and on disk</strong> to provide both high performance and durability. Every write includes a timestamp, write path involves a lot of components: </p><p><img src="https://i.loli.net/2021/09/17/LrMK7ckIS2zEsU1.png" alt="Write Path"></p><ul><li>Each write is appended to a commit log, which is stored on disk</li><li>It is then written to Memtable in memory</li><li>Periodically, MemTables are flushed to SSTables on the disk</li><li>Periodically, compaction runs to merge SSTables</li></ul><h3 id="3-4-1-Commit-Log"><a href="#3-4-1-Commit-Log" class="headerlink" title="3.4.1 Commit Log"></a>3.4.1 Commit Log</h3><ul><li>When a node receives a write request, it immediately writes data to a commit log</li><li>Commit log is a <strong>write ahead log</strong> stored on disk</li><li>Used as a crash recovery mechanism to support Cassandra’s durability goals</li><li>A write will not be considered successful on the node until it’s <strong>written to the commit log</strong><ul><li>This ensures if a write operation does not make it to the in-memory store, it will still be possible to recover the data</li></ul></li><li>If we shut down the node or it crashes unexpectedly, the commit log can ensure that data is not lost; that’s because if the node restart, the commit log gets replayed</li></ul><h3 id="3-4-2-MemTable"><a href="#3-4-2-MemTable" class="headerlink" title="3.4.2 MemTable"></a>3.4.2 MemTable</h3><ul><li>After written to the commit log, the data is written to a memory resident data structure called memTable<ul><li>Each node has a MemTable in memory for each Cassandra table</li><li>Each MemTable contains data for a specific Cassandra table, and it resembles that table in memory</li><li>Each MemTable accrues writes and <strong>provides reads for data not yet flushed to disk</strong></li><li>Commit log stores all the writes in sequential order, with each new write appended to the end; whereas MemTable stores data in the sorted order of partition key and clustering columns</li><li>After writing data to the commit log and MemTable, the node <strong>sends an acknowledgement to the coordinator</strong> that the data has been successfully written</li></ul></li></ul><h3 id="3-4-3-SStable"><a href="#3-4-3-SStable" class="headerlink" title="3.4.3 SStable"></a>3.4.3 SStable</h3><ul><li>When the number of objects stored in the MemTable reaches a threshold, the contents of the MemTable are <strong>flushed to disk</strong> in a file called <strong>SSTable</strong><ul><li>At this point, a new MemTable is created to store subsequent data</li><li>The flush is non blocking operation</li><li>Multiple Memtables may exist for a single table<ul><li>One current, and the rest waiting to be flushed</li></ul></li><li>When the MemTable is flushed to SStables, <strong>corresponding entries in the commit log</strong> are removed</li></ul></li><li>SStable —Sorted String Table<ul><li>Once a MemTable is flushed to disk as an SStable, it is immutable and cannot be changed later</li><li>Each delete or update is considered as a new write operation</li></ul></li><li>The current data state of a Cassandra table consists of its MemTables in memory and SSTables on the disk.<ul><li>Therefore, on reads, Cassandra will read both SSTables and MemTables to find data values, as the MemTable may contain values that have not yet been flushed to the disk.</li><li>The MemTable works like a write-back cache that Cassandra looks up by key</li></ul></li></ul><p><img src="https://i.loli.net/2021/09/17/Qd7x4M6HRrtuAoZ.png" alt="Whole Write Path"></p><h2 id="3-5-Anatomy-of-Cassandra’s-Read-Operation"><a href="#3-5-Anatomy-of-Cassandra’s-Read-Operation" class="headerlink" title="3.5 Anatomy of Cassandra’s Read Operation"></a>3.5 Anatomy of Cassandra’s Read Operation</h2><p><img src="https://i.loli.net/2021/09/17/wIZKE97YqVNAsrP.png" alt="Whole Read Path"></p><h3 id="3-5-1-Caching"><a href="#3-5-1-Caching" class="headerlink" title="3.5.1 Caching"></a>3.5.1 Caching</h3><ul><li>Row Cache<ul><li>Cache frequently read/ hot rows</li><li>Stores a complete data row, which can be returned directly to the client if requested by a read operation</li><li>Could significantly speed up read access for frequently accessed rows, at the cost of more memory usage</li></ul></li><li>Key Cache<ul><li>Stores a map of recently read partition keys to their <strong>SSTable offsets</strong></li><li>This facilitates faster read access into SSTables and improves the read performance</li><li>Use less memory comparing with row cache and provides a considerable improvement for read operations</li></ul></li><li>Chunk Cache<ul><li>Chunk Cache is used to store umcompressed chunks of data read from SSTable files that are accessed frequently</li></ul></li></ul><h3 id="3-5-2-Read-From-MemTable"><a href="#3-5-2-Read-From-MemTable" class="headerlink" title="3.5.2 Read From MemTable"></a>3.5.2 Read From MemTable</h3><ul><li>When a read request come in, node performs a binary search on the partition key to find the required partition and then return the row</li></ul><h3 id="3-5-3-Read-From-SSTable"><a href="#3-5-3-Read-From-SSTable" class="headerlink" title="3.5.3 Read From SSTable"></a>3.5.3 Read From SSTable</h3><ul><li><p>Bloom Filters</p><ul><li>Each SSTable has a Bloom Filter associated with it, which tells if a particular key is present in it or not</li><li>Used to boost performance of read operations</li><li>It’s a very fast, non deterministic algorithms for testing whether an element is a member of a set</li><li>It’s possible to get a false positive but never a false negative</li><li>Theory<ul><li>It works by <strong>mapping the values in a data set into a bit array</strong> and <strong>condensing a larger data set into a digest string</strong> with a hash function</li><li>Filters are stored in memory and are used to improve performance by reducing the need for disk access on key lookups</li></ul></li></ul></li><li><p>How are SSTables stored on the disk?</p><ul><li><p>Consists of two files</p><ul><li><p>Data File</p><ul><li>Actual data is stored here</li><li>It has partitions and rows associated with those partitions</li><li>Partitions are in sorted order</li></ul></li><li><p>Partition Index File</p><ul><li><p>Stored on disk, partition index file stores the sorted partition keys mapped to their SSTable offsets</p></li><li><p>Enable locating a partition exactly in an SSTable rather than scanning data</p><p><img src="https://i.loli.net/2021/09/17/9gUpTXZyLSksDdK.png" alt="Read via Partition Index File"></p></li></ul></li></ul></li></ul></li><li><p>Partition Index Summary File</p><ul><li><p>It’s stored in memory, stores the summary of the partition index file for performance improvement</p><ul><li><p>Two level index, e.g, search for key=19</p></li><li><p>in partition index summary file, it lays to key range 10 - 21</p></li><li><p>then we could go to byte offset 32,</p></li><li><p>in partition index file , we start from 32, to find partition key 19, and then we could go to 5450</p><p><img src="https://i.loli.net/2021/09/17/efsVEmvGAkIldF6.png" alt="Read via Partition Index Summary File"></p></li></ul></li></ul></li><li><p>Read from KeyCache</p><ul><li><p>As the Key Cache stores a map of recently read partition keys to their SSTable offset, it’s the fastest way to find the required row in the SSTable</p><p>  <img src="https://i.loli.net/2021/09/17/5KPTohGmWpecr1a.png" alt="Read From KeyCache"></p></li></ul></li><li><p>Overall workflow</p><p>  <img src="https://i.loli.net/2021/09/17/2zKlRtS48NQYkud.png" alt="Overall Workflow"></p></li></ul><h2 id="3-6-Compaction"><a href="#3-6-Compaction" class="headerlink" title="3.6 Compaction"></a>3.6 Compaction</h2><h3 id="3-6-1-Why-we-need-compaction-And-How-it-Works"><a href="#3-6-1-Why-we-need-compaction-And-How-it-Works" class="headerlink" title="3.6.1 Why we need compaction? And How it Works?"></a>3.6.1 Why we need compaction? And How it Works?</h3><p><img src="https://i.loli.net/2021/09/17/2DgirVjkeq6AI4T.png" alt="Compaction"></p><ul><li>SSTables are immutable, which helps Cassandra achieve high write speeds</li><li>And flushing from MemTable to SSTable is a continuous process, which means we could have a large number of SSTables lying on the disk</li><li>It’s tedious to scan all these SSTables while reading</li><li>We need compaction thus we could merge multiple related SSTables into a single one to improve reading speed</li><li>During compaction, the data in SSTables is merged, keys are merged, columns are combined, obsolete values are discarded, and a new index is created</li></ul><h3 id="3-6-2-Compaction-Strategies"><a href="#3-6-2-Compaction-Strategies" class="headerlink" title="3.6.2 Compaction Strategies"></a>3.6.2 Compaction Strategies</h3><ul><li>SizeTiered Compaction Strategy<ul><li>Suitable for insert-heavy and general workloads</li><li>Triggered when multiple SSTables of a similar size are present</li></ul></li><li>Leveled Compaction Strategy<ul><li>Optimize read performance</li><li>Groups SSTables into levels, each of which has a fixed size limit which is ten times larger than the previous level</li></ul></li><li>Time Window Compaction Strategy<ul><li>Work on time series data</li><li>Compact SSTables within a configured time window</li><li>Ideal for time series data which is immutable after a fixed time interval</li></ul></li></ul><h3 id="3-6-3-Sequential-Writes"><a href="#3-6-3-Sequential-Writes" class="headerlink" title="3.6.3 Sequential Writes"></a>3.6.3 Sequential Writes</h3><ul><li>Main reason that writes perform so well in Cassandra</li><li>No reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations</li><li>Compaction is intended to amortize the reorganization of data, but it uses sequential I/O to do so, which makes it efficient</li></ul><h2 id="3-7-Tombstones"><a href="#3-7-Tombstones" class="headerlink" title="3.7 Tombstones"></a>3.7 Tombstones</h2><h3 id="3-7-1-What-are-Tombstones"><a href="#3-7-1-What-are-Tombstones" class="headerlink" title="3.7.1 What are Tombstones?"></a>3.7.1 What are Tombstones?</h3><ul><li>Scenario<ul><li>We delete some data for a node that is down or unreachable, it would miss a delete</li><li>When the node com back online later and a repair occurs, the node could resurrect the data due to re-sharing it with other nodes</li><li>To prevent deleted data from being reintroduced, Cassandra used a concept of a Tombstone</li></ul></li><li>Tombstone<ul><li>Similar to the idea of soft delete from the relational database</li><li>When we delete, Cassandra does not delete it right away, instead, it associated a tombstone with it, with Time to Expiry</li><li>It’s a marker to indicate data that has been deleted</li><li>When we execute a delete operation, data is not immediately deleted</li><li>Instead, it’s treated as an update operation that places a tombstone on the value</li><li>Default Time to Expiry is set to 10 days<ul><li>If the node is down longer than this value, it should be treated as failed and replaced</li></ul></li><li>Tombstones are removed as part of compaction</li></ul></li></ul><h3 id="3-7-2-Common-problems-associated-with-Tombstones"><a href="#3-7-2-Common-problems-associated-with-Tombstones" class="headerlink" title="3.7.2 Common problems associated with Tombstones"></a>3.7.2 Common problems associated with Tombstones</h3><ul><li>Takes storage space</li><li>When a table accumulates many tombstones, read queries on that table could become slow and can cause serious performance problems like timeouts.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/09/17/IGdzKBJwD9TbZ6f.png&quot; alt=&quot;Cassandra MindMap.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introdu
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
      <category term="Cassandra" scheme="https://www.llchen60.com/tags/Cassandra/"/>
    
  </entry>
  
  <entry>
    <title>Logical Fallacies</title>
    <link href="https://www.llchen60.com/Logical-Fallacies/"/>
    <id>https://www.llchen60.com/Logical-Fallacies/</id>
    <published>2021-09-12T01:23:39.000Z</published>
    <updated>2021-09-12T01:50:35.560Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Logical-Fallacies"><a href="#Logical-Fallacies" class="headerlink" title="Logical Fallacies"></a>Logical Fallacies</h1><h2 id="Overconfidence"><a href="#Overconfidence" class="headerlink" title="Overconfidence"></a>Overconfidence</h2><ul><li>Overconfidence — wishful thinking bias<ul><li>most people think they are above avg</li><li>overestimate possibilities that they want to happen</li><li><strong>this could explain the trade in financial market</strong></li><li>overconfidence in friends and leaders</li></ul></li></ul><h2 id="Cognitive-Dissonance"><a href="#Cognitive-Dissonance" class="headerlink" title="Cognitive Dissonance"></a>Cognitive Dissonance</h2><ul><li>Cognitive Dissonance  认知失调<ul><li>this concept used to describe the mental discomfort that results from holding two conflicting beliefs, values or attitudes</li><li>People tend to seek consistency in their attitudes and perceptions, so this conflict causes feelings of unease or discomfort</li><li>This inconsistency between <strong>what people believe and how they behave</strong> motivates people to <strong>engage in actions</strong> that will help minimize feelings of discomfort</li><li>when we made decision, most people will still look for info about it, to self prove hisself right… in a lot different aspects… to make themselves happy, and to prove they are make right decision</li><li>disposition effect — gonna avoid that</li><li>what’s the causes for that?<ul><li>Forced Compliance<ul><li>Engaging in behaviors that are opposed to your own beliefs due to external expectations, often for work, school, or a social situation</li></ul></li><li>New Information</li><li>Decisions<ul><li>People make decisions both large and small, on a daily basis</li><li>When faced with two similar choices, people often are left with feelings of dissonance because both options are equally appealing</li><li>Once they make decisions, people need to find a way to <strong>reduce feelings of discomfort</strong></li><li>Accomplish by justifying why their choice was the best option so that they can believe they made the right decision</li></ul></li></ul></li></ul></li></ul><h2 id="Mental-Compartments"><a href="#Mental-Compartments" class="headerlink" title="Mental Compartments"></a>Mental Compartments</h2><ul><li>Mental compartments<ul><li>people don’t look at whole portfolio, in fact, people has two or more portfolio<ul><li>usually they have a safe part and a risky part</li></ul></li></ul></li></ul><h2 id="Attention-Anomalies"><a href="#Attention-Anomalies" class="headerlink" title="Attention Anomalies"></a>Attention Anomalies</h2><ul><li>Attention Anomalies<ul><li>We cannot pay attention to anything</li><li>Attention is fundamental aspect of human intelligence and its limits</li><li>Social Basis for attention<ul><li>We incline to pay more attention to what other s pay attention to</li></ul></li></ul></li></ul><h2 id="Anchoring"><a href="#Anchoring" class="headerlink" title="Anchoring"></a>Anchoring</h2><ul><li>Anchoring<ul><li>A tendency in ambiguous situations to allow one’s decisions to be affected by some anchor</li><li>Our subconscious will do anchoring for us, lol</li><li>subjects unaware of their own anchoring behavior</li><li>stock prices anchored to past values, or to other stock in same market</li></ul></li></ul><h2 id="Representativeness-Heuristic"><a href="#Representativeness-Heuristic" class="headerlink" title="Representativeness Heuristic"></a>Representativeness Heuristic</h2><ul><li>Representativeness Heuristic<ul><li>People judge by similarity to familiar types, without regard to <strong>base rate probabilities</strong><ul><li>For example, we describe a person as artist, and skeptical, then what’s the highest possible occupation of him/ her?<ul><li>two choice: banker, and sculptress</li><li>should be banker, cause there are so many more bank tellers than sculptresses</li></ul></li></ul></li><li>Tendency to see patterns in what is really random walk</li><li>Stock price manipulators try to create patterns to fool investors</li></ul></li></ul><h2 id="Disjunction-Effect"><a href="#Disjunction-Effect" class="headerlink" title="Disjunction Effect"></a>Disjunction Effect</h2><ul><li>inability to make decisions in advance in anticipation of future information</li></ul><h2 id="Magical-Thinking-amp-Quasi-Magical-Thinking"><a href="#Magical-Thinking-amp-Quasi-Magical-Thinking" class="headerlink" title="Magical Thinking  &amp; Quasi Magical Thinking"></a>Magical Thinking  &amp; Quasi Magical Thinking</h2><ul><li>Some coincidence lead you to build superstitious, but there are actually no karma (cause and effect)</li><li>Belief that unrelated events are causally connected despite the absence of any plausible causal link between them, particularly as a result of supernatural effects.</li><li>E.G<ul><li>For voting, though our vote actually has basically 0 possibility to influence president election, but a lot people do it</li><li>For lottery, we somehow put more money if we select the number</li></ul></li></ul><h2 id="Personality-Disorders"><a href="#Personality-Disorders" class="headerlink" title="Personality Disorders"></a>Personality Disorders</h2><ul><li>culture and social contagion — collective memory<ul><li>same effect, same memory, then similar decisions</li></ul></li><li>Antisocial Personality Disorder</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Logical-Fallacies&quot;&gt;&lt;a href=&quot;#Logical-Fallacies&quot; class=&quot;headerlink&quot; title=&quot;Logical Fallacies&quot;&gt;&lt;/a&gt;Logical Fallacies&lt;/h1&gt;&lt;h2 id=&quot;Overc
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>抗逆力</title>
    <link href="https://www.llchen60.com/%E6%8A%97%E9%80%86%E5%8A%9B/"/>
    <id>https://www.llchen60.com/%E6%8A%97%E9%80%86%E5%8A%9B/</id>
    <published>2021-08-28T19:28:59.000Z</published>
    <updated>2021-08-28T19:30:58.113Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>很庆幸听了这场讲座，回想起原先做的一些决定，会囿于固定性思维，因为害怕失败，害怕被证实不够聪明，失去了很多很多成长的机会。感觉无论是工作还是生活，在度过了刚刚开始的熟悉了解阶段以后，后期除了专业技能上的提升之外，更加重要的是mindset的提升 — 关于你是怎么看待自己的，怎么看待你经历的事情。不知道最终答案是什么，但是这篇里面说的东西至少告诉了我想要达到理想的状态，你需要每天做些什么 :)  值得过一段时间回来再看看各种action items 呀</p></blockquote><h1 id="1-心理韧性"><a href="#1-心理韧性" class="headerlink" title="1. 心理韧性"></a>1. 心理韧性</h1><ul><li>高心理韧性是成功者的共性<ul><li>因为对于任何一个成功者来说，磨难是必不可少的一部分</li><li>成功者 坚韧不拔的精神</li><li>心理学研究<ul><li>预测成功的概率 和 坚韧不拔的特质有很明显的正相关，和智商的关系反倒并不是很强</li><li>生存下来的不是最强大的生物，也不是最聪明的生物，而是最能够适应变化的生物</li></ul></li></ul></li></ul><h2 id="1-1-心理学定义"><a href="#1-1-心理学定义" class="headerlink" title="1.1 心理学定义"></a>1.1 心理学定义</h2><ul><li>复原力 resilence<ul><li>人从逆境，冲突，痛楚，失败，压力当中迅速恢复的心理能力</li></ul></li><li>坚毅力 grip</li><li>创伤后的成长 PTG — Post Traumatic Growth<ul><li>不消沉，奋进</li></ul></li></ul><h2 id="1-2-高心理韧性人的特质"><a href="#1-2-高心理韧性人的特质" class="headerlink" title="1.2 高心理韧性人的特质"></a>1.2 高心理韧性人的特质</h2><ul><li>能力<ul><li>适应力</li><li>成长力</li><li>抗挫力</li><li>积极力 — 情绪的调节的方法</li><li>关系力 — 如何建立关系</li><li>控制力 — 淡定从容，自我控制</li></ul></li><li>特质<ul><li>有积极的认知方式  — the power of positive thinking<ul><li>决定我们的幸福指数的不是事情本身，而是我们如何看待这个事情</li></ul></li><li>乐观的情绪调节</li><li>健康的身心状态</li><li>强大的自我效能感  — 感觉自己能成，感觉自己有用  lol<ul><li>结婚能让男性长寿7年 lol</li></ul></li><li>解决问题的行动精神</li><li>良好的人际关系</li></ul></li></ul><h1 id="2-如何提升心理韧性"><a href="#2-如何提升心理韧性" class="headerlink" title="2. 如何提升心理韧性"></a>2. 如何提升心理韧性</h1><h2 id="2-1-自我效能感的提升"><a href="#2-1-自我效能感的提升" class="headerlink" title="2.1 自我效能感的提升"></a>2.1 自我效能感的提升</h2><ul><li>自我效能感的提升 Self Efficacy<ul><li>定义 — 是个人对自己完成某方面工作能力的主观评估，通过两条路径体现出来</li><li>高自我效能感的人，甚至会把压力 挫折 打击当做一种证明自己的能力的机遇</li><li>体现路径<ul><li>结果预期<ul><li>相信自己，认为我可以做到，是一种自我实现的预言</li></ul></li><li>效能预期<ul><li>我认为我能做到不是因为运气好或者环境好，而是因为我的能力</li><li>因此我要施展我的能力，为结果做足准备</li></ul></li></ul></li></ul></li><li>如何去做<ul><li>做出成功的模样<ul><li>装积极，是会变成真积极的</li><li>步伐更快</li><li>说话更多</li><li>做事主动</li><li>穿衣更正式些</li><li>锻炼更频繁些</li></ul></li><li>被成功者接纳<ul><li>与积极的人同行</li><li>替代性强化<ul><li>观察者看到榜样或者他人收到强化，成功了; 从而使得自己也倾向于做出榜样的行为</li></ul></li></ul></li><li>社会支持<ul><li>进化选择的是合作者</li><li>社会网络面积越大，更容易产生优势效应</li><li><strong>弱联系的强势效应</strong><ul><li>弱联系有着很快的低成本和高效能的传播效率</li><li>在六度分隔试验当中，正是层层叠加的弱联系将世界上原本毫不相关的人联系到了一起</li></ul></li></ul></li><li>模拟实战<ul><li>预见</li><li>大脑休闲的时候处于默认模式状态  hh<ul><li>会畅想未来，是一种竞争优势的~</li><li>对于事情进行遇见，是对我们帮助很大的</li></ul></li><li>Visualization  预见想象<ul><li>将自己将要做的事情去提前想象一下</li><li>过一遍自己需要怎么做</li><li>训练越多，意向越清晰</li><li>设想遇到打击，困难的时候你要怎么做</li></ul></li></ul></li><li>不断积累成功<ul><li>人最可怕的是发现自己一成不变</li><li>要去做</li></ul></li></ul></li></ul><h2 id="2-2-培养成长性思维"><a href="#2-2-培养成长性思维" class="headerlink" title="2.2 培养成长性思维"></a>2.2 培养成长性思维</h2><ul><li><p>人的思维模式</p><ul><li>成长性思维  Growth Mindset<ul><li>天赋只是起点</li><li>态度和努力可以决定一切</li><li>可以学会任何我想学会的东西</li><li>喜欢自我挑战</li><li>当我失败的时候，我学到了很多东西</li><li>我希望你表扬我很努力</li><li>如果别人成功了，我会收到别人的启发</li></ul></li><li>固定性思维 — 卓越的包袱<ul><li>我的聪明才智决定了一切</li><li>我擅长某些事，不擅长另外一些事</li><li>我不想尝试我可能不擅长的东西</li><li>如果我失败了，我就无地自容了</li><li>我希望你表扬我很聪明</li><li>如果别人成功了，他会威胁到我</li></ul></li></ul></li><li><p>固定性思维对于人的影响很大</p><ul><li>你会因为认为自己聪明，不敢做更大的挑战，因为一旦失败，你会害怕别人认为你不聪明了 会越来越难达到别人的预期的</li><li>被表扬努力的往往会选择更加困难的任务，也会更愿意通过学习，去尝试解决方案</li><li>卓越的包袱<ul><li>装酷的孩子的包袱</li><li>不愿意去冒险，不愿意去奋斗</li><li>努力愚蠢，装聪明</li><li>精英父母的过高的期望造成的心理压力和心理阴影</li><li>优秀女孩的诅咒，这种包袱往往对女孩的打击更大，她们往往更在意外在的评价，不敢冒险和努力</li><li>we are supposed to be dumb all the way, hhh</li></ul></li></ul></li><li><p>如何培养成长性思维</p><ul><li>改变考核的标准<ul><li>关注进步，而不是结果</li></ul></li><li>改变沟通的方式<ul><li>在评价表现的时候，用暂时不行代替就是不行</li><li>短暂 局部 可以改的</li><li>不要把事情说成稳定的长期的不可改变的</li><li>not yet instead of failed</li></ul></li><li>改变认知的习惯 — Albert Ellis 的认知治疗ABC</li><li>发挥辩证思维的优势 — 从负面体验中吸取成功的经验<ul><li>当一个人出于自我保护而抗拒内心的地狱的时候，他一并切断了通往内在天堂的道路。<ul><li>不承认自己内心的阴暗龌龊，那么就无从改进了</li></ul></li></ul></li></ul></li></ul><ul><li>认知治疗ABC<ul><li>构成<ul><li>A — Activating Events  诱发刺激</li><li>B — Beliefs  信念反应</li><li>C — Consequences  行为后果</li></ul></li><li>原理<ul><li>我们是改不了A的，但是我们可以改B，然后C就会发生变化！！</li><li>关键是你怎么看待A的 ！ 改变认知</li><li>真正困扰我们的并不是发生在我们身上的事情，而是我们围绕这个事情对它编织的故事，和由此引起的身心反应</li></ul></li></ul></li><li>情绪的ABCD理论 — 对于孩子而言<ul><li>出现了ABC以后，给一个机会让其反驳</li><li>让孩子去反驳他当时的念头</li><li>干预B  从而干预C</li></ul></li></ul><h2 id="2-3-提高自我调控的能力"><a href="#2-3-提高自我调控的能力" class="headerlink" title="2.3 提高自我调控的能力"></a>2.3 提高自我调控的能力</h2><ul><li>延迟满足， 自我控制</li><li>自我调控能力是可以锻炼从而获得提升的</li><li>如何进行训练<ul><li>体育锻炼</li><li>正念冥想 — 做事情沉浸其中就好啊！！</li><li>自我挑战</li><li>目标想象</li><li>有效休息</li><li>积极心态</li></ul></li></ul><h1 id="3-组织韧性"><a href="#3-组织韧性" class="headerlink" title="3. 组织韧性"></a>3. 组织韧性</h1><ul><li><p>复原力</p><ul><li>企业遇到困难后，如何回归正常</li></ul></li><li><p>复原后的发展能力</p></li><li><p>影响组织韧性的维度</p><ul><li>组织资本<ul><li>人力资源的保障<ul><li>什么政策</li></ul></li></ul></li><li>组织承诺<ul><li>员工对于组织的感情</li><li>信任</li></ul></li><li>组织领导<ul><li>leader本身的态度，思考，韧性</li></ul></li><li>组织学习</li><li>组织文化<ul><li>组织的传统和信仰</li></ul></li><li>社会网络</li></ul></li><li><p>提升组织韧性的方式</p><ul><li>Staff  选择心理韧性高的人才，锻炼心理韧性<ul><li>积极的自我认识</li><li>提倡积极的思维</li><li>加强关系建设</li><li>未来导向</li><li>乐观主义精神<ul><li>对于路径的乐观</li><li>对于结果的乐观</li></ul></li></ul></li><li>System 创造积极的心理健康环境</li><li>Skill</li></ul></li></ul><h1 id="4-压力的应对技巧"><a href="#4-压力的应对技巧" class="headerlink" title="4. 压力的应对技巧"></a>4. 压力的应对技巧</h1><h2 id="4-1-压力的应激反应"><a href="#4-1-压力的应激反应" class="headerlink" title="4.1 压力的应激反应"></a>4.1 压力的应激反应</h2><ul><li>应激反应的三轴心<ul><li>下丘脑</li><li>垂体</li><li>肾上腺</li></ul></li><li>三个器官会释放压力激素，使得我们的反应是fight or flight lol</li><li>而后激素水平下降</li><li>各种情绪<ul><li>焦虑  为未来的恐慌</li><li>抑郁  为过去伤心</li><li>自残</li></ul></li></ul><h2 id="4-2-与情绪有关的脑区"><a href="#4-2-与情绪有关的脑区" class="headerlink" title="4.2 与情绪有关的脑区"></a>4.2 与情绪有关的脑区</h2><ul><li><p>杏仁核  Amygdala</p><ul><li>会影响我们的情绪</li><li>情绪不好的时候会让杏仁核充血，然后杏仁核温度升高</li></ul></li><li><p>大脑皮层 Cerebral Cortex</p></li><li><p>如何应对压力 — <strong>抑制</strong>杏仁核的活动</p><ul><li>吸入凉气，降低杏仁核的温度  hhh</li><li>香气  让我们产生愉悦的感觉</li><li>写写日记</li></ul></li><li><p>激活大脑的奖励中枢</p><ul><li><p>神经元之间的间隙 靠神经递质连接</p></li><li><p>当奖赏中枢释放神经递质的时候，会释放积极的情绪</p></li><li><p>！！心理活动不是一个一个点，而是一片一片的产生的</p></li><li><p>多巴胺</p><ul><li>庆祝自己的成功 — 让自己的成功和快乐的体验延续一段时间<ul><li>将快乐的体验延续4分钟，就可以在大脑中形成记忆，从而形成一个快乐的神经网络</li></ul></li><li>做自己喜欢做的事情</li><li>享受艺术的美妙</li></ul></li><li><p>血清素</p><ul><li>能够振奋人的心情</li><li>什么时候会分泌<ul><li>体验到自我的价值</li><li>帮助别人的时候</li><li>自尊心的呵护<ul><li>保护自尊心  体现其价值</li></ul></li></ul></li><li>一些行为<ul><li>晒太阳~</li></ul></li></ul></li><li><p>内啡肽</p><ul><li>只有我们身心痛苦的时候，才会释放</li><li>行为<ul><li>有规律的运动</li><li>先苦后甜的体验</li><li>看喜剧，相声~ 烧脑的幽默</li></ul></li></ul></li><li><p>催产素</p><ul><li>男人也有催产素</li><li>主要作用是增加人的爱的感受，而不是为了怀孕</li><li>行为<ul><li>夸奖，赞美</li><li>陪伴</li></ul></li></ul><h2 id="4-3-应对压力的长期策略"><a href="#4-3-应对压力的长期策略" class="headerlink" title="4.3 应对压力的长期策略"></a>4.3 应对压力的长期策略</h2></li><li><p>压力容易让人失控失常</p></li><li><p>培养应对压力的积极习惯</p><ul><li>strength based approach  发挥自己的长处优势  找到自己的优势，然后充分在工作生活当中使用  使用自己的优势！！！</li><li>找到自己的流  find your flow<ul><li>喜欢做的事情，进入心流状态</li></ul></li><li>借助一些科学方法，自修，同修，专修<ul><li>reading &amp; learning</li></ul></li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>彭凯平 演讲  — 抗逆力— 重压下的心理韧性与成功</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;很庆幸听了这场讲座，回想起原先做的一些决定，会囿于固定性思维，因为害怕失败，害怕被证实不够聪明，失去了很多很多成长的机会。感觉无论是工作还是生活，在度过了刚刚开始的熟悉了解阶段以后，后期除了专业技能上的提升之外，更加重要的是mindset的提升 —
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="积极心理学" scheme="https://www.llchen60.com/tags/%E7%A7%AF%E6%9E%81%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Make Body Language Your Super Power</title>
    <link href="https://www.llchen60.com/Make-Body-Language-Your-Super-Power/"/>
    <id>https://www.llchen60.com/Make-Body-Language-Your-Super-Power/</id>
    <published>2021-08-28T02:24:14.000Z</published>
    <updated>2021-08-28T02:25:19.176Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Stand Strong</li><li>Gesture Effectively</li><li>Mind Your Audience</li></ul><h1 id="1-Posture"><a href="#1-Posture" class="headerlink" title="1. Posture"></a>1. Posture</h1><ul><li>Communication begins before you open your mouth</li><li>How to stand<ul><li>no<ul><li>hands in the pocket  — cannot convey strong msg</li><li>hands in the hip — tend to look overwhelming and powerful</li><li>hands in front of family jewels lol</li></ul></li><li>yes<ul><li>base posture — feet should be shoulder width apart<ul><li>that’s the first impression</li></ul></li><li>movement<ul><li>give</li><li>show</li><li>chop — strong msg</li></ul></li><li>palms up has better impact! comparing with palm down and pointing</li></ul></li></ul></li><li>Where to stand<ul><li>face your audience</li><li>move around in the center box</li><li>get rid of potential distraction<ul><li>like window, by nature we are attracted by moving thing, will break the concentration</li></ul></li></ul></li></ul><h1 id="2-Audience"><a href="#2-Audience" class="headerlink" title="2. Audience"></a>2. Audience</h1><ul><li>Speaker need to understand what audience is doing , make sure we are all in the journey</li><li>How to engage with audience more<ul><li>gesture</li><li>notice<ul><li>how your audience sitting</li><li>eye contact</li></ul></li><li>surprise<ul><li>cold call, lol</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.youtube.com/watch?v=cFLjudWTuGQ&ab_channel=StanfordGraduateSchoolofBusiness">https://www.youtube.com/watch?v=cFLjudWTuGQ&amp;ab_channel=StanfordGraduateSchoolofBusiness</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Stand Strong&lt;/li&gt;
&lt;li&gt;Gesture Effectively&lt;/li&gt;
&lt;li&gt;Mind Your Audience&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;1-Posture&quot;&gt;&lt;a href=&quot;#1-Posture&quot; class=&quot;hea
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Distributed Messaging System: Kafka</title>
    <link href="https://www.llchen60.com/Distributed-Messaging-System-Kafka/"/>
    <id>https://www.llchen60.com/Distributed-Messaging-System-Kafka/</id>
    <published>2021-08-24T17:07:33.000Z</published>
    <updated>2021-08-24T17:10:35.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview-of-Messaging-Systems"><a href="#1-Overview-of-Messaging-Systems" class="headerlink" title="1. Overview of Messaging Systems"></a>1. Overview of Messaging Systems</h1><h2 id="1-1-Why-we-need-a-messaging-system"><a href="#1-1-Why-we-need-a-messaging-system" class="headerlink" title="1.1 Why we need a messaging system"></a>1.1 Why we need a messaging system</h2><ul><li><p>Aim:</p><ul><li>Reliably transfer a high throughput of messages between different entities</li></ul></li><li><p>Challenges</p><ul><li>how we handle a spike of messages</li><li>how we divide the work among a set of instances</li><li>how could we receive messages from different types of sources</li><li>what will happen if the service is down?</li></ul></li><li><p>We need messaging systems in distributed architecture due to challenges above</p></li></ul><h2 id="1-2-What-is-a-messaging-system"><a href="#1-2-What-is-a-messaging-system" class="headerlink" title="1.2 What is a messaging system?"></a>1.2 What is a messaging system?</h2><ul><li><p>responsible for transferring data among services /applications/ processes/ servers</p></li><li><p>help decouple different parts of a distributed system by providing an asynchronous way of transferring messaging between the sender and the receiver</p></li><li><p>Two common ways to handle messages</p><ul><li>Queuing<ul><li>msgs are stored sequentially in a queue</li><li>producers push msg to the rear of the queue</li><li>consumers extract the msgs from the front of the queue</li><li>a particular msg can be consumed by a <strong>max of one consumer</strong> only</li></ul></li><li>Publish - Subscribe<ul><li>messages are divided into topics</li><li>a publisher sends a message to a topic</li><li>subscribers subscribe to a topic to receive every message published to that topic</li><li>msg system that stores and maintains the msg named as <strong>message broker</strong></li></ul></li></ul></li></ul><h1 id="2-Kafka"><a href="#2-Kafka" class="headerlink" title="2. Kafka"></a>2. Kafka</h1><h2 id="2-1-General"><a href="#2-1-General" class="headerlink" title="2.1 General"></a>2.1 General</h2><ul><li><strong>publish subscribe based</strong> messaging system</li><li>takes streams of messages from applications known as producers, stores them reliably on a central cluster, and allows those messages to be received by applications that process the messages</li><li>kafka is mainly used for<ul><li>reliably storing a huge amount of data</li><li>enabling high throughput of message transfer between different entities</li><li>streaming real time data</li></ul></li><li>kafka is a distributed commit log — write ahead log<ul><li>append-only data structure that can <strong>persistently store a sequence of records</strong></li><li>all messages are stored <strong>on disk</strong></li><li>since all reads and writes happen <strong>in sequence</strong>, Kafka takes advantage of <strong>sequential disk reads</strong></li></ul></li></ul><h2 id="2-2-Use-Cases"><a href="#2-2-Use-Cases" class="headerlink" title="2.2 Use Cases"></a>2.2 Use Cases</h2><ul><li>Metrics<ul><li>collect and aggregate monitoring data</li></ul></li><li>Log Aggregation<ul><li>collect logs from multiple sources and make them available in a standard format to multiple consumers</li></ul></li><li>Stream Processing<ul><li>the raw data consumed from a topic is transformed, enriched, or aggregated and pushed to a <strong>new topic</strong> for further consumption. This way of data processing is known as stream processing.</li></ul></li><li>Commit Log<ul><li>can be used as an external commit log for any distributed system</li><li>Distributed services can log their transactions to Kafka to keep track of what is happening. This transaction data can be used for replication between nodes and also becomes very useful for disaster recovery, for example, to help failed nodes to recover their states.</li></ul></li><li>Website activity tracking<ul><li>Build a user activity tracking pipeline</li><li>User activities like page clicks, searches, etc., are published to Kafka into separate topics. These topics are available for subscription for a range of use cases, including real-time processing, real-time monitoring, or loading into Hadoop or data warehousing systems for offline processing and reporting</li></ul></li><li>Product Suggestion</li></ul><h1 id="3-High-Level-Architecture"><a href="#3-High-Level-Architecture" class="headerlink" title="3. High Level Architecture"></a>3. High Level Architecture</h1><h2 id="3-1-Common-Terms"><a href="#3-1-Common-Terms" class="headerlink" title="3.1 Common Terms"></a>3.1 Common Terms</h2><ul><li>Brokers<ul><li>A Kafka server</li><li>responsible for reliably storing data provided by the producers and making it available to the consumers</li></ul></li><li>Records<ul><li>A message or an event that get stored in Kafka</li><li>A record contains<ul><li>key</li><li>value</li><li>timestamp</li><li>optional metadata headers</li></ul></li></ul></li><li>Topics<ul><li>Messages are divided into categories called topics</li><li>Each msg that Kafka receives from a producer is associated with a topic</li><li>consumers can subscribe to a topic to get notified when new messages are added to the topic</li><li>a topic can have multiple subscribers that read messages from it</li><li>a topic is identified by its name and must be unique</li><li>mes in a topic can be read as often as needed — message are not deleted after consumption, instead, Kafka <strong>retains messages for a configurable amount of time or until a storage size is exceeded</strong></li></ul></li><li>Producers<ul><li>Applications that publish or write records to Kafka</li></ul></li><li>Consumers<ul><li>Applications that subscribe to read and process data from Kafka topics</li><li>Consumers subscribe to one or more topics and consume published messages by pulling data from the brokers</li><li>In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for</li></ul></li></ul><h2 id="3-2-Architecture"><a href="#3-2-Architecture" class="headerlink" title="3.2 Architecture"></a>3.2 Architecture</h2><p><img src="https://i.loli.net/2021/08/25/Chuwtvg4mNfFU58.png" alt="Overall Architecture"></p><ul><li>Kafka cluster<ul><li>Kafka is run as a cluster of one or more servers, where each server is responsible for running one Kafka broker</li></ul></li><li>ZooKeeper<ul><li>Distributed key value store</li><li>Used for coordination and storing configurations</li><li>Kafka uses ZooKeeper to coordinate between Kafka brokers; ZooKeeper maintains metadata information about the Kafka cluster</li></ul></li></ul><h2 id="3-3-Performance-concern"><a href="#3-3-Performance-concern" class="headerlink" title="3.3 Performance concern"></a>3.3 Performance concern</h2><h3 id="3-3-1-Storing-messages-to-disks"><a href="#3-3-1-Storing-messages-to-disks" class="headerlink" title="3.3.1 Storing messages to disks"></a>3.3.1 Storing messages to disks</h3><ul><li>there is a huge difference in disk performance between <strong>random block access and sequential access</strong>. Random block access is slower because of <strong>numerous disk seeks</strong>, whereas the sequential nature of writing or reading, enables disk operations to be <strong>thousands of times faster</strong> than random access.</li><li>OS level optimization<ul><li>Read Ahead — prefetch large block multiples</li><li>Write Behind — group small logical writes into big physical writes</li><li>PageCache — cache the disk in free RAM</li></ul></li><li>Zero Copy optimization<ul><li>OS copy data from the pageCache directly to a socket, effectively bypassing the kafka broker application entirely</li></ul></li><li>Kafka protocol to group msg together<ul><li>reduce network overhead</li></ul></li></ul><h1 id="4-Dive-Deep-in-Kafka-Cluster"><a href="#4-Dive-Deep-in-Kafka-Cluster" class="headerlink" title="4. Dive Deep in Kafka Cluster"></a>4. Dive Deep in Kafka Cluster</h1><h2 id="4-1-Topic-Partitions"><a href="#4-1-Topic-Partitions" class="headerlink" title="4.1 Topic Partitions"></a>4.1 Topic Partitions</h2><ul><li><p>Topics are partitioned, spread over a number of fragments</p></li><li><p>Each partition can be placed on a separate Kafka broker</p></li><li><p>A new message get appended to one of the topic’s partition</p><ul><li>producer controls which partition it publishes to based on the data</li></ul></li><li><p>One partition is an <strong>ordered sequence</strong> of messages</p><ul><li>producers continually append new messages to partition</li><li>ordering of messages is <strong>maintained at the partition level, not across the topic</strong></li></ul></li><li><p>Unique sequence ID — offset</p><ul><li>It will get assigned to every message that enters a partition</li><li>used to identify every message’s sequential position within a topic’s partition</li><li>offset sequences are unique only to each partition</li><li>to locate a specific message<ul><li>topic</li><li>partition</li><li>offset number</li></ul></li><li>producers can choose to publish a message to any partition<ul><li>if ordering within a partition is not needed, a round robin partition strategy can be used</li><li>Placing each partition on separate Kafka brokers enables multiple consumers to read from a topic in parallel. That means, different consumers can concurrently read different partitions present on separate brokers</li></ul></li></ul></li><li><p>Messages once written to partitions are immutable and cannot be updated.</p></li><li><p>Kafka guarantees that messages with the same key are written to the same partition.</p></li></ul><h2 id="4-2-Dumb-Broker-and-Smart-Consumer"><a href="#4-2-Dumb-Broker-and-Smart-Consumer" class="headerlink" title="4.2 Dumb Broker and Smart Consumer"></a>4.2 Dumb Broker and Smart Consumer</h2><ul><li>Kafka does not keep track of what records are read by the consumer</li><li>Consumers themselves poll kafka for new messages and say what records they want to read<ul><li>this allow them to increment/ decrement the offset they are as they wish</li></ul></li></ul><h2 id="4-3-Leader-and-Follower"><a href="#4-3-Leader-and-Follower" class="headerlink" title="4.3 Leader and Follower"></a>4.3 Leader and Follower</h2><p>Every topic can be replicated to multiple Kafka brokers to make the data fault-tolerant and highly available. Each topic partition has one leader broker and multiple replica (follower) brokers. </p><ul><li>Structure<ul><li>the broker cluster could have multiple brokers, each broker could have multiple partitions which belong to different topics</li><li>Each topic partition would have one lead broker and multiple replica brokers</li></ul></li></ul><h3 id="4-3-1-Leader"><a href="#4-3-1-Leader" class="headerlink" title="4.3.1 Leader"></a>4.3.1 Leader</h3><ul><li>A leader is the node responsible for all reads and writes for the given partition</li><li>Each partition has one kafka broker acting as a leader</li></ul><h3 id="4-3-2-Follower"><a href="#4-3-2-Follower" class="headerlink" title="4.3.2 Follower"></a>4.3.2 Follower</h3><ul><li><p>To handle single point of failure, Kafka replicate partitions and distribute them across multiple broker servers called followers.</p></li><li><p>Each follower’s responsibility is to replicate the leader’s data to serve as a backup partition</p><ul><li><p>any follower can take over the leadership if the leader goes down</p><ul><li><p>from the image below, you could see only the leader take read and write requests, follower acts as replica but not take any read and write reqeusts</p><p><img src="https://i.loli.net/2021/08/25/4t7kVJfv3jliZyK.png" alt="Leader and Follower"></p></li></ul></li></ul></li></ul><h3 id="4-3-3-In-Sync-Replicas"><a href="#4-3-3-In-Sync-Replicas" class="headerlink" title="4.3.3 In Sync Replicas"></a>4.3.3 In Sync Replicas</h3><ul><li>In Sync Replicas means the broker has the latest data for a given partition</li><li>A leader is always an in sync replica</li><li>A follower is an in sync replica only if it has fully caught up to the partition it is following</li><li>Only ISRs are eligible to become partition leaders.</li><li>Kafka can choose the minimum number of ISRs required before the data becomes available for consumers to read</li></ul><h3 id="4-3-4-High-Water-mark"><a href="#4-3-4-High-Water-mark" class="headerlink" title="4.3.4 High Water mark"></a>4.3.4 High Water mark</h3><ul><li><p>To ensure data consistency, the leader broker never returns (or exposes) messages which have not been replicated to a minimum set of ISRs</p></li><li><p>For this, brokers keep track of the high-water mark, which is the highest offset that all ISRs of a particular partition share</p></li><li><p>The leader exposes data only up to the high-water mark offset and propagates the high-water mark offset to all followers</p><p>  <img src="https://i.loli.net/2021/08/25/pyDKzGCRow6O2Wu.png" alt="High Water Mark"></p></li></ul><h1 id="5-Consumer-Group"><a href="#5-Consumer-Group" class="headerlink" title="5. Consumer Group"></a>5. Consumer Group</h1><ul><li>A set of one or more consumers working together in parallel to consume messages from topic partitions, messages are equally divided among all the consumers of a group. with no two consumers receiving the same message</li></ul><h2 id="5-1-How-to-distribute-a-specific-message-to-only-a-single-consumer"><a href="#5-1-How-to-distribute-a-specific-message-to-only-a-single-consumer" class="headerlink" title="5.1 How to distribute a specific message to only a single consumer"></a>5.1 How to distribute a specific message to only a single consumer</h2><ul><li><p>only a single consumer reads messages from any partition within a consumer group</p><ul><li>means only one consumer can work on a partition in a consumer group at a time</li><li>every time a consumer is added to or removed from a group, the consumption is rebalanced within the group</li></ul></li><li><p>with consumer groups, consumers can be parallelized so that multiple consumers can read from multiple partitions on a topic, allowing a very high message processing throughput</p></li><li><p>number of partitions impacts consumers’ maximum parallelism. as there cannot be more consumers than partitions</p></li><li><p>Kafka stores the <strong>current offset per consumer group per topic per partition</strong>, as it would for a single consumer. This means that unique messages are only sent to a single consumer in a consumer group, and the load is balanced across consumers as equally as possible</p></li><li><p><strong>Number of consumers in a group = number of partitions:</strong> each consumer consumes one partition.</p></li><li><p><strong>Number of consumers in a group &gt; number of partitions:</strong> some consumers will be idle.</p></li><li><p><strong>Number of consumers in a group &lt; number of partitions:</strong> some consumers will consume more partitions than others.</p></li></ul><h1 id="6-Kafka-Workflow"><a href="#6-Kafka-Workflow" class="headerlink" title="6. Kafka Workflow"></a>6. Kafka Workflow</h1><h2 id="6-1-Pub-sub-messaging"><a href="#6-1-Pub-sub-messaging" class="headerlink" title="6.1 Pub sub messaging"></a>6.1 Pub sub messaging</h2><ul><li>Producer publish messages on a topic</li><li>Kafka broker stores messages in the partitions configured for that particular topic.<ul><li>If the producer did not specify the partition in which the msg should be stored, the broker ensures that the msg are equally shared between partitions</li><li>If the producer sends two msgs and there are two partitions, Kafka will store those two in two partitions separately.</li></ul></li><li>consumer subscribe to a specific topic</li><li>Kafka will provide the current offset of the topic to the consumer and also saves that offset in the zookeeper</li><li>consumer request kafka at regular intervals for new msgs</li><li>once kafka receives msg from producers, it forward these messages to the consumer</li><li>consumer will receive msg and process it</li><li>once processed, consumer will send an acknowledgement to the kafka broker</li><li>upon receiving the acknowledgement, kafka <strong>increments the offset and updates it in the zooKeeper</strong><ul><li>this info is stored in zooKeeper, thus consumer could read the next msg correctly even during broker outages</li></ul></li><li>consumers can rewind/ skip to the desired offset of a topic at any time and read all the subsequent messages</li></ul><h2 id="6-2-Kafka-workflow-for-consumer-group"><a href="#6-2-Kafka-workflow-for-consumer-group" class="headerlink" title="6.2 Kafka workflow for consumer group"></a>6.2 Kafka workflow for consumer group</h2><ul><li>Producers publish messages on a topic.</li><li>Kafka stores all messages in the partitions configured for that particular topic, similar to the earlier scenario.</li><li>A single consumer subscribes to a specific topic, assume <code>Topic-01</code> with Group ID as <code>Group-1</code>.</li><li>Kafka interacts with the consumer in the same way as pub-sub messaging until a new consumer subscribes to the same topic, <code>Topic-01</code>, with the same Group ID as <code>Group-1</code>.</li><li>Once the new consumer arrives, Kafka switches its operation to share mode, such that each message is passed to only one of the subscribers<br>of the consumer group <code>Group-1</code>. This message transfer is<br>similar to queue-based messaging, as only one consumer of the group<br>consumes a message. Contrary to queue-based messaging, messages are not<br>removed after consumption.</li><li>This message transfer can go on until the number of consumers<br>reaches the number of partitions configured for that particular topic.</li><li>Once the number of consumers exceeds the number of partitions, the<br>new consumer will not receive any message until an existing consumer<br>unsubscribes. This scenario arises because each consumer in Kafka will<br>be assigned a minimum of one partition. Once all the partitions are<br>assigned to the existing consumers, the new consumers will have to wait.</li></ul><h1 id="7-ZooKeeper"><a href="#7-ZooKeeper" class="headerlink" title="7. ZooKeeper"></a>7. ZooKeeper</h1><h2 id="7-1-What-is-ZooKeeper"><a href="#7-1-What-is-ZooKeeper" class="headerlink" title="7.1 What is ZooKeeper"></a>7.1 What is ZooKeeper</h2><ul><li>A distributed configuration and synchronization service</li><li>In Kafka case, help to store basic metadata<ul><li>information about brokers</li><li>topics</li><li>partitions</li><li>partition leader/ followers</li><li>consumer offsets</li></ul></li></ul><h2 id="7-2-Act-as-central-coordinator"><a href="#7-2-Act-as-central-coordinator" class="headerlink" title="7.2 Act as central coordinator"></a>7.2 Act as central coordinator</h2><p>ZooKeeper is used for storing all sorts of metadata about the Kafka cluster:</p><ul><li>It maintains the <strong>last offset position</strong> of each consumer group per partition, so that consumers can quickly recover from the last position in case of a failure (although modern clients store offsets in a<br>separate Kafka topic).</li><li>It tracks the topics, number of partitions assigned to those topics, and leaders’/followers’ location in each partition.</li><li>It also manages the access control lists (ACLs) to different topics in the cluster. ACLs are used to enforce access or authorization.</li></ul><h2 id="7-3-How-to-find-leaders"><a href="#7-3-How-to-find-leaders" class="headerlink" title="7.3 How to find leaders"></a>7.3 How to find leaders</h2><ul><li>The producer connects to any broker and asks for the leader of Partition 1<ul><li>each broker contains metadata</li><li>each brokers will talk to zooKeeper to get the latest metadata</li></ul></li><li>The broker responds with the identification of the leader broker responsible for partition 1</li><li>The producer connects to the leader broker to publish the message</li></ul><h1 id="8-Controller-Broker"><a href="#8-Controller-Broker" class="headerlink" title="8. Controller Broker"></a>8. Controller Broker</h1><ul><li>Within the Kafka cluster, one broker will be elected as the Controller</li><li>Responsibility<ul><li>admin operations<ul><li>creating/ deleting a topic</li><li>adding partitions</li><li>assigning leaders to partitions</li><li>monitoring broker failures</li></ul></li><li>check the health of other brokers in the system periodically</li><li>communicates the result of the partition leader election to other brokers in the system</li></ul></li></ul><h2 id="8-1-Split-brain-issue"><a href="#8-1-Split-brain-issue" class="headerlink" title="8.1 Split brain issue"></a>8.1 Split brain issue</h2><ul><li>some controller has temporary issue, during the period, we assign a new controller, but the previous one auto recover, so we have two controllers and it could bring inconsistency easily.</li><li>Solution:<ul><li>Generation Clock<ul><li>simply a monotonically increasing number to indicate a server’s generation</li><li>If the old leader had an epoch number of ‘1’, the new one would have ‘2’.</li><li>This epoch is included in every request that is sent from the Controller to other brokers.</li><li>This way, brokers can now easily differentiate the real Controller by simply trusting the Controller with the highest number.</li><li>The Controller with the highest number is undoubtedly the latest one, since the epoch number is always increasing.</li><li>This epoch number is stored in ZooKeeper.</li></ul></li></ul></li></ul><h1 id="9-Delivery-Semantics"><a href="#9-Delivery-Semantics" class="headerlink" title="9. Delivery Semantics"></a>9. Delivery Semantics</h1><h2 id="9-1-Producer-Delivery-Semantics"><a href="#9-1-Producer-Delivery-Semantics" class="headerlink" title="9.1 Producer Delivery Semantics"></a>9.1 Producer Delivery Semantics</h2><ul><li>How can a producer know that the data is successfully stored at the leader or that the followers are keeping up with the leader</li><li>Kafka offers three options to denote the <strong>number of brokers</strong> that <strong>must receive the record</strong> before the <strong>producer considers the write as successful</strong><ul><li>Async<ul><li>Producer sends a msg to kafka and does not wait for acknowledgement from the server</li><li>fire-and-forget approach gives the best performance as we can write data to Kafka at network speed, but <strong>no guarantee can be made</strong> that the server has received the record in this case.</li></ul></li><li>Committed to Leader<ul><li>Producer waits for an acknowledgment from the leader.</li><li>This ensures that the data is committed at the leader; it will be slower than the ‘Async’ option, as the data has to be written on disk on the leader.</li><li>Under this scenario, the leader will respond without waiting for acknowledgments from the followers.</li><li>In this case, the record <strong>will be lost if the leader crashes immediately after acknowledging the producer but before the followers have replicated it</strong>.</li></ul></li><li>Committed to Leader and Quorum<ul><li>Producer waits for an acknowledgment from the leader and the quorum. This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This will be the slowest write but guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee.</li></ul></li></ul></li></ul><h2 id="9-2-Consumer-Delivery-Semantics"><a href="#9-2-Consumer-Delivery-Semantics" class="headerlink" title="9.2 Consumer Delivery Semantics"></a>9.2 Consumer Delivery Semantics</h2><ul><li>Ways to provide consistency to the consumer<ul><li>At most once<ul><li>Message may be lost but are never redelivered</li><li>Under this option, the consumer upon receiving a message, commit (or increment) the offset to the broker. Now, if the consumer crashes before fully consuming the message, that message will be lost, as when the consumer restarts, it will receive the next message from the last committed offset.</li></ul></li><li>At least once<ul><li>Messages are never lost but maybe redelivered</li><li>This scenario occurs when the consumer receives a message from Kafka, and it does not immediately commit the offset.</li><li>Instead, it waits till it completes the processing.</li><li>So, if the consumer crashes after processing the message but before committing the offset, it has to reread the message upon restart.</li><li>Since, in this case, the consumer never committed the offset to the broker, the broker will redeliver the same message. Thus, duplicate message delivery could happen in such a scenario.</li></ul></li><li>Exactly once<ul><li>It is very hard to achieve this unless the consumer is working with a transactional system.</li><li>Under this option, the consumer puts the message processing and the offset increment in one transaction.</li><li>This will ensure that the offset increment will happen only if the whole transaction is complete.</li><li>If the consumer crashes while processing, the transaction will be rolled back, and the offset will not be incremented. When the consumer restarts, it can reread the message as it failed to process it last time. This option leads to no data duplication and no data loss but can lead to decreased throughput.</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview-of-Messaging-Systems&quot;&gt;&lt;a href=&quot;#1-Overview-of-Messaging-Systems&quot; class=&quot;headerlink&quot; title=&quot;1. Overview of Messaging Syste
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>货运代理法律风险</title>
    <link href="https://www.llchen60.com/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E6%B3%95%E5%BE%8B%E9%A3%8E%E9%99%A9/"/>
    <id>https://www.llchen60.com/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E6%B3%95%E5%BE%8B%E9%A3%8E%E9%99%A9/</id>
    <published>2021-08-17T04:29:17.000Z</published>
    <updated>2021-08-19T23:01:27.883Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-货运代理法律风险基本内容"><a href="#1-货运代理法律风险基本内容" class="headerlink" title="1. 货运代理法律风险基本内容"></a>1. 货运代理法律风险基本内容</h1><h2 id="1-1-国际货运代理人定义"><a href="#1-1-国际货运代理人定义" class="headerlink" title="1.1 国际货运代理人定义"></a>1.1 国际货运代理人定义</h2><ul><li>历史视角上来看<ul><li>国际货运代理经历了由显明代理人到隐名代理人再到<strong>运输合同当事人</strong>的过程  Fright Agency —→ Freight Forwarder</li><li>研究内容从单纯的代理法律关系向运输法律关系扩展</li><li>现代货运代理的定义 1992 《货运代理》<ul><li>提供并安排货物运输以取得报酬，</li><li>或者为货物合并拼箱并承担将这些货物由收货地运至目的地的<strong>运输责任</strong></li></ul></li><li>2002 《货运代理法》  Legal Classification of Fright Forwarders<ul><li>奠定了国际货运代理制度的FIATA的立法模式下，以货物运输合同的当事人 (as principal) 和非当事人 (except as principal) 来区分国际货物代理企业作为<strong>契约承运人和纯粹代理人</strong>的情况</li></ul></li><li>FIATA (International Federation of Freight Forwarders Association)《国际货运代理业示范法》<ul><li>国际货运代理人是指与客户达成货运代理协定，为其提供各类运输相关服务及其他辅助和咨询服务，或者在前述服务之外还以使用自有运输工具或者签发自己的运输单据的方式为客户承运货物的人</li></ul></li></ul></li><li>无船承运人<ul><li>对于实际货主而言，作为公共承运人与之订立海上货物运输合同</li><li>对于实际承运人而言，又承担着托运人的义务</li></ul></li><li>国际货运代理人定义<ul><li>International Freight Forwarder</li></ul></li><li>对货代的两类界定<ul><li>代理人说<ul><li>将国际货运代理人规定为受委托人的指示为其货物在国际间的运输及其他有关事务提供合理审慎服务的自然人或经济组织，本身业务不涉及货物的承运</li><li>与货主或委托人之间是纯粹的代理关系</li></ul></li><li>双重身份说<ul><li>规定了国际货运代理人是为委托方代办国际货运事务的代理人</li><li>规定了在一定条件下可以<strong>成为运输合同的当事人并对外承担承运人的责任</strong></li></ul></li></ul></li></ul><h2 id="1-2-货运代理法律关系辨析"><a href="#1-2-货运代理法律关系辨析" class="headerlink" title="1.2 货运代理法律关系辨析"></a>1.2 货运代理法律关系辨析</h2><ul><li>法律关系<ul><li>指相关海运国际公约，各国国内法以及行业规范等在调整国际海上货运代理行为的过程中形成的各有关主体间的权利和义务关系</li><li>国际海上货运代理法律关系包括<ul><li>国际货运代理企业作为海运代理人的法律关系</li><li>作为无船承运人的法律关系</li></ul></li></ul></li><li>作为海运代理人的法律关系<ul><li>在提供海上货物运输的相关代理业务的时候，呈现出的关系</li><li>内部委托法律关系<ul><li>国际货运代理企业与托运人之间的委托代理合同法律关系</li></ul></li><li>外部代理法律关系<ul><li>国际货运代理企业为托运人的利益和承运人签订海上货物运输合同而产生的运输合同法律关系</li><li>货代企业往往会以自己的名义代替多笔散货托运人与承运人签订一个总的运输合同，自己再分别同各托运人签订货运代理协议</li></ul></li><li>我国合同法对于包括货运代理合同在内的委托合同采用过错责任原则<ul><li>货代仅在自身确实受托事项存在过错并造成托运人损失的情况下承担违约损害赔偿责任</li><li>且只要在第三方选任上能够证明已经履行了合理和谨慎的义务，对由于第三方造成的托运人损失，可以免于承担责任</li></ul></li></ul></li><li>作为无船承运人的法律关系<ul><li>契约承运人的一种，即不拥有或者不经营船舶，不进行实际的货物运输活动，以签发无船承运人提单(Non Vessel Operating Common Carrier Bill of Loading)的方式明示或者默示对运输负有责任的人，承运责任来源于和托运人签订的货物运输契约，而不是实际的运输行为</li><li>法律关系<ul><li>无船承运人和托运人之间的海上货物运输合同关系</li><li>无船承运人和船公司之间的海上货物运输合同关系<ul><li>货代以托运人的身份和船公司 — 实际承运人签订运输合同并获得海运提单的 (Master Bill of Loading MBL)</li><li><strong>根据海商法， 无船承运人所需要承担的法律责任和实际承运人的责任是一样的</strong></li><li>无船承运人不享有不完全责任制<ul><li>当免责事由发生的时候，船公司可以免于对无船承运人承担责任，而无船承运人无法以相同的事由对托运人免责</li></ul></li><li>当承运人和实际承运人需要对货损货差需要进行赔偿的时候，二者在责任范围内承担连带责任</li><li>对于除了海上运输之外实际托运人提供的其他运输服务，无船承运人均承担严格责任</li></ul></li><li>无船承运人和船公司代理人之间的海上货物运输合同关系</li></ul></li></ul></li></ul><h1 id="2-风险状况分析"><a href="#2-风险状况分析" class="headerlink" title="2. 风险状况分析"></a>2. 风险状况分析</h1><ul><li><p>风险的定义</p><ul><li>由于国际货运代理企业在开展海上业务过程中受到客观法律环境，包括自身在内的各海洋运输相关主体所实施的法律行为的影响，导致其权利义务状态发生改变，从而产生的可能由该企业承担的法律上的不利后果</li><li>法律风险由环境诱因 — 客观法律环境，行为诱因 — 实施的法律行为及二者所引发的不利后果构成<ul><li>法律的不完善和不确定性<ul><li>不完善 — 制定法因为无法避免地滞后于社会的发展而必然存在的漏洞和空白</li><li>不确定性 — 由于法律条文意义晦涩之处需要法官来阐释说明，法官对于法律的理解会有所不同，从而导致了裁判结果不总是一致的</li></ul></li><li>行为诱因<ul><li>货代的一些不规范的法律行为，</li></ul></li></ul></li></ul></li><li><p>货代纠纷案件研究 争议焦点主要在：</p><ul><li>涉案主体间货运代理合同关系的认定<ul><li>因为转委托而引发的对双方间是否存在直接法律关系的异议</li><li>因为货运代理人和无船承运人身份识别而引发的对合同关系性质是货物运输合同法律关系还是货物运输代理合同法律关系的异议</li></ul></li></ul></li><li><p>主要风险分类</p><ul><li>身份认定上的风险<ul><li>指对国际货运代理企业是涉案货物的海运代理人还是无船承运人的认定结果和企业自身对此的认识不同造成的问题</li></ul></li><li>转委托和双方代理上的风险</li><li>涉外法律适用上的风险<ul><li>譬如美国统一商法典，允许以记名提单的方式放货，如果双方在法律适用条款当中约定适用美国法，那么海外代理提单无单放货的行为就不属于过错行为</li></ul></li></ul></li></ul><h1 id="3-风险成因分析"><a href="#3-风险成因分析" class="headerlink" title="3. 风险成因分析"></a>3. 风险成因分析</h1><h2 id="3-1-身份认定上的法律风险成因"><a href="#3-1-身份认定上的法律风险成因" class="headerlink" title="3.1 身份认定上的法律风险成因"></a>3.1 身份认定上的法律风险成因</h2><ul><li>国际货运代理企业是涉案货物的海运代理人还是无船承运人的认定结果的不同，会导致企业需要承担的法律责任和风险完全不同<ul><li>立法上的小混乱<ul><li>因为很有可能货运代理企业同时担当着无船承运人还有代理人的角色</li></ul></li><li>司法裁判的不确定<ul><li>《海上货代纠纷规定》的一些判断逻辑<ul><li>双方之间是否订立了海上货运代理合同，反映出来的是代理协议还是运输协议</li><li>受托人向委托人是否签发了提单，是承运提单还是代理人的分提单</li><li>受托人收取费用的名义是佣金还是运费</li><li>双方以往的交易历史和交易习惯</li></ul></li></ul></li><li>从业者自身行为的不规范<ul><li>是否保留了重要的往来文件，发票和提单</li></ul></li></ul></li></ul><h2 id="3-2-转委托与双方代理上的法律风险成因"><a href="#3-2-转委托与双方代理上的法律风险成因" class="headerlink" title="3.2 转委托与双方代理上的法律风险成因"></a>3.2 转委托与双方代理上的法律风险成因</h2><h3 id="3-2-1-转委托行为的法律风险成因"><a href="#3-2-1-转委托行为的法律风险成因" class="headerlink" title="3.2.1 转委托行为的法律风险成因"></a>3.2.1 转委托行为的法律风险成因</h3><ul><li>什么是转委托<ul><li>受托人将委托人委托其代为处理的事务转交给第三人处理的行为</li></ul></li><li>转委托行为的法律风险成因<ul><li>合同法规定，对于委托事务，除了经过委托人同意或者出现紧急状况可以转委托之外，受托人均应当亲自处理，否则就要为第三人的行为承担责任</li><li>但是对于国际货运代理行业来说，转委托是一个常规方式<ul><li>原因在于海运货代委托人更为看重成本和效率，让货代企业去完全处理每一件委托事务是不经济也不现实的</li></ul></li><li>造成货代企业转委托风险的是是否取得了委托人的同意</li><li>当前《海上货代纠纷规定》明确排除了推定托运人默示同意货运代理人转委托的可能<ul><li>转委托具体权限约定不明的时候，委托人将负有就不明权限想委托人报告的义务</li><li>委托人在受托人指示下与第三人的通常接触行为不能认定为委托人以该行为对转委托的明确同意</li></ul></li></ul></li></ul><h3 id="3-2-2-双方代理行为上的法律风险成因"><a href="#3-2-2-双方代理行为上的法律风险成因" class="headerlink" title="3.2.2 双方代理行为上的法律风险成因"></a>3.2.2 双方代理行为上的法律风险成因</h3><ul><li>什么是双方代理？<ul><li>指在同一法律关系内，一方当事人的代理人同时又接受另一方当事人委托，并为其代理的行为</li><li>由于合同关系中双方是相对的，双方代理会使得本是冲突的合同双方意思表示被代理的个人意志予以替代，偏离了合同的本质属性</li></ul></li><li>海运货代行业需要这样做，因为效率上的提升。但会有法律上的风险</li></ul><h2 id="3-3-涉外法律适用上的风险成因"><a href="#3-3-涉外法律适用上的风险成因" class="headerlink" title="3.3 涉外法律适用上的风险成因"></a>3.3 涉外法律适用上的风险成因</h2><ul><li>作为法院裁判依据的国外法律规定可能会让国际货运代理企业在诉讼中处于不利地位</li><li>涉外商事代理法律关系，会受到途经国家的法律管辖</li><li>如果双方未约定准据法，或涉诉的国际货代企业没有准确理解和把握已约定的准据法，就会大大增加诉讼中的不稳定因素</li><li>英美法系，判例法； 成文法国家，有专门的货运代理法律</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>《国际海上货运代理法律风险研究》  邓大鸣</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-货运代理法律风险基本内容&quot;&gt;&lt;a href=&quot;#1-货运代理法律风险基本内容&quot; class=&quot;headerlink&quot; title=&quot;1. 货运代理法律风险基本内容&quot;&gt;&lt;/a&gt;1. 货运代理法律风险基本内容&lt;/h1&gt;&lt;h2 id=&quot;1-1-国际货运代理人定义&quot;&gt;
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="货运代理" scheme="https://www.llchen60.com/tags/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>国际货运代理行业初探</title>
    <link href="https://www.llchen60.com/%E5%9B%BD%E9%99%85%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E8%A1%8C%E4%B8%9A%E5%88%9D%E6%8E%A2/"/>
    <id>https://www.llchen60.com/%E5%9B%BD%E9%99%85%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E8%A1%8C%E4%B8%9A%E5%88%9D%E6%8E%A2/</id>
    <published>2021-08-15T14:52:04.000Z</published>
    <updated>2021-08-19T23:01:51.975Z</updated>
    
    <content type="html"><![CDATA[<h1 id="国际货运代理行业初探"><a href="#国际货运代理行业初探" class="headerlink" title="国际货运代理行业初探"></a>国际货运代理行业初探</h1><h1 id="1-货运公司业务与工作内容"><a href="#1-货运公司业务与工作内容" class="headerlink" title="1. 货运公司业务与工作内容"></a>1. 货运公司业务与工作内容</h1><h2 id="1-1-主要业务"><a href="#1-1-主要业务" class="headerlink" title="1.1 主要业务"></a>1.1 主要业务</h2><h3 id="1-1-1-Overview"><a href="#1-1-1-Overview" class="headerlink" title="1.1.1 Overview"></a>1.1.1 Overview</h3><blockquote><p>客户委托货运代理运输货物，客户本人并不承担承运责任；货代接受发货人委托后，为发货人提供相关服务，满足发货人的具体要求，同时要求发货人根据服务来支付一定报酬的行业</p></blockquote><h3 id="1-1-2-主营业务"><a href="#1-1-2-主营业务" class="headerlink" title="1.1.2 主营业务"></a>1.1.2 主营业务</h3><ul><li>纯粹代理人的业务<ul><li>取得委托人授权</li><li>负责货物离开港口以及到达港口的公务处理工作<ul><li>报关</li><li>报检</li><li>保险</li></ul></li><li>这个清关环节主要是配合当地海关进行文件准备工作</li></ul></li><li>国际多式联运业务<ul><li>多式联运，发展自集装箱运输</li></ul></li><li>无船承运业务<ul><li>具有无船公共承运人NVOCC (Non Vessel Operating Common Carrier)资质的货代企业可以以承运人的身份接受货载委托</li><li>货物清关的时候用货代企业的提单，承担承运人的责任，完成货物离港到岸的国际海运经营业务</li><li>无船承运人只是契约承运人，而实际完成运输的承运人是货代企业所委托的其他国际船舶运输经营者</li></ul></li><li>物流业务<ul><li>可以针对不同客户提供定制化的供应链解决方案，囊括了货物从生产，出厂，转运，清仓报关，到达目标市场等各个环节</li></ul></li><li>国际快递业务</li></ul><h2 id="1-2-基本工作流程"><a href="#1-2-基本工作流程" class="headerlink" title="1.2 基本工作流程"></a>1.2 基本工作流程</h2><ul><li>海运流程<ul><li>他国买主和中国企业就贸易行为签订贸易合同</li><li>中国工厂确定货运代理企业，并与之商讨出货</li><li>中国货运代理企业和船运公司商讨订舱事宜</li><li>拿到承载货物的船的名字和达到目的地的准确信息</li><li>国内外货运代理企业共同确认货物运输信息，以及到达目的地的时间信息，并将具体信息传递给国外买家</li></ul></li><li>location / key points<ul><li>卖方，出口商地点</li><li>出口单证手续</li><li>边境/ 机场/ 码头交货</li><li>装运港</li><li>船舷</li><li>船上</li><li>船上</li><li>船舷</li><li>到达卸货港</li><li>指定目的地交货; 边境/ 机场/ 码头</li><li>进口单证手续</li><li>卖方/ 进口商地点</li></ul></li></ul><h2 id="1-3-货代企业等级"><a href="#1-3-货代企业等级" class="headerlink" title="1.3 货代企业等级"></a>1.3 货代企业等级</h2><ul><li>一级货代企业<ul><li>需要获得国际货代资格证书</li><li>商务部颁发的，一级货代企业有权从中国不同港口订舱</li></ul></li><li>小型货代公司<ul><li>依托于一级货代企业，来进行订舱的公司</li></ul></li></ul><h1 id="2-国际货运行业发展的外部环境"><a href="#2-国际货运行业发展的外部环境" class="headerlink" title="2. 国际货运行业发展的外部环境"></a>2. 国际货运行业发展的外部环境</h1><ul><li><p>PEST分析方式</p><ul><li>维度<ul><li>政治<ul><li>2010年货代标准化委员会</li><li>2015年《推动共建丝绸之路经济带和21世纪海上丝绸之路的愿景与行动》</li><li>2016年《关于加快国际货运代理物流业健康发展的指导意见》</li><li>《中华人民共和国海商法》</li></ul></li><li>经济</li><li>技术</li><li>社会文化</li></ul></li><li>外部的变化是组织无法控制的，脱离于组织内部，却环绕在组织四周，从不同方面作用在组织内部</li></ul></li><li><p>波特五力竞争理论</p><ul><li>维度<ul><li>替代者<ul><li>货代的供应商</li></ul></li><li>客户</li><li>供应商<ul><li>供应商 尤其是船舶公司航空公司有着比较强的议价能力，尤其对于中小型货代而言</li></ul></li><li>潜在竞争者<ul><li>政策壁垒整体在降低</li><li>传统业务 — 报关报检，码头物流，短途专线运输</li><li>新兴业务 — 供应链管理，非常规货物托运，多式联运</li></ul></li><li>同行业竞争者<ul><li>船舶公司旗下自己设立的货代物流公司<ul><li>有直接客户源</li><li>底价海运费和订舱费减免的优势</li><li>可以通过船公司完善的海外网络指定国内的出口供应商，可以提供更多种的贸易方式<ul><li>贸易方式<ul><li>Exxxx<ul><li>EXW<ul><li>卖方仅在自己的地点为买房备妥货物交付，出工厂以后就没有费用和安全责任了</li></ul></li></ul></li><li>Fxxx  — 卖方需要将货物交到制定的承运人处<ul><li>FCA  货交承运人</li><li>FAS  装运港 船边  交货 — 到码头</li><li>FOB 装运港 船上 交货  — 到船舷<ul><li>价格计算  = (产品含税成本 + 利润 + 国内运输费用 - 出口退税)/ 汇率</li><li>卖方的义务<ul><li>将合同规定的货物交到买房所指派的船上并及时通知买方</li><li>承担货物越过装运港船舷之前的一切风险</li><li>办理货物的出口手续</li><li>提交商业发票等所需的凭证</li></ul></li><li>买方的义务<ul><li>租船订舱，支付运费</li><li>将船名，装货地点和要求交货的时间及时通知卖方</li><li>受领货物，支付货款</li><li>承担货物越过装运港船舷之后的一切风险</li><li>办理货物的进口手续</li></ul></li></ul></li></ul></li><li>Cxxx — 卖方需要订立运输合同，但是对于货物损失的风险以及装船和启运之后发生的意外所产生的额外费用，卖方不承担责任<ul><li>CFR  成本+运费<ul><li>CFR = FOB + 海运费</li></ul></li><li>CIF  成本运费保险<ul><li>CIF = FOB + 海运费 + 海运保险费</li></ul></li><li>CIP  运费/ 保险费付到目的地</li></ul></li><li>Dxxx — 卖方承担将货物交到出口国边境或者目的国所需的全部费用和风险<ul><li>DAF  出口国边境交货</li><li>DES  目的港船上交货</li><li>DEQ  目的港码头交货</li><li>DDU  未完税交货</li><li>DDP  完税后交货</li></ul></li><li>FOB</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>SWOT 战略管理分析</p><ul><li>S - Strength<ul><li>企业内部优势</li></ul></li><li>W - Weakness<ul><li>企业面临的竞争弱势</li></ul></li><li>O - Opportunity<ul><li>发展过程当中的有利外部环境</li></ul></li><li>T  - Threat<ul><li>企业自身业务的外部威胁</li><li>当外部环境不利于企业的发展，企业为了避免外部环境带来的威胁，应当及时调整经营战略</li></ul></li></ul></li></ul><h1 id="3-内部环境分析"><a href="#3-内部环境分析" class="headerlink" title="3. 内部环境分析"></a>3. 内部环境分析</h1><ul><li>价值链分析法<ul><li>将活动分为基础活动以及辅助活动两大类<ul><li>基础活动<ul><li>内部物流活动<ul><li>各个部门和环节之间的合作，如何能直观了解到项目的进展此类问题</li></ul></li><li>生产经营活动<ul><li>生产 销售  供应 财务</li></ul></li><li>外部物流活动<ul><li>货物的周转</li><li>报关 报检 查验 清关 缴税等</li></ul></li><li>服务性活动</li></ul></li><li>辅助活动<ul><li>人力资源管理活动</li><li>改善基础设施条件的活动</li><li>原材料采购活动</li><li>新产品研发活动等</li></ul></li></ul></li><li>只有在一些特定环节，才会真的去创造价值，这些是战略环节，需要在此构建战略优势</li></ul></li></ul><h1 id="4-Other-Notes"><a href="#4-Other-Notes" class="headerlink" title="4. Other Notes"></a>4. Other Notes</h1><ul><li><p>企业战略</p><ul><li>用于整合与重新优化配置的措施及可行性方案</li><li>企业组织结构要跟随企业战略进行调整</li></ul></li><li><p>如何选择合适的竞争战略</p><ul><li>对有吸引力，高潜力的产业的正确选择</li><li>在选择的行业当中确立自己的竞争优势地位</li><li>除此以外还需要从企业内部环境，研究价值链</li></ul></li><li><p>企业内部性的深入研究 — 企业核心能力</p><ul><li>让企业构成其他企业并不具备的能力，同时资源无法在不同的企业之间进行流通</li><li>企业内部的资源的独特性，直接关系到企业利润的获得以及竞争优势的保持</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>1.<a href="https://www.52by.com/article/2584">https://www.52by.com/article/2584</a><br>2. 《小型国际货运代理X公司的发展战略研究》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;国际货运代理行业初探&quot;&gt;&lt;a href=&quot;#国际货运代理行业初探&quot; class=&quot;headerlink&quot; title=&quot;国际货运代理行业初探&quot;&gt;&lt;/a&gt;国际货运代理行业初探&lt;/h1&gt;&lt;h1 id=&quot;1-货运公司业务与工作内容&quot;&gt;&lt;a href=&quot;#1-货运公司业务
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="货运代理" scheme="https://www.llchen60.com/tags/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>System Design Patterns - Quorum</title>
    <link href="https://www.llchen60.com/System-Design-Patterns-Quorum/"/>
    <id>https://www.llchen60.com/System-Design-Patterns-Quorum/</id>
    <published>2021-08-11T04:26:01.000Z</published>
    <updated>2021-08-11T04:26:42.996Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h1><p>In distributed system, data is replicated across multiple servers for fault tolerance and high availability.</p><p>Once system decides to maintain multiple copies of data, another problem arises: how to make sure that all replicas are consistent?? </p><h1 id="2-Dive-Deep"><a href="#2-Dive-Deep" class="headerlink" title="2. Dive Deep"></a>2. Dive Deep</h1><h2 id="2-1-Definition"><a href="#2-1-Definition" class="headerlink" title="2.1 Definition"></a>2.1 Definition</h2><ul><li>Quorum<ul><li>Minimum number of servers on which a distributed operation needs to be performed successfully before declaring the operation’s overall success</li></ul></li></ul><h2 id="2-2-How-it-works"><a href="#2-2-How-it-works" class="headerlink" title="2.2 How it works?"></a>2.2 How it works?</h2><ul><li>Suppose a database is replicated on 5 machines, then quorum refers to the minimum number of machines that perform the same action for a given transaction in order to decide the final operation for that transaction</li><li>So in a set of 5, three machines form the majority quorum, quorum <strong>enforces the consistency requirement</strong> needed for distributed operations</li><li>Quorum Number<ul><li>N / 2 + 1</li></ul></li><li>Quorum is achieved when nodes follow the below protocol R + W &gt; N<ul><li>R  minimum read nodes</li><li>W minimum write nodes</li><li>N  nodes in the quorum group</li></ul></li></ul><h2 id="2-3-Where-is-it-used"><a href="#2-3-Where-is-it-used" class="headerlink" title="2.3 Where is it used?"></a>2.3 Where is it used?</h2><ul><li>Chubby<ul><li>Use paxos for leader election, which use quorum to ensure strong consistency</li></ul></li><li>Cassandra<ul><li>Ensure data consistency, each write request can be configured to be successful only if the data has been written to at least a quorum of replica nodes</li></ul></li><li>Dynamodb<ul><li>Writes to a sloppy quorum of other nodes in the system</li><li>All read/ write operations are performed on the first N healthy nodes from the preference list, which may not always be the first N nodes encountered walking the consistent hashing ring</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Background&quot;&gt;&lt;a href=&quot;#1-Background&quot; class=&quot;headerlink&quot; title=&quot;1. Background&quot;&gt;&lt;/a&gt;1. Background&lt;/h1&gt;&lt;p&gt;In distributed system, data 
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>System Design Patterns - Bloom Filters</title>
    <link href="https://www.llchen60.com/System-Design-Patterns-Bloom-Filters/"/>
    <id>https://www.llchen60.com/System-Design-Patterns-Bloom-Filters/</id>
    <published>2021-08-10T02:33:45.000Z</published>
    <updated>2021-08-10T02:34:11.571Z</updated>
    
    <content type="html"><![CDATA[<h1 id="System-Design-Patterns-Bloom-Filters"><a href="#System-Design-Patterns-Bloom-Filters" class="headerlink" title="System Design Patterns - Bloom Filters"></a>System Design Patterns - Bloom Filters</h1><p>Created: August 8, 2021 10:06 PM<br>Status: Finished<br>Tags: System Design<br>Type: Tech Resource</p><h1 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h1><ul><li>Suppose we have a large set of structured data(identified by record IDs) stored in a set of data files, and we want to know which file might contain our required data<ul><li>we don’t want to read each file, as it’s slow and we have to read a lot of data from the disk</li></ul></li><li>Solution 1: Build an index on each data file and store it in a separate index file, to map each record ID to its offset in the data file; each index file will be sorted on the record ID. Then we could do a binary search in index file</li><li>Solution 2: We could use Bloom Filters</li></ul><h1 id="2-How-does-Bloom-Filter-work"><a href="#2-How-does-Bloom-Filter-work" class="headerlink" title="2. How does Bloom Filter work?"></a>2. How does Bloom Filter work?</h1><ul><li>The Bloom filter data structure tells whether an element <strong>may be in a set, or definitely is not</strong><ul><li>which means the only possible errors are false positives</li></ul></li><li>How it looks<ul><li>An empty bloom filter is a bit array of m bits, all set to 0</li><li>There are also k different hash functions, each of which maps a set element to one of the m bit positions</li></ul></li><li>Workflow<ul><li>To add an element, feed it to the hash functions to get k bit positions, and set the bits at these positions to 1</li><li>To test if an element is in the set, feed it to the hash functions to get k bit positions<ul><li>if any of the bits at these positions is 0, the element is definitely not in the set</li><li>if all are 1, then the element may be in the set</li></ul></li></ul></li></ul><h1 id="3-When-will-we-use-Bloom-Filter"><a href="#3-When-will-we-use-Bloom-Filter" class="headerlink" title="3. When will we use Bloom Filter?"></a>3. When will we use Bloom Filter?</h1><ul><li>In BigTable, any read operation has to read from all SSTables that make up a tablet<ul><li>if these SSTables are not in memory, the read operation may end up doing many disk accesses</li><li>BigTable uses bloom filters to reduce the number of disk accesses</li><li>Store of BloomFilter could drastically reduces the number of disk seeks, thereby improving read performance</li></ul></li></ul><h1 id="4-How-to-use-Bloom-Filter-in-Java"><a href="#4-How-to-use-Bloom-Filter-in-Java" class="headerlink" title="4. How to use Bloom Filter in Java?"></a>4. How to use Bloom Filter in Java?</h1><ul><li>We could use BloomFilter class from the Guava library to achieve this</li></ul><pre><code class="jsx">BloomFilter&lt;Integer&gt; filter = BloomFilter.create(    Funnels.integerFunnel(),     500,     0.01);</code></pre><ul><li>How to implement from scratch   <a href="https://www.inlighting.org/archives/java-implement-bloom-filter/">https://www.inlighting.org/archives/java-implement-bloom-filter/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;System-Design-Patterns-Bloom-Filters&quot;&gt;&lt;a href=&quot;#System-Design-Patterns-Bloom-Filters&quot; class=&quot;headerlink&quot; title=&quot;System Design Patter
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis集合统计模式</title>
    <link href="https://www.llchen60.com/Redis%E9%9B%86%E5%90%88%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    <id>https://www.llchen60.com/Redis%E9%9B%86%E5%90%88%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F/</id>
    <published>2021-08-08T23:32:13.000Z</published>
    <updated>2021-08-08T23:32:56.696Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis集合统计模式"><a href="#Redis集合统计模式" class="headerlink" title="Redis集合统计模式"></a>Redis集合统计模式</h1><h1 id="1-聚合统计"><a href="#1-聚合统计" class="headerlink" title="1. 聚合统计"></a>1. 聚合统计</h1><h2 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h2><ul><li>统计多个集合元素的聚合结果<ul><li>交集统计 — 统计多个集合的共有元素</li><li>差集统计 — 统计其中一个集合独有的元素</li><li>并集统计 — 统计多个集合的所有元素</li></ul></li><li>聚合统计可以使用Set类型来做</li><li>但是Set的差集，并集，交集的计算复杂度比较高，在数据量比较大的情况下，直接执行可能会导致Redis实例阻塞。</li><li>可以从主从集群当中选择一个从库，使其专门负责聚合计算，或者将数据读取到客户端，在客户端完成聚合统计</li></ul><h2 id="1-2-案例分析"><a href="#1-2-案例分析" class="headerlink" title="1.2 案例分析"></a>1.2 案例分析</h2><ul><li>统计一个手机App的每天的新增用户数和第二天的留存用户数</li><li>用一个集合记录所有登陆过App的用户Id<ul><li>key — user:id</li><li>value — set类型 记录用户id</li></ul></li><li>另外一个集合记录每天用户set<ul><li>key — user:id:20210808</li><li>value — set类型  记录用户id</li></ul></li></ul><pre><code class="jsx">// 差值统计SUNIONSTORE user:id user:id user:id:20210808// 计算新用户SDIFFSTORE user:new user:id:20210808 user:id // 计算留存用户SINTERSTOPRE user:id:rem user:id:20210808 user:id:20210809</code></pre><h1 id="2-排序统计"><a href="#2-排序统计" class="headerlink" title="2. 排序统计"></a>2. 排序统计</h1><h2 id="2-1-概念"><a href="#2-1-概念" class="headerlink" title="2.1 概念"></a>2.1 概念</h2><ul><li>需要能对输出进行排序，Redis常用的4个集合类型当中 (List, Hash, Set, Sorted Set)， List和Sorted Set属于有序集合</li><li>List按照元素进入List的顺序排序，而Sorted Set可以根据元素的权重来排序</li></ul><h2 id="2-2-案例"><a href="#2-2-案例" class="headerlink" title="2.2 案例"></a>2.2 案例</h2><ul><li>电商网站上提供最新评论列表的场景</li><li>List在这个场景里面的问题<ul><li>因为根据位置排序，当有新的评价加进来，那么可能会有一些评价会在不同页面重复出现</li></ul></li><li>Sorted Set不存在这个问题，因为它是根据元素的实际权重来排序和获取数据的<ul><li>我们可以按照评论时间的先后给每条评论设置一个权重值，然后将评论保存到Sorted Set当中</li><li>ZRANGEBYSCORE命令就可以按照权重排序以后返回元素</li></ul></li></ul><h1 id="3-二值状态统计"><a href="#3-二值状态统计" class="headerlink" title="3. 二值状态统计"></a>3. 二值状态统计</h1><h2 id="3-1-概念"><a href="#3-1-概念" class="headerlink" title="3.1 概念"></a>3.1 概念</h2><ul><li>指集合元素的取值只有0和1两种</li><li>计算海量二值状态数据的时候，bitmap可以有效减少所需的内存空间</li></ul><h2 id="3-2-案例"><a href="#3-2-案例" class="headerlink" title="3.2 案例"></a>3.2 案例</h2><ul><li>签到统计<ul><li>每个用户一天的签到用一个bit位就能表示</li><li>因此并不需要非常复杂的数据类型，使用bitmap就可以了</li></ul></li><li>Redis提供了Bitmap类型<ul><li>GETBIT</li><li>SETBIT<ul><li>将某一位设置为1</li></ul></li><li>BITCOUNT<ul><li>用来统计所有1的个数</li></ul></li></ul></li></ul><pre><code class="jsx">// 记录用户8 3 签到了SETBIT uid:sign:3000:202008 2 1 // 检查是否8 3 签到了GETBIT uid:sign:3000:202008 2 // 统计该用户8月份的签到次数BITCOUNT uid:sign:3000:202008 </code></pre><ul><li>如何统计一亿个用户连续10天的签到情况<ul><li>将每天日期作为key，每个key对应一个1亿位的bitmap  每一个bit对应一个用户当天的签到情况</li><li>对10个bitmap做与操作</li><li>然后用BITCOUNT统计下最终生成的Bitmap当中1的个数</li></ul></li></ul><h1 id="4-基数统计"><a href="#4-基数统计" class="headerlink" title="4. 基数统计"></a>4. 基数统计</h1><h2 id="4-1-概念"><a href="#4-1-概念" class="headerlink" title="4.1 概念"></a>4.1 概念</h2><ul><li>基数统计指统计一个集合中不重复的元素的个数</li></ul><h2 id="4-2-案例"><a href="#4-2-案例" class="headerlink" title="4.2 案例"></a>4.2 案例</h2><ul><li>统计一个网页的UV<ul><li>需要去重，一个用户一天内的多次访问只能算一次</li></ul></li><li>可以使用SET或者HASH类型来进行记录，但是会消耗很大的内存空间</li><li>可以使用HyperLogLog<ul><li>用于统计基数的数据集合类型</li><li>优势在于当集合元素数量非常多的时候，计算基数所需的空间总是固定的，而且还很小</li><li>Redis中每个HyperLogLog只需要使用12KB内存，就可以计算接近2^64个元素的基数</li></ul></li><li>HyperLogLog的统计规则是基于概率完成的，因此其给出的统计结果是有一定误差的，标准误算率为0.81% ; 如果应用场景是必须非常精确，那就还需要使用Set或者Hash类型</li></ul><pre><code class="jsx">PFADD page1:uv  user1 user2 user3 user4 PFCOUNT page1:uv </code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis集合统计模式&quot;&gt;&lt;a href=&quot;#Redis集合统计模式&quot; class=&quot;headerlink&quot; title=&quot;Redis集合统计模式&quot;&gt;&lt;/a&gt;Redis集合统计模式&lt;/h1&gt;&lt;h1 id=&quot;1-聚合统计&quot;&gt;&lt;a href=&quot;#1-聚合统计&quot; cla
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>DynamoDB Architecture (Paper Reading)</title>
    <link href="https://www.llchen60.com/DynamoDB-Architecture-Paper-Reading/"/>
    <id>https://www.llchen60.com/DynamoDB-Architecture-Paper-Reading/</id>
    <published>2021-08-07T20:14:38.000Z</published>
    <updated>2021-08-08T18:51:22.531Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>Designed to be always on</li><li>Dynamo falls within the category of AP systems (available and partition tolerant) and is designed for high availability and partition tolerance at the expense of strong consistency</li></ul><h2 id="1-1-Design-Goals"><a href="#1-1-Design-Goals" class="headerlink" title="1.1 Design Goals"></a>1.1 Design Goals</h2><ul><li>Scalable<ul><li>System need to be highly scalable. We should be able to throw a machine into the system to see proportional improvement</li></ul></li><li>Decentralized<ul><li>To avoid single points of failures and performance bottlenecks, there should not be any central/ leader process</li></ul></li><li>Eventually Consistent<ul><li>Data can be optimistically replicated to become eventually consistent</li></ul></li></ul><h2 id="1-2-Use-cases"><a href="#1-2-Use-cases" class="headerlink" title="1.2 Use cases"></a>1.2 Use cases</h2><ul><li>eventually consistent database</li></ul><h2 id="1-3-System-APIs"><a href="#1-3-System-APIs" class="headerlink" title="1.3 System APIs"></a>1.3 System APIs</h2><ul><li>put(key, context, object)<ul><li>find the nodes where the object associated with the given key should locate</li><li>context is a value that is returned with a get operation and then sent back with the put operation</li><li>context is always stored along with the object</li><li>used like a cookie to verify the validity of the object supplied in the put request</li></ul></li><li>get(key)<ul><li>find the nodes where the object associated with the given key is located</li><li>return a single object or a list of objects with conflicting versions along with a context</li><li>context contains encoded metadata about the object, and <strong>version of the object</strong></li></ul></li></ul><h1 id="2-High-Level-Design"><a href="#2-High-Level-Design" class="headerlink" title="2. High Level Design"></a>2. High Level Design</h1><h2 id="2-1-Data-Distribution"><a href="#2-1-Data-Distribution" class="headerlink" title="2.1 Data Distribution"></a>2.1 Data Distribution</h2><h3 id="2-1-1-What-is-it"><a href="#2-1-1-What-is-it" class="headerlink" title="2.1.1 What is it"></a>2.1.1 What is it</h3><ul><li>Consistent hashing to distribute its data among nodes</li><li>Also make it easy to add/ remove nodes from a dynamo cluster</li></ul><h3 id="2-1-2-Challenge"><a href="#2-1-2-Challenge" class="headerlink" title="2.1.2 Challenge"></a>2.1.2 Challenge</h3><ul><li>how do we know on which node a particular piece of data will be stored?</li><li>when we add/ remove nodes, how do we know what data will be moved from existing nodes to the new nodes?</li><li>how can we minimize data movement when nodes join or leave?</li></ul><h3 id="2-1-3-Consistent-Hashing"><a href="#2-1-3-Consistent-Hashing" class="headerlink" title="2.1.3 Consistent Hashing"></a>2.1.3 Consistent Hashing</h3><ul><li><p>Represents the data managed by a cluster as a ring</p></li><li><p>Each node in the ring is assigned a range of data</p></li><li><p>Token</p><ul><li><p>The start of the range is called a token</p></li><li><p>each node will be assigned with one token</p><p>  <img src="https://i.loli.net/2021/08/08/8jFPMEDNmn4cXaf.png" alt=""></p></li></ul></li><li><p>Process for a put or get request</p><ul><li>DDB performs a MD5 hashing algorithm to the key</li><li>Output determines within which range the data lies —→ which node the data will be stored</li></ul></li><li><p>Problems for only use physical nodes</p><ul><li>for adding or removing nodes, it only influence the next node, but it would cause uneven distribution of traffic</li><li>recomputing the tokens causing a significant administrative overhead for a large cluster</li><li>Since each node is assigned one large range, if the data is not evenly distributed, some nodes can become hotspots</li><li>Since each node’s data is replicated on a fixed number of nodes (discussed later), when we need to rebuild a node, only its replica nodes can provide the data. This puts a lot of pressure on the replica nodes and can lead to service degradation</li></ul></li></ul><h3 id="2-1-4-Virtual-Nodes"><a href="#2-1-4-Virtual-Nodes" class="headerlink" title="2.1.4 Virtual Nodes"></a>2.1.4 Virtual Nodes</h3><p><img src="https://i.loli.net/2021/08/08/YWwxt9eMmToQ3qn.png" alt=""></p><ul><li><p>Hash range is divided into multiple smaller ranges, and each physical node is assigned multiple of these smaller ranges</p></li><li><p>Each of these subranges is called a Vnode</p></li><li><p>Vnodes are randomly distributed across the cluster and are generally non contiguous (不连续的)</p><p>  <img src="https://i.loli.net/2021/08/08/RVgrPm8T19nbKZB.png" alt=""></p></li><li><p>Benefits of Vnodes</p><ul><li>Help spread the load more evenly across the physical nodes on the cluster by dividing the hash range into smaller subranges<ul><li>speeds up the rebalancing process after adding or removing nodes</li><li>When a new node is added, it receives many Vnodes from the existing nodes to maintain a balanced cluster. Similarly, when a node needs to be rebuilt, instead of getting data from a fixed number of replicas, many nodes participate in the rebuild process.</li></ul></li><li>Vnodes make it easier to maintain a cluster containing heterogeneous machines. This means, with Vnodes, we can assign a high number of ranges to a powerful server and a lower number of ranges to a less powerful server</li><li>Since Vnodes help assign smaller ranges to each physical node, the probability of hotspots is much less than the basic Consistent Hashing scheme which uses one big range per node</li></ul></li></ul><h2 id="2-2-Data-Replication-and-Consistency"><a href="#2-2-Data-Replication-and-Consistency" class="headerlink" title="2.2 Data Replication and Consistency"></a>2.2 Data Replication and Consistency</h2><ul><li>Data is replicated optimistically</li><li>Dynamo provides eventual consistency</li></ul><h3 id="2-2-1-Optimistic-Replication"><a href="#2-2-1-Optimistic-Replication" class="headerlink" title="2.2.1 Optimistic Replication"></a>2.2.1 Optimistic Replication</h3><ul><li><p>To ensure high availability and durability, Dynamo replicates each data item on multiple N nodes in the system where the value N is equivalent to the <strong>replication factor</strong>, also is configured per instance of Dynamo</p></li><li><p>Each key is assigned to a coordinator node, which first stores the data locally and then replicates it to N-1 clockwise successor nodes on the ring</p><ul><li>Thus each node owns the region on the ring between it and its Nth predecessor</li></ul></li><li><p>Replication is done asynchronously and Dynamo provides an eventually consistent model</p></li><li><p>It’s called optimistic replication, as the replicas are not guaranteed to be identical at all times</p><p>  <img src="https://i.loli.net/2021/08/08/QaHhmPTI6XcYfwu.png" alt=""></p></li><li><p>Preference List</p><ul><li>List of nodes responsible for storing a particular key</li><li>Dynamo is designed so that every node in the system can determine which nodes should be in the list for any specific key</li><li>The list contains more than N nodes to account for failures and skip virtual nodes on the ring so that the list only contains distinct physical nodes</li></ul></li></ul><h2 id="2-3-Handling-Temporary-Failures"><a href="#2-3-Handling-Temporary-Failures" class="headerlink" title="2.3 Handling Temporary Failures"></a>2.3 Handling Temporary Failures</h2><ul><li>To handle temporary failures, dynamo replicates data to a <strong>sloppy quorum</strong> of other nodes in the system instead of a strict majority quorum</li></ul><h3 id="2-3-1-Quorum-Approach"><a href="#2-3-1-Quorum-Approach" class="headerlink" title="2.3.1 Quorum Approach"></a>2.3.1 Quorum Approach</h3><ul><li>Traditional quorum approach<ul><li>any distributed system becomes unavailable during server failures or network partitions and would have reduced availability even under simple failure conditions</li></ul></li><li>Sloppy quorum<ul><li>all read/ write operations are performed on the first N healthy nodes from the preference list. may not always be the first N nodes encountered while moving clockwise on the consistent hashing ring</li></ul></li></ul><h3 id="2-3-2-Hinted-Handoff"><a href="#2-3-2-Hinted-Handoff" class="headerlink" title="2.3.2 Hinted Handoff"></a>2.3.2 Hinted Handoff</h3><ul><li>When a node is unreachable, another node can accept writes on its behalf</li><li>Write is then kept in a local buffer and sent out once the destination node is reachable again</li><li>Problem<ul><li>Sloppy quorum is not a strict majority, the data can and will diverge</li><li>It is possible for two concurrent writes to the same key to be accepted by non-overlapping sets of nodes. This means that multiple conflicting values against the same key can exist in the system, and we can get stale or conflicting data while reading. Dynamo allows this and resolves these conflicts using Vector Clocks.</li></ul></li></ul><h2 id="2-4-Inter-node-communication-and-failure-detection"><a href="#2-4-Inter-node-communication-and-failure-detection" class="headerlink" title="2.4 Inter node communication and failure detection"></a>2.4 Inter node communication and failure detection</h2><ul><li>Use gossip protocol to keep track of the cluster state</li></ul><h3 id="2-4-1-Gossip-Protocol"><a href="#2-4-1-Gossip-Protocol" class="headerlink" title="2.4.1 Gossip Protocol"></a>2.4.1 Gossip Protocol</h3><ul><li><p>Enable each node to keep track of state information about the other nodes in the cluster</p><ul><li>which nodes are reachable</li><li>what key ranges they are responsible for</li></ul></li><li><p>Gossip Protocol</p><ul><li><p>Peer to peer communication mechanism</p></li><li><p>nodes periodically exchange state information about themselves and other nodes they know about</p></li><li><p>each node initiate a gossip round every second with a random node</p><p>  <img src="https://i.loli.net/2021/08/08/hgBPER5e2kuxvMw.png" alt=""></p></li></ul></li><li><p>External discovery through seed nodes</p><ul><li>An administrator joins node A to the ring and then joins node B to the ring. Nodes A and B consider themselves part of the ring, yet neither would be immediately aware of each other. To prevent these logical partitions, Dynamo introduced the concept of seed nodes. Seed nodes are fully functional nodes and can be obtained either from a static configuration or a configuration service. This way, all nodes are aware of seed nodes. Each node communicates with seed nodes through gossip protocol to reconcile membership changes; therefore, logical partitions are highly unlikely.</li></ul></li></ul><h2 id="2-5-Conflict-Resolution-and-Handling-permanent-failures"><a href="#2-5-Conflict-Resolution-and-Handling-permanent-failures" class="headerlink" title="2.5 Conflict Resolution and Handling permanent failures"></a>2.5 Conflict Resolution and Handling permanent failures</h2><h3 id="2-5-1-Clock-Skew"><a href="#2-5-1-Clock-Skew" class="headerlink" title="2.5.1 Clock Skew"></a>2.5.1 Clock Skew</h3><ul><li>Dynamo resolves potential conflicts using below mechanisms<ul><li>use vector clocks to keep track of value history and reconcile divergent histories at read time</li><li>in the background, dynamo use an <strong>anti entropy mechanism</strong> like <strong>Merkle trees</strong> to handle permanent failures</li></ul></li></ul><h3 id="2-5-2-Vector-Clock"><a href="#2-5-2-Vector-Clock" class="headerlink" title="2.5.2 Vector Clock"></a>2.5.2 Vector Clock</h3><ul><li><p>Used to capture causality between different versions of the same object</p></li><li><p>A vector clock is a <code>node, counter</code> pair</p></li><li><p>Each version of every object associate with a vector clock</p><ul><li>one can determine whether two versions of an object are on parallel branches or have a causal ordering by examining vector clocks</li><li>If the counters on the first object’s clock are less-than-or-equal to all of the nodes in the second clock, then the first is an ancestor of the second and can be forgotten. Otherwise, the two changes are considered to be in conflict and require reconciliation.</li></ul></li><li><p>  <img src="https://i.loli.net/2021/08/08/9fKB418IrMezTun.png" alt=""></p></li><li><p>Issue occur when there are network partition, that same data cannot be shared / communicated via different servers</p></li><li><p>In this case, DynamoDB will return it back and let client reads and reconciles</p></li></ul><h3 id="2-5-3-Conflict-free-replicated-data-types-CRDTs"><a href="#2-5-3-Conflict-free-replicated-data-types-CRDTs" class="headerlink" title="2.5.3 Conflict free replicated data types (CRDTs)"></a>2.5.3 Conflict free replicated data types (CRDTs)</h3><ul><li>we need to model our data in such a way that concurrent changes can be applied to the data in any order and will produce the same end result</li></ul><h2 id="2-6-put-and-get-Operations"><a href="#2-6-put-and-get-Operations" class="headerlink" title="2.6 put() and get() Operations"></a>2.6 put() and get() Operations</h2><h3 id="2-6-1-Strategies-for-choosing-the-coordinator-node"><a href="#2-6-1-Strategies-for-choosing-the-coordinator-node" class="headerlink" title="2.6.1 Strategies for choosing the coordinator node"></a>2.6.1 Strategies for choosing the coordinator node</h3><ul><li><p>Strategies</p><p>  <img src="https://i.loli.net/2021/08/08/MB8qFbQGdwhoZAJ.png" alt=""></p><ul><li>Clients can route their requests through a generic load balancer<ul><li>client is unaware of the dynamo ring<ul><li>helps scalability</li><li>make ddb architecture loosely coupled</li></ul></li><li>it’s possible node it select is not part of the perference list, this will result in an extra hop</li></ul></li><li>Clients can use a partition aware client library that routes the request to the appropriate coordinator nodes with lower latency<ul><li>helps to achieve lower latency  — achieve zero hop</li><li>DDB doesn’t have much control over the load distribution and request handling</li></ul></li></ul></li></ul><h3 id="2-6-2-Consistency-Protocol"><a href="#2-6-2-Consistency-Protocol" class="headerlink" title="2.6.2 Consistency Protocol"></a>2.6.2 Consistency Protocol</h3><ul><li>R W is the min number of nodes that must participate in a successful read/ write operation</li><li>R + W &gt; N yields a quorun like system</li><li>A Common (N,R,WN, R, WN,R,W) configuration used by Dynamo is (3, 2, 2)</li><li>In general, low values of WWW and RRR increase the risk of inconsistency, as write requests are deemed successful and returned to the clients even if a majority of replicas have not processed them. This also introduces a vulnerability window for durability when a write request is successfully returned to the client even though it has been persisted at only a small number of nodes</li></ul><h3 id="2-6-3-put-process"><a href="#2-6-3-put-process" class="headerlink" title="2.6.3  put() process"></a>2.6.3  <code>put()</code> process</h3><ul><li>the coordinator generates a new data version and vector clock component</li><li>saves new data locally</li><li>sends the write request to N-1 highest ranked healthy nodes from the preference list</li><li>the put() operation is considered successful after receiving W - 1 confirmation</li></ul><h3 id="2-6-4-get-process"><a href="#2-6-4-get-process" class="headerlink" title="2.6.4 get() process"></a>2.6.4 <code>get()</code> process</h3><ul><li>coordinator requests the data version from N - 1 highest ranked healthy nodes from the preference list</li><li>waits until R - 1 replies</li><li>coordinator handles causal data versions through a vector clock</li><li>returns all relevant data versions to the caller</li></ul><h3 id="2-6-5-Request-handling-through-state-machine"><a href="#2-6-5-Request-handling-through-state-machine" class="headerlink" title="2.6.5 Request handling through state machine"></a>2.6.5 Request handling through state machine</h3><ul><li>Each client request results in creating a state machine on the node that received the client request</li><li>A read operation workflow would be:<ul><li>send read requests to the nodes</li><li>wait for the minimum number of required responses</li><li>if too few replies were received within a given time limit, fail the request</li><li>otherwise, gather all the data versions and determine the ones to be returned</li><li>if versioning is enabled, perform syntactic reconciliation and generate an opaque write context that contains the vector clock that subsumes all the remaining versions</li><li>At this point, read response has been returned to the caller</li><li>the state machine waits for a short period to receive any outstanding responces</li><li>if stale versions were returned in any of the responses, the coordinator updates those nodes with the latest version — READ REPAIR</li></ul></li></ul><h2 id="2-7-Anti-entropy-Through-Merkle-Trees"><a href="#2-7-Anti-entropy-Through-Merkle-Trees" class="headerlink" title="2.7 Anti-entropy Through Merkle Trees"></a>2.7 Anti-entropy Through Merkle Trees</h2><ul><li>Vector clocks are useful to remove conflicts while serving read requests</li><li>But if a replica falls significantly behind others, it might take a very long time to resolve conflicts using just vector clocks</li></ul><hr><p>—&gt; we need to quickly compare two copies of a range of data residing on different replicas and figure out exactly which parts are different </p><h3 id="2-7-1-What-are-MerkleTrees"><a href="#2-7-1-What-are-MerkleTrees" class="headerlink" title="2.7.1 What are MerkleTrees?"></a>2.7.1 What are MerkleTrees?</h3><ul><li><p>Dynamo use Merkel Trees to compare replicas of a range</p></li><li><p>A merkle tree is a binary tree of hashes, where each internal node is the hash of its two children, each leaf node is a hash of a portion of the original data</p></li><li><p>Then compare the merkle tree come to be super easy, just compare the root hashes of both trees, if equal, then stop; else, recurse on the left and right children</p></li><li><p>The principal advantage of using a Merkle tree is that each branch of the tree can be checked independently without requiring nodes to download the entire tree or the whole data set. Hence, Merkle trees minimize the amount of data that needs to be transferred for synchronization and reduce the number of disk reads performed during the anti-entropy process.</p></li><li><p>The disadvantage of using Merkle trees is that many key ranges can change when a node joins or leaves, and as a result, the trees need to be recalculated.</p></li></ul><h2 id="2-8-Dynamo-Characteristics"><a href="#2-8-Dynamo-Characteristics" class="headerlink" title="2.8 Dynamo Characteristics"></a>2.8 Dynamo Characteristics</h2><h3 id="2-8-1-Dynamo’s-Node-Responsibilities"><a href="#2-8-1-Dynamo’s-Node-Responsibilities" class="headerlink" title="2.8.1 Dynamo’s Node Responsibilities"></a>2.8.1 Dynamo’s Node Responsibilities</h3><ul><li>Each node serves three functions:<ul><li>Managing <code>get()</code> and <code>put()</code> requests<ul><li>A node may act as a coordinator and manage all operations for a particular key</li><li>A node also could forward the request to the appropriate node</li></ul></li></ul></li><li>Keep track of membership and detecting failures<ul><li>Every node uses gossip protocol to keep track of other nodes in the system and their associated hash ranges</li></ul></li><li>Local persistent storage<ul><li>Each node is responsible for being either the primary or replica store for keys that hash to a specific range of values</li><li>These pairs are stored within that node using various storage systems depending on application needs</li><li>E.G<ul><li>BerkeleyDB Transactional Data Store</li><li>MySQL</li><li>In memory buffer backed by persistent storage</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Designed to be always on&lt;/li&gt;

      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
      <category term="DynamoDB" scheme="https://www.llchen60.com/tags/DynamoDB/"/>
    
  </entry>
  
</feed>
