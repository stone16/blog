<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leilei&#39;s Blog | 磊磊的博客</title>
  
  <subtitle>Because it&#39;s there</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.llchen60.com/"/>
  <updated>2020-11-01T18:53:41.564Z</updated>
  <id>https://www.llchen60.com/</id>
  
  <author>
    <name>Leilei Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LC - Linked List</title>
    <link href="https://www.llchen60.com/LC-Linked-List/"/>
    <id>https://www.llchen60.com/LC-Linked-List/</id>
    <published>2020-10-25T03:33:44.000Z</published>
    <updated>2020-11-01T18:53:41.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LC-23-Merge-K-Sorted-Lists"><a href="#LC-23-Merge-K-Sorted-Lists" class="headerlink" title="LC-23 Merge K Sorted Lists"></a>LC-23 Merge K Sorted Lists</h1><ul><li>给定k个链表，每个链表都按照升序排列，合并链表使得最终的链表也按照升序排列</li><li>最直观的想法</li><li>以一个链表为基准，拿出一个链表来做比较，各保留一个指针，然后来做合并操作</li></ul><h2 id="Solution1-Brute-Force"><a href="#Solution1-Brute-Force" class="headerlink" title="Solution1: Brute Force"></a>Solution1: Brute Force</h2><ul><li><p>都放到一个arraylist当中</p></li><li><p>使用Collection的排序算法，对其进行排序</p></li><li><p>然后创建一个新的链表，来返回结果</p><pre><code>class Solution {  public ListNode mergeKLists(ListNode[] lists) {      if (lists == null || lists.length == 0) {          return null;      }      List&lt;Integer&gt; list = new ArrayList();      for (ListNode ln : lists) {          while (ln != null) {              list.add(ln.val);              ln = ln.next;          }      }      Collections.sort(list);      ListNode dummy = new ListNode(0);      ListNode result = dummy;      for (int i = 0; i &lt; list.size(); i++) {          result.next = new ListNode(list.get(i));          result = result.next;      }      return dummy.next;  }}</code></pre></li></ul><h2 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution 2"></a>Solution 2</h2><ul><li>比较每个链表头，将最小的放到result中，一个一个这样比较，相当于只遍历了以便就生成了结果</li><li>注意终止循环的判断，应该是几个链表都到头了才退出</li></ul><pre><code>class Solution {    public ListNode mergeKLists(ListNode[] lists) {        if (lists == null || lists.length == 0) {            return null;        }        ListNode result = new ListNode(0);        ListNode head = result;        while (true) {            int min = Integer.MAX_VALUE;            int min_index = -1;            boolean shouldBreak = true;            for (int i = 0; i &lt; lists.length; i++) {                if (lists[i] != null &amp;&amp; lists[i].val &lt; min) {                    min = lists[i].val;                    min_index = i;                    shouldBreak = false;                }            }            if (shouldBreak) break;            // get one value min             head.next = lists[min_index];            head = head.next;            lists[min_index] = lists[min_index].next;        }        return result.next;    }}</code></pre><h2 id="Solution-3-使用Priority-Queue"><a href="#Solution-3-使用Priority-Queue" class="headerlink" title="Solution 3: 使用Priority Queue"></a>Solution 3: 使用Priority Queue</h2><pre><code>public ListNode mergeKLists(ListNode[] lists) {         PriorityQueue q = new PriorityQueue&lt;&gt;((o1, o2) -&gt; o1.val - o2.val);        for(ListNode l : lists){            if(l!=null){                // 这里加进去的不是value 而是几个ListNode                q.add(l);            }                }        ListNode head = new ListNode(0);        ListNode point = head;        while(!q.isEmpty()){             point.next = q.poll();            point = point.next;             ListNode next = point.next;            if(next!=null){                // 顺次下移以后比较下一个节点                q.add(next);            }        }        return head.next;    }</code></pre><h1 id="LC-24-Swap-Nodes-in-Pairs"><a href="#LC-24-Swap-Nodes-in-Pairs" class="headerlink" title="LC-24 Swap Nodes in Pairs"></a>LC-24 Swap Nodes in Pairs</h1><ul><li><p>交换相邻节点</p></li><li><p>临界条件</p><ul><li>单个节点</li><li>没有节点</li></ul></li><li><p>因为会涉及到首节点的替换变动，所以为了简化问题，需要设置一个 dummy node，使得<code>dummy.next = head</code></p></li><li><p>画一个基本的swap示意图可以发现整个替换会涉及四个节点</p><ul><li>首节点的上个节点原先指向首节点的指针</li><li>首节点本身</li><li>次节点本身</li><li>次节点原先指向次节点下一个节点的指针</li></ul></li><li><p>因为是单向链表，我们没法用prev指针来找到上一个节点，所以需要在过程中做好prev的记录，而对于次节点的下一个节点，完全可以用next指针来表示</p></li><li><p>另外要注意做swap的时候的顺序问题</p></li></ul><pre><code>// Iteration的版本class Solution {    public ListNode swapPairs(ListNode head) {        ListNode dummy = new ListNode(0);        dummy.next = head;        ListNode prev = dummy;        while (head != null &amp;&amp; head.next != null) {            ListNode cur = head;            ListNode next = head.next;            // Swapping             prev.next = next;            cur.next = next.next;            next.next = cur;            prev = cur;            head = cur.next;        }        return dummy.next;    }}</code></pre><h1 id="LC-234-Palindrome-Linked-List"><a href="#LC-234-Palindrome-Linked-List" class="headerlink" title="LC-234 Palindrome Linked List"></a>LC-234 Palindrome Linked List</h1><h2 id="Solution-1-递归"><a href="#Solution-1-递归" class="headerlink" title="Solution 1: 递归"></a>Solution 1: 递归</h2><ul><li><p>反向输出链表的递归逻辑</p><pre><code>function print_values_in_reverse(ListNode head)  if head is NOT null      print_values_in_reverse(head.next)      print head.val</code></pre></li><li><p>题解</p></li></ul><pre><code>class Solution {    private ListNode front;    private boolean recursiveCheck(ListNode cur) {        if (cur != null) {            if (!recursiveCheck(cur.next)) return false;            if (cur.val != front.val) return false;            front = front.next;        }        return true;    }    public boolean isPalindrome(ListNode head) {        front = head;        return recursiveCheck(head);    }}</code></pre><h2 id="Solution-2-反向后半链表来做比较"><a href="#Solution-2-反向后半链表来做比较" class="headerlink" title="Solution 2: 反向后半链表来做比较"></a>Solution 2: 反向后半链表来做比较</h2><pre><code>class Solution {    public boolean isPalindrome(ListNode head) {        if (head == null) return true;        // Find the end of first half and reverse second half.        ListNode firstHalfEnd = endOfFirstHalf(head);        ListNode secondHalfStart = reverseList(firstHalfEnd.next);        // Check whether or not there is a palindrome.        ListNode p1 = head;        ListNode p2 = secondHalfStart;        boolean result = true;        while (result &amp;&amp; p2 != null) {            if (p1.val != p2.val) result = false;            p1 = p1.next;            p2 = p2.next;        }                // Restore the list and return the result.        firstHalfEnd.next = reverseList(secondHalfStart);        return result;    }    // Taken from https://leetcode.com/problems/reverse-linked-list/solution/    private ListNode reverseList(ListNode head) {        ListNode prev = null;        ListNode curr = head;        while (curr != null) {            ListNode nextTemp = curr.next;            curr.next = prev;            prev = curr;            curr = nextTemp;        }        return prev;    }    private ListNode endOfFirstHalf(ListNode head) {        ListNode fast = head;        ListNode slow = head;        while (fast.next != null &amp;&amp; fast.next.next != null) {            fast = fast.next.next;            slow = slow.next;        }        return slow;    }}</code></pre><h1 id="LC-1474-Delete-N-nodes-after-M-nodes-of-a-linked-list"><a href="#LC-1474-Delete-N-nodes-after-M-nodes-of-a-linked-list" class="headerlink" title="LC-1474 Delete N nodes after M nodes of a linked list"></a>LC-1474 Delete N nodes after M nodes of a linked list</h1><ul><li><p>链表热身题</p></li><li><p>注意边界条件的判定，题中已给m和n的限定范围，所以不需要额外做判断了</p></li><li><p>对于n的周期性去除的node，注意可以不用额外空间，通过改变指针的指向即可</p><pre><code>class Solution {  public ListNode deleteNodes(ListNode head, int m, int n) {      ListNode cur = head;      ListNode last = head;      while (cur != null) {          int mCount = m;          int nCount = n;          while (cur != null &amp;&amp; mCount &gt; 0 ) {              last = cur;              cur = cur.next;              mCount --;          }          while (cur != null &amp;&amp; nCount &gt; 0) {              cur = cur.next;              nCount --;          }          last.next = cur;      }      return head;  }}</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;LC-23-Merge-K-Sorted-Lists&quot;&gt;&lt;a href=&quot;#LC-23-Merge-K-Sorted-Lists&quot; class=&quot;headerlink&quot; title=&quot;LC-23 Merge K Sorted Lists&quot;&gt;&lt;/a&gt;LC-23 Me
      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="LinkedList" scheme="https://www.llchen60.com/tags/LinkedList/"/>
    
  </entry>
  
  <entry>
    <title>System Design Basics</title>
    <link href="https://www.llchen60.com/System-Design-Basics/"/>
    <id>https://www.llchen60.com/System-Design-Basics/</id>
    <published>2020-10-12T04:54:08.000Z</published>
    <updated>2020-10-30T04:08:15.415Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Key-Characteristics-of-Distributed-Systems"><a href="#1-Key-Characteristics-of-Distributed-Systems" class="headerlink" title="1. Key Characteristics of Distributed Systems"></a>1. Key Characteristics of Distributed Systems</h1><h2 id="1-1-Scalability"><a href="#1-1-Scalability" class="headerlink" title="1.1 Scalability"></a>1.1 Scalability</h2><ul><li><p>Capability of a system, process, or a network to grow and manage increased demand. Achieve the scaling without performance loss </p></li><li><p>Why need to scale?</p><ul><li>Increased data volume </li><li>Increased amount of work <ul><li>number of transactions </li></ul></li></ul></li><li><p>Performance curve </p><ul><li>Usually performance of a system would decline with the system size due to the management or environment cost <ul><li>network speed come to be slower because machines tend to be far apart from one another </li><li>some tasks may not be distributed, either because of their inherent atomic nature or because of some flaw in the system design </li></ul></li></ul></li></ul><ul><li><p>Horizontal vs Vertical Scaling </p><ul><li><p>Horizontal Scaling </p><ul><li>Scale by adding more servers into your pool of resources </li><li>Easier to scale dynamically by adding more machines into the existing pool </li></ul></li><li><p>Vertical scaling </p><ul><li><p>Scale by adding more power to an existing server </p><ul><li>CPU</li><li>RAM</li><li>Storage </li></ul></li><li><p>limited to the capacity of a single server, and scaling beyond that capacity often <strong>involves downtime</strong> and comes with an upper limit </p></li></ul></li></ul></li></ul><h2 id="1-2-Reliability"><a href="#1-2-Reliability" class="headerlink" title="1.2 Reliability"></a>1.2 Reliability</h2><ul><li>Probability a system will fail in a given period</li><li>A distributed system is considered reliable if it keeps delivering its services even when one or more software or hardware components fail </li><li>A reliable distributed system achieves this through redundancy of both the software components and data <ul><li>Eliminate every single point of failure</li></ul></li></ul><h2 id="1-3-Availability"><a href="#1-3-Availability" class="headerlink" title="1.3 Availability"></a>1.3 Availability</h2><ul><li>The time a system remains operational to perform its required function in s specific period </li><li>Percentage of time that a system, service, or a machine remains operational under normal conditions </li><li>Reliability  is availability over time considering the full range of possible real world conditions that can occur</li><li>If a system is reliable, it is available. However, if it is available, it is not necessarily reliable. In other words, high reliability contributes to high availability, but it is possible to achieve a high availability even with an unreliable product by minimizing repair time and ensuring that spares are always available when they are needed</li></ul><h2 id="1-4-Efficiency"><a href="#1-4-Efficiency" class="headerlink" title="1.4 Efficiency"></a>1.4 Efficiency</h2><ul><li><p>Standard measures of its efficiency </p><ul><li><p>Response time/ latency </p><ul><li>denotes the delay to obtain the first item </li></ul></li><li><p>Throughput / bandwidth</p><ul><li>number of items delivered in a given time unit </li></ul></li></ul></li><li><p>two measures above correspond to the following unit costs </p><ul><li>number of messages globally sent by the nodes of the system regardless of the message size </li><li>size of messages representing the volume of data exchanges </li></ul></li></ul><h2 id="1-5-Serviceability-Manageability"><a href="#1-5-Serviceability-Manageability" class="headerlink" title="1.5 Serviceability / Manageability"></a>1.5 Serviceability / Manageability</h2><ul><li>How easy it is to operate and maintain </li><li>Simplicity and speed with which a system can be repared or maintained </li><li>Ease of diagnosing and understanding problems when they occur, ease of making updates or modifications, </li></ul><h1 id="2-Load-Balancing"><a href="#2-Load-Balancing" class="headerlink" title="2. Load Balancing"></a>2. Load Balancing</h1><h2 id="2-1-What-is-Load-Balancer"><a href="#2-1-What-is-Load-Balancer" class="headerlink" title="2.1 What is Load Balancer?"></a>2.1 What is Load Balancer?</h2><ul><li><p>A critical component of any distributed system</p><ul><li><p>help to spread the traffic across a cluster of servers to improve responsiveness and availability of applications/ websites/ databases </p></li><li><p>also keep track of the status of all the resources while distributing requests </p><ul><li>if one server is not available to take new requests, not responding, has elevated error rate </li><li>LB will stop sending traffic to such server </li></ul></li><li><p>sit between client and the server accepting incoming network and application traffic, distribute traffic across multiple backend servers using various algorithm</p></li><li><p>could prevents any one application server from becoming a single point of failure </p></li></ul></li><li><p>where to add LB?</p><ul><li>between the user and the web server </li><li>between web servers and an internal platform layer, like application servers or cache servers </li><li>between internal platform layer and database <h2 id="2-2-Benefits"><a href="#2-2-Benefits" class="headerlink" title="2.2 Benefits"></a>2.2 Benefits</h2></li></ul></li><li><p>from user side </p><ul><li>faster, uninterrupted service; their requests could be immediately passed on to a more readily available resource </li></ul></li><li><p>from service provider side </p><ul><li>experience less downtime and higher throughput </li></ul></li><li><p>long term benefits</p><ul><li>smart load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen </li></ul></li></ul><h2 id="2-3-Load-Balancing-Algorithms"><a href="#2-3-Load-Balancing-Algorithms" class="headerlink" title="2.3 Load Balancing Algorithms"></a>2.3 Load Balancing Algorithms</h2><ul><li><p>How does the load balancer choose the backend server?</p><ul><li><ol><li>Make sure servers could respond appropriately to requests <ul><li>routinely do health check <ul><li>regularly attempt to connect to backend servers to ensure that servers are listening </li></ul></li></ul></li></ol></li><li><ol start="2"><li><p>Use pre configured algorithm to select one from the set of healthy servers </p><ul><li><p>Least connection Method </p><ul><li>direct traffic to the server with fewest active connections </li><li>useful when there are a large number of persistent client connections which are unevenly distributed between the servers</li></ul></li><li><p>Least Response Time Method </p><ul><li>direct traffic to the server with the fewest active connections and the lowest average response time </li></ul></li><li><p>Least Bandwidth Method </p><ul><li>selects the server that is currently serving the least amount of traffic measured in megabits per second (Mbps)</li></ul></li><li><p>Round Robin Method </p><ul><li>cycles through a list of servers and sends each new request to the next server </li><li>most useful when the servers are of equal specification and there are not many persistent connections</li></ul></li><li><p>weighted round robin </p><ul><li>desifned to better handle servers with different processing capacities </li><li>each server is assigned a weight which indicates the processing capacity </li></ul></li><li><p>IP Hash </p><ul><li>A hash of the IP address of the client is calculated to redirect the request to a server </li></ul></li></ul></li></ol></li></ul></li></ul><h2 id="2-4-Redundant-Load-Balancers"><a href="#2-4-Redundant-Load-Balancers" class="headerlink" title="2.4 Redundant Load Balancers"></a>2.4 Redundant Load Balancers</h2><ul><li>Load Balancer can be a single point of failure <ul><li>thus we need a second load balancer, to form a cluster </li><li>each LB monitors the health of the other</li><li>passive one could be switched to be active anytime since they keep monitoring same </li></ul></li></ul><h1 id="3-Caching"><a href="#3-Caching" class="headerlink" title="3. Caching"></a>3. Caching</h1><p>Caching enable you to make vastly better use of the resources you already have as well as make otherwise unattainable product requirements feasible。 </p><p>It takes advantage of the locality of reference principle: recently requested data is likely to be requested again, could be used in almost every layer of computing </p><h2 id="3-1-Application-Server-Cache"><a href="#3-1-Application-Server-Cache" class="headerlink" title="3.1 Application Server Cache"></a>3.1 Application Server Cache</h2><ul><li><p>place a cache directly on a request layer node </p></li><li><p>cache could be located both in memory and on the node’s local disk </p></li><li><p>One note here </p><ul><li>if expand this to many nodes, depends on your load balancer behavior, if it randomly distributes requests across the nodes, the same request will go to different nodes, thus increasing cache misses. <ul><li>could use either global caches or distributed caches for it </li></ul></li></ul></li></ul><h2 id="3-2-Content-Distribution-Network"><a href="#3-2-Content-Distribution-Network" class="headerlink" title="3.2 Content Distribution Network"></a>3.2 Content Distribution Network</h2><ul><li><p>For sites serving large amounts of static media </p></li><li><p>A typical workflow </p><ul><li>A request first ask the CDN for a piece of static media </li><li>CDN will serve the content if it has it locally available </li><li>If not, CDN will query the back end servers for the file </li><li>Then cache it locally, and serve it to the requesting user <h2 id="3-3-Cache-Invalidation"><a href="#3-3-Cache-Invalidation" class="headerlink" title="3.3 Cache Invalidation"></a>3.3 Cache Invalidation</h2></li></ul></li><li><p>Cache needs maintenance for keeping cache coherent with the source of truth</p><ul><li>if data is modified in db, should be invalidated in the cache </li></ul></li><li><p>Write Through Cache </p><ul><li>Data is written into the cache and the corresponding database at the same time </li><li>It could minimize the risk of data loss, but since every write operation mush be done twice before returning success to the client, latency would be higher </li></ul></li><li><p>Write Around Cache </p><ul><li>Data is written directly to permanent storage, bypassing the cache </li><li>Could reduce the cache being flooded with write operations that will not subsequently be re-read</li><li>But a read request for recently written data will create a cache miss </li></ul></li><li><p>Write Back Cache </p><ul><li>Data is written to cache alone</li><li>Write to permanent storage is done after specified intervals or under certain conditions </li><li>Low latency and high throughput for write intensive applications </li><li>However this speed up could cause issue of data loss in case of a crash or other adverse event because the only copy of the written data is in the cache </li></ul></li></ul><h2 id="3-4-Cache-Eviction-Policies"><a href="#3-4-Cache-Eviction-Policies" class="headerlink" title="3.4 Cache Eviction Policies"></a>3.4 Cache Eviction Policies</h2><ul><li>First In First Out </li><li>Last In First Out </li><li>Least Recently Used </li><li>Most Recently Used </li><li>Least Frequently Used </li><li>Randowm Replacement </li></ul><h1 id="4-Data-Partitioning"><a href="#4-Data-Partitioning" class="headerlink" title="4. Data Partitioning"></a>4. Data Partitioning</h1><p>It aims to break up a big database into many smaller parts. It’s a process of splitting up a DB/ table across multiple machines to improve the manageability, performance, availability, and load balancing of an application. </p><p>The justification for data partitioning is after a certain scale point, it’s cheaper and more feasible to scale horizontally by adding more machines that to grow it vertically by adding beefier servers </p><h2 id="4-1-Partitioning-Methods"><a href="#4-1-Partitioning-Methods" class="headerlink" title="4.1 Partitioning Methods"></a>4.1 Partitioning Methods</h2><h3 id="4-1-1-Horizontal-Partitioning-Sharding"><a href="#4-1-1-Horizontal-Partitioning-Sharding" class="headerlink" title="4.1.1 Horizontal Partitioning/ Sharding"></a>4.1.1 Horizontal Partitioning/ Sharding</h3><ul><li><p>Put different rows into different tables </p></li><li><p>range based partitioning as we store different ranges of data in separate tables </p></li><li><p>Probelm here</p><ul><li>is the range value isn’t chosen carefully, the partitioning scheme will lead to unbalanced servers <h3 id="4-1-2-Vertical-Partitioning"><a href="#4-1-2-Vertical-Partitioning" class="headerlink" title="4.1.2 Vertical Partitioning"></a>4.1.2 Vertical Partitioning</h3></li></ul></li><li><p>store data related to a specific feature in their own server </p><ul><li>like photo in one server, video in another, people they follow in another </li></ul></li><li><p>not quite scalable, if our app experience some high traffic, then the single server will not be enough to handle such traffic </p></li></ul><h3 id="4-1-3-Directory-Based-Partitioning"><a href="#4-1-3-Directory-Based-Partitioning" class="headerlink" title="4.1.3 Directory Based Partitioning"></a>4.1.3 Directory Based Partitioning</h3><ul><li>Create a lookup service which knows your current partitioning scheme and abstracts it away from the DB access code </li><li>To find a particular data entity, query the directory server that holds the mapping between each tuple key to its DB server</li></ul><h2 id="4-2-Partitioning-Criteria"><a href="#4-2-Partitioning-Criteria" class="headerlink" title="4.2 Partitioning Criteria"></a>4.2 Partitioning Criteria</h2><ul><li><p>Key or hash based partitioning </p><ul><li>apply a hash function to some key attributes of the entity we are storing </li><li>need to ensure a uniform allocation of data among servers </li><li>it will change the hash function when every time you add / remove some servers, the workaround is to use consistent hashing </li></ul></li><li><p>List Partitioning </p><ul><li>each partition is assigned a list of values </li></ul></li><li><p>Round robind partitioning </p></li><li><p>Composite Partitioning </p><ul><li>combination of criteria above</li></ul></li></ul><h2 id="4-3-Common-Problems-of-Data-Partitioning"><a href="#4-3-Common-Problems-of-Data-Partitioning" class="headerlink" title="4.3 Common Problems of Data Partitioning"></a>4.3 Common Problems of Data Partitioning</h2><ul><li><p>Joins and Denormalization </p><ul><li>Performing joins on a database that runs on several different servers </li><li>will not be performance efficient </li><li>Workaround is to denormalize database so queries perviously requiring joins can be performed from a single table <ul><li>but need to deal with data inconsistency issue </li></ul></li></ul></li><li><p>Referential Integrity </p><ul><li>nforce data integrity constraints such as foreign keys in a partitioned database can be extremely difficult</li></ul></li><li><p>Rebalancing </p><ul><li><p>need to do that due to </p><ul><li>data distribution is not uniform </li><li>could be a lot of load on one partition </li></ul></li><li><p>In such cases, either we have to create more DB partitions or have to rebalance existing partitions, which means the partitioning scheme changed and all existing data moved to new locations. Doing this without incurring downtime is extremely difficult. Using a scheme like directory based partitioning does make rebalancing a more palatable experience at the cost of increasing the complexity of the system and creating a new single point of failure </p></li></ul></li></ul><h1 id="5-Indexes"><a href="#5-Indexes" class="headerlink" title="5. Indexes"></a>5. Indexes</h1><p>Leverage on indexes when current database performance is no longer satisfactory. Indexing could help make search faster, it could be created using one or more columns of a ddb table, providing the basis for both rapid random lookups and efficient access of ordered records. </p><p>Index can dramatically speed up data retrieval but may itself be large due to the additional keys, which will slow down data insertion and update. </p><p>When adding rows or making updates to existing rows for a table with an active index, we not only have to write the data but also have to update the index. This will decrease the write performance. </p><p>This performance degradation applies to all insert, update, and delete operations for the table. For this reason, adding unnecessary indexes on tables should be avoided and indexes that are no longer used should be removed.</p><p>If the goal of ddb is often written to and rarely read from, in that case, decreasing the performance of the more common operation, which is writing, is probably not worth the increase in performance we get from reading.</p><h1 id="6-Proxies"><a href="#6-Proxies" class="headerlink" title="6. Proxies"></a>6. Proxies</h1><h2 id="6-1-What-is-Proxy-Server"><a href="#6-1-What-is-Proxy-Server" class="headerlink" title="6.1 What is Proxy Server?"></a>6.1 What is Proxy Server?</h2><ul><li><p>Intermediate server between the client and the backend server </p></li><li><p>Clients connect to proxy servers to make a request for a service like</p><ul><li>web page</li><li>file connection </li></ul></li><li><p>Proxy server is a piece of software or hardware that acts as an intermediary for requests from clients seeking resources from other servers </p></li><li><p>Proxy are used to </p><ul><li><p>filter requests </p></li><li><p>transform requests </p><ul><li>add/ remove headers </li><li>encrypt and decrypt</li><li>compress a resource </li></ul></li><li><p>caching </p><ul><li>if multiple clients access a particular resource, the proxy server can cache it and serve it to all clients without going to the remote server </li></ul></li></ul></li></ul><h2 id="6-2-Types"><a href="#6-2-Types" class="headerlink" title="6.2 Types"></a>6.2 Types</h2><ul><li><p>Open Proxy </p><ul><li>A proxy server that is accessible by any internet user </li><li>type<ul><li>anonymous proxy <ul><li>reveals its identity as a server but does not disclose the initial IP address </li></ul></li><li>transparent proxy <ul><li>Identify itself and with the suppot of HTTP headers </li><li>IP address could be viewed </li><li>main benefit of using this sort of server is its ability to cache the websites </li></ul></li></ul></li></ul></li><li><p>reverse proxy </p><ul><li>retrieve resources on behalf of a client from one or more servers </li><li>these resources are then returned to the client, appearing as if they originated from the proxy server itself </li></ul></li></ul><h1 id="7-Redundancy-and-Replication"><a href="#7-Redundancy-and-Replication" class="headerlink" title="7. Redundancy and Replication"></a>7. Redundancy and Replication</h1><ul><li>redundancy <ul><li>duplication of critical components or functions of a system with the intention of increasing the reliability of the system <ul><li>backup</li><li>fail saft </li><li>direct improvement on actual system performance </li></ul></li></ul></li></ul><ul><li><p>replication </p><ul><li><p>shareing information to ensure consistency between redundant resources</p><ul><li>to improve reliability</li><li>fault tolerance</li><li>accessibility </li></ul></li><li><p>primary replica relationship </p><ul><li>primary server gets all the updates </li><li>then ripple through to the replica servers </li><li>each replica outputs a message stating that it has received the update successfully <h1 id="8-SQL-vs-NoSQL"><a href="#8-SQL-vs-NoSQL" class="headerlink" title="8. SQL vs NoSQL"></a>8. SQL vs NoSQL</h1></li></ul></li></ul></li></ul><h2 id="8-1-Concepts"><a href="#8-1-Concepts" class="headerlink" title="8.1 Concepts"></a>8.1 Concepts</h2><ul><li><p>Relational databases </p><ul><li>structured </li><li>predefiend schemas </li></ul></li><li><p>Non-relational database </p><ul><li>unstrutured </li><li>distributed</li><li>dynamic schema </li></ul></li></ul><ul><li><p>SQL</p><ul><li><p>store data in rows and columns </p></li><li><p>each row contains:</p><ul><li>all the info about one entity </li></ul></li><li><p>each column contains:</p><ul><li>all separate data points </li></ul></li></ul></li><li><p>NoSQL</p><ul><li><p>Key-Value Stores </p><ul><li>store in an arry of key value pairs</li><li>key is an attribute name which is linked to a vlue <ul><li>redis</li><li>voldemort</li><li>dynamo</li></ul></li></ul></li><li><p>document database</p><ul><li>data is stored in documents (instead of rows and columns in a table)</li><li>documents are grouped together in collections </li><li>each document can have an entirely different structure</li></ul></li><li><p>wide column databases</p><ul><li>have column families, which are containers for rows </li><li>no need to know all the columns up front and each row doesn’t have to have the same number of columns</li><li>best suited for analyzing large datasets </li><li>type <ul><li>HBase</li><li>Cassandra</li></ul></li></ul></li><li><p>Graph Database</p><ul><li>used to store data whose relations are best represented in a graph </li><li>data is saved in graph structures with:<ul><li>nodes <ul><li>entities</li></ul></li><li>properties<ul><li>information about the entities </li></ul></li><li>lines <ul><li>connections between the entities</li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="8-2-Differences-between-SQL-and-NoSQL"><a href="#8-2-Differences-between-SQL-and-NoSQL" class="headerlink" title="8.2 Differences between SQL and NoSQL"></a>8.2 Differences between SQL and NoSQL</h2><ul><li><p>Storage</p><ul><li><p>SQL </p><ul><li>each row represents an entity </li><li>each column represents a data point about the entity </li></ul></li><li><p>NoSQL</p><ul><li>could be key value</li><li>document</li><li>graph </li></ul></li></ul></li><li><p>Schema</p><ul><li><p>SQL</p><ul><li>each record conforms to a fixed schema <ul><li>columns must be decided and chosen before data entry </li><li>each row must have data for each column </li></ul></li><li>schema modification need to involve modifying the whole database and go offline </li></ul></li><li><p>NoSQL</p><ul><li>schemas are dynamic </li><li>columns can be added on the fly, and each row doesn’t have to contain data for each column</li></ul></li></ul></li><li><p>Query </p><ul><li><p>SQL </p><ul><li>Structured query language for defining and manipulating the data </li></ul></li><li><p>NoSQL </p><ul><li>Query focus on a collection of documents </li><li>UnQL - unstructured query language </li></ul></li></ul></li></ul><ul><li><p>Scalability</p><ul><li><p>SQL</p><ul><li>vetically scalable <ul><li>by increase the horsepower (memory, CPU, etc.) of the hardware </li></ul></li></ul></li><li><p>NoSQL</p><ul><li>horizontally scalable <ul><li>we could add more servers easily in database infrastructure to handle more traffic </li><li>any cheap hardware could host NoSQL database</li></ul></li></ul></li></ul></li><li><p>Reliability or ACID comliancy</p><ul><li><p>ACID</p><ul><li>atomocity</li><li>consistency</li><li>isolation</li><li>durability </li></ul></li><li><p>most NoSQL solutions sacrifice ACID compliance for performance and scalability </p></li></ul></li></ul><h2 id="8-3-Choose-which-one"><a href="#8-3-Choose-which-one" class="headerlink" title="8.3 Choose which one?"></a>8.3 Choose which one?</h2><ul><li>Reasons for use SQL<ul><li>Need ACID compliance <ul><li>ACID reduces anomalies and protects the integrity of your db by prescribing exactly how transactions interact with the database </li></ul></li><li>Data is structured and unchanging </li></ul></li></ul><ul><li><p>Reasons for use NoSQL </p><ul><li><p>Store large volumens of data that often have little to no structure </p><ul><li>NoSQL allows use to add new types </li><li>with document based databases, you can store data in one place without having to define what types of data those are in advance </li></ul></li><li><p>Making the most of cloud computing and storage </p><ul><li>cloud based storage requires data to be easily spread across multiple servers to scale up </li></ul></li><li><p>Rapid development </p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Key-Characteristics-of-Distributed-Systems&quot;&gt;&lt;a href=&quot;#1-Key-Characteristics-of-Distributed-Systems&quot; class=&quot;headerlink&quot; title=&quot;1. K
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>System Design General Guides - from Grokking SDI</title>
    <link href="https://www.llchen60.com/System-Design-General-Guides-from-Grokking-SDI/"/>
    <id>https://www.llchen60.com/System-Design-General-Guides-from-Grokking-SDI/</id>
    <published>2020-10-09T00:55:45.000Z</published>
    <updated>2020-10-09T00:56:11.183Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Requirements-Clarifications"><a href="#1-Requirements-Clarifications" class="headerlink" title="1. Requirements Clarifications"></a>1. Requirements Clarifications</h1><ul><li>Ask about the exact scope of the problem <ul><li>For a twitter like system design <ul><li>will users of our service be able to post tweets and follow others? </li><li>Create and display user’s timeline?</li><li>Tweets contain photos and videos?</li><li>Front end design too?</li><li>Searchable tweets?</li><li>Display hot trending topics?</li><li>Push notification for new tweets? </li></ul></li></ul></li></ul><h1 id="2-Back-of-the-envelope-estimation"><a href="#2-Back-of-the-envelope-estimation" class="headerlink" title="2. Back of the envelope estimation"></a>2. Back of the envelope estimation</h1><ul><li><p>estimate the scale of the system we are going to design </p><ul><li><p>could help later when we will be focusing on scaling, partitioning, load balancing and caching</p><ul><li><p>what scale is expected </p><ul><li>number of new tweets</li><li>number of tweet views</li><li>number of timeline generations per sec </li></ul></li><li><p>how much storage will we need</p><ul><li>videos and photos will play important role on our estimation </li></ul></li><li><p>what network bandwidth usage are we expecting</p><ul><li>crucial in deciding how we will manage traffic and balance load between servers </li></ul></li></ul></li></ul></li></ul><h1 id="3-System-Interface-Definition"><a href="#3-System-Interface-Definition" class="headerlink" title="3. System Interface Definition"></a>3. System Interface Definition</h1><p>Establish exact contract expected from the system </p><pre><code>postTweet(user_id, tweet_data, tweet_location, user_location, timestamp, …)  generateTimeline(user_id, current_time, user_location, …)  markTweetFavorite(user_id, tweet_id, timestamp, …)  </code></pre><h1 id="4-Define-Data-Model"><a href="#4-Define-Data-Model" class="headerlink" title="4. Define Data Model"></a>4. Define Data Model</h1><p>Define the data model, to clarify how data will flow between different components of the system. Later, will guide for data partitioning and management. </p><p>User: UserID, Name, Email, DoB, CreationData, LastLogin, etc.<br>Tweet: TweetID, Content, TweetLocation, NumberOfLikes, TimeStamp, etc.<br>UserFollow: UserID1, UserID2<br>FavoriteTweets: UserID, TweetID, TimeStamp</p><h1 id="5-High-level-design"><a href="#5-High-level-design" class="headerlink" title="5. High level design"></a>5. High level design</h1><ul><li>Draw a block diagram with 5 - 6 boxes representing the core components of our system. We need to identify enough components that are needed to solve the actual problem from end2end. </li><li>For twitter <ul><li>Multiple application servers to serve all the read/ write requests with load balancers in front of them for traffic distributions </li><li>we could have separate servers for handling specific traffic (like much more read than write)</li><li>Efficient database that can store all the tweets and can support a huge number of reads </li><li>Also need a distributed file storage system for storing photos and videos </li></ul></li></ul><h1 id="6-Detailed-Design"><a href="#6-Detailed-Design" class="headerlink" title="6. Detailed Design"></a>6. Detailed Design</h1><ul><li>Dig deeper to 2 or 3 components <ul><li>present different approaches</li><li>pros and cons </li><li>explain why we prefer one on the other </li><li>consider tradeoffs between different options while keeping system contraints in mind </li></ul></li></ul><ul><li>Question Examples<ul><li>Since we will be storing a massive amount of data, how should we partition our data to distribute it to multiple databases? Should we try to store all the data of a user on the same database? What issue could it cause?</li><li>How will we handle hot users who tweet a lot or follow lots of people?</li><li>Since users’ timeline will contain the most recent (and relevant) tweets, should we try to store our data in such a way that is optimized for scanning the latest tweets?</li><li>How much and at which layer should we introduce cache to speed things up?</li><li>What components need better load balancing?</li></ul></li></ul><h1 id="7-Identifying-and-resolving-bottlenecks"><a href="#7-Identifying-and-resolving-bottlenecks" class="headerlink" title="7 Identifying and resolving bottlenecks"></a>7 Identifying and resolving bottlenecks</h1><ul><li>Discuss as many bottlenecks as possible and different approaches to mitigate them <ul><li>Any single point of failure in our system? </li><li>Do we have enough replicas of the data so that if we lose a few servers, we can still serve our users? </li><li>Do we have enough copies of different services running such that a fewe failures will not cause a total system shutdown?</li><li>How to monitor performance of our service? </li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Requirements-Clarifications&quot;&gt;&lt;a href=&quot;#1-Requirements-Clarifications&quot; class=&quot;headerlink&quot; title=&quot;1. Requirements Clarifications&quot;&gt;&lt;/
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>缓存的常见问题</title>
    <link href="https://www.llchen60.com/%E7%BC%93%E5%AD%98%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>https://www.llchen60.com/%E7%BC%93%E5%AD%98%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/</id>
    <published>2020-10-06T22:55:09.000Z</published>
    <updated>2020-10-06T22:55:54.435Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-缓存雪崩"><a href="#1-缓存雪崩" class="headerlink" title="1. 缓存雪崩"></a>1. 缓存雪崩</h1><p>缓存系统的IOPS比数据库高很多，因此需要注意短时间内的大量缓存失效的情况。这种情况一旦发生，可能就会在瞬间有大量的数据需要回源到数据库查询，对数据库造成极大的压力，极限情况下甚至导致后端数据库直接崩溃。这就是我们说的缓存雪崩。</p><p>产生雪崩的原因有两种：</p><ul><li>缓存系统本身不可用，导致大量请求直接回源到数据库</li><li>应用设计层面大量的Key在同一时间过期，导致大量的数据回源<ul><li>解决方法<ul><li>差异化缓存过期时间<ul><li>不让大量的Key在同一时间过期</li><li>初始化缓存的时候，设置缓存的过期时间为30秒 + 10秒以内的随机延迟（扰动值）。这样key就不会集中在30秒这个时刻过期，而是会分散在30 - 40秒之间</li></ul></li><li>让缓存不主动过期<ul><li>初始化缓存数据的时候设置缓存永不过期，然后启动一个后台线程30秒一次定时将所有数据更新到缓存，通过适当的休眠，控制从数据库更新数据的频率，降低数据库的压力</li><li>如果无法全量缓存所有的数据，那么就无法使用该种方案</li></ul></li></ul></li></ul></li></ul><pre><code>// 差异化缓存过期时间@PostConstructpublic void rightInit1() {    //这次缓存的过期时间是30秒+10秒内的随机延迟    IntStream.rangeClosed(1, 1000).forEach(i -&gt; stringRedisTemplate.opsForValue().set(&quot;city&quot; + i, getCityFromDb(i), 30 + ThreadLocalRandom.current().nextInt(10), TimeUnit.SECONDS));    log.info(&quot;Cache init finished&quot;);    //同样1秒一次输出数据库QPS：   Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {        log.info(&quot;DB QPS : {}&quot;, atomicInteger.getAndSet(0));    }, 0, 1, TimeUnit.SECONDS);}// 让缓存不主动过期@PostConstructpublic void rightInit2() throws InterruptedException {    CountDownLatch countDownLatch = new CountDownLatch(1);    //每隔30秒全量更新一次缓存     Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {        IntStream.rangeClosed(1, 1000).forEach(i -&gt; {            String data = getCityFromDb(i);            //模拟更新缓存需要一定的时间            try {                TimeUnit.MILLISECONDS.sleep(20);            } catch (InterruptedException e) { }            if (!StringUtils.isEmpty(data)) {                //缓存永不过期，被动更新                stringRedisTemplate.opsForValue().set(&quot;city&quot; + i, data);            }        });        log.info(&quot;Cache update finished&quot;);        //启动程序的时候需要等待首次更新缓存完成        countDownLatch.countDown();    }, 0, 30, TimeUnit.SECONDS);    Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {        log.info(&quot;DB QPS : {}&quot;, atomicInteger.getAndSet(0));    }, 0, 1, TimeUnit.SECONDS);    countDownLatch.await();}</code></pre><h1 id="2-缓存击穿"><a href="#2-缓存击穿" class="headerlink" title="2. 缓存击穿"></a>2. 缓存击穿</h1><p>在某些Key属于极端热点数据并且并发量很大的情况下，如果这个Key过期，可能会在某个瞬间出现大量的并发请求同时回源，相当于大量的并发请求直接打到了数据库。这就是我们所说的缓存击穿或缓存并发问题。</p><p>如果说回源操作比较昂贵的话，那么这种并发就不能忽略不计了。可以考虑使用锁机制来限制回源的并发，或者可以使用类似Semaphore的工具来限制并发数，比如限制为10.这样子既限制了回源并发数不至于太大，又能够使得一定量的线程可以同时回源。</p><h1 id="3-缓存穿透"><a href="#3-缓存穿透" class="headerlink" title="3. 缓存穿透"></a>3. 缓存穿透</h1><p>缓存穿透指的是实际上缓存里有key 和value，但是其value可能为空，如果没做正确处理，那我们的逻辑可能会认为没有对当前的key做好缓存，会对所有的请求都回源到数据库上，这就会给数据库造成压力了。</p><p>针对这种问题，可以用以下方案解决：</p><ul><li><p>对于不存在的数据，同样设置一个特殊的Value到缓存当中，比如NODATA，这样子就不会有缓存穿透的问题</p><ul><li>可能会将大量无效的数据加入到缓存当中</li></ul></li><li><p>使用布隆过滤器</p><ul><li>放在缓存数据读取前先进行过滤操作</li><li>Google Guava BloomFilter </li></ul></li></ul><pre><code>private BloomFilter&lt;Integer&gt; bloomFilter;@PostConstructpublic void init() {    //创建布隆过滤器，元素数量10000，期望误判率1%    bloomFilter = BloomFilter.create(Funnels.integerFunnel(), 10000, 0.01);    //填充布隆过滤器    IntStream.rangeClosed(1, 10000).forEach(bloomFilter::put);}@GetMapping(&quot;right2&quot;)public String right2(@RequestParam(&quot;id&quot;) int id) {    String data = &quot;&quot;;    //通过布隆过滤器先判断    if (bloomFilter.mightContain(id)) {        String key = &quot;user&quot; + id;        //走缓存查询        data = stringRedisTemplate.opsForValue().get(key);        if (StringUtils.isEmpty(data)) {            //走数据库查询            data = getCityFromDb(id);            stringRedisTemplate.opsForValue().set(key, data, 30, TimeUnit.SECONDS);        }    }    return data;}</code></pre><h1 id="4-缓存数据同步策略"><a href="#4-缓存数据同步策略" class="headerlink" title="4. 缓存数据同步策略"></a>4. 缓存数据同步策略</h1><ul><li>当原始数据被修改了以后，我们很可能会采用主动更新缓存的策略<ul><li>可能的策略有<ul><li>先更新缓存，再更新数据库<ul><li>不可行</li><li>数据库操作失败是有可能的，会导致缓存和数据库当中的数据不一致</li></ul></li><li>先更新数据库，再更新缓存<ul><li>不可行</li><li>多线程情况下数据库中更新的顺序和缓存更新的顺序会不同，可能会导致旧数据最后到，导致问题的出现</li></ul></li><li>先删除缓存，再更新数据库，访问的时候按需加载数据到缓存当中<ul><li>很可能删除缓存以后还没来得及更新数据库，就有另外一个线程先读取了旧值到缓存中</li></ul></li><li>先更新数据库，再删除缓存，访问的时候按需加载数据到缓存当中<ul><li>出现缓存不一致的概率很低</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-缓存雪崩&quot;&gt;&lt;a href=&quot;#1-缓存雪崩&quot; class=&quot;headerlink&quot; title=&quot;1. 缓存雪崩&quot;&gt;&lt;/a&gt;1. 缓存雪崩&lt;/h1&gt;&lt;p&gt;缓存系统的IOPS比数据库高很多，因此需要注意短时间内的大量缓存失效的情况。这种情况一旦发生，可能就会在
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
  </entry>
  
  <entry>
    <title>利用注解 + 反射消除重复代码</title>
    <link href="https://www.llchen60.com/%E5%88%A9%E7%94%A8%E6%B3%A8%E8%A7%A3-%E5%8F%8D%E5%B0%84%E6%B6%88%E9%99%A4%E9%87%8D%E5%A4%8D%E4%BB%A3%E7%A0%81/"/>
    <id>https://www.llchen60.com/%E5%88%A9%E7%94%A8%E6%B3%A8%E8%A7%A3-%E5%8F%8D%E5%B0%84%E6%B6%88%E9%99%A4%E9%87%8D%E5%A4%8D%E4%BB%A3%E7%A0%81/</id>
    <published>2020-10-04T00:37:32.000Z</published>
    <updated>2020-10-04T00:38:18.211Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-案例分析"><a href="#1-案例分析" class="headerlink" title="1. 案例分析"></a>1. 案例分析</h1><h2 id="1-1-案例场景"><a href="#1-1-案例场景" class="headerlink" title="1.1 案例场景"></a>1.1 案例场景</h2><p>假设银行提供了一些 API 接口，对参数的序列化有点特殊，不使用 JSON，而是需要我们把参数依次拼在一起构成一个大字符串</p><ul><li>按照银行提供的API文档顺序，将所有的参数构成定长的数据，并且拼接在一起作为一整个字符串</li><li>因为每一种参数都有固定长度，未达到长度需要进行填充处理<ul><li>字符串类型参数不满长度部分要以下划线右填充，即字符串内容靠左</li><li>数字类型的参数不满长度部分以0左填充，即实际数字靠右</li><li>货币类型的表示需要把金额向下舍入2位到分，以分为单位，作为数字类型同样进行左填充</li><li>参数做MD5 操作作为签名</li></ul></li></ul><h2 id="1-2-初步代码实现"><a href="#1-2-初步代码实现" class="headerlink" title="1.2 初步代码实现"></a>1.2 初步代码实现</h2><pre><code>public class BankService {    //创建用户方法    public static String createUser(String name, String identity, String mobile, int age) throws IOException {        StringBuilder stringBuilder = new StringBuilder();        //字符串靠左，多余的地方填充_        stringBuilder.append(String.format(&quot;%-10s&quot;, name).replace(&#39; &#39;, &#39;_&#39;));        //字符串靠左，多余的地方填充_        stringBuilder.append(String.format(&quot;%-18s&quot;, identity).replace(&#39; &#39;, &#39;_&#39;));        //数字靠右，多余的地方用0填充        stringBuilder.append(String.format(&quot;%05d&quot;, age));        //字符串靠左，多余的地方用_填充        stringBuilder.append(String.format(&quot;%-11s&quot;, mobile).replace(&#39; &#39;, &#39;_&#39;));        //最后加上MD5作为签名        stringBuilder.append(DigestUtils.md2Hex(stringBuilder.toString()));        return Request.Post(&quot;http://localhost:45678/reflection/bank/createUser&quot;)                .bodyString(stringBuilder.toString(), ContentType.APPLICATION_JSON)                .execute().returnContent().asString();    }    //支付方法    public static String pay(long userId, BigDecimal amount) throws IOException {        StringBuilder stringBuilder = new StringBuilder();        //数字靠右，多余的地方用0填充        stringBuilder.append(String.format(&quot;%020d&quot;, userId));        //金额向下舍入2位到分，以分为单位，作为数字靠右，多余的地方用0填充        stringBuilder.append(String.format(&quot;%010d&quot;, amount.setScale(2, RoundingMode.DOWN).multiply(new BigDecimal(&quot;100&quot;)).longValue()));        //最后加上MD5作为签名        stringBuilder.append(DigestUtils.md2Hex(stringBuilder.toString()));        return Request.Post(&quot;http://localhost:45678/reflection/bank/pay&quot;)                .bodyString(stringBuilder.toString(), ContentType.APPLICATION_JSON)                .execute().returnContent().asString();    }}</code></pre><ul><li>这样做能够基本满足需求，但是存在一些问题：<ul><li>处理逻辑互相之间有重复，稍有不慎就会出现Bug</li><li>处理流程中字符串拼接、加签和发请求的逻辑，在所有方法重复</li><li>实际方法的入参的参数类型和顺序，不一定和接口要求一致，容易出错</li><li>代码层面参数硬编码，无法清晰进行核对</li></ul></li></ul><h2 id="1-3-使用接口和反射优化代码"><a href="#1-3-使用接口和反射优化代码" class="headerlink" title="1.3 使用接口和反射优化代码"></a>1.3 使用接口和反射优化代码</h2><h3 id="1-3-1-实现定义了所有接口参数的POJO类"><a href="#1-3-1-实现定义了所有接口参数的POJO类" class="headerlink" title="1.3.1 实现定义了所有接口参数的POJO类"></a>1.3.1 实现定义了所有接口参数的POJO类</h3><pre><code>@Datapublic class CreateUserAPI {    private String name;    private String identity;    private String mobile;    private int age;}</code></pre><h3 id="1-3-2-定义注解本身"><a href="#1-3-2-定义注解本身" class="headerlink" title="1.3.2 定义注解本身"></a>1.3.2 定义注解本身</h3><pre><code>@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Documented@Inheritedpublic @interface BankAPI {    String desc() default &quot;&quot;;    String url() default &quot;&quot;;}@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.FIELD)@Documented@Inheritedpublic @interface BankAPIField {    int order() default -1;    int length() default -1;    String type() default &quot;&quot;;}</code></pre><h3 id="1-3-3-反射配合注解实现动态的接口参数组装"><a href="#1-3-3-反射配合注解实现动态的接口参数组装" class="headerlink" title="1.3.3 反射配合注解实现动态的接口参数组装"></a>1.3.3 反射配合注解实现动态的接口参数组装</h3><pre><code>private static String remoteCall(AbstractAPI api) throws IOException {    //从BankAPI注解获取请求地址    BankAPI bankAPI = api.getClass().getAnnotation(BankAPI.class);    bankAPI.url();    StringBuilder stringBuilder = new StringBuilder();    Arrays.stream(api.getClass().getDeclaredFields()) //获得所有字段            .filter(field -&gt; field.isAnnotationPresent(BankAPIField.class)) //查找标记了注解的字段            .sorted(Comparator.comparingInt(a -&gt; a.getAnnotation(BankAPIField.class).order())) //根据注解中的order对字段排序            .peek(field -&gt; field.setAccessible(true)) //设置可以访问私有字段            .forEach(field -&gt; {                //获得注解                BankAPIField bankAPIField = field.getAnnotation(BankAPIField.class);                Object value = &quot;&quot;;                try {                    //反射获取字段值                    value = field.get(api);                } catch (IllegalAccessException e) {                    e.printStackTrace();                }                //根据字段类型以正确的填充方式格式化字符串                switch (bankAPIField.type()) {                    case &quot;S&quot;: {                        stringBuilder.append(String.format(&quot;%-&quot; + bankAPIField.length() + &quot;s&quot;, value.toString()).replace(&#39; &#39;, &#39;_&#39;));                        break;                    }                    case &quot;N&quot;: {                        stringBuilder.append(String.format(&quot;%&quot; + bankAPIField.length() + &quot;s&quot;, value.toString()).replace(&#39; &#39;, &#39;0&#39;));                        break;                    }                    case &quot;M&quot;: {                        if (!(value instanceof BigDecimal))                            throw new RuntimeException(String.format(&quot;{} 的 {} 必须是BigDecimal&quot;, api, field));                        stringBuilder.append(String.format(&quot;%0&quot; + bankAPIField.length() + &quot;d&quot;, ((BigDecimal) value).setScale(2, RoundingMode.DOWN).multiply(new BigDecimal(&quot;100&quot;)).longValue()));                        break;                    }                    default:                        break;                }            });    //签名逻辑   stringBuilder.append(DigestUtils.md2Hex(stringBuilder.toString()));    String param = stringBuilder.toString();    long begin = System.currentTimeMillis();    //发请求    String result = Request.Post(&quot;http://localhost:45678/reflection&quot; + bankAPI.url())            .bodyString(param, ContentType.APPLICATION_JSON)            .execute().returnContent().asString();    log.info(&quot;调用银行API {} url:{} 参数:{} 耗时:{}ms&quot;, bankAPI.desc(), bankAPI.url(), param, System.currentTimeMillis() - begin);    return result;}</code></pre><p>通过反射来动态获得class的信息，并在runtime的时候完成组装过程。这样做的好处是开发的时候会方便直观很多，然后将逻辑与细节隐藏起来，并且集中放到了一个方法当中，减少了重复，以及维护当中bug的出现。</p><h3 id="1-3-4-在代码中的应用"><a href="#1-3-4-在代码中的应用" class="headerlink" title="1.3.4 在代码中的应用"></a>1.3.4 在代码中的应用</h3><pre><code>@BankAPI(url = &quot;/bank/createUser&quot;, desc = &quot;创建用户接口&quot;)@Datapublic class CreateUserAPI extends AbstractAPI {    @BankAPIField(order = 1, type = &quot;S&quot;, length = 10)    private String name;    @BankAPIField(order = 2, type = &quot;S&quot;, length = 18)    private String identity;    @BankAPIField(order = 4, type = &quot;S&quot;, length = 11) //注意这里的order需要按照API表格中的顺序    private String mobile;    @BankAPIField(order = 3, type = &quot;N&quot;, length = 5)    private int age;}@BankAPI(url = &quot;/bank/pay&quot;, desc = &quot;支付接口&quot;)@Datapublic class PayAPI extends AbstractAPI {    @BankAPIField(order = 1, type = &quot;N&quot;, length = 20)    private long userId;    @BankAPIField(order = 2, type = &quot;M&quot;, length = 10)    private BigDecimal amount;}</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-案例分析&quot;&gt;&lt;a href=&quot;#1-案例分析&quot; class=&quot;headerlink&quot; title=&quot;1. 案例分析&quot;&gt;&lt;/a&gt;1. 案例分析&lt;/h1&gt;&lt;h2 id=&quot;1-1-案例场景&quot;&gt;&lt;a href=&quot;#1-1-案例场景&quot; class=&quot;headerlink
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Java" scheme="https://www.llchen60.com/tags/Java/"/>
    
      <category term="Annotations" scheme="https://www.llchen60.com/tags/Annotations/"/>
    
      <category term="Reflection" scheme="https://www.llchen60.com/tags/Reflection/"/>
    
      <category term="注解" scheme="https://www.llchen60.com/tags/%E6%B3%A8%E8%A7%A3/"/>
    
      <category term="反射" scheme="https://www.llchen60.com/tags/%E5%8F%8D%E5%B0%84/"/>
    
  </entry>
  
  <entry>
    <title>Spring IoC, AOP浅析</title>
    <link href="https://www.llchen60.com/Spring-IoC-AOP%E6%B5%85%E6%9E%90/"/>
    <id>https://www.llchen60.com/Spring-IoC-AOP%E6%B5%85%E6%9E%90/</id>
    <published>2020-10-02T02:43:05.000Z</published>
    <updated>2020-10-02T02:43:41.325Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-IoC"><a href="#1-IoC" class="headerlink" title="1. IoC"></a>1. IoC</h1><h2 id="1-1-Overview"><a href="#1-1-Overview" class="headerlink" title="1.1 Overview"></a>1.1 Overview</h2><ul><li><p>Inversion of control </p><ul><li>控制反转</li><li>将设计好的对象交给Spring容器来控制，而不是在对象内部控制</li></ul></li><li><p>好处</p><ul><li>可以无侵入的调整对象的关系</li><li>同时可以无侵入的调整对象的属性，甚至实现对象的替换</li></ul></li></ul><h2 id="1-2-单例Bean注入Prototype的Bean"><a href="#1-2-单例Bean注入Prototype的Bean" class="headerlink" title="1.2 单例Bean注入Prototype的Bean"></a>1.2 单例Bean注入Prototype的Bean</h2><p>Spring创建的Bean默认是单例的，但是当Bean遇到继承的时候，是会忽略这一点的。</p><pre><code>@Slf4jpublic abstract class SayService {    List&lt;String&gt; data = new ArrayList&lt;&gt;();    public void say() {        data.add(IntStream.rangeClosed(1, 1000000)                .mapToObj(__ -&gt; &quot;a&quot;)                .collect(Collectors.joining(&quot;&quot;)) + UUID.randomUUID().toString());        log.info(&quot;I&#39;m {} size:{}&quot;, this, data.size());    }}@Service@Slf4jpublic class SayHello extends SayService {    @Override    public void say() {        super.say();        log.info(&quot;hello&quot;);    }}@Service@Slf4jpublic class SayBye extends SayService {    @Override    public void say() {        super.say();        log.info(&quot;bye&quot;);    }}</code></pre><p>上述代码中，基类SayService是有状态的，dataList一直在增加。当SayHello类继承基类，并且声明为Service的时候，将其注册为Bean。这时候有状态的基类就很有可能造成内存泄露或者线程安全的问题了。</p><p>正确做法是在将类标记为<code>@Service</code>并交给容器进行管理之前，需要首先评估一下类是否有状态，然后为Bean设置合适的Scope。</p><pre><code>@Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE, proxyMode = ScopedProxyMode.TARGET_CLASS)</code></pre><p>让其以代理的方式注入，这样虽然controller还是单例的，但是每次都从代理那里获得Service，这样prototype范围的配置才会真正生效。</p><h1 id="2-AOP"><a href="#2-AOP" class="headerlink" title="2. AOP"></a>2. AOP</h1><ul><li><p>体现了松耦合，高内聚</p></li><li><p>在切面集中实现横切关注点</p><ul><li>缓存</li><li>权限</li><li>日志</li></ul></li><li><p>然后通过切点配置将代码注入到合适的地方</p></li><li><p>关键点</p><ul><li><p>连接点 Join Point </p><ul><li>实现AOP的地方</li><li>方法执行</li></ul></li><li><p>切点 PointCut</p><ul><li>告诉程序在哪里做切入</li><li>Spring中默认使用AspectJ查询表达式，通过在连接点运行查询表达式来匹配切入点</li></ul></li><li><p>增强 Advice</p><ul><li>定义了切入切点后增强的方式<ul><li>前</li><li>后</li><li>环绕</li></ul></li></ul></li><li><p>切面 Aspect</p><ul><li>切面 = 切点 + 增强 </li><li>实现整个AOP操作</li></ul></li></ul></li></ul><h2 id="2-1-实现整个日志记录，异常处理，方法耗时的统一切面"><a href="#2-1-实现整个日志记录，异常处理，方法耗时的统一切面" class="headerlink" title="2.1 实现整个日志记录，异常处理，方法耗时的统一切面"></a>2.1 实现整个日志记录，异常处理，方法耗时的统一切面</h2><pre><code>// 定义一个自定义注解Metrics @Retention(RetentionPolicy.RUNTIME)@Target({ElementType.METHOD, ElementType.TYPE})public @interface Metrics {    /**     * 在方法成功执行后打点，记录方法的执行时间发送到指标系统，默认开启     *     * @return     */    boolean recordSuccessMetrics() default true;    /**     * 在方法成功失败后打点，记录方法的执行时间发送到指标系统，默认开启     *     * @return     */    boolean recordFailMetrics() default true;    /**     * 通过日志记录请求参数，默认开启     *     * @return     */    boolean logParameters() default true;    /**     * 通过日志记录方法返回值，默认开启     *     * @return     */    boolean logReturn() default true;    /**     * 出现异常后通过日志记录异常信息，默认开启     *     * @return     */    boolean logException() default true;    /**     * 出现异常后忽略异常返回默认值，默认关闭     *     * @return     */    boolean ignoreException() default false;}</code></pre><pre><code>// 实现一个切面完成Metrics注解提供的功能@Aspect@Component@Slf4jpublic class MetricsAspect {    @Autowired    private ObjectMapper ObjectMapper;    //实现一个返回Java基本类型默认值的工具。其实，你也可以逐一写很多if-else判断类型，然后手动设置其默认值。    //这里为了减少代码量用了一个小技巧，即通过初始化一个具有1个元素的数组，然后通过获取这个数组的值来获取基本类型默认值     //Array.newInstance(Class&lt;?&gt; componentType, int length) 创建一个有特定的类的类型和长度的对象    private static final Map&lt;Class&lt;?&gt;, Object&gt; DEFAULT_VALUES =         Stream.of(boolean.class, byte.class, char.class, double.class, float.class, int.class, long.class, short.class)             .collect(toMap(clazz -&gt; (Class) clazz, clazz -&gt; Array.get(Array.newInstance(clazz, 1), 0)));     public static T getDefaultValue(Class clazz) {         return (T) DEFAULT_VALUES.get(clazz);     }    // 实现了对标记了Metrics注解的方法进行匹配    @PointCut(&quot;within(@org.cleilei.commonmistakes.springpart1.aopmetrics.Metrics *)&quot;)    public void withMetricsAnnotation() {    }    // 实现了匹配类型上标记了@RestController注解的方法    @PointCut(&quot;within(@org.springframework.web.bind.annotation.RestController *)&quot;)    public void controllerBean() {    }    @Around(&quot;controllerBean() || withMetricsAnnotation()&quot;)    public Object metrics(ProceedingJoinPoint pjp) throws Throwable {        // 通过连接点获取方法签名和方法上Metrics的注解，并根据方法签名生成日志中要输出的方法定义描述        MethodSignature signature = (MethodSignature)pjp.getSignature();        Metrics metrics = signature.getMethod().getAnnotation(Metrics.class);        String name = String.format(&quot;%s %s&quot;, signature.getDeclaringType().toString(), signature.toLongString());        if (metrics == null) {            @Metrics            final class c {}             metrics = c.class.getAnnotation(Metrics.class);        }        // 尝试从上下文获取请求的URL，来方便定位问题        RequestAttributes RequestAttributes = RequestContextHolder.getRequestAttributes();        if (requestAttributes != null) {            HttpServletRequest request = ((ServletRequestAttributes) requestAttributes).getRequest();            if (request != null) {                name += String.format(&quot; %s &quot;, request.getRequestURL().toString());            }        }        // 记录参数        if (metrics.logParameters()) {            log.info(String.format(&quot;Call method %s with parameters %s&quot;, name. objectMapper.writeValueAsString(pjp.getArgs())));        }        // 记录方法的执行，break points, 异常时记录        Object returnValue;        Instant start = Instant.now();        try {            returnValue = pjp.proceed();            if (metrics.recordSuccessMetrics())                //在生产级代码中，我们应考虑使用类似Micrometer的指标框架，把打点信息记录到时间序列数据库中，实现通过图表来查看方法的调用次数和执行时间                log.info(String.format(&quot;Call method %s succeed，time used：%d ms&quot;, name, Duration.between(start, Instant.now()).toMillis()));        } catch (Exception ex) {            if (metrics.recordFailMetrics())                log.info(String.format(&quot;Call method %s fail，time used：%d ms&quot;, name, Duration.between(start, Instant.now()).toMillis()));            if (metrics.logException())                log.error(String.format(&quot;Call method %s with exceptions&quot;, name), ex);            //忽略异常的时候，使用一开始定义的getDefaultValue方法，来获取基本类型的默认值            if (metrics.ignoreException())                returnValue = getDefaultValue(signature.getReturnType());            else                throw ex;        }        //实现了返回值的日志输出        if (metrics.logReturn())            log.info(String.format(&quot;Call method %s with result: %s&quot;, name, returnValue));        return returnValue;    }}</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-IoC&quot;&gt;&lt;a href=&quot;#1-IoC&quot; class=&quot;headerlink&quot; title=&quot;1. IoC&quot;&gt;&lt;/a&gt;1. IoC&lt;/h1&gt;&lt;h2 id=&quot;1-1-Overview&quot;&gt;&lt;a href=&quot;#1-1-Overview&quot; class=&quot;header
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Spring" scheme="https://www.llchen60.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>Java8 日期时间类</title>
    <link href="https://www.llchen60.com/Java8-%E6%97%A5%E6%9C%9F%E6%97%B6%E9%97%B4%E7%B1%BB/"/>
    <id>https://www.llchen60.com/Java8-%E6%97%A5%E6%9C%9F%E6%97%B6%E9%97%B4%E7%B1%BB/</id>
    <published>2020-09-28T21:20:22.000Z</published>
    <updated>2020-09-28T21:20:43.184Z</updated>
    
    <content type="html"><![CDATA[<p>在Java8之前，处理日期时间需求的时候需要使用Data, Calendar, SimpleDateFormat来声明时间戳，使用日历处理日期和格式化解析日期时间，Java8之后有了新的日期时间类，定义的比原来要清晰很多，且支持线程安全。</p><p>这篇文章看看时间错乱问题背后的原因，看看使用遗留的日期时间类，来处理日期时间初始化，格式化，解析，计算等可能会遇到的问题，以及如何使用新日期时间类来解决。</p><h1 id="初始化日期时间"><a href="#初始化日期时间" class="headerlink" title="初始化日期时间"></a>初始化日期时间</h1><h2 id="使用Date初始化日期时间"><a href="#使用Date初始化日期时间" class="headerlink" title="使用Date初始化日期时间"></a>使用Date初始化日期时间</h2><pre><code>Date date = new Date(2019, 12, 31, 11, 12, 13);System.out.println(date);// Output Sat Jan 31 11:12:13 CST 3920</code></pre><p>这是因为Date初始化时间的时候是用的和1970的差值，月的值是从0- 11 的</p><p>我们可以使用Calendar来定义时区的信息 </p><h1 id="时区问题"><a href="#时区问题" class="headerlink" title="时区问题"></a>时区问题</h1><pre><code>Calendar calendar = Calendar.getInstance();calendar.set(2019, 11, 31, 11, 12, 13);System.out.println(calendar.getTime());Calendar calendar2 = Calendar.getInstance(TimeZone.getTimeZone(&quot;America/New_York&quot;));calendar2.set(2019, Calendar.DECEMBER, 31, 11, 12, 13);System.out.println(calendar2.getTime());</code></pre><ul><li>Date类<ul><li>Date本身没有时区的问题，都是保存的UTC时间</li><li>Date当中保存的是一个时间戳，是从1970-1-1 0点到现在的毫秒数</li><li>保存方法<ul><li>以UTC保存</li><li>以字面量保存<ul><li>年月日 时分秒  时区信息</li></ul></li></ul></li></ul></li></ul><ul><li><p>ZoneId </p><ul><li><code>ZoneId.of</code>用来初始化一个标准的时区</li><li><code>ZoneOffset.ofHours</code> 通过offset来初始化具有指定的时间差的时区</li></ul></li><li><p>LocalDateTime</p><ul><li>不带有时区属性，是本地时区的日期时间</li></ul></li><li><p>ZonedDateTime </p><ul><li>= LocalDateTime + ZoneId </li></ul></li><li><p>DateTimeFormatter</p><ul><li>可以通过withZone方法直接设置需要使用的时区</li></ul></li><li><p>因此对于国际化时间的处理，应当使用Java8的日期时间类，通过ZonedDateTime来保存时间</p></li></ul><h1 id="日期时间格式化和解析"><a href="#日期时间格式化和解析" class="headerlink" title="日期时间格式化和解析"></a>日期时间格式化和解析</h1><ul><li>SimpleDateFormat的问题<ul><li>注意使用SimpleDateFormat的时候，大写的YYYY表示的是week year，即所在的周属于哪一年，yyyy表示的是年</li><li>static 的SimpleDateFormat可能会出现线程安全的问题，其解析和格式化操作是非线程安全的</li><li>SimpleDateFormat对于格式不匹配表现的非常宽容，可能会隐藏一些错误，要注意格式上的不同</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在Java8之前，处理日期时间需求的时候需要使用Data, Calendar, SimpleDateFormat来声明时间戳，使用日历处理日期和格式化解析日期时间，Java8之后有了新的日期时间类，定义的比原来要清晰很多，且支持线程安全。&lt;/p&gt;
&lt;p&gt;这篇文章看看时间错乱
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
  </entry>
  
  <entry>
    <title>Java文件读写Tips</title>
    <link href="https://www.llchen60.com/Java%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99Tips/"/>
    <id>https://www.llchen60.com/Java%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99Tips/</id>
    <published>2020-09-22T04:11:35.000Z</published>
    <updated>2020-09-22T04:12:02.059Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-字符编码问题"><a href="#1-字符编码问题" class="headerlink" title="1. 字符编码问题"></a>1. 字符编码问题</h1><p>FileReader是以当前机器的默认字符集来读取文件的，如果希望指定字符集的话，需要直接使用InputStreamReader和FileInputStream </p><p>因此对于字符编码问题，我们应当使用FileInputStream来读取文件流，然后使用InputStreamReader读取字符流，并且制定字符集</p><h1 id="2-Files类的静态方法"><a href="#2-Files类的静态方法" class="headerlink" title="2. Files类的静态方法"></a>2. Files类的静态方法</h1><ul><li><p><code>Files.readAllLines</code> </p><ul><li>可以很方便的一行代码完成文件内容的读取</li><li>但是读取超出内存大小的大文件的时候会出现OOM<ul><li>是因为readAllLines读取文件所有内容之后会放到一个List<String>当中返回，如果内存无法容纳这个List，就会OOM </li></ul></li></ul></li><li><p><code>Files.lines</code></p><ul><li>返回的是Stream<String></li><li>使得我们在需要的时候可以不断读取，使用文件中的内容，而不是一次性的把所有内容都读取到内存当中，因此避免了OOM</li></ul></li><li><p>对于返回是Stream的方法要注意关闭句柄，可以通过使用try with resources 的方式来确保流的close方法可以调用释放资源</p></li></ul><pre><code>LongAdder longAdder = new LongAdder();IntStream.rangeClosed(1, 1000000).forEach(i -&gt; {    try (Stream&lt;String&gt; lines = Files.lines(Paths.get(&quot;demo.txt&quot;))) {        lines.forEach(line -&gt; longAdder.increment());    } catch (IOException e) {        e.printStackTrace();    }});log.info(&quot;total : {}&quot;, longAdder.longValue());</code></pre><h1 id="3-读写文件的缓冲区设置"><a href="#3-读写文件的缓冲区设置" class="headerlink" title="3. 读写文件的缓冲区设置"></a>3. 读写文件的缓冲区设置</h1><h2 id="3-1-为什么执行缓慢？"><a href="#3-1-为什么执行缓慢？" class="headerlink" title="3.1 为什么执行缓慢？"></a>3.1 为什么执行缓慢？</h2><pre><code>private static void perByteOperation() throws IOException {    try (FileInputStream fileInputStream = new FileInputStream(&quot;src.txt&quot;);         FileOutputStream fileOutputStream = new FileOutputStream(&quot;dest.txt&quot;)) {        int i;        while ((i = fileInputStream.read()) != -1) {            fileOutputStream.write(i);        }    }}</code></pre><ul><li>上述代码运行起来功能上完全没有问题<ul><li>但是会非常缓慢</li><li>原因是每读取一个字节，每写入一个字节就进行一次IO操作，代价太大了</li><li>解决方案是可以使用缓冲区作为过渡，一次性从原文件当中读取一定数量的数据到缓冲区当中，一次性写入一定数量的数据到目标文件</li></ul></li></ul><pre><code>private static void bufferOperationWith100Buffer() throws IOException {    try (FileInputStream fileInputStream = new FileInputStream(&quot;src.txt&quot;);         FileOutputStream fileOutputStream = new FileOutputStream(&quot;dest.txt&quot;)) {        byte[] buffer = new byte[100];        int len = 0;        while ((len = fileInputStream.read(buffer)) != -1) {            fileOutputStream.write(buffer, 0, len);        }    }}</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-字符编码问题&quot;&gt;&lt;a href=&quot;#1-字符编码问题&quot; class=&quot;headerlink&quot; title=&quot;1. 字符编码问题&quot;&gt;&lt;/a&gt;1. 字符编码问题&lt;/h1&gt;&lt;p&gt;FileReader是以当前机器的默认字符集来读取文件的，如果希望指定字符集的话，需要直
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="IO" scheme="https://www.llchen60.com/tags/IO/"/>
    
  </entry>
  
  <entry>
    <title>前端请求的timeout设置</title>
    <link href="https://www.llchen60.com/%E5%89%8D%E7%AB%AF%E8%AF%B7%E6%B1%82%E7%9A%84timeout%E8%AE%BE%E7%BD%AE/"/>
    <id>https://www.llchen60.com/%E5%89%8D%E7%AB%AF%E8%AF%B7%E6%B1%82%E7%9A%84timeout%E8%AE%BE%E7%BD%AE/</id>
    <published>2020-09-18T04:09:01.000Z</published>
    <updated>2020-09-18T04:09:42.640Z</updated>
    
    <content type="html"><![CDATA[<p>当我们发出一个网络请求，但是没有做超时设置，一个隐含的假设是我们认为这个请求一定会成功。然而，我们无法做出请求一定会成功的保证的。</p><ul><li>当你发出的同步请求从没有返回的时候，线程会一直被占用的</li><li>异步请求未返回的线程也无法继续复用，因为sockets会有泄露，socket池的容量是有限的，未返回结果的线程会一直开着连接，最终可能会导致连接的短缺</li></ul><p>因此best practice应当是对于ajax请求，做好timeout的设置，因为XMLHttpRequest的默认timeout是0，即没有超时设置。</p><p>Client端的timeout设置和server端一样重要，浏览器可以开的socket的数量也是有限的，我们应该通过设置timeout来充分利用socket pool。Fetch API是当前比较流行的XMLHttpRequest API的替代品，然而现在还没有一个直接的设置timeout的方法，最近刚刚推出了abort API，可以用来支持timeout。</p><p>用法比如： </p><pre><code>const controller = new AbortController();const signal = controller.signal;const fetchPromise = fetch(url, {signal});  // No timeout by default!setTimeout(() =&gt; controller.abort(), 10000); </code></pre><p>而对于Jquery的ajax call，我们可以使用： </p><pre><code>$.ajax({    url: &quot;test.html&quot;,    error: function(){        // will fire when timeout is reached    },    success: function(){        //do something    },    timeout: 3000 // sets timeout to 3 seconds});</code></pre><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://robertovitillo.com/default-timeouts/" target="_blank" rel="noopener">https://robertovitillo.com/default-timeouts/</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当我们发出一个网络请求，但是没有做超时设置，一个隐含的假设是我们认为这个请求一定会成功。然而，我们无法做出请求一定会成功的保证的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当你发出的同步请求从没有返回的时候，线程会一直被占用的&lt;/li&gt;
&lt;li&gt;异步请求未返回的线程也无法继续复用，因为s
      
    
    </summary>
    
    
      <category term="FrontEnd" scheme="https://www.llchen60.com/categories/FrontEnd/"/>
    
    
  </entry>
  
  <entry>
    <title>Java日志记录Tips</title>
    <link href="https://www.llchen60.com/Java%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95Tips/"/>
    <id>https://www.llchen60.com/Java%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95Tips/</id>
    <published>2020-09-15T03:03:32.000Z</published>
    <updated>2020-09-15T03:04:02.755Z</updated>
    
    <content type="html"><![CDATA[<p>使用Java来记录日志有几个需要注意的地方的</p><ul><li>首先日志框架很多，不同的类库有可能会使用不同的日志框架，如何兼容是一个问题</li><li>配置文件的复杂性</li></ul><p>Java体系的日志框架有：</p><ul><li>Logback</li><li>Log4j</li><li>Log4j2</li><li>commons-logging </li><li>JDK 自带的Java.util.logging</li></ul><p>如果不同的包使用不同的日志框架的话，那管理就会变得非常麻烦。为了解决这个问题，就有了SLF4J – Simple Logging Facade For Java </p><ul><li>提供了统一的日志门面API，实现了中立的日志记录API</li><li>桥接功能<ul><li>可以将各种日志框架的API桥接到SLF4J API上。这样一来，即便你的程序试用了各种日志API记录日志，最终都可以桥接到Slf4j门面API上</li></ul></li><li>适配功能<ul><li>实现slf4j和实际日志框架的绑定</li><li>slf4j知识日志标准，还是需要一个实际的日志框架</li></ul></li></ul><p>下面一起梳理下常见的日志记录中的错误</p><h1 id="1-理解Logback配置，避免重复记录"><a href="#1-理解Logback配置，避免重复记录" class="headerlink" title="1. 理解Logback配置，避免重复记录"></a>1. 理解Logback配置，避免重复记录</h1><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;configuration&gt;    &lt;appender name=&quot;CONSOLE&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;        &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt;            &lt;pattern&gt;[%d{yyyy-MM-dd HH:mm:ss.SSS}] [%thread] [%-5level] [%logger{40}:%line] - %msg%n&lt;/pattern&gt;        &lt;/layout&gt;    &lt;/appender&gt;    &lt;logger name=&quot;org.geekbang.time.commonmistakes.logging&quot; level=&quot;DEBUG&quot;&gt;        &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt;    &lt;/logger&gt;    &lt;root level=&quot;INFO&quot;&gt;        &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt;    &lt;/root&gt;&lt;/configuration&gt;</code></pre><p>要注意在使用appender的时候，appender是如何挂载的，上述代码将appender挂载在了两个不同的地方，而且两个都定义在了root下，所以会造成重复。 </p><p>对于需要将不同的日志放到不同的文件的应用场景，可以通过设置Logger的additivity属性来实现这个操作</p><pre><code> &lt;logger name=&quot;org.geekbang.time.commonmistakes.logging&quot; level=&quot;DEBUG&quot; additivity=&quot;false&quot;&gt;            &lt;appender-ref ref=&quot;FILE&quot;/&gt;     &lt;/logger&gt;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用Java来记录日志有几个需要注意的地方的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先日志框架很多，不同的类库有可能会使用不同的日志框架，如何兼容是一个问题&lt;/li&gt;
&lt;li&gt;配置文件的复杂性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Java体系的日志框架有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logb
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
  </entry>
  
  <entry>
    <title>《网易一千零一夜》读书笔记</title>
    <link href="https://www.llchen60.com/%E3%80%8A%E7%BD%91%E6%98%93%E4%B8%80%E5%8D%83%E9%9B%B6%E4%B8%80%E5%A4%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>https://www.llchen60.com/%E3%80%8A%E7%BD%91%E6%98%93%E4%B8%80%E5%8D%83%E9%9B%B6%E4%B8%80%E5%A4%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</id>
    <published>2020-09-09T04:35:15.000Z</published>
    <updated>2020-09-16T04:29:15.133Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-为什么开始看这本书？"><a href="#0-为什么开始看这本书？" class="headerlink" title="0. 为什么开始看这本书？"></a>0. 为什么开始看这本书？</h1><p>接到了让人兴奋的项目，小振奋，然后突然意识到项目的复杂程度，需要涉及的方方面面可能已经超过了我能够处理的能力范畴，所以需要补课了…</p><p>看了很多知乎的答案啊，看着看着意识到当自己在努力寻找沉淀下来的经过思考的单领域的内容的时候，从知乎或者其他快节奏的平台找倒是真的有点舍本逐末哎。项目管理还是有很多核心理念是可以也已经沉淀下来的，所以就开始了找书看，对照着自己现在的境况，一点点解决的状态。</p><p>另外踩的一个小坑是太追求优质的工具，即疯狂比较各种管理记录的软件，比较omniplan vs xxx vs blablabla, 瞎倒腾了好一顿才意识到对于一个不懂项目管理的我来说，倒腾哪个都是白瞎。一定会按照完全不是软件本身设定的方式，来使用它，应该说是完全解决不了问题。</p><p>所以还是踏踏实实的带着敬畏感的从一本不太学术的书开始，从互联网公司 – 网易的产品经理的视角，来看看怎么做项目管理，怎么跟进项目的进程，如何应对各种过程中的设计变化，如何做各种取舍。</p><h1 id="1-项目管理Overview"><a href="#1-项目管理Overview" class="headerlink" title="1. 项目管理Overview"></a>1. 项目管理Overview</h1><h2 id="1-1-High-Level的建议"><a href="#1-1-High-Level的建议" class="headerlink" title="1.1 High Level的建议"></a>1.1 High Level的建议</h2><ul><li><p>项目到底需要什么</p><ul><li>接手一个项目之前，应当与项目的重要干系人加强沟通，理解前因后果<ul><li>对于技术项目，理解整个大致技术实现的思路，其中的痛点难点，潜在的不确定因素<ul><li>比如跨组的合作</li></ul></li></ul></li><li>而后，理解项目到底需要什么<ul><li>时间成本质量要素的权衡与取舍<ul><li>范围</li><li>时间</li><li>成本和质量</li></ul></li><li>各个角色目前的痛点</li></ul></li><li>大家对项目管理的认知和接受度<ul><li>通过怎样的途径，是全面推进，还是步步改善？ </li><li>从哪一个角度切入？ </li><li>蓝图是否清晰？</li><li>是否与项目负责人沟通到位并且达成一致？ </li></ul></li></ul></li><li><p>不要凡事事必躬亲</p><ul><li>替别人待办他们本该做的事情，对于团队来说反而效率最低</li><li>需要努力让别人能够做好这件事情<ul><li>Awareness<ul><li>使得团队成员知道需要做什么 </li></ul></li><li>Desire<ul><li>需要给予某方面的动能<ul><li>技术挑战程度？</li><li>项目完成以后的影响力？ </li><li>升职加薪？ </li></ul></li></ul></li><li>Ability <ul><li>确保其有足够的能力来做好这件事情<ul><li>初期的辅助</li><li>必要的培训等</li></ul></li></ul></li></ul></li></ul></li></ul><ul><li><p>不要追在别人屁股后面做监工</p><ul><li>项目经历不是监督事情做得怎么样的人</li><li>他是应该和大家一起将整个事情环节捋顺的人，需要建议<u><strong>一套对应的流程规则</strong></u>，明确各个角色在过程中的职责</li><li>获得认同，使得这个机制自行运转起来</li><li>努力做到是<strong>规则在约束大家的行为</strong>，而不是靠人看着来做</li></ul></li><li><p>言必信，行必果</p><ul><li><p>无权力下的领导力 – leadership without authority </p><ul><li>在弱矩阵结构下项目经历必修课</li></ul></li><li><p>需要构建起团队对你的信任，建立自己的可信度，打造个人品牌</p></li><li><p>信任的获取需要一点一滴的积累了</p></li><li><p>专业度上</p><ul><li>确定自己足够专业</li><li>至少相对更专业</li></ul></li><li><p>跟进</p><ul><li>会议，发布，邮件，承诺</li></ul></li></ul></li><li><p>处理争端</p><ul><li>与人一起解决问题，会因为不同的对于事情的看法产生很多争论</li><li>需要找出一致的地方并且努力放大</li></ul></li></ul><h2 id="1-2-关于时间估算"><a href="#1-2-关于时间估算" class="headerlink" title="1.2 关于时间估算"></a>1.2 关于时间估算</h2><ul><li><p>有没有必要进行时间估算以及进行到什么程度的时间估算？</p><ul><li><p>现状</p><ul><li>用了很多时间，但是最终的实际使用时间往往和估算的有不小的偏差</li></ul></li><li><p>房子从凌乱到整洁需要20%的努力，从整洁到一尘不染可能需要80%</p><ul><li>有估算实际上是完成了相对性价比比较高的一段</li></ul></li><li><p>估算可以给一个相对合理的计划，使得用户，管理层和团队都有一个稳定的预期</p></li></ul></li></ul><ul><li><p>估算单位</p><ul><li><p>理想人日</p><ul><li>指成员在不受干扰的情况下，全部时间用来开发需求所需的天数</li><li>劣势<ul><li>人的不同会导致整个估算的不同</li><li>这样的差异会导致我们队任务规模认识的偏差，很难衡量项目的实际大小</li></ul></li></ul></li><li><p>理想人时</p><ul><li>对应理想人日而存在</li><li>在充分理解需求的情况下，能帮助团队做到更靠近真实值的估算</li></ul></li><li><p>故事点</p><ul><li><p>对任务规模的估计，是一种相对的概念</p></li><li><p>优势</p><ul><li>基于故事点的估算不会因为开发人员的变更，时间的推移而改变</li></ul></li><li><p>劣势</p><ul><li>难以找到合适的估算单位</li></ul></li></ul></li></ul></li><li><p>估算的方式</p><ul><li><p>自下而上的估算</p><ul><li><p>每个开发人员估算自己的任务时间，然后将所有的任务汇总</p></li><li><p>团队特征</p><ul><li>成员间业务独立性强</li><li>相互之间业务熟悉度不高，熟悉成本高</li><li>各成员相对经验比较丰富</li></ul></li><li><p>优势</p><ul><li>估算效率高</li><li>准确度也会比较高</li></ul></li></ul></li><li><p>专家判断</p><ul><li>专家根据响应开发的情况给出任务的估算值</li></ul></li><li><p>扑克估算</p><ul><li><p>流程</p><ul><li>每个估计者都会分到一叠扑克牌，每张上有一个数值</li><li>由负责人对某个需求进行估算的需求或者任务进行讲解</li><li>讲解后，所有人都可以向该负责人提问关于该条需求或者任务的问题，直至足够了解</li><li>然后所有成员挑选一张扑克牌代表自己对该条目的估算</li><li>如果差值比较大，就需要人员说明各自给出这个估值的理由，然后再进行下一轮的估算</li><li>最后取平均值</li></ul></li><li><p>优势</p><ul><li>多成员一件，更客观</li><li>估算过程中，强化了大家对于需求和任务的理解，将任务考虑得更加细致，降低了不确定性给计划带来的冲击</li><li>使得相对严肃的计划和估算变得更加有趣，但是会花费更多的时间成本</li><li>需求探索的会更加深入，估算也会更加全面细致</li><li>让潜在的冲突公开化，台面化，让大家去充分碰撞，然后用一种近似游戏化的方式再去化解掉</li></ul></li></ul></li></ul></li><li><p>估算的注意事项</p><ul><li>估算仅仅是预测，当对外承诺项目完成时间的时候，最好提供一个日期范围，让听者知道你的估算只是预测</li><li>将任务分成更细的粒度是会有利于估算的</li><li>团队需要练习估算方式并且收集反馈，有迭代，有过去的数据，就可以做分析，来进行优化</li><li>估算需要进行反复进行，当项目进行一半，发现估算过于乐观的话，就需要对剩下的工作进行重新估算</li></ul></li><li><p>估算与Scrum</p><ul><li>在Scrum项目当中，我们会以迭代Sprint为周期来做增量交付</li><li>和传统的项目不一样的是在每个迭代计划当中我们不需要确定日期，只需要估算一个迭代我们能完成多少工作</li></ul></li></ul><h2 id="1-3-进度计划"><a href="#1-3-进度计划" class="headerlink" title="1.3 进度计划"></a>1.3 进度计划</h2><ul><li><p>制定计划的重要性</p><ul><li>计划的本质是团队对何时完成任务的承诺</li><li>排斥做计划的原因，在于人们不愿轻易做出承诺。人们都会有一种言行一致的愿望，一旦做出承诺，来自内心和外部的压力会迫使我们按照承诺去做。</li></ul></li><li><p>制定计划的时间点</p><ul><li>应该尽量早的制定出计划<ul><li>因为在混沌不清的时候，需要某种方式来做锚定，相当于挖个坑，根据少量的信息给出期望值，然后让人们一点点来填满它</li><li>当有了坑以后，就让大家一起来填</li></ul></li></ul></li><li><p>调整计划的注意事项</p><ul><li>计划是调整的基础和依据，但是调整计划需要注意：<ul><li>确保项目的每一个人都知道当前的计划是什么</li><li>调整计划需要怎样的决策过程</li><li>需要谁参与决策</li></ul></li></ul></li><li><p>如何做好计划</p><ul><li><p>项目立项前</p><ul><li>将目标按照功能体系分割成几个重大的里程碑</li><li>这个时候注意要给出立项的时间表 – 能够使得各个资源方有明确的预期，以便提前做好资源的调配<ul><li>什么时候完成初期调研和评估</li><li>何时做好立项准备</li><li>何时启动项目</li></ul></li></ul></li><li><p>项目立项后</p><ul><li>根据启动过程当中对于里程碑的大致预期，进一步推导出<ul><li>需求确认</li><li>设计确认</li><li>功能完成</li></ul></li></ul></li><li><p>需求确认后</p><ul><li>由设计，开发，测试一起做WBS，将工作细化分解<ul><li>注意的几个节点<ul><li>设计确认</li><li>功能完成</li><li>no bug</li><li>发布前的代码冻结</li></ul></li></ul></li></ul></li><li><p>完成标准</p><ul><li><p>需求设计确认</p><ul><li>团队要准备好怎么编写，在哪里编写代码</li></ul></li><li><p>功能完成</p><ul><li>所有定义的功能都已经完成，已经通过测试，允许质量差距和少量的Bug存在</li></ul></li><li><p>里程碑完成</p><ul><li>质量已达到适当水平，可以上线发布，或者开始下一个里程碑</li></ul></li></ul></li><li><p>Tips</p><ul><li>计划本质上是一种承诺，因此应该让团队共同参与制定出来</li><li>想让承诺有效，必须要是当事人积极公开且经过一番努力后自由选择得来的  </li></ul></li></ul></li></ul><h2 id="1-4-每日站会"><a href="#1-4-每日站会" class="headerlink" title="1.4 每日站会"></a>1.4 每日站会</h2><ul><li>定位在沟通交流<ul><li>不是汇报会</li><li>是用来分享信息，做出承诺以及提出路障的，解决的是团队协同的问题</li><li>需要让人参与进来<ul><li>红黄绿三牌的方式<ul><li>黄牌 – 进行相关提问，向发言者了解协同和依赖的信息</li><li>红牌 – 用来打断谈话，避免过度的讨论和无结果的时间浪费，提高站会效率</li><li>绿牌 – 代表每个人的发言权，将牌归还给主持人则意味着站会结束</li></ul></li></ul></li></ul></li></ul><h2 id="1-5-周会周报"><a href="#1-5-周会周报" class="headerlink" title="1.5 周会周报"></a>1.5 周会周报</h2><ul><li><p>周会的目的</p><ul><li>不仅仅是同步状态，汇总团队信息，这些是邮件，文件共享就可以实现的</li><li>目的在于<ul><li>面对面感受项目当前的整体状态，重要问题，接下去的目标，以及所需的调整</li><li>借此对项目当前重要的问题有一致的认识，进行小幅度的讨论，并形成下一步的工作事项 </li></ul></li></ul></li><li><p>Tips</p><ul><li><p>控制规模和时间</p><ul><li>最多10 - 15</li><li>1.5h maximum </li></ul></li><li><p>要不要轮流汇报？</p><ul><li>大部分项目整体情况可以通过项目经历事先收集来直接做整体概述</li><li>对于部分方向性或者商务性的小组，可以简单汇报主要工作和主要问题</li></ul></li><li><p>什么适合在周会中讨论？</p><ul><li>急事不适合</li><li>非跨团队的问题不适合周会讨论</li><li>纯执行细节问题不适合周会讨论</li><li>大方向决策问题不适合</li><li>可以讨论跨团队的涉及整体性计划的问题</li><li>中期改进型问题</li></ul></li><li><p>说话比例</p><ul><li>三分<ul><li>会议主持人，更新整体状态，主持讨论</li><li>需要回报的与会者的发言</li><li>所有与会者的讨论</li></ul></li></ul></li></ul></li><li><p>全体类周会</p></li><li><p>组长类周会</p></li><li><p>三方类 – 产品，运营，市场三方周会</p></li></ul><h2 id="1-6-工作汇报"><a href="#1-6-工作汇报" class="headerlink" title="1.6 工作汇报"></a>1.6 工作汇报</h2><ul><li><p>工作汇报的目的</p><ul><li><p>将状态，问题，风险知会相关干系人</p></li><li><p>寻求帮助</p><ul><li>需要在遇到问题的初期就积极主动寻求帮助，进而迅速的去解决</li></ul></li><li><p>自我审视</p><ul><li>周报是一种自我审视的过程，看看自己制定的目标和项目的完成情况</li></ul></li></ul></li><li><p>个人周报</p><ul><li><p>内容</p><ul><li><p>本周工作完成程度</p><ul><li>做了什么</li><li>完成结果如何</li></ul></li><li><p>下周工作计划</p><ul><li>需要做什么</li><li>时间点</li><li>完成的定义</li></ul></li><li><p>工作中遇到的问题和建议</p></li><li><p>个人感言和建议</p><ul><li>工作中的总结和分享，让上司知道你在想什么</li></ul></li></ul></li></ul></li></ul><ul><li>团队工作周报<ul><li>团队周报更多聚焦在结果和计划上，而非个人微观层面的事件总结</li><li>计划的时间跨度根据团队规模大小不同可以从1个月到3个月不等</li><li>内容<ul><li>上周达成的结果<ul><li>量化的结果指标<ul><li>销售额</li><li>用户数等</li></ul></li></ul></li><li>未来一段时间的规划<ul><li>通过图形化的方式言简意赅地列出任务的时间点和期望达到的结果</li></ul></li><li>达成如上规划图的风险/ 需要的协助<ul><li>资源风险</li><li>合作方风险</li><li>建议的应对方案</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;0-为什么开始看这本书？&quot;&gt;&lt;a href=&quot;#0-为什么开始看这本书？&quot; class=&quot;headerlink&quot; title=&quot;0. 为什么开始看这本书？&quot;&gt;&lt;/a&gt;0. 为什么开始看这本书？&lt;/h1&gt;&lt;p&gt;接到了让人兴奋的项目，小振奋，然后突然意识到项目的复杂程
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="项目管理" scheme="https://www.llchen60.com/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>《你早该这么玩Excel》</title>
    <link href="https://www.llchen60.com/%E3%80%8A%E4%BD%A0%E6%97%A9%E8%AF%A5%E8%BF%99%E4%B9%88%E7%8E%A9Excel%E3%80%8B/"/>
    <id>https://www.llchen60.com/%E3%80%8A%E4%BD%A0%E6%97%A9%E8%AF%A5%E8%BF%99%E4%B9%88%E7%8E%A9Excel%E3%80%8B/</id>
    <published>2020-09-05T20:06:45.000Z</published>
    <updated>2020-09-07T04:37:36.593Z</updated>
    
    <content type="html"><![CDATA[<p>一次对Excel的了解和正视之旅。相较于编程实现与通过Excel进行数据分析，编程在Scalability上胜出的，但是对于操作的便捷性，以及验证假设的速度上，确确实实Excel要胜出一筹。尤其是学到了三表的操作，VLookUp这种函数之后，开始理解Excel在数据分析上的巨大的作用。</p><ul><li><p>Highlights</p><ul><li>三表<ul><li>参数表</li><li>源数据表</li><li>分类汇总表</li></ul></li><li>巧用各种函数</li></ul></li><li><p>源数据表</p><ul><li>应当只有一张，对于每一个你想要研究的领域</li><li>应当为一维数据格式</li><li>标题内容不要写在表格当中，因为我们很可能需要索引的，<ul><li>可以对工作簿， 工作表进行命名来做区分</li></ul></li><li>源数据顺序<ul><li>应该按照工作当中的逻辑顺序来对列进行排序</li><li>列数据位置调整，shift + 拖动</li></ul></li><li>凡是同一种属性的数据都应该记录在同一列当中的</li><li>多个单元格批量录入<ul><li>选定多个单元格</li><li>在一个单元格当中输入内容</li><li>Ctrl + enter 输入一次的内容会被加载到你选中的所有单元格上</li></ul></li><li>源数据表当中不应该使用合并居中这种操作<ul><li>明细数据应当有一条记录一条</li><li>所有单元格应该被填满</li><li>每一行数据必须完整且整齐</li><li>合并单元格会导致只有首个单元格有数据，其他的都是空白单元格</li></ul></li><li>元数据只保留在一张表当中，放在多张表当中的话合并会非常非常麻烦</li><li>源数据表是为了商业上的use case服务的，需要理清楚需要什么数据<ul><li>按照逻辑顺序来分清各个column</li><li>在这之后可以按照可能的手动输入，复制粘贴来做cluster</li></ul></li></ul></li><li><p>三张表的定义</p><ul><li>参数表<ul><li>系统的配置参数，供源数据表和分类汇总表来调用</li><li>表示数据匹配关系或者某属性明细不会经常变更的数据</li></ul></li><li>源数据表<ul><li>数据的录入</li><li>一切与数据录入相关的工作都应该在源数据表当中进行</li><li>应满足一下条件<ul><li>一维数据</li><li>一个标题行</li><li>字段分类清晰</li><li>数据属性完整</li><li>数据连续</li><li>无合并单元格</li><li>无合计行</li><li>无分隔行/ 列</li><li>无空白单元格</li><li>单元格内容禁用短语句子</li></ul></li><li>分类汇总表<ul><li>希望是通过函数关联等从数据表当中获取一切所需的数据</li></ul></li><li>Thoughts <ul><li>企业信息化是必须的，需要有ERP, CRM, WMS, OA等企业系统</li><li>但是对于信息的个性化处理上来说，Excel会更占上风，可以更快速的给出各类数据</li></ul></li></ul></li></ul></li><li><p>数据透视表</p><ul><li>在源数据表当中选中想要做分析的数据，然后来生成Pivot table</li><li>步骤<ul><li>确认数据来源和待创建的报表类型</li><li>确认选定的数据区域</li><li>标题行需要被包含在内</li></ul></li><li>Tips<ul><li>分类多的字段尽量作为航字段</li></ul></li></ul></li><li><p>录入安全</p><ul><li>设置有关于数据有效性的限制，比如规定必须输入某种日期格式</li><li>对于包含公式的部分，可以直接进行锁定，这样其他人就无法对其进行修改了</li><li>手工录入，复制粘贴，公式链接的数据区域要用不同的填充色区分，来告知使用者什么地方需要填写，什么地方需要复制粘贴</li></ul></li><li><p>Vlookup</p><ul><li>查找引用函数<ul><li>查找某单元格数据在源数据库中是否存在，如果存在，就返回源数据库中同行指定列的单元格内容</li></ul></li><li>四个参数<ul><li>用什么找</li><li>在哪个表找</li><li>找到了返回什么值</li><li>精确找还是模糊找</li></ul></li></ul></li><li><p>图表</p><ul><li>做图表的目的是为了能够更加准确直观的诠释数据</li><li>饼状图<ul><li>说明比例关系</li></ul></li><li>柱状图<ul><li>比较数值</li></ul></li><li>折线图<ul><li>关注趋势</li></ul></li><li>概念图<ul><li>左右对比，适合男女</li><li>生成图标的源数据当中制造负数，来生成这种向两边延伸的效</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一次对Excel的了解和正视之旅。相较于编程实现与通过Excel进行数据分析，编程在Scalability上胜出的，但是对于操作的便捷性，以及验证假设的速度上，确确实实Excel要胜出一筹。尤其是学到了三表的操作，VLookUp这种函数之后，开始理解Excel在数据分析上的
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="Excel" scheme="https://www.llchen60.com/tags/Excel/"/>
    
  </entry>
  
  <entry>
    <title>Java 集合处理/ 空值处理/ 异常处理</title>
    <link href="https://www.llchen60.com/Java-%E9%9B%86%E5%90%88%E5%A4%84%E7%90%86-%E5%92%8C-%E7%A9%BA%E5%80%BC%E5%A4%84%E7%90%86/"/>
    <id>https://www.llchen60.com/Java-%E9%9B%86%E5%90%88%E5%A4%84%E7%90%86-%E5%92%8C-%E7%A9%BA%E5%80%BC%E5%A4%84%E7%90%86/</id>
    <published>2020-09-04T03:48:11.000Z</published>
    <updated>2020-09-07T17:15:59.334Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Arrays-asList"><a href="#1-Arrays-asList" class="headerlink" title="1. Arrays.asList"></a>1. <code>Arrays.asList</code></h1><p>业务开发当中，我们常常会将原始的数组转换为List类数据结构，来继续展开各种Stream操作</p><ul><li><p>Arrays.asList无法转换基本类型的数组，可以使用Arrays.stream来进行转换</p></li><li><p>Arrays.asList返回的list是不支持增删操作的，其返回的List是Arrays的内部类ArrayList。内部继承自AbstractList，没有覆写父类的add方法</p></li><li><p>对原始数组的修改会影响到我们获得的那个List</p><ul><li>ArrayList实际上是使用了原始的数组，因此在使用的时候，最好再使用New ArrayList来实现解耦</li></ul></li></ul><h1 id="2-空值处理"><a href="#2-空值处理" class="headerlink" title="2. 空值处理"></a>2. 空值处理</h1><h2 id="2-1-NullPointerException"><a href="#2-1-NullPointerException" class="headerlink" title="2.1 NullPointerException"></a>2.1 NullPointerException</h2><ul><li>可能出现的场景<ul><li>参数值是Integer等包装类型，使用时因为自动拆箱出现了空指针异常</li><li>字符串比较</li><li>ConcurrentHashMap这种容器不支持Key和Value为null，强行put null的key或Value会出现空指针异常</li><li>方法或远程服务返回的list是null，没做判空就直接调用，出现空指针异常</li><li>联级调用的null check</li></ul></li></ul><ul><li>best practice<ul><li><code>string.equalsTo(variableName)</code></li><li><code>Optional.ofNullable()</code></li><li><code>orElse()</code></li></ul></li></ul><h1 id="3-异常处理"><a href="#3-异常处理" class="headerlink" title="3. 异常处理"></a>3. 异常处理</h1><h2 id="3-1-在业务代码层面考虑异常处理"><a href="#3-1-在业务代码层面考虑异常处理" class="headerlink" title="3.1 在业务代码层面考虑异常处理"></a>3.1 在业务代码层面考虑异常处理</h2><ul><li><p>大多数业务应用都采用三层架构</p><ul><li><p>Controller层</p><ul><li>负责信息收集，参数校验，转换服务层处理的数据适配前端，轻业务逻辑</li><li>Controller 捕获异常，然后需要给用户友好用户的提示</li></ul></li><li><p>Service层</p><ul><li>负责核心业务逻辑，包括外部服务调用，访问数据库，缓存处理，消息处理等</li><li>一般会涉及到数据库事务，出现异常不适合捕获，否则事务无法自动回滚</li></ul></li><li><p>Repository层</p><ul><li>负责数据访问实现，一般没有业务逻辑</li><li>根据情况来做忽略，降级，或者转化为一个友好的异常</li></ul></li></ul></li><li><p>框架层面的异常处理</p><ul><li>尽量不要在框架层面做异常的自动，统一的处理</li><li>框架应当来做兜底工作，如果异常上升到最上层逻辑还是无法处理的话，可以用统一的方式进行异常转换<ul><li><code>@RestControllerAdvice</code></li><li><code>@ExceptionHandler</code></li></ul></li></ul></li></ul><h2 id="3-2-不要直接生吞异常"><a href="#3-2-不要直接生吞异常" class="headerlink" title="3.2 不要直接生吞异常"></a>3.2 不要直接生吞异常</h2><p>捕获了异常以后不应该生吞，因为吞掉的异常如果没有正常处理的话，出现Bug会很难发现。</p><p>需要有合适的转化成用户友好的异常，或者至少在warn， error级别来做log</p><h2 id="3-3-保留原始的信息"><a href="#3-3-保留原始的信息" class="headerlink" title="3.3 保留原始的信息"></a>3.3 保留原始的信息</h2><p>在捕捉了异常之后，一定要记得在log 或者在向外扔出的异常之中记录原始异常信息</p><pre><code>catch (IOException e) {    //只保留了异常消息，栈没有记录    log.error(&quot;文件读取错误, {}&quot;, e.getMessage());    throw new RuntimeException(&quot;系统忙请稍后再试&quot;);}catch (IOException e) {    throw new RuntimeException(&quot;系统忙请稍后再试&quot;, e);}</code></pre><h2 id="3-4-小心finally中的异常-try-with-resources"><a href="#3-4-小心finally中的异常-try-with-resources" class="headerlink" title="3.4 小心finally中的异常 + try with resources"></a>3.4 小心finally中的异常 + try with resources</h2><p>注意在资源释放处理等收尾操作的时候也可能会出现异常，这种时候，如果try block逻辑和finnally逻辑都有异常抛出的话，try当中的异常会被finnally中的异常覆盖掉，这会让问题变得非常不明显</p><pre><code>@GetMapping(&quot;wrong&quot;)public void wrong() {    try {        log.info(&quot;try&quot;);        //异常丢失        throw new RuntimeException(&quot;try&quot;);    } finally {        log.info(&quot;finally&quot;);        throw new RuntimeException(&quot;finally&quot;);    }}</code></pre><p>对于实现了AutoCloseable接口的资源，可以使用try-with-resources来释放资源，就是在try中带资源的声明</p><ul><li>try catch finally vs try with resources </li></ul><pre><code>Scanner scanner = null;try {    scanner = new Scanner(new File(&quot;test.txt&quot;));    while (scanner.hasNext()) {        System.out.println(scanner.nextLine());    }} catch (FileNotFoundException e) {    e.printStackTrace();} finally {    if (scanner != null) {        scanner.close();    }}try (Scanner scanner = new Scanner(new File(&quot;test.txt&quot;))) {    while (scanner.hasNext()) {        System.out.println(scanner.nextLine());    }} catch (FileNotFoundException fnfe) {    fnfe.printStackTrace();}</code></pre><h2 id="3-5-线程池任务的异常处理"><a href="#3-5-线程池任务的异常处理" class="headerlink" title="3.5 线程池任务的异常处理"></a>3.5 线程池任务的异常处理</h2><ul><li>设置自定义的异常处理程序作为保底，比如在声明线程池时自定义线程池的未捕获异常处理程序</li></ul><pre><code>new ThreadFactoryBuilder()  .setNameFormat(prefix+&quot;%d&quot;)  .setUncaughtExceptionHandler((thread, throwable)-&gt; log.error(&quot;ThreadPool {} got exception&quot;, thread, throwable))  .get()</code></pre><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.baeldung.com/java-try-with-resources" target="_blank" rel="noopener">https://www.baeldung.com/java-try-with-resources</a> </li><li><a href="https://time.geekbang.org/column/article/220230" target="_blank" rel="noopener">https://time.geekbang.org/column/article/220230</a> </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Arrays-asList&quot;&gt;&lt;a href=&quot;#1-Arrays-asList&quot; class=&quot;headerlink&quot; title=&quot;1. Arrays.asList&quot;&gt;&lt;/a&gt;1. &lt;code&gt;Arrays.asList&lt;/code&gt;&lt;/h1&gt;&lt;p&gt;业务开
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Java" scheme="https://www.llchen60.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Java数值计算精度问题</title>
    <link href="https://www.llchen60.com/Java%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%E7%B2%BE%E5%BA%A6%E9%97%AE%E9%A2%98/"/>
    <id>https://www.llchen60.com/Java%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%E7%B2%BE%E5%BA%A6%E9%97%AE%E9%A2%98/</id>
    <published>2020-09-01T05:29:02.000Z</published>
    <updated>2020-09-01T05:29:27.985Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Double"><a href="#1-Double" class="headerlink" title="1. Double"></a>1. Double</h1><pre><code>System.out.println(0.1+0.2);System.out.println(1.0-0.8);System.out.println(4.015*100);System.out.println(123.3/100);double amount1 = 2.15;double amount2 = 1.10;if (amount1 - amount2 == 1.05)    System.out.println(&quot;OK&quot;);// Output 0.300000000000000040.19999999999999996401.499999999999941.2329999999999999</code></pre><p>上述问题出现的原因是因为计算机是以二进制存储数值的，浮点数也是如此。 Java采用的是IEEE754标准来实现浮点数的表达和运算，当将一个10进制的数值转化成浮点数的时候，会出现无限循环的结果。当使用二进制表示是无限循环的时候，转换成10进制就会造成精度的缺失了。</p><h1 id="2-BigDecimal"><a href="#2-BigDecimal" class="headerlink" title="2. BigDecimal"></a>2. BigDecimal</h1><p>BigDecimal可以用于浮点数精确表达的场景，但是使用BigDecimal的时候，一定要注意使用字符串的构造方法来初始化</p><pre><code>System.out.println(new BigDecimal(&quot;0.1&quot;).add(new BigDecimal(&quot;0.2&quot;)));System.out.println(new BigDecimal(&quot;1.0&quot;).subtract(new BigDecimal(&quot;0.8&quot;)));System.out.println(new BigDecimal(&quot;4.015&quot;).multiply(new BigDecimal(&quot;100&quot;)));System.out.println(new BigDecimal(&quot;123.3&quot;).divide(new BigDecimal(&quot;100&quot;)));0.30.2401.5001.233</code></pre><ul><li><p>BigDecimal </p><ul><li>有scale, Precision的概念</li><li>scale 表示小数点右边的位数</li><li>precision 表示精度，即有效数字的长度</li></ul></li><li><p>BigDecimal的equals判等</p><ul><li>比较的是value和scale 两个值的！</li></ul></li></ul><pre><code>System.out.println(new BigDecimal(&quot;1.0&quot;).equals(new BigDecimal(&quot;1&quot;)))false</code></pre><pre><code>/** * Compares this {@code BigDecimal} with the specified * {@code Object} for equality.  Unlike {@link * #compareTo(BigDecimal) compareTo}, this method considers two * {@code BigDecimal} objects equal only if they are equal in * value and scale (thus 2.0 is not equal to 2.00 when compared by * this method). * * @param  x {@code Object} to which this {@code BigDecimal} is *         to be compared. * @return {@code true} if and only if the specified {@code Object} is a *         {@code BigDecimal} whose value and scale are equal to this *         {@code BigDecimal}&#39;s. * @see    #compareTo(java.math.BigDecimal) * @see    #hashCode */@Overridepublic boolean equals(Object x)</code></pre><h1 id="3-数值溢出问题"><a href="#3-数值溢出问题" class="headerlink" title="3. 数值溢出问题"></a>3. 数值溢出问题</h1><p>所有的基本数值类型都有超出表达范围的可能性，而且是没有任何异常的默默的溢出</p><ul><li>可以使用Math类的addExact, substractExact等方法进行数值运算，在溢出的时候主动抛出异常</li><li>也可以使用BigInteger，也会主动抛出异常</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Double&quot;&gt;&lt;a href=&quot;#1-Double&quot; class=&quot;headerlink&quot; title=&quot;1. Double&quot;&gt;&lt;/a&gt;1. Double&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;System.out.println(0.1+0.2);
System.
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Java" scheme="https://www.llchen60.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Java 判等问题</title>
    <link href="https://www.llchen60.com/Java-%E5%88%A4%E7%AD%89%E9%97%AE%E9%A2%98/"/>
    <id>https://www.llchen60.com/Java-%E5%88%A4%E7%AD%89%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-26T20:16:43.000Z</published>
    <updated>2020-08-26T20:17:17.758Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-equals-vs"><a href="#1-equals-vs" class="headerlink" title="1. equals vs =="></a>1. equals vs <code>==</code></h1><ul><li>对于基本类型，应使用==，比较的是值 </li><li>对于引用类型，需要使用equals，进行内容判等。使用<code>==</code>判断的是指针 –&gt; 代表的是两个对象在内存中的地址</li></ul><p>这里要注意的是Java是有字符串常量池机制的，当代码中出现双引号形式创建字符串对象的时候，JVM会先对字符串进行检查，如果字符串常量池存在相同内容的字符串对象的引用，就将这个引用返回；否则就创建新的字符串对象，然后将这个引用放入字符串常量池当中，并返回该引用</p><p>另外一个小坑是Integer在[-128,127]之间的数值是会做缓存的，即对于这中间的数值，即便你直接用<code>==</code>进行判断，有可能是直接会过的…</p><pre><code>public static Integer valueOf(int i) {    if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)        return IntegerCache.cache[i + (-IntegerCache.low)];    return new Integer(i);}</code></pre><h2 id="1-1-equals方法的实现"><a href="#1-1-equals方法的实现" class="headerlink" title="1.1 equals方法的实现"></a>1.1 equals方法的实现</h2><ul><li>equals在Object类当中的定义比较的是对象的引用</li></ul><pre><code>public boolean equals(Object obj) {    return (this == obj);}</code></pre><ul><li>而Integer，String类都重写了这个方法</li></ul><pre><code>public boolean equals(Object anObject) {    if (this == anObject) {        return true;    }    if (anObject instanceof String) {        String anotherString = (String)anObject;        int n = value.length;        if (n == anotherString.value.length) {            char v1[] = value;            char v2[] = anotherString.value;            int i = 0;            while (n-- != 0) {                if (v1[i] != v2[i])                    return false;                i++;            }            return true;        }    }    return false;}</code></pre><p>上述代码是先比较了引用，如果引用的地址一致，那么久可以直接返回true了。如果不一致，那就首先判断类的类型，如果是String类，再进行长度判断，如果长度一致，就逐个比较字符</p><ul><li>实现一个equals方法，需要注意<ul><li>首先进行指针判断，如果对象相同直接返回true</li><li>需要对另一方进行判空，空对象和自身的比较结果一定是false</li><li>需要判断两个对象的类型，如果类型都不同，那么直接返回false</li><li>在确保类型相同的情况下进行类型的强制转换，然后逐一判断所有字段<ul><li>需要进行类型强制转换是因为我们override的equals方法默认的输入参数是Object</li></ul></li></ul></li></ul><h2 id="1-2-使用Lombok的小坑"><a href="#1-2-使用Lombok的小坑" class="headerlink" title="1.2 使用Lombok的小坑"></a>1.2 使用Lombok的小坑</h2><p>Lombok的@Data注解会帮助我们实现equals和hashcode方法，但是有继承关系的时候，Lombok自动生成的方法是不会考虑到父类的</p><ul><li><p>对于不想进行equals和hashCode判断的参数，可以使用：</p><ul><li><code>@EqualsAndHashCode.Exclude</code></li></ul></li><li><p>对于想要使用父类属性的场景，可以使用</p><ul><li><code>@EqualsAndHashCode(callSuper = true)</code></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-equals-vs&quot;&gt;&lt;a href=&quot;#1-equals-vs&quot; class=&quot;headerlink&quot; title=&quot;1. equals vs ==&quot;&gt;&lt;/a&gt;1. equals vs &lt;code&gt;==&lt;/code&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;对于基本类型，
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Java" scheme="https://www.llchen60.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Spring @Transactional 事务Tips</title>
    <link href="https://www.llchen60.com/Spring-Transactional-%E4%BA%8B%E5%8A%A1Tips/"/>
    <id>https://www.llchen60.com/Spring-Transactional-%E4%BA%8B%E5%8A%A1Tips/</id>
    <published>2020-08-25T03:31:52.000Z</published>
    <updated>2020-08-26T18:08:52.862Z</updated>
    
    <content type="html"><![CDATA[<p>Spring针对Transaction APi, JDBC, Hibernate, Java Persistence API等事务API，实现了一致的编程模型，而Spring的声明式事务功能提供了非常方便的事务配置方式，使用<code>@Transactional</code>注解，就可以一键开启方法的事务性配置。</p><p>但是不是加上标注就能实现事务的，还是需要去关注事务是否有效，出错以后事务是否会正确回滚，当业务代码设计到多个子业务逻辑的时候，怎么正确处理事务。</p><h1 id="1-事务生效问题"><a href="#1-事务生效问题" class="headerlink" title="1. 事务生效问题"></a>1. 事务生效问题</h1><pre><code>@Entity@Datapublic class UserEntity {    @Id    @GeneratedValue(strategy = AUTO)    private Long id;    private String name;    public UserEntity() { }    public UserEntity(String name) {        this.name = name;    }}@Repositorypublic interface UserRepository extends JpaRepository&lt;UserEntity, Long&gt; {    List&lt;UserEntity&gt; findByName(String name);}@Service@Slf4jpublic class UserService {    @Autowired    private UserRepository userRepository;    //一个公共方法供Controller调用，内部调用事务性的私有方法    public int createUserWrong1(String name) {        try {            this.createUserPrivate(new UserEntity(name));        } catch (Exception ex) {            log.error(&quot;create user failed because {}&quot;, ex.getMessage());        }        return userRepository.findByName(name).size();    }    //标记了@Transactional的private方法    @Transactional    private void createUserPrivate(UserEntity entity) {        userRepository.save(entity);        if (entity.getName().contains(&quot;test&quot;))            throw new RuntimeException(&quot;invalid username!&quot;);    }    //根据用户名查询用户数    public int getUserCount(String name) {        return userRepository.findByName(name).size();    }}</code></pre><p>上述代码使用JPA做数据库访问，Entity定义在UserEntity当中，在服务层，声明了createUsr方法，当名字包含test的时候，希望抛出异常，然后实现数据库的回滚（只是例子，当然实际实现上将判断和数据库存储执行顺序换过来就能避开这里的问题了）</p><p>当调用的时候，发现即使用户名不合法，也能够调用成功，这是因为上述代码将注解定义到了private方法，因此不生效</p><blockquote><p>只有定义在public方法上的@Transactional才能生效，因为Spring默认通过动态代理的方式实现AOP，对目标方法进行增强，private方法无法代理到，Spring也就无法使用动态增强事务处理的逻辑了。</p></blockquote><p>然而就算把上述的private方法改为public transactional依旧不会生效，这是因为：</p><blockquote><p>Transactional需要通过代理过的类从外部调用目标方法才能生效</p></blockquote><p>Spring通过AOP技术对方法进行增强，要调用增强过的方法必然是调用代理之后的对象</p><p>因此我们可以在controller层调用这个逻辑，来实现整个transactional的支持。即你需要使用Spring注入的类，通过代理调用才有机会来进行动态的增强。</p><h1 id="2-事务回滚问题"><a href="#2-事务回滚问题" class="headerlink" title="2. 事务回滚问题"></a>2. 事务回滚问题</h1><p>通过AOP锁实现的事务处理可以理解为使用try catch来包裹标记了<code>@Transactional</code>注解的方法，当方法出现了异常并且满足一定条件的时候，在catch里面我们可以设置事务回滚，没有异常则直接提交事务。</p><ol><li>只有异常传播出标记了注解的方法，事务才能回滚</li></ol><pre><code>try {   // This is an around advice: Invoke the next interceptor in the chain.   // This will normally result in a target object being invoked.   retVal = invocation.proceedWithInvocation();}catch (Throwable ex) {   // target invocation exception   completeTransactionAfterThrowing(txInfo, ex);   throw ex;}finally {   cleanupTransactionInfo(txInfo);}</code></pre><ol start="2"><li>默认情况下，出现RuntimeException或者Error的时候，Spring才会回滚事务</li></ol><p>在必要的时候，可以选择手动进行回滚，以及遇到所有的Exception都回滚事务</p><pre><code>@Transactionalpublic void createUserRight1(String name) {    try {        userRepository.save(new UserEntity(name));        throw new RuntimeException(&quot;error&quot;);    } catch (Exception ex) {        log.error(&quot;create user failed&quot;, ex);        TransactionAspectSupport.currentTransactionStatus().setRollbackOnly();    }}@Transactional(rollbackFor = Exception.class)public void createUserRight2(String name) throws IOException {    userRepository.save(new UserEntity(name));    otherTask();}</code></pre><ul><li>有时我们会遇到嵌套逻辑，分别需要实现事务的问题，而子逻辑事务的回滚不希望影响到父逻辑，可以使用<code>@Transactional(propagation = Propagation.REQUIRES_NEW)</code>, 以此来设置事务传播策略，即执行到这个方法的时候需要开启新的事务，并挂起当前事务。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Spring针对Transaction APi, JDBC, Hibernate, Java Persistence API等事务API，实现了一致的编程模型，而Spring的声明式事务功能提供了非常方便的事务配置方式，使用&lt;code&gt;@Transactional&lt;/cod
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="事务" scheme="https://www.llchen60.com/tags/%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>HTTP调用的超时，并发</title>
    <link href="https://www.llchen60.com/HTTP%E8%B0%83%E7%94%A8%E7%9A%84%E8%B6%85%E6%97%B6%EF%BC%8C%E5%B9%B6%E5%8F%91/"/>
    <id>https://www.llchen60.com/HTTP%E8%B0%83%E7%94%A8%E7%9A%84%E8%B6%85%E6%97%B6%EF%BC%8C%E5%B9%B6%E5%8F%91/</id>
    <published>2020-08-23T21:43:11.000Z</published>
    <updated>2020-08-23T21:44:10.267Z</updated>
    
    <content type="html"><![CDATA[<p>HTTP调用的时候，是通过HTTP协议进行一次网络请求，网络请求会有超时的可能性，我们需要考虑到：</p><ul><li>使用的框架设置的默认超时的合理性</li><li>超时后的请求重试需要考虑到服务端接口的幂等性 – 即任意多次执行所产生的影响是否与一次执行的影响相同</li><li>需要考虑框架是否会限制并发连接数，以免在服务并发很大的情况下，HTTP调用的并发数限制成为瓶颈 </li></ul><p>常用框架： </p><ul><li>Spring Cloud <ul><li>需要使用Feign进行声明式的服务调用</li></ul></li><li>Spring Boot<ul><li>使用Apache HttpClient进行服务调用</li></ul></li></ul><h1 id="1-如何配置连接超时"><a href="#1-如何配置连接超时" class="headerlink" title="1. 如何配置连接超时"></a>1. 如何配置连接超时</h1><ul><li><p>HTTP调用应用层走的是HTTP协议，但是网络层还是TCP/IP协议的</p><ul><li><p>TCP/ IP协议是面向连接的协议，在传输数据之前需要建立连接</p></li><li><p>网络框架会提供两个超时参数</p><ul><li><p>连接超时参数 ConnectTimeout</p><ul><li>建立连接阶段的最长等待时间</li><li>应该配置在1 - 5s之间，因为TCP的三次握手建立连接需要的时间实际上是非常短的，超出往往是网络或者防火墙配置的问题</li></ul></li><li><p>读取超时参数 ReadTimeout</p><ul><li><p>用来控制从Socket上读取数据的最长等待时间</p></li><li><p>读取超时包括</p><ul><li>网络问题</li><li>服务端处理业务逻辑的时间</li></ul></li><li><p>参数配置不应过大</p><ul><li>HTTP请求一般是同步调用，如果超时很长，在等待服务端返回数据的同时，客户端线程也在等待</li><li>当下游服务出现大量超时的时候，程序可能也会受到拖累创建大量线程，最终崩溃</li></ul></li></ul></li></ul></li></ul></li></ul><ul><li>首先对于超时本身<ul><li>是客户端和服务端需要都有贡献的</li><li>有一致的时间估计</li><li>平衡吞吐量和错误率</li></ul></li></ul><h1 id="2-HTTP调用并发问题"><a href="#2-HTTP调用并发问题" class="headerlink" title="2. HTTP调用并发问题"></a>2. HTTP调用并发问题</h1><p>如果使用Apache 的httpClient，在PoolingHttpClientConnectionManager当中，定义的参数： </p><ul><li>defaultMaxPerRoute = 2<ul><li>同一个主机最大的并发请求书为2</li></ul></li><li>maxTotal = 20<ul><li>主机的最大并发为20</li></ul></li></ul><pre><code>httpClient2 = HttpClients.custom().setMaxConnPerRoute(10).setMaxConnTotal(20).build();</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;HTTP调用的时候，是通过HTTP协议进行一次网络请求，网络请求会有超时的可能性，我们需要考虑到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用的框架设置的默认超时的合理性&lt;/li&gt;
&lt;li&gt;超时后的请求重试需要考虑到服务端接口的幂等性 – 即任意多次执行所产生的影响是否与一次执行的影响
      
    
    </summary>
    
    
      <category term="Web" scheme="https://www.llchen60.com/categories/Web/"/>
    
    
      <category term="HTTP" scheme="https://www.llchen60.com/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>线程池创建: Executors  vs ThreadPoolExecutor</title>
    <link href="https://www.llchen60.com/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%88%9B%E5%BB%BA-Executors-vs-ThreadPoolExecutor/"/>
    <id>https://www.llchen60.com/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%88%9B%E5%BB%BA-Executors-vs-ThreadPoolExecutor/</id>
    <published>2020-08-16T22:17:49.000Z</published>
    <updated>2020-08-16T22:18:46.296Z</updated>
    
    <content type="html"><![CDATA[<p>工程上对于线程池的使用必不可少，很多人会选择使用Executors class定义的<code>newCachedThreadPool</code>以及<code>newFixedThreadPool</code>。这篇博文就稍微分析一下二者适用的场景，以及我们应该使用Executors的方法还是直接调用ThreadPoolExecutor来创建线程池。</p><p>首先让我们一起看看二者的源码</p><pre><code>   /**     * Creates a thread pool that reuses a fixed number of threads     * operating off a shared unbounded queue.  At any point, at most     * {@code nThreads} threads will be active processing tasks.     * If additional tasks are submitted when all threads are active,     * they will wait in the queue until a thread is available.     * If any thread terminates due to a failure during execution     * prior to shutdown, a new one will take its place if needed to     * execute subsequent tasks.  The threads in the pool will exist     * until it is explicitly {@link ExecutorService#shutdown shutdown}.     *     * @param nThreads the number of threads in the pool     * @return the newly created thread pool     * @throws IllegalArgumentException if {@code nThreads &lt;= 0}     */    public static ExecutorService newFixedThreadPool(int nThreads) {        return new ThreadPoolExecutor(nThreads, nThreads,                                      0L, TimeUnit.MILLISECONDS,                                      new LinkedBlockingQueue&lt;Runnable&gt;());    }    /**     * Creates a thread pool that creates new threads as needed, but     * will reuse previously constructed threads when they are     * available.  These pools will typically improve the performance     * of programs that execute many short-lived asynchronous tasks.     * Calls to {@code execute} will reuse previously constructed     * threads if available. If no existing thread is available, a new     * thread will be created and added to the pool. Threads that have     * not been used for sixty seconds are terminated and removed from     * the cache. Thus, a pool that remains idle for long enough will     * not consume any resources. Note that pools with similar     * properties but different details (for example, timeout parameters)     * may be created using {@link ThreadPoolExecutor} constructors.     *     * @return the newly created thread pool     */    public static ExecutorService newCachedThreadPool() {        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,                                      60L, TimeUnit.SECONDS,                                      new SynchronousQueue&lt;Runnable&gt;());    } </code></pre><p>二者对比，你会发现实际上他们都是调用的ThreadPoolExecutor,只是参数是不一样的。那让我们看看ThreadPoolExecutor的源码</p><pre><code>    /**     * Creates a new {@code ThreadPoolExecutor} with the given initial     * parameters and default thread factory and rejected execution handler.     * It may be more convenient to use one of the {@link Executors} factory     * methods instead of this general purpose constructor.     *     * @param corePoolSize the number of threads to keep in the pool, even     *        if they are idle, unless {@code allowCoreThreadTimeOut} is set     * @param maximumPoolSize the maximum number of threads to allow in the     *        pool     * @param keepAliveTime when the number of threads is greater than     *        the core, this is the maximum time that excess idle threads     *        will wait for new tasks before terminating.     * @param unit the time unit for the {@code keepAliveTime} argument     * @param workQueue the queue to use for holding tasks before they are     *        executed.  This queue will hold only the {@code Runnable}     *        tasks submitted by the {@code execute} method.     * @throws IllegalArgumentException if one of the following holds:&lt;br&gt;     *         {@code corePoolSize &lt; 0}&lt;br&gt;     *         {@code keepAliveTime &lt; 0}&lt;br&gt;     *         {@code maximumPoolSize &lt;= 0}&lt;br&gt;     *         {@code maximumPoolSize &lt; corePoolSize}     * @throws NullPointerException if {@code workQueue} is null     */    public ThreadPoolExecutor(int corePoolSize,                              int maximumPoolSize,                              long keepAliveTime,                              TimeUnit unit,                              BlockingQueue&lt;Runnable&gt; workQueue) {        this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,             Executors.defaultThreadFactory(), defaultHandler);    }</code></pre><p>看一下其中需要的几个参数：</p><ul><li><p>corePoolSize</p><ul><li>在线程池中最少要保持的线程数量，哪怕已经超过了定义的keepAliveTime </li></ul></li><li><p>maximumPoolSize</p><ul><li>线程池允许的最大线程数量</li></ul></li><li><p>keepAliveTime</p><ul><li>当当前线程数超过核心线程数量的时候，就会检查闲置的线程，如果在这段时间没有新的任务，就暂停当前线程</li></ul></li><li><p>unit</p><ul><li>定义事件单位</li></ul></li><li><p>workQueue</p><ul><li>在任务还没有执行之前，被用来持有这些任务的</li><li>queue之后持有execute方法提交的Runnable任务</li></ul></li></ul><p>带着这些信息我们再来看Executors.newFixedThreadPool的定义，方法传入了线程数量，然后核心线程数和最大线程数被设为一样的数值，让我们来看看在不同情况下他的表现：</p><ul><li><p>任务数小于等于设定的线程数</p><ul><li>一切运行正常</li><li>限制的线程不会被关闭</li></ul></li><li><p>任务数大于设定的线程数</p><ul><li>任务会加入到队列当中，进行等待</li><li>值得注意的是在实例化LinkedBlockingQueue的时候，传入的参数是<code>this(Integer.MAX_VALUE);</code><ul><li>这意味着如果任务在线程中执行的时间非常长，任务可以在队列中堆积到无限大，最终结果会是内存被占满..程序崩溃</li></ul></li></ul></li></ul><p>而对于Executors.newCachedThreadPool来说，其定义的核心线程数量为0，最大线程数是<code>Integer.MAX_VALUE</code>,即理论上是可以有无限多的线程，keepAliveTime是60秒，使用的是SynchrounousQueue。</p><ul><li>当任务进来的时候<ul><li>会增加线程</li><li>有多少任务进来，就会使用ThreadFactory开多少线程，因为允许的最大线程数时无限大，所以可以一直这么开下去</li><li>而其workqueue是SynchrounousQueue,其大小始终为0，在这里我们可以直接任务当任务进来的时候，如果没有空闲的线程，会直接让ThreadFactory来构建新的线程了</li><li>那么当任务无限多的时候，就会创建无数多的线程，直接撑爆内存了</li></ul></li></ul><p>由此可以看出来使用Executors的两个方法直接构建线程池因为设定的参数是无界的，可能会导致OOM的错误，更好的方式是自己根据当前线程池的应用场景，来设定参数。</p><p>根据应用场景的不同，根据doc，我们有三大类的queue可以选择，分别为：</p><ul><li><p><code>Synchronous queue</code></p><ul><li>直接讲任务交给线程</li><li>自己本身不持有任何任务的</li><li>针对的应用场景可以是各个线程之间任务的执行有某些内在的联系，阻碍一个的执行可能会影响另外一个</li><li>为了不拒绝新的线程的创建，就必须设定线程池的大小为Integer.MAX_VALUE</li><li>这样如果处理速度低于新任务的提交速度的话，可能会导致非常非常大的线程池</li></ul></li><li><p><code>LinkedBlockingQueue</code></p><ul><li>使用没有边界的queue</li><li>这样当所有核心线程都忙碌的时候，任务就都会在队列当中排队</li><li>这种方式可以环节突发性的峰值，但是如果处理速度慢于任务堆积的速度，queue会变得很大</li></ul></li><li><p><code>ArrayBlockingQueue</code></p><ul><li>有限长的queue</li><li>这样可以防止资源耗尽，但是也很难做调整和优化</li><li>队列的大小和最大线程数相互影响，很难做到优化</li><li>使用大队列，小线程池可以减少对于CPU的使用，线程切换的损耗，但是单位时间处理速度不会太高</li><li>使用小队列，大线程池可以让CPU更忙碌，但是切换线程会有不小的损耗</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.ibm.com/developerworks/library/j-jtp0730/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/library/j-jtp0730/index.html</a> </li><li><a href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html" target="_blank" rel="noopener">https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;工程上对于线程池的使用必不可少，很多人会选择使用Executors class定义的&lt;code&gt;newCachedThreadPool&lt;/code&gt;以及&lt;code&gt;newFixedThreadPool&lt;/code&gt;。这篇博文就稍微分析一下二者适用的场景，以及我们应该使用Ex
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Executors" scheme="https://www.llchen60.com/tags/Executors/"/>
    
      <category term="ThreadPoolExecutor" scheme="https://www.llchen60.com/tags/ThreadPoolExecutor/"/>
    
  </entry>
  
  <entry>
    <title>AWS CDK Overview</title>
    <link href="https://www.llchen60.com/AWS-CDK-Overview/"/>
    <id>https://www.llchen60.com/AWS-CDK-Overview/</id>
    <published>2020-08-14T03:17:35.000Z</published>
    <updated>2020-08-14T03:18:06.405Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li><p>AWS CDK </p><ul><li><p>Open source software development framework </p></li><li><p>Model and provision your cloud application resources</p></li><li><p>To resolve what issue? </p><ul><li>provision cloud applications is challenging cause require<ul><li>manual actions</li><li>custom scripts </li><li>maintain templates </li><li>domain specific languages </li></ul></li></ul></li><li><p>How does CDK resolve the issue?</p><ul><li>Provides with high level component that pre-configure cloud resources with proven defaults</li><li>Provision your resources through AWS CloudFormation </li></ul></li></ul></li></ul><h2 id="1-1-Workflow"><a href="#1-1-Workflow" class="headerlink" title="1.1 Workflow"></a>1.1 Workflow</h2><ul><li>Creating an Amazon ECS service with AWS Fargate launch type </li></ul><pre><code>public class MyEcsConstructStack extends Stack {    public MyEcsConstructStack(final Construct scope, final String id) {        this(scope, id, null);    }    public MyEcsConstructStack(final Construct scope, final String id,            StackProps props) {        super(scope, id, props);        Vpc vpc = Vpc.Builder.create(this, &quot;MyVpc&quot;).maxAzs(3).build();        Cluster cluster = Cluster.Builder.create(this, &quot;MyCluster&quot;)                .vpc(vpc).build();        ApplicationLoadBalancedFargateService.Builder.create(this, &quot;MyFargateService&quot;)                .cluster(cluster)                .cpu(512)                .desiredCount(6)                .taskImageOptions(                       ApplicationLoadBalancedTaskImageOptions.builder()                               .image(ContainerImage                                       .fromRegistry(&quot;amazon/amazon-ecs-sample&quot;))                               .build()).memoryLimitMiB(2048)                .publicLoadBalancer(true).build();    }}</code></pre><ul><li><p>Basic workflow</p><ul><li>create the app from a template provided by the AWS CDK</li><li>add code to the app to create resources within stacks</li><li>build the app </li><li>synthesize one or more stacks in the app to create an AWS CloudFormation template </li><li>deploy one or more stacks to your AWS account </li></ul></li><li><p>Benefits </p><ul><li>Could use logic when defining infrastructure </li><li>Use object-oriented techniques to create a model of system </li><li>Define high level abstractions</li></ul></li><li><p>Tools</p><ul><li><p><a href="https://docs.aws.amazon.com/cdk/latest/guide/cli.html" target="_blank" rel="noopener">CDK Toolkit</a> </p><ul><li>CLI for interacting with CDK apps </li><li>Enable developers to synthesize artifacts such as AWS CloudFormation templates, deploy stacks to development AWS accounts, and diff against a deployed stack to understand the impact of a code change </li></ul></li><li><p><a href="https://docs.aws.amazon.com/cdk/latest/guide/constructs.html" target="_blank" rel="noopener">AWS Construct Library</a></p><ul><li>contains constructs representing AWS resources </li><li>encapsulate the details of how to create resources for an Amazon or AWS service </li></ul></li></ul></li></ul><h2 id="1-1-1-Create-and-build-the-app"><a href="#1-1-1-Create-and-build-the-app" class="headerlink" title="1.1.1 Create and build the app"></a>1.1.1 Create and build the app</h2><pre><code>mkdir hello-cdk &amp;&amp; cd hello-cdkcdk init TEMPLATE --language LANGUAGE cdk init app --language java// In your IDE, import it as maven project mvn compile cdk ls </code></pre><h3 id="1-1-2-Add-an-Amazon-S3-Bucket"><a href="#1-1-2-Add-an-Amazon-S3-Bucket" class="headerlink" title="1.1.2 Add an Amazon S3 Bucket"></a>1.1.2 Add an Amazon S3 Bucket</h3><pre><code>// Add dependencies to pom.xml &lt;dependency&gt;    &lt;groupId&gt;software.amazon.awscdk&lt;/groupId&gt;    &lt;artifactId&gt;s3&lt;/artifactId&gt;    &lt;version&gt;${cdk.version}&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>Define an Amazon S3 bucket in the stack using L2 construct </p><pre><code>package com.myorg;import software.amazon.awscdk.core.*;import software.amazon.awscdk.services.s3.Bucket;public class HelloCdkStack extends Stack {    public HelloCdkStack(final Construct scope, final String id) {        this(scope, id, null);    }    public HelloCdkStack(final Construct scope, final String id, final StackProps props) {        super(scope, id, props);        Bucket.Builder.create(this, &quot;MyFirstBucket&quot;)            .versioned(true).build();    }}</code></pre><h3 id="1-1-3-Systhesize-an-AWS-CloudFormation-Template"><a href="#1-1-3-Systhesize-an-AWS-CloudFormation-Template" class="headerlink" title="1.1.3 Systhesize an AWS CloudFormation Template"></a>1.1.3 Systhesize an AWS CloudFormation Template</h3><pre><code>cdk synth</code></pre><h3 id="1-1-4-Deploying-the-stack"><a href="#1-1-4-Deploying-the-stack" class="headerlink" title="1.1.4 Deploying the stack"></a>1.1.4 Deploying the stack</h3><p><code>cdk deploy</code></p><h3 id="1-1-5-Modifying-the-stack"><a href="#1-1-5-Modifying-the-stack" class="headerlink" title="1.1.5 Modifying the stack"></a>1.1.5 Modifying the stack</h3><pre><code>// after make your change cdk diff cdk deploy // Possibly destroy cdk destroy </code></pre><ul><li>Synthesize before deploying <h1 id="2-Basic-concepts"><a href="#2-Basic-concepts" class="headerlink" title="2. Basic concepts"></a>2. Basic concepts</h1></li></ul><h2 id="2-1-Constructs"><a href="#2-1-Constructs" class="headerlink" title="2.1 Constructs"></a>2.1 Constructs</h2><h3 id="2-1-1-Basics"><a href="#2-1-1-Basics" class="headerlink" title="2.1.1 Basics"></a>2.1.1 Basics</h3><ul><li><p>Constructs </p><ul><li><p>Basic building blocks </p></li><li><p>represents a cloud component, encapsulates everything AWS CloudFormation needs to create the component </p></li><li><p>[AWS Construct Library](<a href="https://docs.aws.amazon.com/cdk/api/latest/docs/aws-construct-library.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/cdk/api/latest/docs/aws-construct-library.html</a></p></li><li><p>defferent level of constructs </p><ul><li><p>CFN Resources/ L1 </p><ul><li>directly represent all of the AWS resources that are available in AWS CloudFormation </li><li>named as CfnXyz, where xyz is name of the resource </li><li><strong>When you use CFN resources, you must explicitly configure all resource properties, which requires a complete understanding of the details of the underlying AWS CloudFormation resource model</strong></li></ul></li><li><p>AWS Reousources/ L2 Constrcuts</p><ul><li>higher level, intent based API </li><li>provide defaults, boilerplate, and glue logic you’d be writing with a CFN resource construct</li><li>Offer convenient defaults thus reduce the need for the detail of AWS resources  </li></ul></li><li><p>Patterns - higher level constructs </p><ul><li>designed to help you complete common tasks in AWS, often involving multiple kinds of resources </li></ul></li></ul></li></ul></li></ul><pre><code>// L1 ConstructCfnBucket bucket = CfnBucket.Builder.create(this, &quot;MyBucket&quot;)                        .bucketName(&quot;MyBucket&quot;)                        .corsConfiguration(new CfnBucket.CorsConfigurationProperty.Builder()                            .corsRules(Arrays.asList(new CfnBucket.CorsRuleProperty.Builder()                                .allowedOrigins(Arrays.asList(&quot;*&quot;))                                .allowedMethods(Arrays.asList(&quot;GET&quot;))                                .build()))                            .build())                        .build();// L2 Constructimport software.amazon.awscdk.services.s3.*;public class HelloCdkStack extends Stack {    public HelloCdkStack(final Construct scope, final String id) {        this(scope, id, null);    }    public HelloCdkStack(final Construct scope, final String id, final StackProps props) {        super(scope, id, props);        Bucket.Builder.create(this, &quot;MyFirstBucket&quot;)                .versioned(true).build();    }}</code></pre><h3 id="2-1-2-Hierarchy-Composition"><a href="#2-1-2-Hierarchy-Composition" class="headerlink" title="2.1.2 Hierarchy - Composition"></a>2.1.2 Hierarchy - Composition</h3><ul><li><p>Composition </p><ul><li>High level construct can be composed from any number of lower level constructs </li><li>In turn, those could be composed from even lower level constructs</li><li>Scoping pattern results in a hierarchy of constructs known as a construct tree </li></ul></li><li><p>Composition means you can define reusable components and share them like any other code </p></li></ul><h3 id="2-1-3-Initialization"><a href="#2-1-3-Initialization" class="headerlink" title="2.1.3 Initialization"></a>2.1.3 Initialization</h3><ul><li><p>Being implemented in classes that extend the Construct base class </p></li><li><p>3 parameters </p><ul><li><p>scope </p><ul><li>the construct within which this construct is defined </li></ul></li><li><p>id </p><ul><li>an identifier that much be unique within this scope </li><li>serves as a namespace for everything that’s encapsulated within the scope’s subtree </li><li>used to allocate unique identities such as resource names and AWS CloudFormation logical IDs </li></ul></li><li><p>props </p><ul><li>a set of properties or keyword arguments </li><li>define the construct’s initial configuration </li></ul></li></ul></li></ul><h1 id="3-Java-Related"><a href="#3-Java-Related" class="headerlink" title="3. Java Related"></a>3. Java Related</h1><h2 id="3-1-AWS-CDK-idioms-in-Java"><a href="#3-1-AWS-CDK-idioms-in-Java" class="headerlink" title="3.1 AWS CDK idioms in Java"></a>3.1 AWS CDK idioms in Java</h2><ul><li>Props <ul><li>expressed with Builder pattern </li><li>define a bundle of key/ value pairs that the construct uses to configure the resources it creates </li></ul></li></ul><pre><code>Bucket bucket = new Bucket(this, &quot;MyBucket&quot;, new BucketProps.Builder()                           .versioned(true)                           .encryption(BucketEncryption.KMS_MANAGED)                           .build());</code></pre><ul><li>missing values <ul><li>it will be represented by <code>null</code></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;AWS CDK &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Op
      
    
    </summary>
    
    
      <category term="Cloud" scheme="https://www.llchen60.com/categories/Cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>Git 流程</title>
    <link href="https://www.llchen60.com/Git-%E6%B5%81%E7%A8%8B/"/>
    <id>https://www.llchen60.com/Git-%E6%B5%81%E7%A8%8B/</id>
    <published>2020-08-12T00:41:26.000Z</published>
    <updated>2020-08-12T00:41:52.187Z</updated>
    
    <content type="html"><![CDATA[<p>解决git conflict永远都是件很让人头疼的事情，为了让生活更简单，还是需要设定正确的git流程的。现在有如下几种git 流程</p><h1 id="1-基本的Git-流程"><a href="#1-基本的Git-流程" class="headerlink" title="1. 基本的Git 流程"></a>1. 基本的Git 流程</h1><p>只有一个branch – master. 开发者直接commit进去，然后会进入到alpha，beta, gamma, prod等不同的生产状态当中。</p><p>一般来说，除非你在自己单独完成某项小任务，是很不推荐这样做的。</p><p>缺陷在于：</p><ul><li>代码上的合作变得很困难，可能会有多次冲突，需要逐次进行解决</li></ul><h1 id="2-Git-feature分支流程"><a href="#2-Git-feature分支流程" class="headerlink" title="2. Git feature分支流程"></a>2. Git feature分支流程</h1><p>当在同一个codebase我们有多个工程师共同工作的时候，使用feature分治就变成了必不可少的事情了。</p><p>如果现在有两个工程师在同一个branch上工作，来提交自己的代码，那最终一定是冲突不断的，很容易出现各种问题。</p><p>为了避免出现这种情况，两个开发者可以创建两个不同的分支，分别在自己的分治上来开发自己的项目。</p><p>这样做的好处是不用担心大量需要解决的冲突了。</p><h1 id="3-Git-feature分支流程与Develop分支"><a href="#3-Git-feature分支流程与Develop分支" class="headerlink" title="3. Git feature分支流程与Develop分支"></a>3. Git feature分支流程与Develop分支</h1><p>和上述的feature分支流程很类似，只是又加了一个Develop分支，在这个流程下，master 分支永远反映一个prod ready的状态。</p><p>无论何时，当小组想要将代码部署到prod的时候，他们从master分支来进行部署</p><p>develop branch反映的是带着最新的为了下次发布准备的所有改动。开发者fork develop 分支的代码，来做独立开发。一旦项目做好，经过了测试，就合并到develop分支当中，在develop分支来做充分的测试，然后再merge到master分支当中去。</p><p>这样做的好处是能够允许小组持续merge新的功能，做持续集成。不过过程相对比较麻烦。个人观点是在小规模的前提下，使用特征分支就足够了，再加上持续集成的工具，譬如Jerkins，很安全，效率也很不错。</p><p><a href="https://zepel.io/blog/5-git-workflows-to-improve-development/" target="_blank" rel="noopener">https://zepel.io/blog/5-git-workflows-to-improve-development/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;解决git conflict永远都是件很让人头疼的事情，为了让生活更简单，还是需要设定正确的git流程的。现在有如下几种git 流程&lt;/p&gt;
&lt;h1 id=&quot;1-基本的Git-流程&quot;&gt;&lt;a href=&quot;#1-基本的Git-流程&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="git" scheme="https://www.llchen60.com/tags/git/"/>
    
  </entry>
  
</feed>
