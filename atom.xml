<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leilei&#39;s Blog | 磊磊的博客</title>
  
  <subtitle>Because it&#39;s there</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.llchen60.com/"/>
  <updated>2022-12-07T11:19:07.324Z</updated>
  <id>https://www.llchen60.com/</id>
  
  <author>
    <name>Leilei Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Singleton Pattern</title>
    <link href="https://www.llchen60.com/Singleton-Pattern/"/>
    <id>https://www.llchen60.com/Singleton-Pattern/</id>
    <published>2022-12-07T11:18:26.000Z</published>
    <updated>2022-12-07T11:19:07.324Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-why-we-need"><a href="#1-why-we-need" class="headerlink" title="1. why we need"></a>1. why we need</h1><ul><li>Registry Setting</li><li>Thread Pool / Connection Pool</li></ul><blockquote><p>The singleton Pattern ensures a class has only one instance, and provides a global point of access to it.</p></blockquote><ul><li>let a class to manage a single instance of itself<ul><li>also prevent other classes from creating a new instance on its own</li></ul></li><li>provide a global access point to the instance</li></ul><h1 id="2-Implementation"><a href="#2-Implementation" class="headerlink" title="2. Implementation"></a>2. Implementation</h1><h2 id="2-1-1st-Iteration"><a href="#2-1-1st-Iteration" class="headerlink" title="2.1 1st Iteration"></a>2.1 1st Iteration</h2><pre><code class="java">public class Singleton &#123;    private static Singleton uniqueInstance;    private Singleton() &#123;&#125;    public static Singleton getInstance() &#123;        if (uniqueInstance == null) &#123;            uniqueInstance = new Singleton();        &#125;        return uniqueInstance;    &#125;&#125;</code></pre><ul><li>Issue when multi threading</li></ul><p>Thread A </p><p>if (uniqueInstance == null) { // return true </p><p>new Singleton() </p><p>Thread B </p><p>if (uniqueInstance == null) { // return true </p><p>new Singleton() </p><ul><li>Under such case, we’ll create two new instances</li></ul><h2 id="2-2-Implementation-with-multi-threading-support"><a href="#2-2-Implementation-with-multi-threading-support" class="headerlink" title="2.2 Implementation with multi-threading support"></a>2.2 Implementation with multi-threading support</h2><pre><code class="java">public class Singleton &#123;    private static Singleton uniqueInstance;    private Singleton() &#123;&#125;    public static synchronized Singleton getInstance() &#123;        if (uniqueInstance == null) &#123;            uniqueInstance = new Singleton();        &#125;        return uniqueInstance;    &#125;&#125;</code></pre><ul><li>synchronized<ul><li>force every thread to wait its turn before it can enter the method</li><li>no two threads may enter the method at the same time</li></ul></li><li>Issue<ul><li>we only need synchronized when there is no instance, once there are, no need to use synchronized, it’s just overhead</li></ul></li></ul><h2 id="2-3-Implementation-with-multi-threading-support-—-Eagerly-created-instance"><a href="#2-3-Implementation-with-multi-threading-support-—-Eagerly-created-instance" class="headerlink" title="2.3  Implementation with multi-threading support  — Eagerly created instance"></a>2.3  Implementation with multi-threading support  — Eagerly created instance</h2><pre><code class="java">public class Singleton &#123;    private static Singleton uniqueInstance = new Singleton();    private Singleton() &#123;&#125;    public static synchronized Singleton getInstance() &#123;        return uniqueInstance;    &#125;&#125;</code></pre><ul><li>We rely on the JVM to create the unique instance of the Singleton when the class is loaded</li><li>JVM guarantees the instance will be created before any thread accesses the static unique instance variable</li></ul><h2 id="2-4-Implementation-with-multi-threading-support-—-Double-Checked-Locking"><a href="#2-4-Implementation-with-multi-threading-support-—-Double-Checked-Locking" class="headerlink" title="2.4  Implementation with multi-threading support  — Double Checked Locking"></a>2.4  Implementation with multi-threading support  — Double Checked Locking</h2><pre><code class="java">public class Singleton &#123;    private volatile static Singleton uniqueInstance;    private Singleton() &#123;&#125;    public static synchronized Singleton getInstance() &#123;        if (uniqueInstance == null) &#123;            synchronized (Singleton.class) &#123;                if (uniqueInstance == null) &#123;                    uniqueInstance = new Singleton();                &#125;            &#125;        &#125;        return uniqueInstance;        &#125;&#125;</code></pre><ul><li>volatile<ul><li>ensure multiple threads handle the unique instance variable correctly when it is initialized to the singleton instance</li></ul></li></ul><h2 id="2-5-Implementation-with-multi-threading-support-—-Using-Enum"><a href="#2-5-Implementation-with-multi-threading-support-—-Using-Enum" class="headerlink" title="2.5 Implementation with multi-threading support — Using Enum"></a>2.5 Implementation with multi-threading support — Using Enum</h2><pre><code class="java">public enum Singleton &#123;    UNIQUE_INSTANCE;&#125;public class SingletonClient &#123;    Singleton singleton = Singleton.UNIQUE_INSTANCE;&#125;</code></pre><p>ClassLoader </p><ul><li>Responsible for loading java classes dynamically to the JVM during runtime</li><li>Responsible for loading classes into memory</li></ul><p>type </p><ul><li><p>application class loader</p><ul><li><strong>An application or system class loader loads our own files in the classpath.</strong></li></ul></li><li><p>extension class loader</p><ul><li><strong>Extension class loaders load classes that are an extension of the standard core Java classes.</strong></li></ul></li><li><p>bootstrap class loader</p><ul><li><strong>A bootstrap or primordial class loader is the parent of all the others.</strong></li><li><strong>This is because the bootstrap class loader is written in native code, not Java, so it doesn’t show up as a Java class</strong></li></ul></li><li><p>how does it work</p><ul><li>JVM request a class</li><li>class loader try to locate the class and load the definition into the runtime using <strong>fully qualified class name</strong></li></ul></li><li><p>delegation model</p><ul><li>delegate the search of the class/ resource to the parent class loader</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.baeldung.com/java-classloaders">https://www.baeldung.com/java-classloaders</a> </li><li><a href="https://learning.oreilly.com/library/view/head-first-design/9781492077992/ch05.html">https://learning.oreilly.com/library/view/head-first-design/9781492077992/ch05.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-why-we-need&quot;&gt;&lt;a href=&quot;#1-why-we-need&quot; class=&quot;headerlink&quot; title=&quot;1. why we need&quot;&gt;&lt;/a&gt;1. why we need&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Registry Setting&lt;/
      
    
    </summary>
    
    
      <category term="Design Pattern" scheme="https://www.llchen60.com/categories/Design-Pattern/"/>
    
    
  </entry>
  
  <entry>
    <title>工厂模式</title>
    <link href="https://www.llchen60.com/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"/>
    <id>https://www.llchen60.com/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/</id>
    <published>2022-12-05T13:24:32.000Z</published>
    <updated>2022-12-05T13:26:47.688Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-工厂模式Overview"><a href="#1-工厂模式Overview" class="headerlink" title="1.工厂模式Overview"></a>1.工厂模式Overview</h1><blockquote><p>Define an interface for creating an object, but let’s subclasses decide which class to instantiate. Factory method lets a class defer instantiation to subclasses.</p></blockquote><ul><li>出现工厂模式是因为我们不希望将实现和类的声明进行强绑定，我们希望整体在一个松耦合的状态<ul><li>我们希望达到的理想状态是 Open for extension, Close for modification.</li></ul></li><li>Interface 定义出了一个contract，规定了我们是如何在各个类之间进行交互的</li><li>我们使用面向接口的编程方式，主要目的是对各种改动会更加友好，写的代码会通过Java多态的特性自动对实现了这个接口的新类可用</li><li>Factory — 封装对象的创建过程<ul><li>专门负责创建出新的对象，让工厂根据条件去生产出新的对象</li><li>使用static factory<ul><li>因为按道理也不应该因为要使用create方法就实例化一个对象</li><li>但是坏处是不能够去创建子类，或者改变create方法本身了</li></ul></li></ul></li><li>creator side<ul><li>one abstract class as abstract method<ul><li>could delegate some method to subclass</li><li>but for common one, we should define here, and subclass could override if they want</li></ul></li><li>a set of subclass implements the abstract methods</li></ul></li><li>product side<ul><li>abstract product</li><li>concrete products which extends the abstract one</li></ul></li></ul><h1 id="2-使用场景"><a href="#2-使用场景" class="headerlink" title="2. 使用场景"></a>2. 使用场景</h1><ul><li>当我们的对象生成需要某些逻辑运算的时候，比如需要根据一些传入的参数来决定生成什么样子的对象</li></ul><h1 id="3-使用目的"><a href="#3-使用目的" class="headerlink" title="3. 使用目的"></a>3. 使用目的</h1><ul><li>将对象的生成和使用解耦，方便对于一组对象的维护，根据一些条件来生成指定的对象</li></ul><h1 id="4-具体实现"><a href="#4-具体实现" class="headerlink" title="4. 具体实现"></a>4. 具体实现</h1><h2 id="4-1-接口和数据对象定义"><a href="#4-1-接口和数据对象定义" class="headerlink" title="4.1 接口和数据对象定义"></a>4.1 接口和数据对象定义</h2><p><img src="https://s2.loli.net/2022/12/05/irb9nDuMsw2lhBY.png" alt="class diagram"></p><pre><code class="java">public abstract class Animal &#123;    String name;    public abstract void bark();&#125;public interface IAnimalFactory &#123;    Animal createAnimal(String type);&#125;</code></pre><h2 id="4-2-具体实现类的定义"><a href="#4-2-具体实现类的定义" class="headerlink" title="4.2 具体实现类的定义"></a>4.2 具体实现类的定义</h2><pre><code class="java">public class Cat extends Animal &#123;    public Cat(String name) &#123;        this.name = name;    &#125;    @Override    public void bark() &#123;        System.out.println(&quot;miao&quot;);    &#125;&#125;public class Dog extends Animal &#123;    public Dog(String name) &#123;        this.name = name;    &#125;    @Override    public void bark() &#123;        System.out.println(&quot;wang&quot;);    &#125;&#125;public class RandomFactory implements IAnimalFactory &#123;    @Override    public Animal createAnimal(String name) &#123;        Random random = new Random();        int num = random.nextInt(2);        var list = Arrays.asList(&quot;cat&quot;, &quot;dog&quot;);        switch(list.get(num)) &#123;            case &quot;cat&quot;:                return new Cat(name);            case &quot;dog&quot;:                return new Dog(name);            default:                return null;        &#125;    &#125;&#125;</code></pre><h2 id="4-3-测试"><a href="#4-3-测试" class="headerlink" title="4.3 测试"></a>4.3 测试</h2><pre><code class="java">@GetMapping(&quot;/factory&quot;)    public String factory() &#123;        RandomFactory randomFactory = new RandomFactory();        randomFactory.createAnimal(&quot;test&quot;).bark();        randomFactory.createAnimal(&quot;test&quot;).bark();        randomFactory.createAnimal(&quot;test&quot;).bark();        randomFactory.createAnimal(&quot;test&quot;).bark();        randomFactory.createAnimal(&quot;test&quot;).bark();        return &quot;check the log&quot;;    &#125;</code></pre><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.youtube.com/watch?v=EcFVTgRHJLM">https://www.youtube.com/watch?v=EcFVTgRHJLM</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-工厂模式Overview&quot;&gt;&lt;a href=&quot;#1-工厂模式Overview&quot; class=&quot;headerlink&quot; title=&quot;1.工厂模式Overview&quot;&gt;&lt;/a&gt;1.工厂模式Overview&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Define an
      
    
    </summary>
    
    
      <category term="Design Pattern" scheme="https://www.llchen60.com/categories/Design-Pattern/"/>
    
    
  </entry>
  
  <entry>
    <title>策略模式</title>
    <link href="https://www.llchen60.com/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/"/>
    <id>https://www.llchen60.com/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/</id>
    <published>2022-09-25T06:21:58.000Z</published>
    <updated>2022-09-25T06:23:31.170Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Strategy-Pattern-Overview"><a href="#1-Strategy-Pattern-Overview" class="headerlink" title="1. Strategy Pattern Overview"></a>1. Strategy Pattern Overview</h1><p>Using composition rather than inheritance </p><p>Strategy patten define a set of algorithms, encapsulate each of them and make then exchangable. And we could switch them at run time , decouple the algorithm with the place they are used </p><h1 id="2-Why-use-strategy-pattern"><a href="#2-Why-use-strategy-pattern" class="headerlink" title="2. Why use strategy pattern?"></a>2. Why use strategy pattern?</h1><ul><li>when we have a set of classes, they belong to same object, they should could a certain types of actions, but the detail would vary. Then we need a way to regulate they have such type of behaviors, and also differ them in a maintainable manner</li><li>literaly composition over inheritance</li><li>其实主要还是在考虑代码的可扩展性，多考虑下如何应对可能的改动，Design Pattern整个就是为了应对改动来服务的，并不太需要神化，毕竟只是对于常见的一些应用场景的抽象，想要达到的目的也就是抽象出来，以后做的时候会在范式下，更专注在业务逻辑上的问题</li></ul><h1 id="3-Implementation"><a href="#3-Implementation" class="headerlink" title="3. Implementation"></a>3. Implementation</h1><p><img src="https://s2.loli.net/2022/09/25/hvGXQ3TWYSH5gnd.png" alt="Class Diagram"></p><h2 id="3-1-Interface"><a href="#3-1-Interface" class="headerlink" title="3.1 Interface"></a>3.1 Interface</h2><pre><code class="java">public interface IFlyBehavior &#123;    void fly();&#125;public interface IQuackBehavior &#123;    void quack();&#125;</code></pre><h2 id="3-2-Implementation"><a href="#3-2-Implementation" class="headerlink" title="3.2 Implementation"></a>3.2 Implementation</h2><pre><code class="java">public class Duck &#123;    IFlyBehavior flyBehavior;    IQuackBehavior quackBehavior;    public void fly() &#123;        flyBehavior.fly();    &#125;    public void quack() &#123;        quackBehavior.quack();    &#125;&#125;public class JetFly implements IFlyBehavior &#123;    @Override    public void fly() &#123;        System.out.println(&quot;Jet Fly&quot;);    &#125;&#125;public class SimplyFly implements IFlyBehavior &#123;    @Override    public void fly() &#123;        System.out.println(&quot;Simple flying&quot;);    &#125;&#125;public class LoudQuack implements IQuackBehavior &#123;    @Override    public void quack() &#123;        System.out.println(&quot;Make noise!&quot;);    &#125;&#125;public class NoQuack implements IQuackBehavior &#123;    @Override    public void quack() &#123;        // do nothing, as no quack exist    &#125;&#125;public class ToyDuck extends Duck &#123;    public ToyDuck(IFlyBehavior flyBehavior, IQuackBehavior quackBehavior) &#123;        this.flyBehavior = flyBehavior;        this.quackBehavior = quackBehavior;    &#125;&#125;</code></pre><h2 id="3-3-Test"><a href="#3-3-Test" class="headerlink" title="3.3 Test"></a>3.3 Test</h2><pre><code class="java">@GetMapping(&quot;/strategy&quot;)    public String strategy() &#123;        Duck testDuck = new ToyDuck(new JetFly(), new LoudQuack());        testDuck.fly();        testDuck.quack();        return &quot;check the log&quot;;    &#125;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Strategy-Pattern-Overview&quot;&gt;&lt;a href=&quot;#1-Strategy-Pattern-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Strategy Pattern Overview&quot;&gt;&lt;/a&gt;1. S
      
    
    </summary>
    
    
      <category term="Design Pattern" scheme="https://www.llchen60.com/categories/Design-Pattern/"/>
    
    
  </entry>
  
  <entry>
    <title>装饰者模式</title>
    <link href="https://www.llchen60.com/%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F/"/>
    <id>https://www.llchen60.com/%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F/</id>
    <published>2022-09-19T14:48:01.000Z</published>
    <updated>2022-09-19T14:49:32.984Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-装饰者模式Overview"><a href="#1-装饰者模式Overview" class="headerlink" title="1. 装饰者模式Overview"></a>1. 装饰者模式Overview</h1><ul><li>Attach additional responsibilities to an object <strong>dynamically</strong>. Decorator provides a flexible alternative to subclassing for extending functionality.<ul><li>inheriting behavior at runtime through composition and delegation</li><li>open for extension but close for modification</li><li>decorator has the same super type as the objects they decorate</li><li>we could use one or more decorators to wrap an object</li><li>the decorator <strong>adds its own behavior</strong> before and/or after delegating to the object it decorates to do the rest of the job</li></ul></li></ul><h1 id="2-使用场景"><a href="#2-使用场景" class="headerlink" title="2. 使用场景"></a>2. 使用场景</h1><ul><li>when we want to give some other responsibility at run time</li><li>kind of like continue to wrap the intrinsic object with some new responsibility, still an is-a relationship</li></ul><h1 id="3-使用目的"><a href="#3-使用目的" class="headerlink" title="3. 使用目的"></a>3. 使用目的</h1><ul><li>Extends the class at run time instead of compile time</li><li>To give objects some new responsibilities without making any code changes to the underlying classes</li><li>pitfall<ul><li>sometimes it could lead to a lot of new classes</li></ul></li></ul><h1 id="4-具体实现"><a href="#4-具体实现" class="headerlink" title="4. 具体实现"></a>4. 具体实现</h1><ul><li><p>One highlight</p><ul><li><p>the condimentDecorator is a beverage, and also need to has a beverage</p></li><li><p>we are using inheritance to achieve the type matching, but we aren’t using inheritance to get behavior</p><p><img src="https://s2.loli.net/2022/09/19/YshIa9SnT65NH17.png" alt="Class Diagram"></p></li></ul></li></ul><h2 id="4-1-接口和数据对象定义"><a href="#4-1-接口和数据对象定义" class="headerlink" title="4.1 接口和数据对象定义"></a>4.1 接口和数据对象定义</h2><pre><code class="java">public abstract class Beverage &#123;    String description = &quot;Unknown Beverage&quot;;    public abstract double cost();    public String getDescription() &#123;        return description;    &#125;&#125;public abstract class CondimentDecorator extends Beverage&#123;    Beverage beverage;    public abstract String getDescription();&#125;</code></pre><h2 id="4-2-具体实现类的定义"><a href="#4-2-具体实现类的定义" class="headerlink" title="4.2 具体实现类的定义"></a>4.2 具体实现类的定义</h2><pre><code class="java">public class DecafCoffee extends Beverage &#123;    @Override    public double cost() &#123;        return 2.5f;    &#125;    String description() &#123;        return &quot;Decat coffee&quot;;    &#125;&#125;public class Espresso extends Beverage&#123;    public Espresso() &#123;        description = &quot;Espresso&quot;;    &#125;    @Override    public double cost() &#123;        return 4;    &#125;&#125;public class HouseBlend extends Beverage&#123;    public HouseBlend() &#123;        description = &quot;House Blend Coffee&quot;;    &#125;    @Override    public double cost() &#123;        return 0.99;    &#125;&#125;public class MilkDecorator extends CondimentDecorator&#123;    public MilkDecorator(Beverage beverage) &#123;        this.beverage = beverage;    &#125;    @Override    public double cost() &#123;        return beverage.cost() + 0.8;    &#125;    @Override    public String getDescription() &#123;        return beverage.getDescription() + &quot;with milk&quot;;    &#125;&#125;public class MochaDecorator extends CondimentDecorator&#123;    public MochaDecorator(Beverage beverage) &#123;        this.beverage = beverage;    &#125;    @Override    public double cost() &#123;        return beverage.cost() + 0.5;    &#125;    @Override    public String getDescription() &#123;        return beverage.getDescription() + &quot;with Mocha&quot;;    &#125;&#125;</code></pre><h2 id="4-3-测试"><a href="#4-3-测试" class="headerlink" title="4.3 测试"></a>4.3 测试</h2><pre><code class="java">@GetMapping(&quot;/decorator&quot;)    public String decorator() &#123;        Beverage decafCoffee = new DecafCoffee();        log.info(&quot;before decoration&quot; + decafCoffee.cost());        Beverage decafWithMocha = new MochaDecorator(decafCoffee);        log.info(&quot;after decoration&quot; + decafWithMocha.cost());        log.info(&quot;after decoration 2&quot; + new MochaDecorator(new MochaDecorator(decafCoffee)).cost());        return &quot;check the log&quot;;    &#125;</code></pre><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://learning.oreilly.com/library/view/head-first-design/9781492077992/ch02.html">https://learning.oreilly.com/library/view/head-first-design/9781492077992/ch02.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-装饰者模式Overview&quot;&gt;&lt;a href=&quot;#1-装饰者模式Overview&quot; class=&quot;headerlink&quot; title=&quot;1. 装饰者模式Overview&quot;&gt;&lt;/a&gt;1. 装饰者模式Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Attach add
      
    
    </summary>
    
    
      <category term="Design Pattern" scheme="https://www.llchen60.com/categories/Design-Pattern/"/>
    
    
  </entry>
  
  <entry>
    <title>观察者模式实现</title>
    <link href="https://www.llchen60.com/%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F-1/"/>
    <id>https://www.llchen60.com/%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F-1/</id>
    <published>2022-09-17T03:49:49.000Z</published>
    <updated>2022-09-17T03:53:15.657Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-观察者模式Overview"><a href="#1-观察者模式Overview" class="headerlink" title="1. 观察者模式Overview"></a>1. 观察者模式Overview</h1><ul><li>定义了一种一对多的交互方式，可以被观察的对象可以通知所有观察者他们期待的对象的变化，Pull vs Push.<ul><li>当一个对象改变状态的时候，它的所有依赖者都会收到通知并且会自动更新</li></ul></li><li>Subject/ Observable<ul><li>管理某些数据</li><li>当数据改变的时候，通知订阅者</li></ul></li><li>Observer<ul><li>在订阅的数据发生改变了以后，得到通知</li></ul></li></ul><h1 id="2-使用场景"><a href="#2-使用场景" class="headerlink" title="2. 使用场景"></a>2. 使用场景</h1><ul><li>出版者 + 订阅者</li><li>Check Java Observable, though deprecated</li></ul><h1 id="3-使用目的"><a href="#3-使用目的" class="headerlink" title="3. 使用目的"></a>3. 使用目的</h1><ul><li>责任的分开，只有一份数据，由Observable来进行管理，</li><li>Observer只是拿到了数据的更新，然后做自己特定的事情，observable完全不需要知道这里的细节。用这种方式实现了交互对象之间的松耦合</li></ul><h1 id="4-具体实现"><a href="#4-具体实现" class="headerlink" title="4. 具体实现"></a>4. 具体实现</h1><p><img src="https://s2.loli.net/2022/09/17/pSb5gUW3ynxAroB.png" alt="Class Diagram"></p><h2 id="4-1-接口和数据对象定义"><a href="#4-1-接口和数据对象定义" class="headerlink" title="4.1 接口和数据对象定义"></a>4.1 接口和数据对象定义</h2><pre><code class="java">/** * Observer other abilities */public interface IDisplay &#123;    void display();&#125;/*** Observable interface, used to register/remove/notify observers */public interface IObservable &#123;    ActionResult registerObserver(IObserver observer);    ActionResult removeObserver(IObserver observer);    void notifyObservers();&#125;/** * Observable will call update method for regiestered observer to update the status in observer side */public interface IObserver &#123;    void update(SharebleData data);@Data@RequiredArgsConstructor@AllArgsConstructorpublic class SharebleData &#123;    Double temperature;    Double humidity;    Double pressure;&#125;public enum Status &#123;    SUCCESS,    FAILURE&#125;@Data@AllArgsConstructorpublic class ActionResult &#123;    @NonNull    Status status;    @Nullable    List&lt;String&gt; errorReason;&#125;</code></pre><h2 id="4-2-具体实现类的定义"><a href="#4-2-具体实现类的定义" class="headerlink" title="4.2 具体实现类的定义"></a>4.2 具体实现类的定义</h2><pre><code class="java">public class ObservableImpl implements IObservable&#123;    private static List&lt;IObserver&gt; observerList;    private SharebleData data;    public ObservableImpl() &#123;        observerList = new ArrayList&lt;&gt;();        data = new SharebleData();    &#125;    public void setSharebleData(double tem, double humidity, double pressure) &#123;        data.setHumidity(humidity);        data.setTemperature(tem);        data.setPressure(pressure);        notifyObservers();    &#125;    @Override    public ActionResult registerObserver(IObserver observer) &#123;        observerList.add(observer);        return new ActionResult(Status.SUCCESS, null);    &#125;    @Override    public ActionResult removeObserver(IObserver observer) &#123;        observerList.remove(observer);        return new ActionResult(Status.SUCCESS, null);    &#125;    @Override    public void notifyObservers() &#123;        observerList.forEach(observer -&gt; &#123;observer.update(data);&#125;);    &#125;&#125;public class CurrentConditionDisplay implements IObserver, IDisplay&#123;    private double temp;    private double pressure;    private IObservable observable;    public CurrentConditionDisplay(IObservable subject) &#123;        this.observable = subject;        observable.registerObserver(this);    &#125;    @Override    public void display() &#123;        System.out.println(String.format(&quot;======= print out current tem and pressure! temp: %f, pressure: %f &quot;, temp, pressure));    &#125;    @Override    public void update(SharebleData data) &#123;        temp = data.getTemperature();        pressure = data.getPressure();        display();    &#125;&#125;</code></pre><h2 id="4-3-测试"><a href="#4-3-测试" class="headerlink" title="4.3 测试"></a>4.3 测试</h2><pre><code class="java">@RestControllerpublic class TestController &#123;    @GetMapping(&quot;/observer&quot;)    public String observer() &#123;        StringBuilder sb = new StringBuilder();        ObservableImpl observable = new ObservableImpl();        observable.setSharebleData(30, 0.7, 80);        CurrentConditionDisplay currentConditionDisplay = new CurrentConditionDisplay(observable);        observable.setSharebleData(31, 0.7, 80);        observable.setSharebleData(32, 0.7, 80);        observable.setSharebleData(33, 0.7, 80);        return &quot;Please check log&quot;;    &#125;&#125;// Output from console ======= print out current tem and pressure! temp: 31.000000, pressure: 80.000000 ======= print out current tem and pressure! temp: 32.000000, pressure: 80.000000 ======= print out current tem and pressure! temp: 33.000000, pressure: 80.000000 </code></pre><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://learning.oreilly.com/library/view/head-first-design/9781492077992/ch02.html">https://learning.oreilly.com/library/view/head-first-design/9781492077992/ch02.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-观察者模式Overview&quot;&gt;&lt;a href=&quot;#1-观察者模式Overview&quot; class=&quot;headerlink&quot; title=&quot;1. 观察者模式Overview&quot;&gt;&lt;/a&gt;1. 观察者模式Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;定义了一种一对多的交
      
    
    </summary>
    
    
      <category term="Design Pattern" scheme="https://www.llchen60.com/categories/Design-Pattern/"/>
    
    
  </entry>
  
  <entry>
    <title>How to use Sprint transactional annotation</title>
    <link href="https://www.llchen60.com/How-to-use-Sprint-transactional-annotation/"/>
    <id>https://www.llchen60.com/How-to-use-Sprint-transactional-annotation/</id>
    <published>2022-07-15T12:58:18.000Z</published>
    <updated>2022-07-15T12:59:24.908Z</updated>
    
    <content type="html"><![CDATA[<h1 id="How-to-use-Sprint-transactional-annotation"><a href="#How-to-use-Sprint-transactional-annotation" class="headerlink" title="How to use Sprint transactional annotation"></a>How to use Sprint transactional annotation</h1><h1 id="1-Attributes"><a href="#1-Attributes" class="headerlink" title="1. Attributes"></a>1. Attributes</h1><ul><li><code>value</code> and <code>transactionManager</code><ul><li>used to provide a TransactionManager reference to be used when handling the transaction for the annotated block</li></ul></li><li><code>propagation</code><ul><li>define how the transaction boundaries propagate to other methods that will be called either directly or indirectly from within the annotated block</li><li>default is <code>REQUIRED</code><ul><li>means a transaction is started if no transaction is already available</li><li>otherwise use the current running thread</li></ul></li></ul></li><li><code>timeout</code> and <code>timeoutString</code><ul><li>define the max number of seconds the current method is allowed to run</li></ul></li><li><code>readOnly</code><ul><li>defines if the current transaction is read-only or read-write</li></ul></li><li><code>rollbackFor</code> and <code>rollbackForClassName</code><ul><li>define one or more Throwable classes for which the current transaction will be rolled back</li><li>default is RuntimeException or an Error is thrown, but not for a checked Exception</li></ul></li><li><code>noRollbackFor</code> and <code>noRollbackForClassName</code><ul><li>used for one or more RuntimeException</li></ul></li></ul><h1 id="2-Transaction-Override"><a href="#2-Transaction-Override" class="headerlink" title="2. Transaction Override"></a>2. Transaction Override</h1><ul><li><code>addStatementReportOperation</code> is using the serializeble level, which override the class level readonly transaction</li></ul><pre><code class="java">@Service@Transactional(readOnly = true)public class OperationService &#123;    @Transactional(isolation = Isolation.SERIALIZABLE)    public boolean addStatementReportOperation(        String statementFileName,        long statementFileSize,        int statementChecksum,        OperationType reportType) &#123;        ...    &#125;&#125;</code></pre><ul><li>we use readOnly in class level because Spring could perform some read only optimization<ul><li>we could save memory when loading read only entities since the loaded state is discarded right away, and not kept for the whole duration of the currently running persistence context</li><li>also, in the cluster, read only data source could be redirected to DB replica instead of DB primary, this could also reduce the burden for primary</li></ul></li></ul><ol><li><a href="https://stackoverflow.com/questions/10394857/how-to-use-transactional-with-spring-data#:~:text=Thus%20we%20recommend%20using%20%40Transactional,re%2Ddecorated%20in%20that%20interface">https://stackoverflow.com/questions/10394857/how-to-use-transactional-with-spring-data#:~:text=Thus we recommend using %40Transactional,re-decorated in that interface</a> </li><li><a href="https://vladmihalcea.com/spring-read-only-transaction-hibernate-optimization/">https://vladmihalcea.com/spring-read-only-transaction-hibernate-optimization/</a>  </li><li><a href="https://vladmihalcea.com/spring-transactional-annotation/">https://vladmihalcea.com/spring-transactional-annotation/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;How-to-use-Sprint-transactional-annotation&quot;&gt;&lt;a href=&quot;#How-to-use-Sprint-transactional-annotation&quot; class=&quot;headerlink&quot; title=&quot;How to u
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Spring" scheme="https://www.llchen60.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>PG::DuplicatePstatement Error prepared statement axxx already exists</title>
    <link href="https://www.llchen60.com/PG-DuplicatePstatement-Error-prepared-statement-axxx-already-exists/"/>
    <id>https://www.llchen60.com/PG-DuplicatePstatement-Error-prepared-statement-axxx-already-exists/</id>
    <published>2022-07-01T14:04:04.000Z</published>
    <updated>2022-07-01T14:05:31.350Z</updated>
    
    <content type="html"><![CDATA[<p>That’s an interesting issue we faced in production during one deployment which only contain some frontend changes. </p><p>Upon check, this happens in such scenario: </p><pre><code class="java">A prepared statement is generated in postgresql, but never stored in rails. Since the code was interrupted before storing the statement, the @counter variable was never incremented even though it was used to generate a prepared statement.</code></pre><p>That pretty much described the issue, prepared statement on postgres side is a server side object that can be used to optimize performance. When the <code>PREPARE</code><br> statement is executed, the specified statement is parsed, analyzed, and rewritten. When an <code>EXECUTE</code>command is subsequently issued, the prepared statement is planned and executed.  </p><p>When the identifiers already bound to existing prepared statements but rails does not realize it, this issue will be happened. </p><p>here is the fix </p><p><a href="https://github.com/rails/rails/pull/41356/files">https://github.com/rails/rails/pull/41356/files</a></p><pre><code class="java">def next_key     &quot;a#&#123;@counter + 1&#125;&quot;end def next_key     &quot;a#&#123;@counter += 1&#125;&quot;end This change make the postgres prepared statement counter before makeing a prepared statementThus if the statemnt is aborted in rails side, app won&#39;t end up in perpetual crash state </code></pre><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://github.com/rails/rails/issues/1627">https://github.com/rails/rails/issues/1627</a></li><li><a href="https://github.com/rails/rails/pull/25827">https://github.com/rails/rails/pull/25827</a></li><li><a href="https://github.com/rails/rails/pull/17607">https://github.com/rails/rails/pull/17607</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;That’s an interesting issue we faced in production during one deployment which only contain some frontend changes. &lt;/p&gt;
&lt;p&gt;Upon check, th
      
    
    </summary>
    
    
      <category term="Ruby" scheme="https://www.llchen60.com/categories/Ruby/"/>
    
    
      <category term="Postgres" scheme="https://www.llchen60.com/tags/Postgres/"/>
    
  </entry>
  
  <entry>
    <title>Equity 101</title>
    <link href="https://www.llchen60.com/Equity-101/"/>
    <id>https://www.llchen60.com/Equity-101/</id>
    <published>2022-04-02T09:08:11.000Z</published>
    <updated>2022-04-02T09:09:25.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-What-is-equity"><a href="#1-What-is-equity" class="headerlink" title="1. What is equity?"></a>1. What is equity?</h1><ul><li>A form of compensation<ul><li>give you partial ownership in the company<ul><li>shares<ul><li>options</li><li>RSUs</li></ul></li></ul></li><li>can have seriously life changing result</li></ul></li><li>Liquidity<ul><li>the ability to sell your shares</li></ul></li></ul><h1 id="2-Cap-Table"><a href="#2-Cap-Table" class="headerlink" title="2. Cap Table"></a>2. Cap Table</h1><ul><li>A document that details who has ownership in a company</li><li>records<ul><li>how and when you’ll get your shares</li><li>how many shares you’ll get</li><li>how you’ll be paid out</li></ul></li><li>employee option pool<ul><li>investors require it, to hire talented people</li><li>let talented stay longer</li></ul></li><li>different types of equity<ul><li>preferred shares — investors</li><li>stock options — employees</li><li>advisory shares — advisors</li></ul></li></ul><h1 id="3-Equity-grants"><a href="#3-Equity-grants" class="headerlink" title="3. Equity grants"></a>3. Equity grants</h1><ul><li><p>equity grant agreements</p><ul><li>how many shares you are getting</li><li>how much each share is worth</li><li>how long it will take to vest all your shares</li></ul></li><li><p>types of equity</p><ul><li>stock option — early stage<ul><li>you have the option to buy shares, but not obligated to</li><li>only vest on a single trigger</li><li>taxed when you buy/ sell your shares</li><li>option types<ul><li>incentive stock option<ul><li>pay tax when you sell your shares</li></ul></li><li>non qualified stock option<ul><li>pay taxes when you buy and sell shares</li></ul></li></ul></li></ul></li><li>restricted stock units - large startup<ul><li>you receive shares that you own automatically</li><li>often vest on a double trigger</li><li>taxed when get vested</li></ul></li></ul></li></ul><h1 id="4-How-vesting-work"><a href="#4-How-vesting-work" class="headerlink" title="4. How vesting work"></a>4. How vesting work</h1><ul><li>Earning the right to buy your shares<ul><li>when you are taxed</li><li>when you are able to buy shares</li></ul></li><li>vesting structures<ul><li>time based vesting</li><li>event based vesting</li><li>hybrid vesting</li></ul></li><li>option outstanding<ul><li>options has not been granted</li></ul></li></ul><h1 id="5-How-exercising-equity-works"><a href="#5-How-exercising-equity-works" class="headerlink" title="5. How exercising equity works"></a>5. How exercising equity works</h1><ul><li>Being granted stock options doesn’t mean you’re automatically a shareholder in a company.</li><li>Stock option not means share, you need to buy it</li><li>exercise<ul><li>means you purchase stock at a specified price</li><li>purchase your options</li><li>then company give stock certificate</li></ul></li><li>strike price<ul><li>option price you could leverage on to buy stocks</li><li>fixed price</li></ul></li><li>409A valuation<ul><li>an analysis of your company’s financials, which determines the value of the company’s shares</li><li>usually done every 12 months</li><li>Fair Market Value - FMV<ul><li>the value of one share of common stock</li></ul></li></ul></li><li>Spread/ Gain<ul><li>the difference between stock price and strike price</li></ul></li><li>PTEP - post termination exercise period<ul><li>the amount of time you have to exercise your options after you leave the company</li></ul></li><li>early exercise<ul><li>company let you buy everything today</li><li>then you just need to pay taxes at strike price</li><li>83b election<ul><li>a form sent to IRS</li><li>you want to pay tax now</li></ul></li></ul></li></ul><h1 id="6-How-to-sell-shares"><a href="#6-How-to-sell-shares" class="headerlink" title="6. How to sell shares"></a>6. How to sell shares</h1><ul><li>How and when can you sell your shares<ul><li>think about<ul><li>any restrictions on selling?</li><li>do you need the cash?</li><li>is the value increasing?</li><li>only chance to sell?</li></ul></li></ul></li><li>ways<ul><li>public company<ul><li>public shares pretty liquid,<ul><li>as it’s easy to convert them into cash at market price</li></ul></li></ul></li><li>private company<ul><li>strongly restrict sell usually</li><li>ways<ul><li>IPO<ul><li>usually 6 month lock up period after going public</li></ul></li><li>M&amp;A<ul><li>bought or merge   acquisition</li><li>another company buys your shares<ul><li>pay for cash or other stocks</li></ul></li></ul></li><li>tender offer<ul><li>company buy from employee</li><li>could happen while the company still private</li></ul></li></ul></li></ul></li></ul></li></ul><h1 id="7-How-are-equity-grants-taxed"><a href="#7-How-are-equity-grants-taxed" class="headerlink" title="7. How are equity grants taxed?"></a>7. How are equity grants taxed?</h1><ul><li>ISO shares<ul><li>when sell it, tax rate come to be different</li></ul></li><li>qualifying disposition<ul><li>1 year after exercise</li><li>2 years after option grant date</li></ul></li><li>long term capital gains tax<ul><li>0% 0 20%</li></ul></li><li>short term capital gains<ul><li>taxed at ordinary income rates</li></ul></li><li>AMT<ul><li>alternative minimum tax</li><li>a low threshold, or floor, defining the minimum amount of taxes you are obligated to pay</li></ul></li></ul><h1 id="8-Valuations"><a href="#8-Valuations" class="headerlink" title="8. Valuations"></a>8. Valuations</h1><ul><li>dilution<ul><li>affect the percentage you have</li></ul></li><li>how valuation works<ul><li>term sheet<ul><li>a legal document defining the terms of an investment</li></ul></li><li>valuation<ul><li>an appraisal of a company’s worth</li><li>affect<ul><li>how many shares</li><li>price per shares</li><li>percentage</li></ul></li></ul></li></ul></li><li>stage<ul><li>pre money valuation<ul><li>post money matters you</li><li>the employee stock option pool—that percentage of the company that gets carved out for new hires and new employees? Well, if you think about how dilution works, when an option pool gets carved out, it reduces the ownership percentage of all of the other shareholders. So that’s why a lot of the time, when an investor puts money into a company, they’ll require that this option pool gets sliced out of the company on a pre-money basis.</li></ul></li><li>investment</li><li>post money valuation</li></ul></li><li>how valuations are calculated<ul><li>valuation for startup come to be complex,<ul><li>revenue, profit, tax, etc.</li><li>growth rate</li><li>user count</li><li>industry</li><li>founding team</li></ul></li><li>section 409A<ul><li>let 3rd party help do valuation for the startup</li><li>every 12 months</li><li>or every time do a new fund raising</li></ul></li></ul></li></ul><h1 id="9-Who-can-invest-in-startups"><a href="#9-Who-can-invest-in-startups" class="headerlink" title="9. Who can invest in startups"></a>9. Who can invest in startups</h1><ul><li>For private company<ul><li>no need to release any info to public</li><li>less info and more risk comparing with public company investment</li></ul></li><li>who can invest in a startup<ul><li>accredited investor<ul><li>more relevant to individuals</li><li>criteria (either one of them)<ul><li>net worth  over a million, not include the primary living place;</li><li>income  net income 200k per year for last 2 years. reasonable expectation in following year; for a family is 300k</li><li>passing a financial exam, hold a license</li></ul></li></ul></li><li>qualified purchaser<ul><li>more relevant for funds, institution,</li><li>all about investment, purchasing power</li><li>an entity that has a significant amount of investment capital — their purchasing power</li></ul></li></ul></li></ul><h1 id="10-How-venture-capital-works"><a href="#10-How-venture-capital-works" class="headerlink" title="10. How venture capital works"></a>10. How venture capital works</h1><ul><li>venture capital system<ul><li>equity financing<ul><li>angel investors</li><li>venture capital fund<ul><li>invest other people’s money through fund</li></ul></li></ul></li><li>debt financing</li></ul></li><li>type of fundraising instruments<ul><li>convertible instrument<ul><li>investor iou, convert the note into shares, owns equity in the future<ul><li>control - startup not lose control in this way</li></ul></li><li>used for very early stage of startups</li></ul></li><li>priced round<ul><li>used at later stages, when the company is growing</li></ul></li></ul></li><li>type of investors<ul><li>accredited investor - angel investor</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://carta.com/equity/learn/what-is-equity/">https://carta.com/equity/learn/what-is-equity/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-What-is-equity&quot;&gt;&lt;a href=&quot;#1-What-is-equity&quot; class=&quot;headerlink&quot; title=&quot;1. What is equity?&quot;&gt;&lt;/a&gt;1. What is equity?&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;A fo
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="Equity" scheme="https://www.llchen60.com/tags/Equity/"/>
    
      <category term="RSU" scheme="https://www.llchen60.com/tags/RSU/"/>
    
  </entry>
  
  <entry>
    <title>System.nanoTime vs System.currentTimeMillis</title>
    <link href="https://www.llchen60.com/System-nanoTime-vs-System-currentTimeMillis/"/>
    <id>https://www.llchen60.com/System-nanoTime-vs-System-currentTimeMillis/</id>
    <published>2022-03-28T10:13:34.000Z</published>
    <updated>2022-03-28T10:14:08.401Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Monotonic-Clock"><a href="#1-Monotonic-Clock" class="headerlink" title="1. Monotonic Clock"></a>1. Monotonic Clock</h1><ul><li>suitable for measuring a duration, such as a timeout or a service’s response time<ul><li><code>clock_gettime(CLOCK_MONOTONIC)</code></li><li><code>System.nanoTime()</code></li></ul></li><li>They are guaranteed to always move forward</li><li>However, the <em>absolute</em> value of the clock is meaningless: it might be the number of nanoseconds since the computer was started, or something similarly arbitrary. In particular, it makes no sense to compare monotonic clock values from two different computers, because they don’t mean the same thing.<ul><li>On a server with multiple CPU sockets, there may be a separate timer per CPU, which is not necessarily synchronized with other CPUs</li></ul></li></ul><h1 id="2-Time-of-day-clock"><a href="#2-Time-of-day-clock" class="headerlink" title="2. Time of day clock"></a>2. Time of day clock</h1><ul><li>It returns the current date and time according to some calendar<ul><li><code>clock_gettime(CLOCK_REALTIME)</code></li><li><code>System.currentTimeMillis()</code><ul><li>return the number of seconds since the epoch</li></ul></li></ul></li><li>this time is synchronized with NTP, which means a timestamp from one machine ideally means the same as a timestamp on another machine</li><li>Oddies<ul><li>In particular, if the local clock is too far ahead of the NTP server, it may be forcibly reset and appear to jump back to a previous point in time. These jumps, as well as similar jumps caused by leap seconds, make time-of-day clocks unsuitable for measuring elapsed time</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Monotonic-Clock&quot;&gt;&lt;a href=&quot;#1-Monotonic-Clock&quot; class=&quot;headerlink&quot; title=&quot;1. Monotonic Clock&quot;&gt;&lt;/a&gt;1. Monotonic Clock&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;su
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Java" scheme="https://www.llchen60.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Transaction Understanding</title>
    <link href="https://www.llchen60.com/Transaction-Understanding/"/>
    <id>https://www.llchen60.com/Transaction-Understanding/</id>
    <published>2022-03-25T03:16:10.000Z</published>
    <updated>2022-03-25T13:52:19.167Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transaction-Understanding"><a href="#Transaction-Understanding" class="headerlink" title="Transaction Understanding"></a>Transaction Understanding</h1><h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><h2 id="1-1-Things-can-go-wrong"><a href="#1-1-Things-can-go-wrong" class="headerlink" title="1.1 Things can go wrong"></a>1.1 Things can go wrong</h2><ul><li>db software or hardware could fail at any time</li><li>application could crash at any time</li><li>interruptions in the network can unexpectedly cut off the application from the db, or one db node from another</li><li>several clients may write to the db at the same time, overwriting each other’s change</li><li>race conditions between clients can cause surprising bugs</li></ul><h2 id="1-2-Transaction"><a href="#1-2-Transaction" class="headerlink" title="1.2 Transaction"></a>1.2 Transaction</h2><ul><li>Transaction is the mechanism for us to simplify those issues<ul><li>Group several reads and writes together is a logical unit</li><li>either the entire transaction succeeds or it fails<ul><li>application can safely retry if it fails</li></ul></li></ul></li><li>Transaction makes error handling much more easier<ul><li>no need to worry about partial failure</li></ul></li><li>Transaction is created with a purpose, to simplify the programming model for applications accessing a database<ul><li>database take care of such issue</li></ul></li><li>for transaction, we need to understand<ul><li>what safety guarantees transactions can provide</li><li>what costs are associated with them</li></ul></li></ul><h1 id="2-ACID"><a href="#2-ACID" class="headerlink" title="2. ACID"></a>2. ACID</h1><ul><li>ACID are the safety guarantees provided by transactions<ul><li>Atomicity</li><li>Consistency</li><li>Isolation</li><li>Durability</li></ul></li></ul><h2 id="2-1-Atomicity"><a href="#2-1-Atomicity" class="headerlink" title="2.1 Atomicity"></a>2.1 Atomicity</h2><aside>💡 The ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity</aside><ul><li>Atomic means sth <strong>cannot be</strong> broken down into smaller parts</li><li>Describes what happens if a client want to make several writes, but a fault occurs after some of the writes have been processed<ul><li>if the writes are grouped together into an atomic transaction</li><li>the transaction cannot be completed due to a fault,</li><li>then the transaction is aborted</li><li>the database must discard or undo any writes it has made so far in that transaction</li></ul></li><li>The ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity</li></ul><h2 id="2-2-Consistency"><a href="#2-2-Consistency" class="headerlink" title="2.2 Consistency"></a>2.2 Consistency</h2><ul><li>Consistency refers to an application specific notion of the database being in a good state</li><li>The idea of ACID consistency is that you have certain statements about your data (<em>invariants</em>) that must always be true—for example, in an accounting system, credits and debits across all accounts must always be balanced</li><li>But this ususally is application code responsibility to define what data is valid or invalid</li></ul><h2 id="2-3-Isolation"><a href="#2-3-Isolation" class="headerlink" title="2.3 Isolation"></a>2.3 Isolation</h2><ul><li>Isolation means that concurrently executing transactions are isolated from each other: they cannot step on each other’s toe</li></ul><h2 id="2-4-Durability"><a href="#2-4-Durability" class="headerlink" title="2.4 Durability"></a>2.4 Durability</h2><ul><li>Purpose of a db system is to provide a safe place where data can be stored without fear of losing it</li><li><em>Durability</em> is the promise that once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes.</li><li>In a single node database<ul><li>means data has been written to nonvolatile storage such as a hard drive or SSD</li><li>could also involves a write ahead log</li></ul></li><li>In a replicated db,<ul><li>means data has been successfully copied to some number of nodes</li></ul></li></ul><h1 id="3-Single-and-Multi-Objects-Operations-Atomicity-Isolation"><a href="#3-Single-and-Multi-Objects-Operations-Atomicity-Isolation" class="headerlink" title="3. Single and Multi Objects Operations - Atomicity + Isolation"></a>3. Single and Multi Objects Operations - Atomicity + Isolation</h1><h2 id="3-1-Multi-Object-Transactions"><a href="#3-1-Multi-Object-Transactions" class="headerlink" title="3.1 Multi Object Transactions"></a>3.1 Multi Object Transactions</h2><h3 id="3-1-1-Why-we-need-it"><a href="#3-1-1-Why-we-need-it" class="headerlink" title="3.1.1 Why we need it"></a>3.1.1 Why we need it</h3><ul><li>Need some way to determine which read and write operations belong to the same transaction<ul><li>In relational db, typically done based on the <strong>client’s TCP connection to the database server</strong></li><li>on any particular connection, everything between a <strong>BEGIN TRANSACTION and a COMMIT statement</strong> is considered to be part of the same transaction</li></ul></li></ul><h3 id="3-1-2-Scenarios"><a href="#3-1-2-Scenarios" class="headerlink" title="3.1.2 Scenarios"></a>3.1.2 Scenarios</h3><ul><li>In a relational data model, a row in one table often has a foreign key reference to a row in another table<ul><li>multi object transactions allow you to ensure that these references remain valid</li></ul></li><li>In a document data model, the fields that need to be updated together are often <strong>within the same document</strong>, which is treated as a single object—no multi-object transactions are needed when updating a single document. However, document databases lacking join functionality also encourage denormalization. When denormalized information needs to be updated, you need to update several documents in one go. Transactions are very useful in this situation to prevent denormalized data from going out of sync</li><li>In db with secondary indexes, indexes also need to be updated every time you change a value<ul><li>These indexes are different database objects from a transaction point of view: for example, without transaction isolation, it’s possible for a record to appear in one index but not another, because the update to the second index hasn’t happened yet.</li></ul></li></ul><h2 id="3-2-Single-Object-Writes"><a href="#3-2-Single-Object-Writes" class="headerlink" title="3.2 Single Object Writes"></a>3.2 Single Object Writes</h2><ul><li>Atomicity and isolation also apply when a single object is being changed<ul><li>all storage engines universally aim to provide atomicity and isolation on the level of a single object on one node</li></ul></li><li>Atomicity<ul><li>Using a log for crash recovery</li><li>increment operation</li><li>compare and set</li></ul></li><li>Isolation<ul><li>Using a lock on each object</li></ul></li></ul><h2 id="3-3-Handling-errors-and-aborts"><a href="#3-3-Handling-errors-and-aborts" class="headerlink" title="3.3 Handling errors and aborts"></a>3.3 Handling errors and aborts</h2><ul><li>ACID DB Philosophy</li></ul><aside>💡 if the database is in danger of violating its guarantee of atomicity, isolation, or durability, it would rather abandon the transaction entirely than allow it to remain half-finished.</aside><ul><li>Aborts of transaction point is to safely retry, thus we should have some retry mechanism build for such scenario</li></ul><h1 id="4-Isolation-Levels"><a href="#4-Isolation-Levels" class="headerlink" title="4. Isolation Levels"></a>4. Isolation Levels</h1><p>Concurrency bugs are hard to find by testing, cause they are rare, and difficult to reproduce. For such reasons, databases have long tried to hide concurrency issues from application developers by providing transaction isolation </p><p>In theory, isolation should make your life easier by letting you pretend that no concurrency is happening: <em>serializable</em> isolation means that the database guarantees that transactions have the same effect as if they ran <em>serially</em></p><p>In practice, isolation has a performance cost, and many databased don’t want to pay that price. Thus it’s common for systems to use weaker levels of isolation, which protect against some concurrency issues, but not all </p><h2 id="4-1-Read-Committed"><a href="#4-1-Read-Committed" class="headerlink" title="4.1 Read Committed"></a>4.1 Read Committed</h2><h3 id="4-1-1-Guarantees"><a href="#4-1-1-Guarantees" class="headerlink" title="4.1.1 Guarantees"></a>4.1.1 Guarantees</h3><ul><li>when reading from the database, you will only see data that has been committed(no dirty reads)</li><li>when writing to the database, you will only overwrite data that has been committed (no dirty writes)</li></ul><h3 id="4-1-2-No-dirty-reads-Explanation"><a href="#4-1-2-No-dirty-reads-Explanation" class="headerlink" title="4.1.2 No dirty reads Explanation"></a>4.1.2 No dirty reads Explanation</h3><p><img src="https://s2.loli.net/2022/03/25/IPV8vCO1M93YxZT.png" alt=""></p><ul><li><p>Prevent dirty read, only committed record could be seen</p></li><li><p>reasons for dirty read prevention</p><ul><li>See the database in a partially updated state is confusing to users and may cause other transactions to take incorrect decisions</li><li>If a transaction aborts, any writes it has made need to be rolled back (like in <a href="https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/ch07.html#fig_transactions_atomicity">Figure 7-3</a>). If the database allows dirty reads, that means a transaction may see data that is later rolled back—i.e., which is never actually committed to the database. Reasoning about the consequences quickly becomes mind-bending.</li></ul></li></ul><h3 id="4-1-3-No-Dirty-Write-Explanation"><a href="#4-1-3-No-Dirty-Write-Explanation" class="headerlink" title="4.1.3 No Dirty Write Explanation"></a>4.1.3 No Dirty Write Explanation</h3><ul><li>If two transactions concurrently try to update the same object in a db, we normally assume that the later write overwrites the earlier write</li><li>Dirty write happen when<ul><li>the earlier write is part of a transaction that has not yet committed, so the later write overwrites an uncommitted value</li></ul></li><li>how does committed isolation level work?<ul><li>by delaying the second write until the first write’s transaction has committed or aborted</li></ul></li></ul><p><img src="https://s2.loli.net/2022/03/25/3Z6NycPtjEGU9dB.png" alt=""></p><h3 id="4-1-4-Implementation"><a href="#4-1-4-Implementation" class="headerlink" title="4.1.4 Implementation"></a>4.1.4 Implementation</h3><ul><li><p>Dirty writes — Row level lock</p><ul><li>when a transaction wants to modify a particular object (row or document), it must first acquire a lock on that object.</li><li>It must then hold that lock until the transaction is committed or aborted.</li><li>Only one transaction can hold the lock for any given object;</li><li>if another transaction wants to write to the same object, it must wait until the first transaction is committed or aborted before it can acquire the lock and continue.</li><li>This locking is done automatically by databases in <strong>read committed mode (or stronger isolation levels)</strong>.</li></ul></li><li><p>Dirty Reads —</p><ul><li><p>still use row level lock</p><ul><li>the approach of requiring read locks does not work well in practice, because one long-running write transaction can force many other transactions to wait until the long-running transaction has completed, even if the other transactions only read and do not write anything to the database</li></ul></li><li><p>This harms the response time of read-only transactions and is bad for operability: a slowdown in one part of an application can have a knock-on effect in a completely different part of the application, due to waiting for locks.</p></li><li><p>most databases prevent dirty reads using the approach illustrated  here</p><p>  <img src="https://s2.loli.net/2022/03/25/IPV8vCO1M93YxZT.png" alt=""></p></li><li><p>for every object that is written, the database remembers <strong>both the old committed value and the new value</strong> set by the transaction that currently holds the write lock.</p></li><li><p>While the transaction is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions switch over to reading the new value.</p></li></ul></li></ul><h2 id="4-2-Snapshot-Isolation-and-Repeatable-Read"><a href="#4-2-Snapshot-Isolation-and-Repeatable-Read" class="headerlink" title="4.2 Snapshot Isolation and Repeatable Read"></a>4.2 Snapshot Isolation and Repeatable Read</h2><h3 id="4-2-1-Issue-with-Read-Committed"><a href="#4-2-1-Issue-with-Read-Committed" class="headerlink" title="4.2.1 Issue with Read Committed"></a>4.2.1 Issue with Read Committed</h3><p><img src="https://s2.loli.net/2022/03/25/1rRcBlD7Funjztg.png" alt=""></p><ul><li>Read committed can still have concurrency bug<ul><li>In the image above, there will be certain time the amount of 2 accounts are not equal to 1000</li><li>called read skew — nonrepeatable read<ul><li>there will be temporary inconsistency</li><li><strong>read the committed data by another transaction</strong></li></ul></li></ul></li><li>But there are certain situations, that cannot tolerate such temporary inconsistency<ul><li>Backup<ul><li>Taking a backup requires making a copy of the entire database, which may take hours on a large database. During the time that the backup process is running, writes will continue to be made to the database. Thus, you could end up with some parts of the backup containing an older version of the data, and other parts containing a newer version. If you need to restore from such a backup, the inconsistencies (such as disappearing money) become permanent.</li></ul></li><li>Analytic queries and integrity checks<ul><li>nonsensical results will be returned if the database is at different points in time</li></ul></li></ul></li></ul><h3 id="4-2-2-Solution-Snapshot-isolation"><a href="#4-2-2-Solution-Snapshot-isolation" class="headerlink" title="4.2.2 Solution: Snapshot isolation"></a>4.2.2 Solution: Snapshot isolation</h3><ul><li>Each transaction reads from a consistent snapshot of the database,<ul><li>the transaction sees all the data that was committed in the db at the start of the transaction</li></ul></li></ul><h3 id="4-2-3-Snapshot-Isolation-Implementation"><a href="#4-2-3-Snapshot-Isolation-Implementation" class="headerlink" title="4.2.3 Snapshot Isolation Implementation"></a>4.2.3 Snapshot Isolation Implementation</h3><ul><li><p>Use write locks to prevent dirty writes</p><ul><li>a transaction that makes a write can block the progress of another transaction that writes to the same object</li></ul></li><li><p>reads do not require any locks.</p><ul><li>From a performance point of view, a key principle of snapshot isolation is <em>readers never block writers, and writers never block readers</em></li><li>This allows a database to handle long-running read queries on a consistent snapshot at the same time as processing writes normally, without any lock contention between the two.</li></ul></li><li><p>To implement snapshot isolation, db uses a generalization of the mechanism</p><ul><li>db must potentially keep several different committed versions of an object, because various in progress transactions may need to see the state of the db at different points in time</li><li>— named as multi version concurrency control — MVCC</li></ul></li><li><p>If a database only needed to provide read committed isolation, but not snapshot isolation, it would be sufficient to keep two versions of an object:</p><ul><li>the committed version and the overwritten-but-not-yet-committed version.</li></ul></li><li><p>However, storage engines that support snapshot isolation typically use MVCC for their read committed isolation level as well.</p><ul><li><strong>A typical approach is that read committed uses a separate snapshot for each query, while snapshot isolation uses the same snapshot for an entire transaction.</strong></li></ul></li></ul><h3 id="4-2-4-Visibility-rules-for-observing-a-consistent-snapshot"><a href="#4-2-4-Visibility-rules-for-observing-a-consistent-snapshot" class="headerlink" title="4.2.4 Visibility rules for observing a consistent snapshot"></a>4.2.4 Visibility rules for observing a consistent snapshot</h3><ul><li>Transaction IDs are used to decide which objects it can see and which are invisible</li><li>Rules<ol><li>At the start of each transaction, the database makes a list of all the other transactions that are in progress (not yet committed or aborted) at that time. Any writes that those transactions have made are ignored, even if the transactions subsequently commit.</li><li>Any writes made by aborted transactions are ignored.</li><li>Any writes made by transactions with a later transaction ID (i.e., which started after the current transaction started) are ignored, regardless of whether those transactions have committed.</li><li>All other writes are visible to the application’s queries.</li></ol></li></ul><h2 id="4-3-Preventing-Lost-Updates"><a href="#4-3-Preventing-Lost-Updates" class="headerlink" title="4.3 Preventing Lost Updates"></a>4.3 Preventing Lost Updates</h2><h3 id="4-3-1-Issues"><a href="#4-3-1-Issues" class="headerlink" title="4.3.1 Issues"></a>4.3.1 Issues</h3><p>The lost update problem can occur if an application reads some value from the database, modifies it, and writes back the modified value (a <em>read-modify-write cycle</em>). <strong>If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification.</strong> (We sometimes say that the later write <em>clobbers</em> the earlier write.) This pattern occurs in various different scenarios: </p><ul><li>Incrementing a counter or updating an account balance (requires reading the current value, calculating the new value, and writing back the updated value)</li><li>Making a local change to a complex value, e.g., adding an element to a list within a JSON document (requires parsing the document, making the change, and writing back the modified document)</li><li>Two users editing a wiki page at the same time, where each user saves their changes by sending the entire page contents to the server, overwriting whatever is currently in the database</li></ul><h3 id="4-3-2-Solution-1-Atomic-Write-Operations"><a href="#4-3-2-Solution-1-Atomic-Write-Operations" class="headerlink" title="4.3.2 Solution 1: Atomic Write Operations"></a>4.3.2 Solution 1: Atomic Write Operations</h3><ul><li>Using atomic update provided by database,</li></ul><pre><code class="ruby">UPDATE counters SET value = value + 1 WHERE key = &#39;foo&#39;;</code></pre><ul><li>Atomic operations are usually implemented by taking an <strong>exclusive lock on the object when it is read</strong> so that no other transaction can read it until the update has been applied</li><li>Also, we could force all atomic operations to be executed on a single thread</li></ul><h3 id="4-3-3-Solution-2-Explicit-Locking"><a href="#4-3-3-Solution-2-Explicit-Locking" class="headerlink" title="4.3.3 Solution 2: Explicit Locking"></a>4.3.3 Solution 2: Explicit Locking</h3><ul><li>Explicitly lock objects that are going to be updated<ul><li>then the application can perform a read modify write cycle</li><li>if any other transaction tries to concurrently read the same object, it is forced to wait until the first read-modify-write cycle has completed</li></ul></li></ul><pre><code class="ruby">BEGIN TRANSACTION;SELECT * FROM figures  WHERE name = &#39;robot&#39; AND game_id = 222// For Update will let databse take a lock o  FOR UPDATE; 1-- Check whether move is valid, then update the position-- of the piece that was returned by the previous SELECT.UPDATE figures SET position = &#39;c4&#39; WHERE id = 1234;COMMIT;</code></pre><h3 id="4-3-4-Solution-3-Automatically-detecting-lost-updates"><a href="#4-3-4-Solution-3-Automatically-detecting-lost-updates" class="headerlink" title="4.3.4 Solution 3: Automatically detecting lost updates"></a>4.3.4 Solution 3: Automatically detecting lost updates</h3><ul><li>Atomic operations and locks are ways of preventing lost updates by forcing the read-modify-write cycles to happen sequentially.</li><li>An alternative is to allow them to <strong>execute in parallel</strong> and, if the transaction manager <strong>detects a lost update, abort the transaction</strong> and <strong>force it to retry its read-modify-write cycle</strong>.</li><li>Lost update detection is great because it doesn’t require application code to use any special database features, you could forget to use a lock or an atomic operation, but lost update detection happens automatically and thus less error prone</li></ul><h3 id="4-3-5-Solution-4-Compare-and-set"><a href="#4-3-5-Solution-4-Compare-and-set" class="headerlink" title="4.3.5 Solution 4: Compare and set"></a>4.3.5 Solution 4: Compare and set</h3><ul><li>Avoid lost updates by allowing an update to happen only if the value has not changed since you last read it</li></ul><pre><code class="ruby">UPDATE wiki_pages SET content = &#39;new content&#39;  WHERE id = 1234 AND content = &#39;old content&#39;;</code></pre><ul><li>But notice the compare and set operation is possible to be unsafe if db by default read from the old snapshot</li></ul><h3 id="4-3-6-Solution-5-Conflict-resolution-and-replication"><a href="#4-3-6-Solution-5-Conflict-resolution-and-replication" class="headerlink" title="4.3.6 Solution 5: Conflict resolution and replication"></a>4.3.6 Solution 5: Conflict resolution and replication</h3><p>In multi leader or leaderless replication system, a common approach in such replicated databases is to allow concurrent writes to create several conflicting versions of a value (also known as <em>siblings</em>), and to use application code or special data structures to resolve and merge these versions after the fact.</p><h2 id="4-4-Write-Skew-and-Phantoms"><a href="#4-4-Write-Skew-and-Phantoms" class="headerlink" title="4.4 Write Skew and Phantoms"></a>4.4 Write Skew and Phantoms</h2><p><img src="https://s2.loli.net/2022/03/25/pFhAQWIU8O4bEZY.png" alt=""></p><ul><li>In each transaction, we first check that two or more doctors are currently on call,<ul><li>since db is using snapshot isolation, both checks return 2, so both of them proceed to the next stage</li></ul></li><li>Write Skew<ul><li>it’s neither a dirty write nor a lost update, because the two transactions are updating two different objects</li><li>But it’s a race condition as the anomalous behavior was only possible because the transactions ran concurrently</li></ul></li><li>You can think of write skew as a generalization of the lost update problem.<ul><li>Write skew can occur if <strong>two transactions read the same objects</strong>, and <strong>then update some of those objects (different transactions may update different objects).</strong></li><li>In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing).</li></ul></li></ul><h3 id="4-4-2-Solution"><a href="#4-4-2-Solution" class="headerlink" title="4.4.2 Solution"></a>4.4.2 Solution</h3><ul><li>We need to explicitly lock the rows as solution in isolation snapshot works for one object, now the case come to be multiple objects, so that comes to be different</li></ul><pre><code class="ruby">BEGIN TRANSACTION;SELECT * FROM doctors  WHERE on_call = true  AND shift_id = 1234 FOR UPDATE; 1UPDATE doctors  SET on_call = false  WHERE name = &#39;Alice&#39;  AND shift_id = 1234;COMMIT;</code></pre><h3 id="4-4-3-Phantoms"><a href="#4-4-3-Phantoms" class="headerlink" title="4.4.3 Phantoms"></a>4.4.3 Phantoms</h3><p>All of these examples follow a similar pattern:</p><ol><li><p>A <code>SELECT</code> query checks whether some requirement is satisfied by searching for rows that match some search condition (there are at least two doctors on call, there are no existing bookings for that room at that time, the position on the board doesn’t already have another figure on it, the username isn’t already taken, there is still money in the account).</p></li><li><p>Depending on the result of the first query, the application code decides how to continue (perhaps to go ahead with the operation, or perhaps to report an error to the user and abort).</p></li><li><p>If the application decides to go ahead, it makes a write (<code>INSERT</code>, <code>UPDATE</code>, or <code>DELETE</code>) to the database and commits the transaction.</p><p> The effect of this write changes the precondition of the decision of step 2. In other words, if you were to repeat the <code>SELECT</code> query from step 1 after committing the write, you would get a different result, because the write changed the set of rows matching the search condition (there is now one fewer doctor on call, the meeting room is now booked for that time, the position on the board is now taken by the figure that was moved, the username is now taken, there is now less money in the account).</p></li></ol><p>The steps may occur in a different order. For example, you could first make the write, then the <code>SELECT</code> query, and finally decide whether to abort or commit based on the result of the query.</p><p>In the case of the doctor on call example, the row being modified in step 3 was one of the rows returned in step 1, so we could make the transaction safe and avoid write skew by locking the rows in step 1 (<code>SELECT FOR UPDATE</code>). However, the other four examples are different: they check for the <em>absence</em> of rows matching some search condition, and the write <em>adds</em> a row matching the same condition. If the query in step 1 doesn’t return any rows, <code>SELECT FOR UPDATE</code> can’t attach locks to anything.</p><p>This effect, where a write in one transaction changes the result of a search query in another transaction, is called a <em>phantom</em> [<a href="https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/ch07.html#Eswaran1976uu">3</a>]. Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions like the examples we discussed, phantoms can lead to particularly tricky cases of write skew.</p><h3 id="4-4-4-Materializing-conflicts-for-Phantoms"><a href="#4-4-4-Materializing-conflicts-for-Phantoms" class="headerlink" title="4.4.4 Materializing conflicts for Phantoms"></a>4.4.4 Materializing conflicts for Phantoms</h3><ul><li>Pre create all rows in the db, thus phantoms problems could be converted to the doctor appointment problem we discussed before</li></ul><h3 id="4-4-5-Predicate-Locks"><a href="#4-4-5-Predicate-Locks" class="headerlink" title="4.4.5 Predicate Locks"></a>4.4.5 Predicate Locks</h3><ul><li>We could use a predicate lock for booking case, works similarly to  the shared/ exclusive lock, but rather than belong to a particular object, it belongs to all objects that match some search condition</li></ul><pre><code class="ruby">SELECT * FROM bookings  WHERE room_id = 123 AND    end_time   &gt; &#39;2018-01-01 12:00&#39; AND    start_time &lt; &#39;2018-01-01 13:00&#39;;</code></pre><ul><li>If transaction A wants to read objects matching some condition, like in that <code>SELECT</code> query, it must acquire a shared-mode predicate lock on the conditions of the query. If another transaction B currently has an exclusive lock on any object matching those conditions, A must wait until B releases its lock before it is allowed to make its query.</li><li>If transaction A wants to insert, update, or delete any object, it must first check whether either the old or the new value matches any existing predicate lock. If there is a matching predicate lock held by transaction B, then A must wait until B has committed or aborted before it can continue.</li></ul><h2 id="4-5-Serializability"><a href="#4-5-Serializability" class="headerlink" title="4.5 Serializability"></a>4.5 Serializability</h2><ul><li>There are a lot of different isolation levels, and each db declare their isolation level slightly different,<ul><li>We could use serializable isolation to simplify it</li></ul></li><li>Serializable isolation<ul><li>Strongest isolation level</li><li>without any concurrency</li></ul></li><li>Serializable Mechanism<ul><li>Literally executing transactions in a serail order</li><li>Two Phase Locking</li><li>Optimistic concurrency control techniques such as serializable snapshot isolation</li></ul></li></ul><h3 id="4-5-1-Actual-Serial-Execution"><a href="#4-5-1-Actual-Serial-Execution" class="headerlink" title="4.5.1 Actual Serial Execution"></a>4.5.1 Actual Serial Execution</h3><ul><li><p>We could try to only execute one transaction at a time, in serial order, on a single thread</p></li><li><p>This comes to be realistic recently around 2007 because</p><ul><li>RAM became cheap enough, thus it’s now feasible to keep the entire active dataset in memory</li><li>OLTP transactions are usually short and only make a small number of reads and writes<ul><li>by contrast, long running analytic queries are typically read only, so they can be run on a consistent snapshot outside of the serial execution loop</li></ul></li></ul></li><li><p>This approach is implemented in VoltDB/ Hstore, Redis, and Datomic</p></li><li><p>Notice</p><ul><li>A system designed for a single threaded execution can sometimes perform better than a system that supports concurrency, because it can avoid the coordination overhead of locking</li></ul></li></ul><h3 id="4-5-2-Encapsulating-transactions-in-stored-procedures"><a href="#4-5-2-Encapsulating-transactions-in-stored-procedures" class="headerlink" title="4.5.2 Encapsulating transactions in stored procedures"></a>4.5.2 Encapsulating transactions in stored procedures</h3><ul><li><p>Philosophy</p><ul><li>Keep transactions short by avoiding interactively waiting for a user within a transaction</li><li>means a transaction is committed within the same HTTP request,</li></ul></li><li><p>IN the interactive style of transaction, network and db will take a lot time, we need to make sure we could handle enough throughput, we need to process multiple transactions concurrently in order to get reasonable performance.</p></li><li><p>Systems with single threaded serial transaction processing could receive the entier transaction code to db ahead of time , as a <strong>stored procedure</strong></p></li></ul><p><img src="https://s2.loli.net/2022/03/25/W1TFbpKA5JGhwXd.png" alt=""></p><ul><li>Pros and Cons<ul><li>Each db has its own language for stored procedures</li><li>Code running in db is difficult to manage, more awkward to keep in version control and deploy</li><li>db is often much more performance sensitive,because a single db instance is often shared by a lot of application servers. A bad written stored procedure can cause much more trouble than equivalent badly written code in an application server</li></ul></li></ul><h3 id="4-5-3-Partitioning"><a href="#4-5-3-Partitioning" class="headerlink" title="4.5.3 Partitioning"></a>4.5.3 Partitioning</h3><ul><li>Partition data to scale to multiple CPU cores, and multiple nodes.</li><li>However, for any transaction that needs to access multiple partitions, the database must coordinate the transaction across all the partitions that it touches. The stored procedure needs to be performed in lock-step across all partitions to ensure serializability across the whole system.</li></ul><h1 id="5-Two-Phase-Locking"><a href="#5-Two-Phase-Locking" class="headerlink" title="5. Two Phase Locking"></a>5. Two Phase Locking</h1><h2 id="5-1-Concepts"><a href="#5-1-Concepts" class="headerlink" title="5.1 Concepts"></a>5.1 Concepts</h2><ul><li>Several transactions are allowed to concurrently read the same object as long as nobody is writing to it</li><li>But as soon as anyone wants to write(modify or delete) an object, exclusive access is required<ul><li>Writes not only block other writers, it also block all readers</li></ul></li></ul><h2 id="5-2-Implementation"><a href="#5-2-Implementation" class="headerlink" title="5.2 Implementation"></a>5.2 Implementation</h2><ul><li>If a transaction wants to read an object, it must first acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simultaneously, but if another transaction already has an exclusive lock on the object, these transactions must wait.</li><li>If a transaction wants to write to an object, it must first acquire the lock in exclusive mode. No other transaction may hold the lock at the same time (either in shared or in exclusive mode), so if there is any existing lock on the object, the transaction must wait.</li><li>If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. The upgrade works the same as getting an exclusive lock directly.</li><li>After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name “two-phase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.</li></ul><h2 id="5-3-Performance"><a href="#5-3-Performance" class="headerlink" title="5.3 Performance"></a>5.3 Performance</h2><ul><li>transaction throughput and response times of queries are significantly worse under two-phase locking than under weak isolation.<ul><li>overhead of <strong>acquiring and releasing all those locks</strong>, but more importantly due to <strong>reduced concurrency</strong>.</li></ul></li></ul><h1 id="6-Serializable-Snapshot-Isolation"><a href="#6-Serializable-Snapshot-Isolation" class="headerlink" title="6. Serializable Snapshot Isolation"></a>6. Serializable Snapshot Isolation</h1><ul><li>It provides full serializability, but has only a small performance penalty compared to snapshot isolation</li><li>Snapshot + Serializable<ul><li>optimistic concurrency control mechanism</li><li>It performs badly if there is high contention (many transactions trying to access the same objects), as this leads to a high proportion of transactions needing to abort. If the system is already close to its maximum throughput, the additional transaction load from retried transactions can make performance worse.</li></ul></li><li>to achieve serialization, db need to know if the query result has been changed within the transaction<ul><li>Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)</li><li>Detecting writes that affect prior reads (the write occurs after the read)</li></ul></li></ul><h2 id="6-1-Detecting-stale-MVCC-reads"><a href="#6-1-Detecting-stale-MVCC-reads" class="headerlink" title="6.1 Detecting stale MVCC reads"></a>6.1 Detecting stale MVCC reads</h2><p>In order to prevent this anomaly, the database needs to track when a transaction ignores another transaction’s writes due to MVCC visibility rules. When the transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted.</p><p><img src="https://s2.loli.net/2022/03/25/kyCJZvs5QB93b1u.png" alt=""></p><h2 id="6-2-Detecting-writes-that-affect-prior-reads"><a href="#6-2-Detecting-writes-that-affect-prior-reads" class="headerlink" title="6.2 Detecting writes that affect prior reads"></a>6.2 Detecting writes that affect prior reads</h2><p>When a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. This process is similar to acquiring a write lock on the affected key range, but rather than blocking until the readers have committed, the lock acts as a tripwire: it simply notifies the transactions that the data they read may no longer be up to date.</p><h2 id="6-3-Performance"><a href="#6-3-Performance" class="headerlink" title="6.3 Performance"></a>6.3 Performance</h2><p>Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one transaction doesn’t need to block waiting for locks held by another transaction. Like under snapshot isolation, writers don’t block readers, and vice versa. This design principle makes query latency much more predictable and less variable. In particular, read-only queries can run on a consistent snapshot without requiring any locks, which is very appealing for read-heavy workloads.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Transaction-Understanding&quot;&gt;&lt;a href=&quot;#Transaction-Understanding&quot; class=&quot;headerlink&quot; title=&quot;Transaction Understanding&quot;&gt;&lt;/a&gt;Transaction
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="transaction" scheme="https://www.llchen60.com/tags/transaction/"/>
    
  </entry>
  
  <entry>
    <title>置身事内 笔记</title>
    <link href="https://www.llchen60.com/%E7%BD%AE%E8%BA%AB%E4%BA%8B%E5%86%85-%E7%AC%94%E8%AE%B0/"/>
    <id>https://www.llchen60.com/%E7%BD%AE%E8%BA%AB%E4%BA%8B%E5%86%85-%E7%AC%94%E8%AE%B0/</id>
    <published>2022-02-02T08:41:20.000Z</published>
    <updated>2022-02-05T09:21:58.786Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-整体框架"><a href="#1-整体框架" class="headerlink" title="1. 整体框架"></a>1. 整体框架</h1><ul><li>微观机制<ul><li>地方政府的权力和事务</li><li>财税与政府行为</li><li>政府投融资和债务</li><li>工业化当中的政府角色</li></ul></li><li>宏观现象<ul><li>城市化和不平衡</li><li>债务与风险</li><li>国内国际失衡</li><li>政府与经济发展</li></ul></li></ul><h1 id="2-地方政府的权力和事务"><a href="#2-地方政府的权力和事务" class="headerlink" title="2. 地方政府的权力和事务"></a>2. 地方政府的权力和事务</h1><aside>💡 地方政府不只提供了公共服务，也深度参与生产和分配。</aside><h2 id="2-1-体制特点"><a href="#2-1-体制特点" class="headerlink" title="2.1 体制特点"></a>2.1 体制特点</h2><ul><li>政府管理体系<ul><li>中央 - 省 - 市 - 县区 - 乡镇</li></ul></li><li>体制特点<ul><li>中央与地方政府<ul><li>日常运作以地方政府为主</li></ul></li><li>党与政府<ul><li>党负责重大决策和人事任免</li><li>政府负责执行</li></ul></li><li>条块分割，多重领导<ul><li>层层复制</li><li>党委政府人大政协四套班子</li><li>条条关系是业务关系，块块关系是领导关系—地方党委和政府可以决定人事任免</li></ul></li><li>上层领导与协调<ul><li>权力分散，决策容易向上集中</li><li>尽量在能达成共识的最低层级上解决问题</li></ul></li><li>官僚体系<ul><li>官员必须学习和贯彻统一的意识形态</li><li>官员由上级任命</li><li>地方主官需要在多地轮换任职</li></ul></li></ul></li><li>政府治理和运作的模式<ul><li>了解权力和资源在政府体系中的分布规则<ul><li>上下级政府间的纵向分布</li><li>同级政府间的横向分布</li></ul></li></ul></li><li>事权划分的原则<ul><li>公共服务的规模经济</li><li>信息的复杂性</li><li>激励相容</li></ul></li></ul><h2 id="2-2-外部性-规模经济"><a href="#2-2-外部性-规模经济" class="headerlink" title="2.2 外部性 规模经济"></a>2.2 外部性 规模经济</h2><ul><li>一个事物是只在划定的区域范围内有影响 还是对外也有影响</li><li>行政区域划分<ul><li>人口密度 — 提供公共物品和服务需要成本</li><li>山川河流</li><li>语言文化差异</li></ul></li><li>城市群的规划<ul><li>因为经济活动和人口集聚，需要打破现有的行政边界，在更大范围内提供无缝对接的标准化公共服务</li></ul></li><li>行政交界地区经济发展弱<ul><li>地理原因</li><li>亚文化区，距离主流文化相对远 — 文化，语言上</li><li>省政府也不会把有限资源优先配置到边界地区</li><li>环境污染谁来治理的问题</li></ul></li></ul><h2 id="2-3-复杂信息"><a href="#2-3-复杂信息" class="headerlink" title="2.3 复杂信息"></a>2.3 复杂信息</h2><ul><li>信息与权力<ul><li>原则上  上级形式权威</li><li>实际上给下级  下级有实际权威</li><li>信息优势  权威的平衡是核心问题</li></ul></li><li>信息获取+隐瞒<ul><li>信息传递很重要<ul><li>大量的会议和文件</li></ul></li><li>复杂的文件和会议制度<ul><li>文件类型，格式，以及报送都有严格的流程</li></ul></li></ul></li><li>上层的监督和审计</li></ul><aside>💡 所谓权力，实质上就是在说不清楚的情况下由谁来拍板决策的问题</aside><h2 id="2-4-激励相容"><a href="#2-4-激励相容" class="headerlink" title="2.4 激励相容"></a>2.4 激励相容</h2><ul><li>垂直管理<ul><li>专业性强，标准化程度高的部门</li><li>面对有双重领导的部门，都有一个根本的激励设计的问题<ul><li>能评价和奖惩工作业绩的上级，能决定工作内容的上级，受下级工作影响最大的上级，应该尽量是同一个上级</li></ul></li></ul></li><li>地方管理<ul><li>更加宏观，更加需要多部门合作的方面，比如经济发展</li><li>需要给地方放权，地方负责且地方能够分享发展成果</li></ul></li></ul><h2 id="2-5-招商引资"><a href="#2-5-招商引资" class="headerlink" title="2.5 招商引资"></a>2.5 招商引资</h2><aside>💡 地方政府不仅在为经济发展创造环境，它本身就是经济发展的深度参与者。</aside><ul><li>全民招商政策<ul><li>即招商不是单个部门要做的，是所有部门都需要熟悉政策，寻找招商机会的</li></ul></li><li>地方政府是城市土地所有者<ul><li>会将工业用地以非常优惠的价格转让给企业使用</li><li>负责对土地进行一系列初期开发 — 七通一平</li></ul></li><li>金融支持<ul><li>政府控制的投资平台入股</li><li>调动本地国企参与投资</li><li>通过各种方式协助企业获得银行贷款</li></ul></li><li>事务上支持<ul><li>各种许可</li></ul></li><li>补贴，税收优惠</li></ul><h1 id="3-财税与政府行为"><a href="#3-财税与政府行为" class="headerlink" title="3. 财税与政府行为"></a>3. 财税与政府行为</h1><ul><li><p>把握政府的真实意图，不能光读文件，还要看政府资金的流向和数量</p></li><li><p>事权与财力匹配 — 事权和支出责任匹配</p></li><li><p>实际情况</p><ul><li>事权与财权高度不匹配<ul><li>自1994年实行分税制改革依赖，地方财政预算支出ji</li></ul></li></ul></li></ul><h2 id="3-1-分税制改革"><a href="#3-1-分税制改革" class="headerlink" title="3.1 分税制改革"></a>3.1 分税制改革</h2><ul><li>1985年 — 1993年 财政包干<ul><li>承包制<ul><li>土地承包</li><li>企业承包</li><li>财政承包</li></ul></li><li>承包制原因，我国基本国策决定了不能对所有权做出根本性变革，只能对使用权和经营权实行承包制</li><li>实行方式<ul><li>1988年北京  以1987年财政收入为基数，设定一个固定的年收入增长率4%，超过4%的部分都归北京，没超过的部分则和中央五五分成</li></ul></li><li>地方实现方式<ul><li>大力发展乡镇企业</li></ul></li><li>影响<ul><li>中央财政预算收入占全国财政预算总收入的比重越来越低</li><li>全国财政预算总收入占GDP的比重也越来越低<ul><li>央地分成比例每过几年就要重新谈判</li><li>预算外收入可独享，因此地方政府疯狂给政府减税，藏富于企业，再通过行政收费，集资，摊派，赞助等方式收回一部分</li></ul></li></ul></li></ul></li><li>1994年分税制改革<ul><li>税务分类<ul><li>中央税</li><li>地方税</li><li>共享税<ul><li>增值税 — 占全国税收的1/4<ul><li>中央拿75% 地方留25%</li><li>转换过程<ul><li>为了防止地方财政收入的剧烈下跌，设立了税收返还的机制，保证改革后地方增值税收入和改革前一样，新增部分才和中央分</li></ul></li></ul></li></ul></li></ul></li><li>机构设置 — 与地方财政脱钩<ul><li>国税</li><li>地税</li></ul></li><li>行政方式<ul><li>省以下税务机关以垂直管理为主，由上级税务机构负责管理人员和工资</li></ul></li></ul></li></ul><aside>💡 公众所接触的信息和看到的现象，大都是已经博弈了的结果，而缺少社会阅历的学生容易把博弈结果错当成博弈过程。成功的政策背后是成功的协商和妥协，而不是机械的命令和执行，所以理解利益冲突，理解协调和解决机制，是理解政策的基础。</aside><ul><li>分税制改革中的权衡<ul><li>93年谈，想以92年作为基年，但是协商后改成93年，因此各个地方政府开始93年突击缴税</li></ul></li><li>分税制改革的影响<ul><li>中央占全国预算总收入的比重从改革前20% 到了55%</li><li>国家预算收入占GDP的比重从11%增加到了20%以上</li></ul></li></ul><h2 id="3-2-土地财政"><a href="#3-2-土地财政" class="headerlink" title="3.2 土地财政"></a>3.2 土地财政</h2><aside>💡 分税制改革减少了地方政府的可支配的财政资源，但是没有改变以经济建设为中心的任务；发展经济所需的诸多额外支出，比如招商引资，土地开发等，需要另筹资金了</aside><ul><li>解决方案<ul><li>努力提高税收规模</li><li>增加预算外收入<ul><li>围绕土地出让和土地开发所产生的的土地财政</li></ul></li></ul></li><li>地方政府青睐重资产的制造业<ul><li>投资规模大，对GDP的拉动作用明显</li><li>增值税在生产环节增收，跟生产规模直接挂钩</li><li>制造业不仅可以吸纳从农业部门转移出来的低技能劳动力，也可以带动第三产业发展，增加相关税收</li></ul></li><li>演变进程<ul><li>1998年 单位停止福利分房</li><li>1997 - 2002年 城镇住宅新开工面积年均增长26%</li><li>2001年 国家推行招标拍卖</li><li>2002年 国土部明确四类经营用地 商业，旅游，娱乐，房地产采用招拍挂的制度</li><li>国有土地转让收入占地方公共预算收入的比重在60%左右 从1999年低于10% 到现在的高度</li></ul></li><li>土地财政囊括<ul><li>土地使用权转让收入</li><li>和土地使用开发有关的各种税收收入<ul><li>大部分税基是价值而非面积</li></ul></li></ul></li><li>问题<ul><li>土地资本化运作，是在将未来的收益抵押到今天来借钱，因为地方官员任期有限，投资质量是难以保证的</li></ul></li></ul><h2 id="3-3-横向纵向的不平衡"><a href="#3-3-横向纵向的不平衡" class="headerlink" title="3.3 横向纵向的不平衡"></a>3.3 横向纵向的不平衡</h2><p>分税制改革以后，中央拿走了大头，但事情还是地方办，地方收支差距需要中央进行转移支付。全国总数上来看，能补得上。但总数不得上不代表每一级政府都能够补得上。</p><ul><li><p>问题</p><ul><li>财权层层上收，事权层层下压<ul><li>2000年 湖北监利县“农民真苦，农村真穷，农业真危险” — 三农问题<ul><li>农村税费改革，制止基层的乱摊派乱收费问题</li><li>废止农业税</li></ul></li></ul></li><li>财税体制的层级问题</li></ul></li><li><p>解决方案</p><ul><li>农村基本公共服务开始 纳入国家公共财政保障范围，中央地方政府共同承担</li><li>在转移支付中加入激励机制，鼓励基层政府达成特定目标，并给予奖励</li><li>将基层财政资源向上一级政府统筹<ul><li>乡财县管</li><li>扩权强县</li><li>财政省直管县</li></ul></li></ul></li><li><p>中央政府通过再分配  转移支付，支援中西部</p></li></ul><h1 id="4-政府投融资与债务"><a href="#4-政府投融资与债务" class="headerlink" title="4. 政府投融资与债务"></a>4. 政府投融资与债务</h1><aside>💡 土地不会移动也不会消失，天然适合作抵押，做各种资本交易的压仓标的，身价自然飙升。土地资本化的魔力，在于可以挣脱物理属性，在抽象的意义上交易承诺和希望，将过去的储蓄，现在的收入，未来的前途，统统汇聚和封存在一小片土地上，使其价值暴增。经济发展的奥秘之一，就是将有形资产转变成为这种抽象资本，从而聚合跨越空间和时间的资源。</aside><h2 id="4-1-城投公司和土地金融-—-土地开发和基础设施投资"><a href="#4-1-城投公司和土地金融-—-土地开发和基础设施投资" class="headerlink" title="4.1 城投公司和土地金融 — 土地开发和基础设施投资"></a>4.1 城投公司和土地金融 — 土地开发和基础设施投资</h2><ul><li>法律规定，地方政府不可以从银行贷款，2015年前也不允许发行债券 — 因此地方政府需要成立公司<ul><li>国有独资企业 — 地方政府融资平台 — 城投公司<ul><li>建设投资 + 投资开发 + 旅游发展</li></ul></li></ul></li><li>城投公司的一般特征<ul><li>持有从政府取得的大量土地使用权<ul><li>土地使用权可以用来撬动银行贷款，以及各种其他资金</li></ul></li><li>盈利状况依赖政府补贴<ul><li>因为承接的项目很多都有基础设施属性</li><li>项目本身盈利能力往往比较弱</li></ul></li><li>政府的隐性担保可以让企业大量借款</li></ul></li><li>土地一级开发<ul><li>平整整理土地</li><li>投入大，利润低，涉及拆迁等问题</li><li>一般由政府融资平台公司完成</li></ul></li><li>土地二级开发<ul><li>土地的建设运营</li><li>由房地产公司来做</li></ul></li><li>华夏幸福的方式—- 政府和社会资本合作 Public Private Partnership — PPP<ul><li>产城结合</li><li>政府委托华夏幸福做住宅用地的一级开发</li><li>这片熟地要还给政府的</li><li>再以招拍挂等公开方式出让给中标的房地产企业</li><li>从工业园区的发展当中，其可以和政府分享税收收益<ul><li>按照法律，政府不能和企业直接分享税收，但是可以购买企业服务，用产业发展服务费的名义来支付约定的分成</li></ul></li></ul></li></ul><h2 id="4-2-地方政府债务"><a href="#4-2-地方政府债务" class="headerlink" title="4.2 地方政府债务"></a>4.2 地方政府债务</h2><ul><li><p>政府依靠土地使用权转让收入支撑起土地财政，并将未来的土地收益资本化，从银行和其他渠道借入天量资金，利用土地金融，快速推动工业化和城市化，但也同时积累了大量债务</p></li><li><p>模式的关键</p><ul><li>土地价格<ul><li>只要不断地投资和建设能带来持续的经济增长，城市就会扩张，地价就会上涨</li><li>这样就能够偿还连本带利越来越多的债务</li><li>但是经济增速一旦放缓，地价下跌，土地出让收入减少，累积的债务就会成为沉重负担，可能压垮融资平台甚至地方政府</li></ul></li></ul></li><li><p>国家开发银行和城投债</p><ul><li>为什么需要上述二者<ul><li>因为地方政府穷，但是还要发展经济，做城市化</li><li>想要在城市建设开发当中引入银行资金，需要解决三个问题<ul><li>需要一个能借款的公司，政府不能直接从银行贷款</li><li>城建开发项目复杂，有些赚钱，有些赔钱，所以需要打包捆绑</li><li>依靠财政预算收入不够还债的，要能把跟土地有关的收益用起来</li></ul></li></ul></li><li>为了解决上述引入资金的问题，出现了<strong>城投公司</strong><ul><li>国家开发银行创立的这种模式<ul><li>贷款，然后可以用土地出让收益作为质押进行还款保证</li></ul></li></ul></li><li>城商行<ul><li>七成左右第一股东为地方政府</li><li>为了方便为融资平台公司和基础设施建设提供贷款</li><li>风险<ul><li>基础设施建设项目周期长，需要中长期贷款<ul><li>国开行是政策性银行，有稳定的长期资金来源</li><li>但是商业银行的存款大都来自短期存款，与中长期贷款期限不匹配，容易产生风险</li></ul></li><li>四大行存款来源庞大，可以承受一定程度期限错配，但是城商行不能，经常需要在资本市场融资，容易出现风险<ul><li>包商银行</li></ul></li></ul></li></ul></li><li>城投公司融资方式<ul><li>银行贷款</li><li>发行债券<ul><li>理论优势<ul><li>分散风险</li><li>债券可交易，价格和利率可变，配置效率高</li></ul></li><li>实际<ul><li>七八成商业银行所有</li><li>隐性背书，尽管有风险，但是大家有点不管不顾</li></ul></li></ul></li></ul></li><li>地方债务<ul><li>隐性负债很多</li><li>局部风险大</li><li>很多地方都靠着中央的补贴保证不违约，但是如果经济遇冷，地价下跌，政府也无法背起这沉重的债务了</li></ul></li></ul></li><li><p>地方债的治理</p><ul><li>债务置换<ul><li>用地方政府发行的公债，替换一部分融资平台公司的银行贷款和城投债<ul><li>好处<ul><li>利率从之前的7% - 8% 降低到4%左右<ul><li>原先因为有政府的隐性担保，所以银行都愿意贷给城投，这样还拉高了一般企业的贷款成本和难度</li></ul></li><li>政府公债期限要长很多<ul><li>降低了期限错配和流动性的风险</li></ul></li><li>信用级别升高</li></ul></li></ul></li><li>如何确定债务置换的规模<ul><li>国务院确定并报全国人大或者人大常委会批准</li></ul></li></ul></li><li>推动融资平台转型<ul><li>剥离为政府融资的功能，同时破除政府的隐性担保</li></ul></li><li>约束银行和各类金融机构，避免大量资金流入融资平台</li><li>问责官员，对于过度负债的行为终身追责</li></ul></li></ul><h2 id="4-3-招商引资当中的地方官员"><a href="#4-3-招商引资当中的地方官员" class="headerlink" title="4.3 招商引资当中的地方官员"></a>4.3 招商引资当中的地方官员</h2><ul><li>官员政绩与激励机制<ul><li>地方主官平均任期三四年，而基建项目一般都要两三年完成</li><li>故而刚上任会上马大量这种项目</li><li>各地的投资 政治投资周期比较频繁<ul><li>新官上任，土地出让数量会增加</li><li>新增的土地供应大多位于城市周边郊区</li><li>摊大饼的态势</li></ul></li></ul></li><li>偏重投资的增长模式带来的问题<ul><li>政府债务的不断攀升</li><li>重视看得见的东西，忽略看不见的</li></ul></li><li>2019考核官员标准发生变化<ul><li>看全面工作，看推动本地区经济建设，政治建设，文化建设，社会建设，生态文明建设</li><li>解决发展不平衡不充分的问题</li><li>满足人民日益增长的美好生活需要的情况和实际成效</li></ul></li></ul><h1 id="5-工业化中的政府角色"><a href="#5-工业化中的政府角色" class="headerlink" title="5. 工业化中的政府角色"></a>5. 工业化中的政府角色</h1><ul><li><p>政府与具体的工业企业的合作</p><ul><li>京东方<ul><li>与其说是一家公司的奋斗史，不如说是液晶屏产业在中国的奋斗史</li></ul></li></ul></li><li><p>现代经济的规模经济效应非常强，新企业的进入门槛非常高，不仅投资额度大，还要面对先进入者已经积累起来的巨大成本和技术的优势</p></li><li><p>东亚经济奇迹，一个很重要的特点</p><ul><li>政府帮助本土企业进入复杂度很高的行业</li><li>充分利用其中的学习效应，规模效应以及技术外溢效应</li><li>迅速提升本土制造业的技术能力和国际竞争力</li></ul></li><li><p>新兴制造业在地理上的集聚效应是很强的，因为扎堆生产可以节约原材料和中间投入的运输成本</p><ul><li>且同行聚集在一起有利于知识和技术交流，外溢效应很强</li><li>因此产业集群一旦形成，自身引力就会不断增强，很难被外力打破</li></ul></li><li><p>光伏产业</p><ul><li>新技术开始成本会非常高，其实都是靠政府补贴赚钱</li><li>政府通过补贴额度的降低来引导各个企业的发展</li></ul></li><li><p>政府产业引导基金与私募基金</p><ul><li>LP limited partner  给钱的</li><li>GP general partner 投资的</li><li>可以投一级市场以及二级市场</li><li>国内的最大一类LP就是政府引导基金</li></ul></li><li><p>产业引导基金的特点</p><ul><li>大多产业引导基金不直接投资企业，而是做LP，将钱交给市场化的私募基金的GP去投资企业<ul><li>因为一支私募不仅有政府资金，还会有很多社会资本，这样可以做产业引导</li></ul></li><li>政府引导基金本身就是一只基金，投资对象是各种私募基金，被称为母基金  funds of funds</li><li>借助市场力量去使用财政资金</li><li>大多数引导基金的最终投向都是战略新兴产业，比如芯片，新能源汽车，而不允许基础设施和房地产</li></ul></li><li><p>设置产业引导基金之后，也需要专门的公司来运营和管理这只基金</p><ul><li>运作模式分类<ul><li>政府独资公司</li><li>混合所有制公司</li><li>政府将钱委托给市场化的母基金管理人去运营</li></ul></li></ul></li><li><p>政府引导基金发展的外部条件</p><ul><li>制度条件<ul><li>制度和政策的指引</li><li>提供政策基础：设立引导基金发挥财政资金的杠杆放大效应，增加创业投资资本的供给，克服单纯通过市场配置创业投资资本的市场失灵问题</li></ul></li><li>产业条件<ul><li>经济需要发展到一定的水平，使得高技术，高风险的战略新兴行业得以出现</li></ul></li><li>资本市场成熟程度<ul><li>GP ，足够大的股权交易市场，退出渠道</li></ul></li></ul></li></ul><h1 id="6-地方政府推动经济发展的模式与特点"><a href="#6-地方政府推动经济发展的模式与特点" class="headerlink" title="6. 地方政府推动经济发展的模式与特点"></a>6. 地方政府推动经济发展的模式与特点</h1><h2 id="6-1-重土地，轻人"><a href="#6-1-重土地，轻人" class="headerlink" title="6.1 重土地，轻人"></a>6.1 重土地，轻人</h2><ul><li>优点<ul><li>可以快速推进城市化和基础设施建设</li></ul></li><li>缺点<ul><li>公共服务供给不足，提高了放假和居民债务负担</li><li>拉大了地区差距和贫富差距</li></ul></li></ul><h2 id="6-2-重规模，重扩张"><a href="#6-2-重规模，重扩张" class="headerlink" title="6.2 重规模，重扩张"></a>6.2 重规模，重扩张</h2><ul><li>优点<ul><li>推动了企业成长和快速工业化</li></ul></li><li>缺点<ul><li>加重了债务负担</li><li>企业，地方政府，居民三部门债务互相作用，加大了经济整体的债务和金融风险</li></ul></li></ul><h2 id="6-3-重投资，重生产，轻消费"><a href="#6-3-重投资，重生产，轻消费" class="headerlink" title="6.3 重投资，重生产，轻消费"></a>6.3 重投资，重生产，轻消费</h2><ul><li>优点<ul><li>拉动了经济的快速增长，扩大了对外贸易</li><li>使得我国迅速成为了制造业强国</li></ul></li><li>缺点<ul><li>经济结构的不平衡</li><li>对内<ul><li>资源向企业部门转移，居民收入和消费占比偏低，不利于经济长期稳定发展</li></ul></li><li>对外<ul><li>国内无法消纳的产能向国外输出，加剧了贸易冲突</li></ul></li></ul></li></ul><h1 id="7-城市化与不平衡"><a href="#7-城市化与不平衡" class="headerlink" title="7. 城市化与不平衡"></a>7. 城市化与不平衡</h1><aside>💡 中国特有的城市土地国有制度，为政府垄断土地一级市场创造了条件，将这笔隐匿的财富变成了启动城市化的巨大资本，但也让地方财源高度依赖土地价值，依赖房地产和房价。房价连着地价，地价连着财政，财政连着基础设施投资，于是经济增长，地方财政，银行，房地产之间形成了“一荣俱荣，一损俱损”的复杂关系。</aside><h2 id="7-1-房价与居民债务"><a href="#7-1-房价与居民债务" class="headerlink" title="7.1 房价与居民债务"></a>7.1 房价与居民债务</h2><ul><li><p>1994年分税制改革是很多重大经济现象的分水岭</p></li><li><p>地区房价差异的主要原因是供需失衡</p><ul><li>500万人和1000万人以上的大城市城区人口增量占全国城区人口增量的近四成，但居住用地增量才占亮程，房价于是就快速上涨了</li></ul></li><li><p>中国对建设用地指标实行严格的管理，每年的新增指标由中央分配到省，再由省分配到地方。</p><ul><li>这些指标无法跨省交易</li><li>但是这些土地倾斜政策无法改变人口流向，人还是不断向东部沿海和大城市聚集</li></ul></li><li><p>房地产</p><ul><li>被称为经济周期之母</li><li>根源在于其内在的供需矛盾<ul><li>银行可以通过按揭创造几乎无限的新购买力</li><li>不可再生的城市土地供给是有限的</li></ul></li><li>这种矛盾经常会导致资产泡沫和破裂的周期循环，是金融和房地产不稳定的核心矛盾</li></ul></li><li><p>中国情况</p><ul><li>中国城镇居民的主要财产是房子，房产占据家庭资产的将近七成<ul><li>六成是住房，一成是商铺</li><li>而美国72%是金融资产，28%是房产</li><li>即中美两国财富的压舱石不一样，也就能看出为什么中国看中房市，而美国看中股市了</li></ul></li><li>房价上涨的影响<ul><li>增加按揭债务负担</li><li>拉大贫富差距</li><li>刺激低收入人群举债消费</li></ul></li><li>书里对于房产，当前情况的描述我认为是浅尝辄止的，只稍微说了下，发出来不容易~</li></ul></li></ul><h2 id="7-2-不平衡与要素市场改革"><a href="#7-2-不平衡与要素市场改革" class="headerlink" title="7.2 不平衡与要素市场改革"></a>7.2 不平衡与要素市场改革</h2><ul><li>人口流动和收入平衡<ul><li>我国人口流动依旧受限<ul><li>重土地轻人，导致民生支出不足，不利于外来人口在城市中真正安家落户</li></ul></li></ul></li><li>要平衡的是人均差距，而不是地区差距</li><li>土地流转<ul><li>当前情况是大城市不仅土地面积有限，而且有对建设用地指标的管制</li><li>如果用地指标能够跟着人口流动，会很大程度环节土地供需矛盾的问题</li><li>还需要让农村集体用地参与流转 — 农村土地归集体所有</li></ul></li><li>改革的目标是为了让各种资源按照常住人口的规模进行配置<ul><li>里面涉及到<ul><li>建立健全城乡统一的建设用地市场</li><li>深化户籍制度的改革</li><li>以经常居住地登记户口制度</li><li>建立城镇教育，就业创业，医疗卫生等基本公共服务和常住人口挂钩的机制</li></ul></li></ul></li></ul><h2 id="7-3-经济发展与贫富差距"><a href="#7-3-经济发展与贫富差距" class="headerlink" title="7.3 经济发展与贫富差距"></a>7.3 经济发展与贫富差距</h2><ul><li>降低了全球的不平等<ul><li>改变了全球收入分布的格局</li></ul></li><li>大家都在变富<ul><li>2019年收入最高的20%人群占有全部收入的48%，而收入最低的20%只占有全部收入的8%</li><li>另外一个视角是30年间，实际收入涨了8 - 13倍 平均下来</li></ul></li><li>经济增长的过程<ul><li>尽管不一定减少收入差距，但是可以一定程度上遏制贫富差距在代际之间传递</li><li>当前情况是财产性收入占收入比重越来越大，人力资本无法在代际之间传承的，但是房产等有形资产是可以的</li><li>经济增速下滑，社会对不平等的容忍程度就会下降了，不安定因素会增加</li></ul></li><li>对收入差距的容忍度<ul><li>隧道效应<ul><li>一条在动，反向不动的时候的极度焦躁</li></ul></li><li>经济形势不好，富人动的慢，穷人可能完全无法有财富增长了</li></ul></li></ul><h1 id="8-债务与风险"><a href="#8-债务与风险" class="headerlink" title="8. 债务与风险"></a>8. 债务与风险</h1><ul><li>债务本身的风险<ul><li>人们在乐观的时候往往会低估负债的风险，过多借债。当风险出现的时候，又会因为债务负担沉重而缺乏腾挪的空间，没有办法应对。</li></ul></li></ul><h2 id="8-1-债务与经济衰退"><a href="#8-1-债务与经济衰退" class="headerlink" title="8.1 债务与经济衰退"></a>8.1 债务与经济衰退</h2><ul><li>负债率搞得经济中，资产价格的下跌往往非常迅猛。若债务太重，收入不够还本，甚至不够还息，就只能变卖资产，抛售的人多了，资产价格就跳水了</li><li>资产价格的下跌会引起信贷收缩，导致资金链的断裂</li><li>经济衰退会加剧不平等<ul><li>因为债务危机对穷人和富人的打击高度不对称</li><li>法律优先保护债权人的索赔权，即欠债的无论是公司还是个人，即使破产也要清算偿债</li></ul></li></ul><h2 id="8-2-债台为何高筑"><a href="#8-2-债台为何高筑" class="headerlink" title="8.2 债台为何高筑"></a>8.2 债台为何高筑</h2><ul><li>资金供给和银行管制<ul><li>资金供给的增加源于金融管制的放松</li><li>银行越做越大，创造的信贷越来越多</li><li>金融创新和衍生品不断增加，导致金融部门的规模和风险也在增加</li></ul></li><li>全球银行的危机的频率和国际资本流动规模高度相关，金融危机基本都伴随着银行的危机<ul><li>银行规模达，杠杆高</li><li>银行借来短期的钱，但是借出去的钱却大都是长期的<ul><li>这种负债和资产的期限不匹配会带来流动性风险</li></ul></li><li>银行信贷大都与房地产有关，常常和土地，以及房产价值一同起落，放大经济波动</li><li>银行风险会传导给其他金融部门<ul><li>比如银行将各种按揭贷款打包成证券组合，卖给其他金融机构</li></ul></li></ul></li><li>脱实向虚，实际上一直是富人的钱 贷给了穷人们</li><li>为什么投资变少了？<ul><li>制造业外迁 — 制造业本身是重资产，重投资的行业</li><li>当今技术提升，需要运用大量软件与服务，而服务业更依赖于人的集聚，也推升了对特定地段的住房和社交空间的需求</li></ul></li></ul><aside>💡 在缺乏增长点的情况下，央行给银行体系提供流动性，但商业银行资金贷不出去，容易流向资产市场。放松货币条件总体上有利于资产持有者，超宽松的货币政策可能加剧财富分化，固化结构扭曲，使危机调整的过程过长。</aside><h2 id="8-3-中国的债务与风险"><a href="#8-3-中国的债务与风险" class="headerlink" title="8.3 中国的债务与风险"></a>8.3 中国的债务与风险</h2><ul><li>始于2008年的财政刺激计划<ul><li>中央放松了对地方融资平台的限制，同时不断降准降息，放宽银行信贷</li><li>这些资金找到了基建和房地产两大载体</li></ul></li><li>2011 因为经济出现过热现象，货币政策收紧</li><li>但是2011年中，欧债危机爆发，国内制造业陷入困境</li><li>2012年又开始降准降息</li><li>2015年股灾<ul><li>美国退出量化宽松，人民币汇率迅速贬值</li><li>中央放松了对房地产的调控</li><li>全国棚户区改造从实物安置转变为货币化安置，带动房价进一步上涨</li></ul></li><li>我国企业债务负担非常重，应对风险能力很有限</li><li>影子银行<ul><li>类似银行的信贷业务，但是不在银行的资产负债表当中，不受到银行监管规则的约束</li><li>比如银行卖给老百姓一个理财产品，利息5%</li><li>然后将筹来的钱委托给信托公司</li><li>信托公司将钱借给公司</li><li>这样做的话，发行的理财产品不算银行储蓄，委托给信托公司的投资不算银行贷款，所以就绕开了银行的监管</li></ul></li></ul><h2 id="8-4-化解债务风险"><a href="#8-4-化解债务风险" class="headerlink" title="8.4 化解债务风险"></a>8.4 化解债务风险</h2><ul><li>解决方案<ul><li>偿还已有债务<ul><li>可能会陷入到经济衰退当中，因为都要勒紧裤腰带过日子</li><li>你的支出就是我的收入了呀</li><li>增发货币<ul><li>增发货币来降低利率<ul><li>可以刺激消费和投资，提振经济</li><li>计算实际收入不增加，随着物价上涨和时间推移，债务负担也会减轻</li></ul></li><li>量化宽松<ul><li>央行增发货币来买入各类资产，将货币注入经济</li><li>托住资产价格，为经济注入流动性</li><li>问题是难以将增发的货币发到穷人手里，因此难以刺激消费支出，还会拉大贫富差距</li><li>购买各种金融资产受益的是各种资产的所有者</li></ul></li><li>债务货币化<ul><li>政府加大财政支出去刺激经济</li><li>由财政部发债融资，央行直接印钱买过来，无需其他金融机构参与也无需支付利息</li></ul></li></ul></li></ul></li><li>遏制新增债务，改革滋生债务的政治经济环境<ul><li>核心点<ul><li>限制房价上涨</li><li>限制土地财政和土地金融</li><li>限制政府担保和国有企业过度借贷</li><li>资本市场改革<ul><li>改变以银行贷款为主的间接融资体系，扩展直接融资渠道<ul><li>能够降低债务负担</li><li>也能提高资金的使用效率</li><li>股权的约束力要更强<ul><li>风险共担</li><li>股权可以转让</li></ul></li></ul></li><li>改革很慢的原因其实是我国金融资产中72%的风险是由金融机构和政府承担的</li></ul></li></ul></li><li>债务问题<ul><li>以出口和投资驱动的经济体系的产物</li></ul></li></ul></li></ul></li></ul><h1 id="9-国内国际的失衡"><a href="#9-国内国际的失衡" class="headerlink" title="9. 国内国际的失衡"></a>9. 国内国际的失衡</h1><ul><li>中国成为制造业大国背后的两个问题<ul><li>内部经济结构失衡<ul><li>重生产，重投资，相对轻民生，轻消费</li><li>导致和巨大的产能对比，国内消费不足，消化不了的产品只能对外输出</li></ul></li><li>国外需求的不稳定和贸易冲突<ul><li>实际制造业中 中国占比从5% 到28%，而七国集团从72%到37%，其他国家基本没有变化</li></ul></li></ul></li></ul><h2 id="9-1-低消费和产能过剩"><a href="#9-1-低消费和产能过剩" class="headerlink" title="9.1 低消费和产能过剩"></a>9.1 低消费和产能过剩</h2><ul><li><p>面临消费不足的问题</p><ul><li>我国居民最终消费占GDP只有44%，美国有70%，欧盟和日本也有55%</li><li>GDP中可供老百姓支配的收入份额下降了，或者老百姓把更大一部分收入存了起来，储蓄类上升了— 我国这两种情况都出现了</li><li>孩子数量减少以后，养儿防老功效降低，父母开始增加储蓄来养老了</li></ul></li><li><p>当前的靠政府 靠投资拉动经济发展会存在几个方面的问题</p><ul><li>基础设施和工业体系已经完善，投资难度加大<ul><li>因此投资决策和调配资源的体制需要改变</li><li>地方政府主导投资的局面需要改变</li></ul></li><li>由于老百姓收入和消费都不足，无法消化投资形成的产能，很多投资不能变成有效的收入，所以债务负担越来越重，带来了一系列的风险</li><li>劳动收入份额下降，资本收入份额的上升，会扩大贫富差距</li><li>由于消费不足和投资过剩，过剩产能会向国外输出，因为我国体量很大，输出产能会家中全球贸易失衡，引发贸易冲突</li></ul></li><li><p>十九大提出了</p><ul><li>提高就业质量和人民收入水平<ul><li>破除妨碍劳动力，人才社会性流动的体制机制弊端</li><li>完善政府，工会，企业共同参与的协商协调机制</li><li>坚持按劳分配，完善按要素分配的体制机制</li><li>扩大中等收入群体，增加低收入者收入，调节过高收入，取缔非法收入</li><li>要努力使居民收入增长快于经济增长</li></ul></li></ul></li><li><p>出口占比高这种经济结构比较脆弱</p><ul><li>外国收到政治经济的影响很大，难以掌控</li></ul></li></ul><h2 id="9-2-中美贸易冲突"><a href="#9-2-中美贸易冲突" class="headerlink" title="9.2 中美贸易冲突"></a>9.2 中美贸易冲突</h2><ul><li><p>反全球化，民粹主义，</p></li><li><p>中美之间的技术冲击，技术竞争才是真正的博弈</p></li><li><p>对于一个站在科技前沿的国家来说，新技术的发明和应用一般从科学研究和实验室开始，再到技术应用和专利阶段，然后再到大规模的工业量产</p></li><li><p>但是对于一个后起的发展中国家来说，很多时候顺序是反的</p><ul><li>先从制造环节开始，边干边学，积累技术和经验</li><li>根据自身需要改进技术，创造一些专利</li><li>产品销量逐步扩大，技术逐步向前沿靠拢以后，拿出更多资源投入研发，推进更为基础，应用范围更广的科研项目</li></ul></li></ul><h2 id="9-3-再平衡与国内大循环"><a href="#9-3-再平衡与国内大循环" class="headerlink" title="9.3 再平衡与国内大循环"></a>9.3 再平衡与国内大循环</h2><ul><li>加快构建以国内大循环为主体，国内国际双循环相互促进的新发展格局<ul><li>需要提高居民的收入和消费</li><li>需要靠服务业的大力发展了，而这些只能发展在人口密度非常高的城市当中</li><li>需要户籍制度，还有土地制度的改革</li><li>需要把更多的西苑从政府和企业手中转移出来，分配给居民</li><li>需要改变地方政府在经济中扮演的角色，遏制其投资冲动，降低其生产性支出，加大民生支出</li></ul></li></ul><h1 id="10-政府与经济发展的总结"><a href="#10-政府与经济发展的总结" class="headerlink" title="10. 政府与经济发展的总结"></a>10. 政府与经济发展的总结</h1><ul><li>一套严格的概念框架无疑有助于厘清问题，但也经常让人错把问题当成答案</li><li>需要将竞争机制引入政府<ul><li>以中央政府为主，部委为单位，</li></ul></li><li>瓦格纳法则<ul><li>国家越富裕，政府在国民经济中所占的比重也往往会越大，而不是越小</li><li>因为随着愈发富裕，民众对政府服务的需求会越来越多，政府在公立教育，医疗，退休金，失业保险等方面的支出也会随之增加</li></ul></li><li>经济发展的核心是提高生产率<ul><li>对于处于技术前沿的发达国家来说，提高生产率的关键是不断探索和创新</li><li>但是对于发展中国家来说，提高生产率的关键不是探索未知和创新，而是学习已知的技术和管理模式，将更多的资源尽快组织和投入到学习过程当中，来提高学习效率</li><li>后进国家虽然有模仿和学习先进国家技术的后发优势，但是其组织学习模式不可能一直持续下去</li><li>当技术和生产率提高到一定的水平以后，需要成功转型成探索创新模式。</li></ul></li><li>城市化 — 是个农民转化为工人的过程</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-整体框架&quot;&gt;&lt;a href=&quot;#1-整体框架&quot; class=&quot;headerlink&quot; title=&quot;1. 整体框架&quot;&gt;&lt;/a&gt;1. 整体框架&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;微观机制&lt;ul&gt;
&lt;li&gt;地方政府的权力和事务&lt;/li&gt;
&lt;li&gt;财税与政府行为&lt;/li&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>人月神话笔记</title>
    <link href="https://www.llchen60.com/%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D%E7%AC%94%E8%AE%B0/"/>
    <id>https://www.llchen60.com/%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D%E7%AC%94%E8%AE%B0/</id>
    <published>2022-01-31T06:34:42.000Z</published>
    <updated>2022-01-31T06:35:57.437Z</updated>
    
    <content type="html"><![CDATA[<ul><li>软件活动的根本任务<ul><li>打造构成抽象软件实体的复杂概念结构</li></ul></li><li>软件活动的次要任务1<ul><li>使用编程语言来表达这些抽象实体，在空间和时间的限制下将他们映射成机器语言</li></ul></li></ul><blockquote><p>一个相互牵制的概念结构是软件实体必不可少的部分，它包括：数据集合，数据条码之间的关系，算法以及功能调用等。这些要素本身是抽象的，体现在不同的表现形式下的概念构造是相同的。</p></blockquote><ul><li>软件系统无法规避的内在特性<ul><li>复杂度<ul><li>元素之间的交互，使得软件的复杂度比非线性增长要高得多</li><li>软件的复杂度是根本属性，抽掉复杂度的软件实体描述实际上也去掉了一些本质属性。所以我们无法做太多的简化的</li></ul></li><li>一致性<ul><li>软件工程当中没有太多的一致性，更多的核心开发工程师本身的理念</li></ul></li><li>可变性<ul><li>软件中的功能，功能属性很容易感受到变更的压力</li></ul></li><li>不可见性<ul><li>软件是不可见以及无法可视化的<ul><li>例如几何抽象是强大的工具，包括机械制图，以及化学分子模型，但是软体的客观存在不具有空间的形体特征</li></ul></li></ul></li></ul></li></ul><ul><li><p>编程系统产品(Programming Systems Product) 开发的工作量是供个人使用的，独立开发的构件程序的9倍。估计软件构件产品化引起了3倍工作量，将软件构件整合成完整系统所需要的设计、集成和测试又加强了3倍的工作量，这些高成本的构件在根本上是相互独立的</p></li><li><p>编程行业满足了我们内心深处的创造渴望和愉悦所有人的共有情感，提供了5种乐趣</p><ul><li>创建事物的快乐</li><li>开发对其他人有用的东西的乐趣</li><li>将可以活动，相互齿合的零部件组装成类似迷宫的东西，这个过程体现出的令人神魂颠倒的美丽</li><li>面对不重复的任务，不断学习的乐趣</li><li>工作在如此易于驾驭的介质上的乐趣—— 纯粹的思维活动 —— 其存在，移动和运转方式完全不同于实际物体</li></ul></li><li><p>同样，这个行业有一些内在固有的烦恼</p><ul><li>将做事方式调整到追求完美是学习编程的最困难部分</li><li>权威不等同于责任；真正的权威来自于每次任务的完成</li><li>任何创造性活动都伴随着枯燥艰苦的劳动，编程也不例外</li><li>人们通常希望项目在接近结束的时候，软件项目能收敛得快一些，然而，情况却是越接近完成，收敛得越慢</li><li>产品在完成前总面临着陈旧过时的威胁，只有实际需要的时候，才会用到最新的设想</li></ul></li><li><p>缺乏合理的时间进度是造成项目滞后的最主要原因，它比其他所有因素的总和影响还大</p></li><li><p>所有编程人员都是乐观主义者，一切都将运作良好</p></li><li><p>由于编程人员通过纯粹的思维活动来开发，我们期待在实现过程当中不会碰到困难。但是我们本身的构思是会有缺陷的，因此总会有bug</p></li><li><p>围绕着成本核算的估计技术，混淆了工作量和项目进展。人月是危险的，因为它暗示着人员数量和时间是可以相互替换的。</p></li><li><p>在若干人员中分解任务会引发额外的沟通工作量 — 培训和相互沟通。</p></li><li><p>Brooks法则： 为进度落后的项目增加人手，只会使得进度更加落后</p><ul><li>增派人手可能增加的工作量<ul><li>任务重新分配本身和所造成的工作中断</li><li>培训新人员</li><li>额外的相互沟通</li></ul></li></ul></li><li><p>同样有两年经验而且在受到同样培训的情况下，优秀的专业程序员的生产力是较差的程序员的10倍。</p></li><li><p>一位首席程序员，类似于外科手术队伍的团队架构提供了一种方法—— 既能获得由少数头脑产生的产品完整性，又能够得到多位协助人员的总体生产率，还彻底减少了沟通的工作量。</p></li><li><p>概念完整性是系统设计中最重要的考虑因素</p><ul><li>为了获得概念的完整性，设计必须由一个人或者具有共识的小型团队来完成</li><li>为了获得概念的完整性，就必须有人控制这些概念，这实际上是一种无需任何歉意的贵族专制统治</li></ul></li><li><p>功能和理解上的复杂程度的比值才是系统设计的最终测试标准，而不仅仅是丰富的功能</p></li><li><p>尽早交流和持续沟通能够使得结构师有较好的成本意识，使开发人员获得对设计的信心，并且不会混淆各自的责任分工</p></li><li><p>交流</p><ul><li>巴比伦项目的失败是因为缺乏交流以及交流的结果 —— 组织</li><li>因为左手不知道右手在做什么，从而进度灾难，功能的不合理，以及系统的缺陷纷纷出现。由于存在对其他人的各种假设，团队成员之间的理解开始出现偏差了。</li></ul></li><li><p>项目工作手册</p><ul><li>项目工作手册是对项目必须产生的一些列文档进行组织的一种结构</li><li>每一个团队成员应该了解所有的材料</li><li>实时更新很重要</li><li>工作手册的使用者应该将注意力集中在上次阅读之后的变更以及关于这些变更重要性的评述上</li><li>每个部分应该被封装，从而没有人需要或者被允许看到其他部分的内部结构，只需要了解接口</li></ul></li><li><p>组织架构</p><ul><li>团队组织的目标是为了减少必要的交流和协作量</li><li>为了减少交流，组织结构包括了人力划分(Division of labor) 和限定职责范围(Specialization of function)</li><li>组织内的交流是网状的，而不是树状结构，因此所有的特殊组织机制都是为了进行调整，来克服树状组织结构中交流缺乏的困难</li></ul></li><li><p>在大型团队当中，各个小组倾向于不断的去做局部优化，来满足自己的目标，而比较少的考虑对用户的整体影响。这种方向性的问题是大型项目的主要危险</p></li><li><p>从系统整体出发和面向用户的态度是软体编程管理人员最重要的职能</p></li><li><p>文档的规范，目标，用户手册，内部文档，进度，预算，组织机构图，和工作空间分配</p><ul><li>每个文档本身就可以作为检查列表或者数据库</li></ul></li><li><p>项目经理的基本职责是使每个人都向着相同的方向前进</p></li><li><p>项目经理的主要日常工作是沟通，而不是做出决定；文档使得各项计划和决策在整个团队范围内得到交流</p></li><li><p>用户的实际需要和用户感觉会随着程序构建，测试和使用而发生变化。</p></li><li><p>对于文档，需要采用定义良好的数字化版本将变更量子化</p></li><li><p>程序员不愿意为设计书写文档，不仅仅是因为惰性，更多的是源于设计人员的踌躇 —— 要为自己尝试性的设计决策进行辩解。</p></li><li><p>只要管理人员和技术人员的天赋允许，老板必须对他们的能力培养给予极大的关注，使得管理人员和技术人员具有互换性；特别是希望在技术和管理角色之间自由的分配人手的时候</p></li><li><p>具有两条晋升线的高效组织机构存在着一些社会性的障碍，人们必须警惕并积极的同它做持续的斗争</p></li><li><p>程序维护基本上不同于硬件的维护：主要由各种变更组成，入修复设计缺陷，新增功能，或者是使用环境或者配置变换引起的调整</p></li><li><p>对于一个广泛使用的程序，其维护总成本通常是开发成本的40%或者更多</p></li><li><p>Campbell指出了一个显示产品生命期中每月bug数的有趣曲线，其先是下降，后面是上升</p></li><li><p>每次修复之后，必须重新运行先前所有的测试用例，确保系统不会以更隐蔽的方式被破坏</p></li><li><p>所有的修改都倾向于破坏系统的架构，增加了系统的混乱程度(熵)。即使是最熟练的软件维护工作，也只是延缓了系统退化到不可修复的混乱状态的进程，以致必须重新进行设计。</p></li><li><p>项目经理应该制定一套策略，并为通用工具的开发分配资源，与此同时，还必须意识到专业工具的需求</p></li><li><p>调试是系统编程中较慢和较困难的部分，而漫长的调试周转时间是调试的祸根</p></li><li><p>在编写任何代码之前，规格说明必须提交给外部的测试人员，来详细的检查说明的完整性和明确性。开发人员自己无法完成这项工作。</p></li><li><p>开发大量的辅助测试平台和测试代码是很值得的，代码量甚至可能有测试对象的一半</p></li><li><p>项目是怎么样被延迟了整整一年的时间的….. 一次一天。一次一天的进度落后比重大灾难更难以识别，更不容易防范和更加难以弥补。</p></li><li><p>根据一个严格的进度表来控制大型项目的第一个步骤是制定进度表，进度表由里程碑和日期组成</p><ul><li>里程碑必须是具体的，特定的，可度量的事件，需要能够进行清晰的定义</li></ul></li><li><p>慢性进度偏离是士气杀手。</p></li><li><p>状态的获取是困难的，因为下属经理有充分的理由不提供信息共享</p></li><li><p>老板的不良反应肯定会对信息的完全公开造成压制；相反，仔细区分状态报告，毫无惊慌地接收报告，决不越俎代庖，将能够鼓励诚实的汇报。</p></li><li><p>必须有评审机制，使得所有成员可以通过它了解真正的状态。出于这个目的，里程碑的进度和完成文档是关键。</p></li><li><p>程序修改人员所使用的文档中，除了描述事情如何，还应当阐述它为什么那样。对于加深理解，目的是非常关键的，即使是高级语言的语法，也不能表达目的</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;软件活动的根本任务&lt;ul&gt;
&lt;li&gt;打造构成抽象软件实体的复杂概念结构&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;软件活动的次要任务1&lt;ul&gt;
&lt;li&gt;使用编程语言来表达这些抽象实体，在空间和时间的限制下将他们映射成机器语言&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="人月神话" scheme="https://www.llchen60.com/tags/%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D/"/>
    
      <category term="Project Management" scheme="https://www.llchen60.com/tags/Project-Management/"/>
    
  </entry>
  
  <entry>
    <title>Real time data’s unifying abstraction</title>
    <link href="https://www.llchen60.com/Real-time-data%E2%80%99s-unifying-abstraction/"/>
    <id>https://www.llchen60.com/Real-time-data%E2%80%99s-unifying-abstraction/</id>
    <published>2022-01-29T02:14:20.000Z</published>
    <updated>2022-01-29T02:20:03.851Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Real-time-data’s-unifying-abstraction"><a href="#Real-time-data’s-unifying-abstraction" class="headerlink" title="Real time data’s unifying abstraction"></a>Real time data’s unifying abstraction</h1><h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>Logs play a key role in distributed data systems and real time application architectures.<ul><li>write ahead log</li><li>commit log</li><li>transaction logs</li></ul></li></ul><h1 id="2-What-is-a-log"><a href="#2-What-is-a-log" class="headerlink" title="2. What is a log?"></a>2. What is a log?</h1><h2 id="2-1-Some-Concepts"><a href="#2-1-Some-Concepts" class="headerlink" title="2.1 Some Concepts"></a>2.1 Some Concepts</h2><ul><li>Storage abstraction<ul><li>append only</li><li>totally ordered sequence of records ordered by time</li></ul></li><li>Records<ul><li>appended to the end of the log</li><li>read from left to right</li><li>each entry has a unique sequential log entry number</li></ul></li><li>Log<ul><li>record what happened and when</li><li>for distributed system, that’s the very heart of the problem</li><li>types<ul><li>text logs<ul><li>meant primarily for humans to read</li></ul></li><li>journal/ data logs<ul><li>built for programmatic access</li></ul></li></ul></li></ul></li></ul><h2 id="2-2-Log-in-different-scenario"><a href="#2-2-Log-in-different-scenario" class="headerlink" title="2.2 Log in different scenario"></a>2.2 Log in different scenario</h2><h3 id="2-2-1-Logs-in-DB"><a href="#2-2-1-Logs-in-DB" class="headerlink" title="2.2.1 Logs in DB"></a>2.2.1 Logs in DB</h3><ul><li>Function 1: Authoritative source for restoring data<ul><li>DB need to keep in sync the variety of data structures and indexes in the presense of crashes</li><li>To make this atomic and durable, a db uses a log to <strong>write out information about the records</strong> they will be modifying, <strong>before applying the changes</strong> to all the various data structures it maintains</li><li>Since the log is <strong>immediately persisted</strong> it is used as the <strong>authoritative source</strong> in restoring all other persistent structures in the event of a crash.</li></ul></li><li>Function 2: Replicating data between DBs<ul><li>Oracle, MySQL and Postgres SQL include <strong>log shipping protocols</strong> to <strong>transmit portions of log to replica databases</strong> which act as slaves</li></ul></li></ul><h3 id="2-2-2-Logs-in-distributed-systems"><a href="#2-2-2-Logs-in-distributed-systems" class="headerlink" title="2.2.2 Logs in distributed systems"></a>2.2.2 Logs in distributed systems</h3><ul><li>Log Centric Approach</li></ul><aside>💡 State Machine Replication Principle: If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.</aside><ul><li>we could reduce the problem of making multiple machines all do the same thing to the problem of <strong>implementing a distributed consistent log to feed these processes input</strong><ul><li>squeeze all the non-determinism out of the input stream to ensure that each replica processing this input stays in sync.</li><li>time stamps that index the log now act as <strong>the clock for the state of the replicas</strong>—you can describe each replica by a single number, the timestamp for the maximum log entry it has processed.</li></ul></li></ul><h1 id="3-Log-Types"><a href="#3-Log-Types" class="headerlink" title="3. Log Types"></a>3. Log Types</h1><ul><li>What we could put in log<ul><li>log the incoming requests to a service</li><li>the state changes the service undergoes in response to request</li><li>the transformation commands it executes.</li></ul></li><li>For DB usage<ul><li>physical logging<ul><li>log the contents of each row that is changed</li></ul></li><li>logical logging<ul><li>log not the changed rows but the SQL commands that lead to the row chagnes</li></ul></li></ul></li><li>For Distributed Systems to process and replicate<ul><li>Primary Backup<ul><li>elect one replica as the leader</li><li>allow this leader to process requests in the order they arrive</li><li>log out the changes to its state from processing the requests.</li><li>The other replicas apply in order the state changes the leader makes so that they will be in sync and ready to take over as leader should the leader fail.</li><li><strong>backup will copy the result from the primary, no logical action to walk through all action primary did</strong></li></ul></li></ul></li></ul><pre><code>- State Machine Replication    - active-active model where we keep a log of the incoming requests and each replica processes each request    - **each machine will do real execution, do the logical stuff**    ![State Machine Replication](https://s2.loli.net/2022/01/29/6JtLFNpmXBYW9wO.png)</code></pre><aside>💡 Below 3 sections has a centric idea:  Log as a stand-alone service. The usefulness of the log comes from simple function that the log provides: **producing a persistent, re-playable record of history**. At the core of these problems is the ability to **have many machines playback history at their own rate in a deterministic manner**</aside><h1 id="4-Changelog-in-Database"><a href="#4-Changelog-in-Database" class="headerlink" title="4. Changelog in Database"></a>4. Changelog in Database</h1><ul><li>duality between a log of changes and a table<ul><li>The log is similar to the list of all credits and debits and bank processes;</li><li>a table is all the current account balances.</li><li>If you have a log of changes, you can apply these changes in order to create the table capturing the current state.</li></ul></li><li>if you have a table taking updates, you can record these changes and publish a “changelog” of all the updates to the state of the table. This changelog is exactly what you need to <strong>support near-real-time replicas</strong>.</li><li>Table support data at rest and logs capture changes</li></ul><aside>💡 The magic of the log is that if it’s complete log of changes, it holds not only the contents of the final version of the table, but also allows recreating all other versions that might have existed. That’s a backup of every previous state of the table</aside><h1 id="5-Data-Integration"><a href="#5-Data-Integration" class="headerlink" title="5. Data Integration"></a>5. Data Integration</h1><blockquote><p>Make all of an organization’s data easily available in all its storage and processing systems</p></blockquote><h2 id="5-1-Expected-workflow-for-data-integration"><a href="#5-1-Expected-workflow-for-data-integration" class="headerlink" title="5.1 Expected workflow for data integration"></a>5.1 Expected workflow for data integration</h2><ul><li>Definition in author’s scope: Making all the data an organization has available in all its services and systems.</li><li>Effective Use of Data<ul><li>Capture all relevant data</li><li>Put it together in an applicable processing env<ul><li>real time query system</li><li>text files</li><li>python scripts, etc.</li></ul></li><li>Infra to process data<ul><li>mapReduce</li><li>Real time query systems</li></ul></li><li>Good data models and consistent well understood semantics</li><li>Sophisticated processing<ul><li>visualization</li><li>reporting</li><li>algorithmic processing and prediction</li></ul></li></ul></li></ul><h2 id="5-2-Problem1-The-event-data-firehose"><a href="#5-2-Problem1-The-event-data-firehose" class="headerlink" title="5.2 Problem1: The event data firehose"></a>5.2 Problem1: The event data firehose</h2><ul><li>Event data rising</li><li>google’s fortune is actually generated by a relevance pipeline built on clicks and impressions — events</li><li>that would be huge amount of data,</li></ul><h2 id="5-3-Problem-2-The-explosion-of-specialized-data-systems"><a href="#5-3-Problem-2-The-explosion-of-specialized-data-systems" class="headerlink" title="5.3 Problem 2: The explosion of specialized data systems"></a>5.3 Problem 2: The explosion of specialized data systems</h2><ul><li>Explosion of specialized data systems</li><li>The combination of more data of more varieties and a desire to get this data into more systems leads to a huge data integration problem.</li></ul><h2 id="5-4-Log-Structured-Data-Flow"><a href="#5-4-Log-Structured-Data-Flow" class="headerlink" title="5.4 Log Structured Data Flow"></a>5.4 Log Structured Data Flow</h2><h3 id="5-4-1-How-the-flow-work"><a href="#5-4-1-How-the-flow-work" class="headerlink" title="5.4.1 How the flow work"></a>5.4.1 How the flow work</h3><ul><li>Recipe: Take all the organization’s data and put it into a central log for <strong>real time subscription</strong></li><li>How the flow works<ul><li>Each logical data source can be modeled as its own log</li><li>A data source could be an application that logs out events, or a db table that accepts modifications</li><li>each subscribing system reads from this log <strong>as quickly as it can</strong>, <strong>applied each new record to its own store</strong>, and <strong>advances its position</strong> in the log</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/29/ykIEs4XwnBD3LUF.png" alt="How the flow work"></p><ul><li><p>Log gives a <strong>logical clock</strong> for each change against which all subscriber can be measured</p><ul><li>Consider a case where there is a database and a collection of caching servers</li><li>log provides a way to synchronize the updates to all these systems and reason about the point of time of each of these systems</li><li>Let’s say we <strong>write a record with log entry X</strong> and then need to do a read from the cache. If we want to guarantee we don’t see stale data, we just need to ensure we <strong>don’t read from any cache which has not replicated up to X.</strong></li></ul></li><li><p>Log also acts as a <strong>buffer</strong> that makes <strong>data production asynchronous from data consumption</strong></p><ul><li>satisfy different requirements like<ul><li>A batch system such as Hadoop or a data warehouse may consume only hourly or daily,</li><li>A real-time query system may need to be up-to-the-second.</li></ul></li></ul></li><li><p>Consumer only need to know about the log and not any details of the system of origin</p></li><li><p>What values most from author perspective</p><ul><li>The pipeline they built for process data, though a bit of a mess, were actually extremely valuable . Just the process of makeing data available in a new processing system (Hadoop) unblocked a lot of possibilities<ul><li>Many new products and analysis just came from putting together multiple pieces of data that had previously been locked up in specialized systems</li></ul></li></ul></li><li><p>LinkedIn Went Through from O(N^2) to O(2N)</p></li></ul><p><img src="https://s2.loli.net/2022/01/29/fXnmrpYFLSVGoR9.png" alt="Pre Architecture LinkedIn"><br><img src="https://s2.loli.net/2022/01/29/9dZa67tYEl1yc8e.png" alt="Cur Architecture LinkedIn"></p><ul><li>Actions for the migration<ul><li>Isolate each consumer from the source of the data</li><li>Create a new data system to be both a data source and a data destination</li><li>Here LinkedIn create Kafka</li><li>Kinesis is similar to Kafka as AWS use it to connects all different distributed systems as a piping</li></ul></li></ul><h3 id="5-4-2-Relationship-to-ETL-and-the-Data-Warehouse"><a href="#5-4-2-Relationship-to-ETL-and-the-Data-Warehouse" class="headerlink" title="5.4.2  Relationship to ETL and the Data Warehouse"></a>5.4.2  Relationship to ETL and the Data Warehouse</h3><ul><li><p>Data Warehouse</p><ul><li>target<ul><li>A repository of the clean, integrated data structured to support analysis</li></ul></li><li>what be involved<ul><li>periodically extracting data from source databases</li><li>munging it into some kind of understandable form</li><li>loading it into a central data warehouse</li></ul></li><li>Problems<ul><li>coupling the clean integrated data to the data warehouse.<ul><li>cannot get real time feed</li></ul></li><li>organization perspective<ul><li>The incentives are not aligned: data producers are often not very aware of the use of the data in the data warehouse and end up creating data that is hard to extract or requires heavy, hard to scale transformation to get into usable form.</li><li>the central team never quite manages to scale to match the pace of the rest of the organization, so data coverage is always spotty, data flow is fragile, and changes are slow.</li></ul></li></ul></li></ul></li><li><p>ETL</p><ul><li>tasks<ul><li>extraction and data cleanup process, liberating data locked up in a variety of systems in the organization and removing an system-specific non-sense</li><li>data is restructured for data warehousing queries (i.e. made to fit the type system of a relational DB, forced into a star or snowflake schema, perhaps broken up into a high performance <a href="http://parquet.io/">column</a> <a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html">format</a>,</li></ul></li><li>problems<ul><li>still, we need such data in real time as well for low latency processing as well as indexing in real time storage systems</li></ul></li></ul></li><li><p>A better approach as ETL and Data Warehouse substitution</p><ul><li><p>Have a central pipeline, the log, with a well defined API for adding data</p></li><li><p>Responsibility Classification</p><ul><li><p>Producer of the data feed: integrating with this pipeline and providing a clean, well-structured data feed</p></li><li><p>Datawarehouse team now <strong>only care about loading structured feeds</strong> of data from the <strong>central log</strong> and <strong>carrying out transformation specific to their system</strong></p><p>  <img src="https://s2.loli.net/2022/01/29/fMl9IQwDGk8TL4o.png" alt="Workflow and ownership classification">            </p></li></ul></li></ul></li></ul><h3 id="5-4-3-Log-Files-and-Events"><a href="#5-4-3-Log-Files-and-Events" class="headerlink" title="5.4.3 Log Files and Events"></a>5.4.3 Log Files and Events</h3><ul><li>Current structure also enables decoupled and event driven systems</li></ul><h3 id="5-4-4-How-to-build-scalable-logs"><a href="#5-4-4-How-to-build-scalable-logs" class="headerlink" title="5.4.4 How to build scalable logs"></a>5.4.4 How to build scalable logs</h3><ul><li>Need a log system that’s fast, cheap, scalable enough to make this practical at scale</li><li>LinkedIn in 2013 actually has already support 60 billion unique message writes through Kafka per day</li><li>Kafka achieve such high throughput via<ul><li>Partitioning the log<ul><li>each partition is a <strong>totally ordered log</strong>, but there is <strong>no global ordering between partitions</strong></li><li>Assignment of the messages to a particular partition is controllable by the writer, with most users choosing to partition by some kind of key</li><li>Replication<ul><li>Each partition is replicated across a configurable number of replicas</li><li>At any time, a single one of them will act as the leader, if the leader fails, one of the replicas will take over as leader</li></ul></li><li>Order Guarantee<ul><li>each partition is order preserving, and Kafka guarantees that appends to a particular partition from a single sender will be delivered in the order they are sent.</li></ul></li></ul></li><li>Optimizing throughput by batching reads and writes<ul><li>occurs when<ul><li>sending data</li><li>writes to disk</li><li>replication between servers</li><li>data transfer to consumers</li><li>acknowledging committed data</li></ul></li></ul></li><li>Avoiding needless data copies<ul><li>Use a simple binary format that is maintained between in memory log, on disk log and in network data transfers</li><li>Thus we could make use of numerous optimizations including zero copy data transfer <a href="https://en.wikipedia.org/wiki/Zero-copy">https://en.wikipedia.org/wiki/Zero-copy</a></li></ul></li></ul></li></ul><h1 id="6-Real-Time-Data-Processing"><a href="#6-Real-Time-Data-Processing" class="headerlink" title="6. Real Time Data Processing"></a>6. Real Time Data Processing</h1><blockquote><p>Computing derived data streams</p></blockquote><h2 id="6-1-Definition-of-Stream-Processing"><a href="#6-1-Definition-of-Stream-Processing" class="headerlink" title="6.1 Definition of Stream Processing"></a>6.1 Definition of Stream Processing</h2><ul><li>Infrastructure for continuous data processing<ul><li>computational model can be general like MapReduce or other distributed processing frameworks,</li><li>need the ability to produce low latency results</li></ul></li><li>Instead of batch get and process, we could do continuous changes</li><li>it is just processing which includes a notion of time in the underlying data being processed and does not require a static snapshot of the data so it can produce output at a user-controlled frequency instead of waiting for the “end” of the data set to be reached. In this sense, stream processing is a generalization of batch processing, and, given the prevalence of real-time data, a very important generalization</li><li>Log role<ul><li>making data available in real-time multi-subscriber data feeds.</li></ul></li></ul><h2 id="6-2-Stateful-Real-Time-Processing"><a href="#6-2-Stateful-Real-Time-Processing" class="headerlink" title="6.2 Stateful Real Time Processing"></a>6.2 Stateful Real Time Processing</h2><ul><li>Stateful real time processing means some more sophisticated operations, like counts, aggregations, or joins over windows in the stream</li><li>We need to maintain certain state in such case</li><li>Strategies for that<ul><li>Keep state in memory<ul><li>cons<ul><li>if the process crash, it would lose its intermediate state</li><li>if the state is only maintained over a window, the process could fall back to the point where the window began</li></ul></li></ul></li><li>Store all state in a remote storage system, and join over the network to that store<ul><li>cons<ul><li>no locality of data and lots of network round trips</li></ul></li></ul></li><li>Duality of tables and logs<ul><li>a stream processor can keep its state in a local table or index — a bdb, leveldb</li><li>the contents of this store is fed from its input streams</li><li>it could journal out a changelog for this local index it keeps to allow it to restore its state in the event of a crash and restart</li><li>This mechanism allows a generic mechanism for keeping co-partitioned state in arbitrary index types local with the incoming stream data.</li><li>when facing process fails<ul><li>recover its index from the changelog</li><li>changelog itself is the transformation of the local state into a sort of incremental record at a time backup</li></ul></li></ul></li></ul></li></ul><h2 id="6-3-Log-Compaction"><a href="#6-3-Log-Compaction" class="headerlink" title="6.3 Log Compaction"></a>6.3 Log Compaction</h2><ul><li>Log need to be cleaned up someway to save the space</li><li>In Kafka, clean up has two options depending on whether the data contains keyed updates or event data<ul><li>for event data, supports just retain a window of data<ul><li>configured to be few days</li><li>also could be configured as space</li></ul></li><li>for keyed data<ul><li>as the complete log give you ability to replay it to recreate the state of the source system</li><li>but we could do log compaction by removing obsolete records — records whose primary key has a more recent update</li></ul></li></ul></li></ul><h1 id="7-Distributed-System-Design"><a href="#7-Distributed-System-Design" class="headerlink" title="7. Distributed System Design"></a>7. Distributed System Design</h1><blockquote><p>How practical systems can be simplified with a log centric design</p></blockquote><h2 id="7-1-Distributed-system-design-thought"><a href="#7-1-Distributed-system-design-thought" class="headerlink" title="7.1 Distributed system design thought"></a>7.1 Distributed system design thought</h2><p>Log here is responsible for data flow, consistency and recovery </p><ul><li><p>Directions</p><ul><li>Coalescing lots of little instances of each system into a few big clusters</li></ul></li><li><p>Possibility 1</p><ul><li>separation of systems remains more or less as it is for a good deal longer.</li><li>an external log that integrates data will be very important.</li></ul></li><li><p>Possibility 2</p><ul><li>re-consolidation in which a single system with enough generality starts to merge back in all the different functions into a single uber-system.</li><li>extremely hard</li></ul></li><li><p>Possibility 3</p><ul><li>data infrastructure could be unbundled into a collection of services and application-facing system apis</li><li>use open source, like in Java stacks<ul><li>zookeeper<ul><li>handle system coordination</li></ul></li><li>mesos and yarn<ul><li>process virtualization</li><li>resource management</li></ul></li><li>netty, jetty<ul><li>handle remote communication</li></ul></li><li>protobuf<ul><li>handle serialization</li></ul></li><li>kafka and bookeeper<ul><li>provide a backing log</li></ul></li></ul></li><li>path towards getting the simplicity of the single system in a more diverse and modular world that continues to evolve. If the implementation time for a distributed system goes from years to weeks because reliable, flexible building blocks emerge, then the pressure to coalesce into a single monolithic system disappears.</li></ul></li></ul><h2 id="7-2-Usage-of-log-in-system-architecture"><a href="#7-2-Usage-of-log-in-system-architecture" class="headerlink" title="7.2 Usage of log in system architecture"></a>7.2 Usage of log in system architecture</h2><ul><li><p>Usage of log in system architecture</p><ul><li>Handle data consistency (whether eventual or immediate) by sequencing concurrent updates to nodes</li><li>Provide data replication between nodes</li><li>Provide “commit” semantics to the writer (i.e. acknowledging only when your write guaranteed not to be lost)</li><li>Provide the external data subscription feed from the system</li><li>Provide the capability to restore failed replicas that lost their data or bootstrap new replicas</li><li>Handle rebalancing of data between nodes.</li></ul></li><li><p>What mentioned above is actually a large portion of what a distributed data system does. left over is mainly related with client facing query API and indexing strategy</p></li><li><p>System Look</p><ul><li>System is divided into two logical pieces<ul><li>log<ul><li>capture the state changes in sequential order</li></ul></li><li>serving layer<ul><li>store whatever index is required to serve queries</li></ul></li></ul></li><li>writes could go directly to the log or may be proxied by the serving layer</li><li>writes to the log yields a logical timestamp, if the system is partitioned, then the log and serving nodes will have the same number of partitions, though they may have very different numbers of machines</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/29/EXfRmnKxbGs8kHT.png" alt="Log and serving layer">    </p><ul><li><p>The client can get <strong>read-your-write semantics</strong> from any node by providing the <strong>timestamp of a write</strong> as part of its query—a serving node receiving such a query will <strong>compare the desired timestamp</strong> to <strong>its own index point</strong> and if necessary delay the request until it has indexed up to at least that time to avoid serving stale data.</p></li><li><p>For handling restoring failed nodes or moving partitions from node to node</p><ul><li>have the log retain only a fixed window of data and combine this with a snapshot of the data stored in the partition</li><li>it’s possible for the log to retain a complete copy of data and garbage collect the log itself</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying</a> </li><li><a href="https://kafka.apache.org/documentation.html#design">https://kafka.apache.org/documentation.html#design</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Real-time-data’s-unifying-abstraction&quot;&gt;&lt;a href=&quot;#Real-time-data’s-unifying-abstraction&quot; class=&quot;headerlink&quot; title=&quot;Real time data’s u
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="log" scheme="https://www.llchen60.com/tags/log/"/>
    
      <category term="distributed system" scheme="https://www.llchen60.com/tags/distributed-system/"/>
    
  </entry>
  
  <entry>
    <title>Flyway</title>
    <link href="https://www.llchen60.com/Flyway/"/>
    <id>https://www.llchen60.com/Flyway/</id>
    <published>2022-01-01T13:57:09.000Z</published>
    <updated>2022-01-01T13:57:52.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>An open source database migration tool</li><li>Favors simplicity and convention over configuration</li><li>has 7 basic commands<ul><li>migrate</li><li>clean</li><li>info</li><li>validate</li><li>undo</li><li>baseline</li><li>repair</li></ul></li></ul><h2 id="1-1-Why-DB-migrations"><a href="#1-1-Why-DB-migrations" class="headerlink" title="1.1 Why DB migrations?"></a>1.1 Why DB migrations?</h2><ul><li>we need a way to version the table</li><li>we need to know what state is the db on this machine</li><li>database migration help us<ul><li>recreate a database from scratch</li><li>make it clear at all times what state a database is in</li><li>migrate in a deterministic way from your current version of the database to a newer one</li></ul></li></ul><h2 id="1-2-How-Flyway-works"><a href="#1-2-How-Flyway-works" class="headerlink" title="1.2 How Flyway works?"></a>1.2 How Flyway works?</h2><ul><li><p>Flyway first try to locate its schema history table</p><ul><li>if db empty, then flyway will create it instead</li><li>this default db named as <code>flyway_schema_history</code></li></ul></li><li><p>Then flyway will begin scanning the filesystem or the classpath of the application for migrations</p></li><li><p>The migrations are then sorted based on the version number and applied in order</p></li><li><p>The schema history table will be updated accordingly as each migration gets applied</p></li><li><p>we use <code>flyway migrate</code> to execute the migration</p></li></ul><h1 id="2-Flyway-Commands"><a href="#2-Flyway-Commands" class="headerlink" title="2. Flyway Commands"></a>2. Flyway Commands</h1><h2 id="2-1-migrate"><a href="#2-1-migrate" class="headerlink" title="2.1 migrate"></a>2.1 <code>migrate</code></h2><ul><li>help migrate the db</li></ul><h2 id="2-2-clean"><a href="#2-2-clean" class="headerlink" title="2.2 clean"></a>2.2 <code>clean</code></h2><ul><li>drop all objects in the confgured schemas</li></ul><h2 id="2-3-info"><a href="#2-3-info" class="headerlink" title="2.3 info"></a>2.3 <code>info</code></h2><ul><li>print the details and status information about all migrations</li></ul><h2 id="2-4-validate"><a href="#2-4-validate" class="headerlink" title="2.4 validate"></a>2.4 <code>validate</code></h2><ul><li>validates the applied migrations against the ones available on the classpath</li></ul><h2 id="2-5-undo"><a href="#2-5-undo" class="headerlink" title="2.5 undo"></a>2.5 <code>undo</code></h2><ul><li>undoes the most recently applied versioned migration</li></ul><h2 id="2-6-baseline"><a href="#2-6-baseline" class="headerlink" title="2.6 baseline"></a>2.6 <code>baseline</code></h2><ul><li>baselines an existing database, excluding all migrations up and including baseline version</li></ul><h2 id="2-7-repair"><a href="#2-7-repair" class="headerlink" title="2.7 repair"></a>2.7 <code>repair</code></h2><ul><li>repair the schema history table</li></ul><h1 id="3-Concepts"><a href="#3-Concepts" class="headerlink" title="3. Concepts"></a>3. Concepts</h1><h2 id="3-1-Migrations"><a href="#3-1-Migrations" class="headerlink" title="3.1 Migrations"></a>3.1 Migrations</h2><ul><li>all changes to the db are called migrations</li><li>migrations can be<ul><li>versioned<ul><li>types<ul><li>regular</li><li>undo<ul><li>the effect can be undone by supplying an undo migration</li></ul></li></ul></li><li>contains<ul><li><strong>version</strong><ul><li>must be unique</li></ul></li><li><strong>description</strong><ul><li>purely informative for you to be able to remember what each migration does</li></ul></li><li><strong>checksum</strong><ul><li>detect accidental changes</li></ul></li></ul></li></ul></li><li>repeatable<ul><li>contains<ul><li><strong>description</strong></li><li><strong>checksum</strong></li></ul></li><li>instead of being run just once, they are re-applied every time their checksum changes</li><li>Within a single migration run, repeatable migrations are always <strong>applied last</strong>, after all pending versioned migrations have been executed. Repeatable migrations are applied in the order of their description</li></ul></li></ul></li></ul><h3 id="3-1-1-Versioned-Migrations"><a href="#3-1-1-Versioned-Migrations" class="headerlink" title="3.1.1 Versioned Migrations"></a>3.1.1 Versioned Migrations</h3><ul><li>contains<ul><li>version<ul><li><strong>must be unique</strong></li><li>applied in order exactly once</li></ul></li><li>description<ul><li>purely informative for you to be able to remember what each migration does</li></ul></li><li>checksum<ul><li>detect accidental changes</li></ul></li></ul></li><li>used for<ul><li>creating/ altering/ dropping tables/ indexes/ foreign keys/ enums</li><li>reference data updates</li><li>user data corrections</li></ul></li></ul><h3 id="3-1-2-Undo-Migrations"><a href="#3-1-2-Undo-Migrations" class="headerlink" title="3.1.2 Undo Migrations"></a>3.1.2 Undo Migrations</h3><ul><li>A migration can fail at any point. If you have 10 statements, it is possible for the 1st, the 5th, the 7th or the 10th to fail. There is simply no way to know in advance. In contrast, undo migrations are written to undo an entire versioned migration and will not help under such conditions.</li><li>we should <strong>maintain backwards compatibility</strong> between the <strong>DB and all versions of the code</strong> currently deployed in production</li></ul><h3 id="3-1-3-Repeatable-Migrations"><a href="#3-1-3-Repeatable-Migrations" class="headerlink" title="3.1.3 Repeatable Migrations"></a>3.1.3 Repeatable Migrations</h3><ul><li><p>contains</p><ul><li>description and a checksum, but no version</li></ul></li><li><p>repeatable migrations are re-applied every time their checksum changes</p></li><li><p>Very useful for managing database objects whose definition can then simply be maintained in a single file in version control</p></li><li><p>Repeatable migrations are always applied last, after all pending versioned migrations have been executed; always applied in the order of their description</p></li></ul><h3 id="3-1-4-SQL-Based-Migrations"><a href="#3-1-4-SQL-Based-Migrations" class="headerlink" title="3.1.4 SQL Based Migrations"></a>3.1.4 SQL Based Migrations</h3><ul><li>used for<ul><li>DDL change — CREATE/ALTER/DROP statements for TABLES,VIEWS,TRIGGERS,SEQUENCES,…</li><li>Simple reference data changes</li><li>simple bulk data changes</li></ul></li><li>Naming  Patterns<ul><li>Prefix<ul><li>v for versioned</li><li>u for undo</li><li>r for repeatable migrations</li></ul></li><li>version<ul><li>with dots or underscores separate as many parts as you like</li></ul></li><li>Separator<ul><li>__ two underscores</li></ul></li><li>Suffix<ul><li><code>.sql</code></li></ul></li></ul></li><li>Discovery<ul><li>Flyway discover sql migrations from directories <strong>referenced by the location property</strong></li></ul></li></ul><h3 id="3-1-5-Script-Based-Migrations"><a href="#3-1-5-Script-Based-Migrations" class="headerlink" title="3.1.5 Script Based Migrations"></a>3.1.5 Script Based Migrations</h3><ul><li>name patten<ul><li>``V1__execute_batch_tool.sh`</li></ul></li><li>could be used for<ul><li>triggering execution of a 3rd party application as part of the migrations</li><li>cleaning up local files</li></ul></li></ul><h3 id="3-1-6-Transactions"><a href="#3-1-6-Transactions" class="headerlink" title="3.1.6 Transactions"></a>3.1.6 Transactions</h3><ul><li>By default, Flyway <strong>wraps the execution of an entire migration within a single transaction</strong></li></ul><h2 id="3-2-Callbacks"><a href="#3-2-Callbacks" class="headerlink" title="3.2 Callbacks"></a>3.2 Callbacks</h2><ul><li><p>For the case we need to execute same action over and over again</p></li><li><p>we could hook into its lifecycle</p></li><li><p>there are certain keywords we could use, and invoke them during the process</p></li></ul><p><a href="https://flywaydb.org/documentation/concepts/callbacks">https://flywaydb.org/documentation/concepts/callbacks</a> </p><h2 id="3-3-Error-Overrides"><a href="#3-3-Error-Overrides" class="headerlink" title="3.3 Error Overrides"></a>3.3 Error Overrides</h2><ul><li>By default, in case an error is returned, flyway displays it with all necessary details, marks the migration as failed and automatically rolls it back if possible</li><li>But we could change the behavior like<ul><li>treat an error as a waring</li><li>treat a waring as an error</li><li>perform an additional action</li></ul></li></ul><h2 id="3-4-Dry-Runs"><a href="#3-4-Dry-Runs" class="headerlink" title="3.4 Dry Runs"></a>3.4 Dry Runs</h2><ul><li>Used for<ul><li>preview changes Flyway will make to the db</li><li>submit the SQL statements for review</li><li>use Flyway to determine what needs updating,</li></ul></li><li>how it works<ul><li>flyway sets up a read only connection to the db,</li><li>assesses what migrations need to run and generates a single SQL file containing all statements it would have executed in case of a regular migration run</li></ul></li></ul><h2 id="3-5-Baseline-Migrations"><a href="#3-5-Baseline-Migrations" class="headerlink" title="3.5 Baseline Migrations"></a>3.5 Baseline Migrations</h2><ul><li><p>Over the lifetime of a project, there would be tons of db objects be created/ destroyed across many migrations</p><ul><li>we want to simplify with a single, cumulative migration that represents the state of db after all of those migrations have been applied without disrupting existing env</li></ul></li><li><p>How it works?</p><ul><li>Prefixed with B followed by the version of your db they represent</li><li>Only used when deploying to new env</li><li>If used in an env where some Flyway migrations have already been applied, <strong>baseline migrations will be ignored,</strong> <strong>new env will choose the latest baseline migration as the starting point</strong><ul><li>every migration with a version below the latest baseline migration’s version is marked as ignored</li></ul></li><li>baseline migration are executed during the migrate process</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://flywaydb.org/documentation/">https://flywaydb.org/documentation/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;An open source database migrat
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="database" scheme="https://www.llchen60.com/tags/database/"/>
    
      <category term="migration" scheme="https://www.llchen60.com/tags/migration/"/>
    
  </entry>
  
  <entry>
    <title>Protobuf Rampup</title>
    <link href="https://www.llchen60.com/Protobuf-Rampup/"/>
    <id>https://www.llchen60.com/Protobuf-Rampup/</id>
    <published>2021-12-25T02:09:49.000Z</published>
    <updated>2021-12-25T02:11:09.003Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Protobuf-Learning"><a href="#Protobuf-Learning" class="headerlink" title="Protobuf Learning"></a>Protobuf Learning</h1><h1 id="1-What-are-Protocol-Buffers"><a href="#1-What-are-Protocol-Buffers" class="headerlink" title="1. What are Protocol Buffers?"></a>1. What are Protocol Buffers?</h1><ul><li>Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler.</li><li>You <strong>define how you want your data to be structured</strong> once, then you <strong>use special generated source code to easily write and read your structured data</strong></li></ul><h1 id="2-Why-use-Protocol-Buffers"><a href="#2-Why-use-Protocol-Buffers" class="headerlink" title="2. Why use Protocol Buffers?"></a>2. Why use Protocol Buffers?</h1><ul><li>XML is human readable and wide language supports<ul><li>but is notoriously space intensive</li><li>encoding/ decoding can impose a huge performance penalty on applications</li></ul></li><li>With protocol buffers<ul><li>write a <code>.proto</code> description of the data structure</li><li>the <strong>protocol buffer compiler</strong> then <strong>creates a class</strong> that implements <strong>automatic encoding and parsing</strong> of the protocol buffer data with an <strong>efficient binary format</strong></li><li>the generated class <strong>provides getters and setters</strong> for the fields</li><li>take care of the details of reading and writing the protocol buffer <strong>as a unit</strong></li></ul></li></ul><h1 id="3-Java-Tutorial-In-Proto2"><a href="#3-Java-Tutorial-In-Proto2" class="headerlink" title="3. Java Tutorial (In Proto2)"></a>3. Java Tutorial (In Proto2)</h1><h2 id="3-1-Define-Protocol-Format"><a href="#3-1-Define-Protocol-Format" class="headerlink" title="3.1 Define Protocol Format"></a>3.1 Define Protocol Format</h2><pre><code class="protobuf">syntax = &quot;proto2&quot;;// starts with package delcaration // we should define this to get rid of name conflict package tutorial;// enable generating a separate .java file for each generated class option java_multiple_files = true;// specify in what java package name your generated classes should live// if not set here, it will simply match the pkg name given by the package declaration option java_package = &quot;com.example.tutorial.protos&quot;;// define the class name of the wrapper class which will represent this file // if not given, it will be auto generated by converting the file name to upper camel case option java_outer_classname = &quot;AddressBookProtos&quot;;/**Message Definition: An aggregate containing a set of typed fields Contain certain standard types    + boo1    + int32     + float     + double     + string we could also add further structure to msgs by using other msg types as field types + marker     + identify the unique tag field use in binary encoding     + try to use 1 - 15 as it neeeds one less byte+ modifier     + optional         + field may or may not be set         + if not, a default value will be used             + we could set our own default values             + or system will provide defaults                 + numeric types -- zero                 + strings -- empty string                 + bools -- false                 + embedded messages -- default instance or prototype of the message, which has none of its fields set     + repeated         + the field may be repeated any number of times [0, xxx)         + order will be preserved in the protocol buffer         + act like a dynamic sized array     + required         + a value for the field must be provided        + try to build an uninitialized msg will throw runtime exception         + parse an uninitialzied msg will throw IOException         + required is not favored as it cannot be backward compatible */message Person &#123;    // =1 marker identify the unique tag that field uses in the binary encoding   optional string name = 1;  optional int32 id = 2;  optional string email = 3;  enum PhoneType &#123;    MOBILE = 0;    HOME = 1;    WORK = 2;  &#125;  message PhoneNumber &#123;    optional string number = 1;    optional PhoneType type = 2 [default = HOME];  &#125;  repeated PhoneNumber phones = 4;&#125;message AddressBook &#123;  repeated Person people = 1;&#125;</code></pre><h2 id="3-2-Compiling-Protocol-Buffers"><a href="#3-2-Compiling-Protocol-Buffers" class="headerlink" title="3.2 Compiling Protocol Buffers"></a>3.2 Compiling Protocol Buffers</h2><ul><li>To generate the classes, we need to run the protocol buffer compiler</li><li>specify the source directory, the destination directory and the path to our <code>.proto</code></li></ul><pre><code class="protobuf">protoc -I=$SRC_DIR --java_out=$DST_DIR $SRC_DIR/addressbook.proto </code></pre><h2 id="3-3-Protocol-Buffer-API"><a href="#3-3-Protocol-Buffer-API" class="headerlink" title="3.3 Protocol Buffer API"></a>3.3 Protocol Buffer API</h2><ul><li>compiler helps auto generate source file<ul><li>getters and setters</li><li>each field also has <code>clear</code> method to set the field back to its empty state</li></ul></li><li>Builders vs Messages<ul><li>message classes are immutable</li><li>builder is used when you first construct a builder, then we could call the builder’s build() method</li></ul></li><li>standard message methods<ul><li><code>isInitialized</code> check if all the required fields have been set</li><li><code>toString</code> returns a human readable representation of the msg</li><li><code>mergeFrom(Message other)</code> merge the contents of other into this msg, overwrite singular scalar fields</li><li><code>clear</code> clear all the fields back to the empty state</li></ul></li><li>Parsing and Serialization<ul><li><code>byte[] toByteArray();</code><ul><li>serializes the msg and returns a byte array containing its raw bytes</li></ul></li><li><code>static xxx parseFrom(byte[] data);</code><ul><li>parse a msg from the given byte array</li></ul></li><li><code>void writeTo(OutputStream output);</code><ul><li>serialize the msg and writes to an OutputStream</li></ul></li><li><code>static xxx parseFrom(InputStream input);</code><ul><li>reads and parses a msg from an InputStream</li></ul></li></ul></li></ul><h2 id="3-4-How-to-extend-a-Protocol-Buffer"><a href="#3-4-How-to-extend-a-Protocol-Buffer" class="headerlink" title="3.4 How to extend a Protocol Buffer"></a>3.4 How to extend a Protocol Buffer</h2><ul><li>In the new version of the protocol buffer<ul><li>must not change the tag numbers of any existing fields</li><li>must not add or delete any required fields</li><li>may delete optional or repeated fields</li><li>may add new optional or repeated fields but must use fresh tag numbers</li></ul></li></ul><h1 id="4-Overall-Guide-In-Proto3"><a href="#4-Overall-Guide-In-Proto3" class="headerlink" title="4. Overall Guide (In Proto3)"></a>4. Overall Guide (In Proto3)</h1><h2 id="4-1-Define-message-type"><a href="#4-1-Define-message-type" class="headerlink" title="4.1 Define message type"></a>4.1 Define message type</h2><ul><li>Each field in the msg definition need to have a <strong>unique number</strong><ul><li>those numbers are used to identify fields in the message binary format</li><li>the number should never be changed</li></ul></li><li>specify field rules<ul><li>singular<ul><li>default field rule for proto3 syntax</li><li>can have <strong>zero or one of this field</strong></li></ul></li><li>repeated<ul><li>can be repeated any number of times (including zero)</li></ul></li></ul></li><li>reserved fields<ul><li>if you update a msg type by entirely removing a field or commenting it out, future users can reuse the field number but it would bring severe issues,</li><li>thus we could reserved the number for deleted fields and tag number</li></ul></li></ul><pre><code class="protobuf">message Foo &#123;  reserved 2, 15, 9 to 11;  reserved &quot;foo&quot;, &quot;bar&quot;;&#125;</code></pre><ul><li>Post compiler running<ul><li>Compiler generates a <code>.java</code> file with a class for each message type, as well as Builder classes for creating message class instances</li></ul></li><li>For enum values<ul><li>every enum definition must contain a constant that maps to zero as its first element</li><li>we can allow alias thus we could assign the same value to different enum constants</li></ul></li><li>import<ul><li>we could do import thus we could use definitions from other <code>.proto</code> file</li></ul></li></ul><h2 id="4-2-Scalar-Value-Types"><a href="#4-2-Scalar-Value-Types" class="headerlink" title="4.2 Scalar Value Types"></a>4.2 Scalar Value Types</h2><p><a href="https://developers.google.com/protocol-buffers/docs/proto3#scalar">Language Guide (proto3) | Protocol Buffers | Google Developers</a></p><h2 id="4-3-Nested-Types"><a href="#4-3-Nested-Types" class="headerlink" title="4.3 Nested Types"></a>4.3 Nested Types</h2><ul><li>we could define and use msg types inside other msg types</li></ul><pre><code class="protobuf">message SearchResponse &#123;  message Result &#123;    string url = 1;    string title = 2;    repeated string snippets = 3;  &#125;  repeated Result results = 1;&#125;// to use the msg type outside its parent message type message SomeOtherMessage &#123;  SearchResponse.Result result = 1;&#125;</code></pre><h2 id="4-4-Updating-a-Message-Type"><a href="#4-4-Updating-a-Message-Type" class="headerlink" title="4.4 Updating a Message Type"></a>4.4 Updating a Message Type</h2><ul><li>don’t change the field numbers for any existing fields</li><li>if you add new fields, any msg serialized by code using your old msg format can still be parsed by your new generated code<ul><li>keep in mind the default values for these elements so that new code can properly interact with msgs generated by old code</li></ul></li><li>to remove a field<ul><li>rename the field with prefix like <code>OBSOLETE_</code></li><li>or make the filed number reserved,</li></ul></li><li>int32, uint32, int64, uint64 and bool are all compatible</li><li>string and bytes are compatible as long as the bytes are valid UTF-8</li></ul><h2 id="4-5-Special-Keywords"><a href="#4-5-Special-Keywords" class="headerlink" title="4.5 Special Keywords"></a>4.5 Special Keywords</h2><h3 id="4-5-1-Any"><a href="#4-5-1-Any" class="headerlink" title="4.5.1 Any"></a>4.5.1 <code>Any</code></h3><ul><li>let you use messages as embedded types without having their .proto definition</li><li>it contains an aribitrary serialized messages as bytes</li></ul><h3 id="4-5-2-Oneof"><a href="#4-5-2-Oneof" class="headerlink" title="4.5.2 Oneof"></a>4.5.2 Oneof</h3><ul><li>if we have a msg with many fields and where at most one field will be set at the same time, we can enforce the behavior and save memory by using the oneof feature</li><li>at most one field can be set at the same time</li><li>setting any member of the oneof automatically clears all the other members</li></ul><h2 id="4-6-Maps"><a href="#4-6-Maps" class="headerlink" title="4.6 Maps"></a>4.6 Maps</h2><ul><li><code>map&lt;key_type, value_type&gt; map_field = N;</code></li></ul><h2 id="4-7-Define-Service"><a href="#4-7-Define-Service" class="headerlink" title="4.7 Define Service"></a>4.7 Define Service</h2><ul><li>If you want to use message types with an RPC system, we can define an RPC service interface in a <code>.proto</code> file</li><li>then the protocol buffer compiler will <strong>generate service interface code and stubs</strong> in chosen language</li></ul><h2 id="4-8-Options"><a href="#4-8-Options" class="headerlink" title="4.8 Options"></a>4.8 Options</h2><ul><li><p>Options do not change the overall meaning of a declaration, but may affect the way it is handled in a particular context.</p></li><li><p>java_package</p><ul><li>pkg you want to use for your generated Java classes</li></ul></li><li><p>java_outer_classname</p><ul><li>class name for the wrapper java class you want to generate</li></ul></li><li><p>java_multiple_files</p></li><li><p><code>optimize_for</code></p><ul><li><code>SPEED</code><ul><li>Compiler will generate code for serializing, parsing and performing other common operations on your msg types.</li><li>Code is highly optimized</li></ul></li><li><code>CODE_SIZE</code><ul><li>generate minimal classes</li><li>operations will be slower</li></ul></li><li><code>LITE_RUNTIME</code><ul><li>only depend on the lite runtime library</li><li>usefyl for apps running on constrained platform like mobile phones</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Overview <a href="https://developers.google.com/protocol-buffers">https://developers.google.com/protocol-buffers</a> </li><li>Language Guide <a href="https://developers.google.com/protocol-buffers/docs/overview">https://developers.google.com/protocol-buffers/docs/overview</a> </li><li>Java Tutorial <a href="https://developers.google.com/protocol-buffers/docs/javatutorial">https://developers.google.com/protocol-buffers/docs/javatutorial</a> </li><li>Java Generated Code <a href="https://developers.google.com/protocol-buffers/docs/reference/java-generated">https://developers.google.com/protocol-buffers/docs/reference/java-generated</a> </li><li>Java Encoding <a href="https://developers.google.com/protocol-buffers/docs/encoding">https://developers.google.com/protocol-buffers/docs/encoding</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Protobuf-Learning&quot;&gt;&lt;a href=&quot;#Protobuf-Learning&quot; class=&quot;headerlink&quot; title=&quot;Protobuf Learning&quot;&gt;&lt;/a&gt;Protobuf Learning&lt;/h1&gt;&lt;h1 id=&quot;1-Wha
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Protobuf" scheme="https://www.llchen60.com/tags/Protobuf/"/>
    
  </entry>
  
  <entry>
    <title>Bazel Intro</title>
    <link href="https://www.llchen60.com/Bazel-Intro/"/>
    <id>https://www.llchen60.com/Bazel-Intro/</id>
    <published>2021-12-22T02:05:15.000Z</published>
    <updated>2021-12-22T02:07:14.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-What-is-Bazel"><a href="#1-What-is-Bazel" class="headerlink" title="1. What is Bazel?"></a>1. What is Bazel?</h1><ul><li><p>Bazel is a build and test tool built that supports building and testing multiple projects for multiple languages and build outputs</p></li><li><p>What</p><ul><li>Build and Test tool similar to Make, Maven, Gradle</li><li>Caches all the previously done work, tests or builds faster everytime</li><li>Support multi languages, multi platforms,</li><li>Support large code base across multi repos</li><li>build, test, and query to trace dependencies in the code</li></ul></li><li><p>Why</p><ul><li>scales</li><li>multi platform</li></ul></li><li><p>How</p><ul><li>Need a BUILD file</li></ul></li></ul><h1 id="2-Concepts"><a href="#2-Concepts" class="headerlink" title="2. Concepts"></a>2. Concepts</h1><ul><li>Workspace - With WORKSPACE<ul><li>dir contains the source file</li><li>considered as root</li></ul></li><li>WORKSPACE<ul><li>a blank text file, which identifies the directory and its content as a Bazel workspace</li><li>at the root of the project’s directory structure</li></ul></li><li>Repos - With WORKSPACE<ul><li>External repos are defined in the WORKSPACE file using workspace rules</li></ul></li><li>Packages - With BUILD<ul><li>A package is defined as a directory containing a file named BUILD or BUILD.bazel</li><li>which reside beneath top level directory in the ws</li><li>This file has instructions on how to <strong>run or build or test</strong> the project</li></ul></li><li>Rules<ul><li>written using a DSL named Starlark</li><li>thus are built for certain language already like rules_java, etc.</li></ul></li><li>Targets<ul><li>Pkg is container, element of pkg —- target</li><li>Most targets are files or rules<ul><li>File<ul><li>source files - written by people</li><li>generated files — generated by build tool</li></ul></li><li>rule<ul><li>specify relationship between set of inputs and output</li><li>output are always generated files</li></ul></li></ul></li></ul></li></ul><h1 id="3-Best-Practices"><a href="#3-Best-Practices" class="headerlink" title="3. Best Practices"></a>3. Best Practices</h1><ul><li>A project should always be able to run <code>bazel build //...</code> and <code>bazel test //...</code></li><li>You may declare third party dependencies<ul><li>either declare them as remote repositories in the WORKSPACE file</li><li>or put them in a directory called third_party under workspace directory</li></ul></li><li>everything should be built from source whenever possible, instead of depending on a library so file, we should create a BUILD file and build so from its sources, then depend on that target</li><li>for project specific options, use the configuration file under <code>workspace/.bazelrc</code></li><li>every directory that contains buildable files should be a package</li></ul><h1 id="4-Build-a-Java-Project"><a href="#4-Build-a-Java-Project" class="headerlink" title="4. Build a Java Project"></a>4. Build a Java Project</h1><h2 id="4-1-Bazel-Jave-Basic"><a href="#4-1-Bazel-Jave-Basic" class="headerlink" title="4.1 Bazel Jave Basic"></a>4.1 Bazel Jave Basic</h2><ul><li><p>Refer <a href="https://docs.bazel.build/versions/1.2.0/tutorial/java.html">https://docs.bazel.build/versions/1.2.0/tutorial/java.html</a></p></li><li><p>build rule tells bazel how to build the desired outputs, executable binaries or libraries</p><ul><li>the java binary rule tells bazel to build a jar file and a wrapper shell script</li></ul></li><li><p><code>bazel build //:ProjectRunner</code></p><ul><li>the <code>//</code> part is the location of our BUILD file relative to the root of the workspace</li><li><code>ProjectRunner</code> is the target name we define in the BUILD file</li></ul></li><li><p>we could review our dependency graph by using</p><ul><li><code>bazel query --notool_deps --noimplicit_deps &quot;deps(//:ProjectRunner)&quot; --output graph</code></li></ul></li></ul><pre><code class="ruby">// generate graph for class in use, and output as a svg file bazel query  --notool_deps --noimplicit_deps &quot;deps(//booking)&quot; --output graph &gt; /Users/lchen1/Documents/bookingGraph.in dot -Tsvg &lt; bookingGraph.in &gt; graph.svg</code></pre><h2 id="4-2-Specify-multiple-build-targets"><a href="#4-2-Specify-multiple-build-targets" class="headerlink" title="4.2 Specify multiple build targets"></a>4.2 Specify multiple build targets</h2><ul><li><p>Package Splits</p><ul><li><p>for larger project, we may want to split into multiple targets and packages to allow for fast incremental builds, this could also speed up builds by building multiple parts of a project at once</p><pre><code class="json">java_binary(  name = &quot;ProjectRunner&quot;,  srcs = [&quot;src/main/java/com/example/ProjectRunner.java&quot;],  main_class = &quot;com.example.ProjectRunner&quot;,  deps = [&quot;:greeter&quot;],)java_library(  name = &quot;greeter&quot;,  srcs = [&quot;src/main/java/com/example/Greeting.java&quot;],)</code></pre></li></ul></li></ul><ul><li>with this configuration, bazel will first build greeter library, then the projectRunner binary<ul><li>deps attribute tells bazel the greeter library is required to build the projectRunner binary</li></ul></li></ul><h2 id="4-3-Use-multiple-packages"><a href="#4-3-Use-multiple-packages" class="headerlink" title="4.3 Use multiple packages"></a>4.3 Use multiple packages</h2><pre><code class="json">java_binary(    name = &quot;runner&quot;,    srcs = [&quot;Runner.java&quot;],    main_class = &quot;com.example.cmdline.Runner&quot;,    deps = [&quot;//:greeter&quot;])</code></pre><ul><li>To make sure above works, we need to let greeter be visible to cmdline.Runner<ul><li>Let the resource owner set the visibility attribute</li><li>we need to do this cause Bazel by default makes target only visible to other targets in the same BUILD file</li><li>bazel uses target visibility to prevent issues such as libraries containing implementation details leaking into public APIs</li></ul></li></ul><pre><code class="json">java_library(    name = &quot;greeter&quot;,    srcs = [&quot;src/main/java/com/example/Greeting.java&quot;],    visibility = [&quot;//src/main/java/com/example/cmdline:__pkg__&quot;],    )</code></pre><h2 id="4-4-Use-labels-to-reference-targets"><a href="#4-4-Use-labels-to-reference-targets" class="headerlink" title="4.4 Use labels to reference targets"></a>4.4 Use labels to reference targets</h2><ul><li>Bazel uses target labels to reference targets<ul><li><code>//:ProjectRunner</code></li><li>sync as follow:<ul><li><code>//path/to/package:target-name</code></li></ul></li></ul></li><li>when referencing targets within the same BUILD file, we can skip the <code>//</code> workspace root identifier and just use <code>:target_name</code></li></ul><h1 id="5-E-G"><a href="#5-E-G" class="headerlink" title="5. E.G"></a>5. E.G</h1><ul><li>java_binary<ul><li>pre defined rule telling bazel to create a binary when a target is invoked</li></ul></li></ul><pre><code class="json">java_binary(        // target name     name = &quot;mymain&quot;,        // all source files, passed as glob, inside the fully qualified directory names on classpath     srcs = glob([&quot;src/main/java/com/abhi/*.java&quot;]),        // main runner class     main_class = &quot;com.abhi.MyMain&quot;,        // dependent classes/ interfaces to be included, not part of srcs     deps = [&quot;//another-dir:animal&quot;])</code></pre><ul><li>java_library<ul><li>pre-defined to create library as the name suggests</li></ul></li></ul><pre><code class="json">java_library(    name = &quot;animal&quot;,    srcs = [&quot;src/main/java/com/abhi/Animal.java&quot;],        // if other class is implemented in a different pkg, it has to be visible to main-dir     visibility = [&quot;//main-dir:__pkg__&quot;])</code></pre><ul><li>CLI Reference<ul><li><code>bazel build //main-dir:mymain</code><ul><li>// means a valid package name</li><li>mymain is the target name</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Bazel best practice <a href="https://docs.bazel.build/versions/main/best-practices.html">https://docs.bazel.build/versions/main/best-practices.html</a> </li><li>Bazel Overview  <a href="https://docs.bazel.build/versions/1.2.0/bazel-overview.html">https://docs.bazel.build/versions/1.2.0/bazel-overview.html</a> </li><li>Java Tutorial <a href="https://docs.bazel.build/versions/1.2.0/tutorial/java.html">https://docs.bazel.build/versions/1.2.0/tutorial/java.html</a> </li><li>How to specify targets to build <a href="https://docs.bazel.build/versions/main/guide.html#target-patterns">https://docs.bazel.build/versions/main/guide.html#target-patterns</a> </li></ol><p><a href="https://docs.bazel.build/versions/4.2.1/command-line-reference.html">Command-Line Reference</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-What-is-Bazel&quot;&gt;&lt;a href=&quot;#1-What-is-Bazel&quot; class=&quot;headerlink&quot; title=&quot;1. What is Bazel?&quot;&gt;&lt;/a&gt;1. What is Bazel?&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bazel
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Bazel" scheme="https://www.llchen60.com/tags/Bazel/"/>
    
      <category term="Package Management" scheme="https://www.llchen60.com/tags/Package-Management/"/>
    
  </entry>
  
  <entry>
    <title>GraphQL Read</title>
    <link href="https://www.llchen60.com/GraphQL-Read/"/>
    <id>https://www.llchen60.com/GraphQL-Read/</id>
    <published>2021-12-04T09:29:50.000Z</published>
    <updated>2021-12-04T09:31:23.613Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GraphQL-Read"><a href="#GraphQL-Read" class="headerlink" title="GraphQL Read"></a>GraphQL Read</h1><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li>QraphQL is<ul><li>a query language</li><li>a server side runtime for executing queries using a type system you define</li></ul></li></ul><h1 id="2-Queries-and-Mutations"><a href="#2-Queries-and-Mutations" class="headerlink" title="2. Queries and Mutations"></a>2. Queries and Mutations</h1><ul><li>Fields<ul><li>GraphQL is about asking specific fields on objects</li><li>Query has the same shape as result<ul><li>server knows exactly what fields the client is asking for</li></ul></li></ul></li></ul><pre><code class="ruby">&#123;    hero &#123;        name    &#125;&#125;&#123;    &quot;data&quot;: &#123;        &quot;hero&quot;: &#123;            &quot;name&quot;: &quot;12test&quot;        &#125;    &#125;&#125;</code></pre><ul><li><p>Arguments</p><ul><li>we could pass arguments to fields</li><li>comparing with Restful, in GraphQL every field and nested object can get its own set of arguments, making GraphQL a complete replacement for making multiple APU fetches</li></ul></li><li><p>Fragments</p><ul><li>That’s the reusable units in GraphQL</li><li>Fragments let you construct sets of fields, and then include them in queries where you need to</li><li>It’s commonly used to split complicated application data requirements into smaller chunks</li></ul></li><li><p>Operation Name</p><ul><li>Operation Type<ul><li>Query</li><li>Mutation</li><li>Subscription</li></ul></li><li>Operation Name</li></ul></li><li><p>Variables</p><ul><li>It want to give dynamic power to graphql, as in most applications, the arguments to fields will be dynamic</li><li>Graphql supports this use case via variables</li><li>we could do:<ul><li>replace the static value in the query with <code>$variable</code></li><li>declare <code>$variable</code> as one of the variables accepted by the query</li><li>pass <code>variable: value</code> in the separate transport specific variables dictionary</li></ul></li><li>using variable could help us denote which arguments are expected to be dynamic</li><li>we should never do string interpolation to construct queries from user supplied values</li></ul></li></ul><pre><code class="ruby">query HeroNameAndFriends($episode: Episode) &#123;  hero(episode: $episode) &#123;    name    friends &#123;      name    &#125;  &#125;&#125;&#123;  &quot;episode&quot;: &quot;JEDI&quot;&#125; </code></pre><ul><li>Default variables</li></ul><pre><code class="ruby">query HeroNameAndFriends($episode: Episode = &quot;defaultOne&quot;) &#123;  hero(episode: $episode) &#123;    name    friends &#123;      name    &#125;  &#125;&#125;&#123;  &quot;episode&quot;: &quot;JEDI&quot;&#125; </code></pre><ul><li><p>Directives</p><ul><li>use this variable to dynamically change the structure and shape of our queries using variables</li><li><code>@include(if: Boolean)</code> only includes this field in the result if the argument is true</li><li><code>@skip(if: Boolean)</code> skip this field if the argument is true</li></ul></li><li><p>Mutations</p><ul><li>A way to modify server side data</li><li>A convention that any operations that cause writes should be sent explicitly via a mutation</li><li>!!! While query fields are executed in parallel, mutation fields run in series, one after the other<ul><li>means if we send two incrementCredits mutations in one request, the first is guranteed to finish before the second begins, ensuring that we don’t end up with a race condition with ourselves</li></ul></li></ul></li><li><p>Inline Fragments</p><ul><li><p>GraphQL schemas include the ability to define interfaces and union types</p></li><li><p>EG below, we need to return different attributes based on hero character</p><pre><code class="ruby">query HeroForEpisode($ep: Episode!) &#123;hero(episode: $ep) &#123;  name  ... on Droid &#123;    primaryFunction  &#125;  ... on Human &#123;    height  &#125;&#125;&#125;</code></pre></li></ul></li><li><p>Meta fields</p><ul><li>there are situations where you don’t know what type you’ll get back from the service</li><li>we need to determine how to handle that data on the client</li><li>we could use <code>__typename</code></li></ul></li></ul><h1 id="3-Schemas-and-Types"><a href="#3-Schemas-and-Types" class="headerlink" title="3. Schemas and Types"></a>3. Schemas and Types</h1><h2 id="3-1-how-the-schema-work"><a href="#3-1-how-the-schema-work" class="headerlink" title="3.1 how the schema work"></a>3.1 how the schema work</h2><ul><li><p>How does GraphQL work</p><ul><li><p>start with a <code>root</code> object</p></li><li><p>select the hero field on that</p></li><li><p>for the object returned by hero, we select the name and appearsIn fields</p><pre><code class="ruby">&#123;hero &#123;  name  appearsIn&#125;&#125;</code></pre></li></ul></li><li><p>we should know what we could query for</p><ul><li>an exact description of the data we can ask for</li><li>what kind of objects might they return</li><li>what fields are available on those sub objects</li></ul></li><li><p>Schema</p><ul><li>Each graphQL services defines a set of types which completely describe the set of possible data you can query on the service</li></ul></li></ul><h2 id="3-2-Type-Language"><a href="#3-2-Type-Language" class="headerlink" title="3.2 Type Language"></a>3.2 Type Language</h2><ul><li>GraphQL use its won Schema Language</li></ul><h3 id="3-2-1-Object-Types-and-Fields"><a href="#3-2-1-Object-Types-and-Fields" class="headerlink" title="3.2.1 Object Types and Fields"></a>3.2.1 Object Types and Fields</h3><ul><li>Object types<ul><li>represent a kind of object you can fetch from your service, and what fields it has</li></ul></li></ul><pre><code class="ruby">type Character &#123;  name: String!// means an array of Episode objects, notnull, 0 or more items, and each item would be an episode object   appearsIn: [Episode!]!&#125;</code></pre><h3 id="3-2-2-Query-and-Mutation-types"><a href="#3-2-2-Query-and-Mutation-types" class="headerlink" title="3.2.2 Query and Mutation types"></a>3.2.2 Query and Mutation types</h3><ul><li>Entry points into the schema</li></ul><h3 id="3-2-3-Scalar-Types"><a href="#3-2-3-Scalar-Types" class="headerlink" title="3.2.3 Scalar Types"></a>3.2.3 Scalar Types</h3><ul><li>Scalar types represent the leaves of the query</li><li>default scalar types<ul><li>Int</li><li>Float</li><li>String</li><li>Boolean</li><li>ID<ul><li>it represents a unique identifier</li><li>The ID type is serialized in the same way as a String, but it means it’s not intended to be human readable</li></ul></li></ul></li><li>We could also define our own Scalar type in this way<ul><li><code>scalar Date</code></li></ul></li></ul><h3 id="3-2-4-Enumeration-Types"><a href="#3-2-4-Enumeration-Types" class="headerlink" title="3.2.4 Enumeration Types"></a>3.2.4 Enumeration Types</h3><ul><li>Restricted to a particular set of allowed values</li><li>Allow you to<ul><li>validate that any arguments of this type are one of the allowed values</li><li>communicate through the type system that a field will always be one of a finite set of values</li></ul></li></ul><h3 id="3-2-5-Lists-and-Non-Null"><a href="#3-2-5-Lists-and-Non-Null" class="headerlink" title="3.2.5 Lists and Non-Null"></a>3.2.5 Lists and Non-Null</h3><pre><code>type Character &#123;  name: String!  appearsIn: [Episode]!&#125;</code></pre><ul><li>We could use <code>!</code> to indicate it should never return null</li><li>We could use <code>[]</code> to indicate that should be an array</li></ul><h3 id="3-2-6-Interfaces"><a href="#3-2-6-Interfaces" class="headerlink" title="3.2.6 Interfaces"></a>3.2.6 Interfaces</h3><ul><li>An abstract type that includes a certain set of fields that a type must include to implement the interface</li></ul><pre><code class="ruby">interface Character &#123;    id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!&#125;type Human implements Character &#123;  id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!  starships: [Starship]  totalCredits: Int&#125;type Droid implements Character &#123;  id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!  primaryFunction: String&#125;</code></pre><ul><li>Type implement the interface need to have all those fields, but they could also have their own fields</li></ul><h3 id="3-2-7-Union-Types"><a href="#3-2-7-Union-Types" class="headerlink" title="3.2.7 Union Types"></a>3.2.7 Union Types</h3><p><code>union SearchResult = Human | Droid | Starship</code></p><h3 id="3-2-8-Input-Types"><a href="#3-2-8-Input-Types" class="headerlink" title="3.2.8 Input Types"></a>3.2.8 Input Types</h3><ul><li>we need to pass complex objects especially when we are using mutations, where we want to pass in a whole object to be created</li></ul><pre><code class="ruby">input ReviewInput &#123;  stars: Int!  commentary: String&#125;mutation CreateReviewForEpisode($ep: Episode!, $review: ReviewInput!) &#123;  createReview(episode: $ep, review: $review) &#123;    stars    commentary  &#125;&#125;</code></pre><h1 id="4-Validation"><a href="#4-Validation" class="headerlink" title="4. Validation"></a>4. Validation</h1><ul><li>Graph ql has validation module to fulfill the validation phase of fulfilling a graphQL result</li></ul><h1 id="5-Execution"><a href="#5-Execution" class="headerlink" title="5. Execution"></a>5. Execution</h1><h2 id="5-1-Resolvers"><a href="#5-1-Resolvers" class="headerlink" title="5.1 Resolvers"></a>5.1 Resolvers</h2><ul><li>Each field in a GraphQL query is a function or method of the previous type which returns the next type</li><li>Each field is backed by a function called the resolver. When a field is executed, the corresponding resolver is called to produce the next value</li><li>The resolver continue to work until reach scalar values</li></ul><h2 id="5-2-Root-fields"><a href="#5-2-Root-fields" class="headerlink" title="5.2 Root fields"></a>5.2 Root fields</h2><pre><code class="ruby">Query: &#123;  human(obj, args, context, info) &#123;    return context.db.loadHumanByID(args.id).then(      userData =&gt; new Human(userData)    )  &#125;&#125;</code></pre><ul><li>obj<ul><li>previous object</li></ul></li><li>args<ul><li>arguments provided to the field in the graphQL query</li></ul></li><li>context<ul><li>a value which is provided to every resolver and holds important contextual information like the currently logged in user, or access to a database</li></ul></li><li>info<ul><li>a value which holds field specific information relevant to the current query as well as the schema details</li></ul></li></ul><h1 id="6-Best-Practices"><a href="#6-Best-Practices" class="headerlink" title="6. Best Practices"></a>6. Best Practices</h1><h2 id="6-1-HTTP"><a href="#6-1-HTTP" class="headerlink" title="6.1 HTTP"></a>6.1 HTTP</h2><ul><li><p>Mostly use HTTP with graphQL</p></li><li><p>Normally web frameworks use a pipeline model where requests are passed through a stack of middle ware</p></li><li><p>Requests could be inspected, transformed, modified or terminated with a response</p></li><li><p>GraphQL should be placed after all authentication middleware— thus you have access to the same session and user info you would in your HTTP endpoints handler</p></li><li><p>GraphQL server operates on a single URL/ endpoint, usually <code>graphql</code> , and all graphql requests for a given service should be directed at this endpoint</p></li></ul><h2 id="6-2-JSON-with-GZIP"><a href="#6-2-JSON-with-GZIP" class="headerlink" title="6.2 JSON with GZIP"></a>6.2 JSON with GZIP</h2><ul><li>typically respond using JSON, and we compress it with GZIP</li></ul><h2 id="6-3-Versioning"><a href="#6-3-Versioning" class="headerlink" title="6.3 Versioning"></a>6.3 Versioning</h2><ul><li>No need to do versioning for graphql api</li><li>Why do most APIs version? When there’s limited control over the data that’s returned from an API endpoint, <em>any change</em> can be considered a breaking change, and breaking changes require a new version. If adding new features to an API requires a new version, then a tradeoff emerges between releasing often and having many incremental versions versus the understandability and maintainability of the API.</li><li>In contrast, GraphQL only returns the data that’s explicitly requested, so new capabilities can be added via new types and new fields on those types without creating a breaking change. This has led to a common practice of always avoiding breaking changes and serving a versionless API.</li></ul><h2 id="6-4-Nullability"><a href="#6-4-Nullability" class="headerlink" title="6.4 Nullability"></a>6.4 Nullability</h2><ul><li>GraphQL default to nullable unless you specifically declare nonnull</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.graphql-java-kickstart.com/">https://www.graphql-java-kickstart.com/</a>  </li><li><a href="https://graphql.org/learn/">https://graphql.org/learn/</a>  </li><li><a href="https://www.apollographql.com/docs/federation/">https://www.apollographql.com/docs/federation/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;GraphQL-Read&quot;&gt;&lt;a href=&quot;#GraphQL-Read&quot; class=&quot;headerlink&quot; title=&quot;GraphQL Read&quot;&gt;&lt;/a&gt;GraphQL Read&lt;/h1&gt;&lt;h1 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="Web" scheme="https://www.llchen60.com/categories/Web/"/>
    
    
  </entry>
  
  <entry>
    <title>如何缓解疲劳</title>
    <link href="https://www.llchen60.com/%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3%E7%96%B2%E5%8A%B3/"/>
    <id>https://www.llchen60.com/%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3%E7%96%B2%E5%8A%B3/</id>
    <published>2021-11-02T13:31:04.000Z</published>
    <updated>2021-11-02T13:34:58.934Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/11/02/zEvGV8uDnJxoITr.png" alt="如何抵抗缓解疲劳.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/11/02/zEvGV8uDnJxoITr.png&quot; alt=&quot;如何抵抗缓解疲劳.png&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>碳达峰与碳中和</title>
    <link href="https://www.llchen60.com/%E7%A2%B3%E8%BE%BE%E5%B3%B0%E4%B8%8E%E7%A2%B3%E4%B8%AD%E5%92%8C/"/>
    <id>https://www.llchen60.com/%E7%A2%B3%E8%BE%BE%E5%B3%B0%E4%B8%8E%E7%A2%B3%E4%B8%AD%E5%92%8C/</id>
    <published>2021-09-30T02:21:14.000Z</published>
    <updated>2021-09-30T02:22:44.858Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/09/30/XGS6Qey4qHki2FC.png" alt="碳达峰与碳中和"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/09/30/XGS6Qey4qHki2FC.png&quot; alt=&quot;碳达峰与碳中和&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Cassandra</title>
    <link href="https://www.llchen60.com/Cassandra/"/>
    <id>https://www.llchen60.com/Cassandra/</id>
    <published>2021-09-17T13:05:55.000Z</published>
    <updated>2021-09-17T13:41:43.051Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/09/17/IGdzKBJwD9TbZ6f.png" alt="Cassandra MindMap.png"></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><h2 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h2><ul><li>We wanna have a distributed and scalable system that can store a <strong>huge amount of structured data</strong>, which is indexed by a row key where each row can have an <strong>unbounded</strong> number of columns.</li><li>Cassandra was originally developed at Facebook in 2007 for index search feature. It’s designed to provide scalability, availability, and reliability to store large amounts of data.</li><li>It combines nature of Dynamo which is a <strong>key value store</strong> and the data model of Bigtable which is a <strong>column based</strong> data store</li><li>Cassandra is in favor of availability and partition tolerance, it could be tuned with <strong>replication factor</strong> and <strong>consistency levels</strong> to meet <strong>strong consistency</strong> requirements, and of course with a performance cost.</li><li>It uses peer to peer architecture, with each node connected to all other nodes</li><li>Each Cassandra node performs all database operations and can serve client requests without the need for any leader node.</li></ul><h2 id="1-2-Use-cases"><a href="#1-2-Use-cases" class="headerlink" title="1.2 Use cases"></a>1.2 Use cases</h2><ul><li>Store key value data with high availability</li><li>Time series data model<ul><li>Due to its data model and log structured storage engine, cassandra benefits from high performing write operations, This also make it well suited for storing and analyzing sequentially captured metrics</li></ul></li><li>Write Heavy Applications<ul><li>Suited for write intensive applications such as time series streaming services, sensor logs, and IoT applications</li></ul></li></ul><h1 id="2-High-Level-Architecture"><a href="#2-High-Level-Architecture" class="headerlink" title="2. High Level Architecture"></a>2. High Level Architecture</h1><h2 id="2-1-Common-Terms"><a href="#2-1-Common-Terms" class="headerlink" title="2.1 Common Terms"></a>2.1 Common Terms</h2><p><img src="https://i.loli.net/2021/09/17/IrfBD5HFqAX76NJ.png" alt="Primary and Clustering Keys"></p><ul><li>Column<ul><li>A key value pair and is the most basic unit of data structure</li><li>Column Key: Uniquely identifies a column in a row</li><li>Column Value: Store a value or a collection of values</li></ul></li><li>Row<ul><li>A container for columns referenced by primary key. Cassandra does not store a column that has a null value, this saves a lot of space</li></ul></li><li>Table<ul><li>A container of rows</li></ul></li><li>Keyspace<ul><li>A container for tables that span over one or more Cassandra nodes</li></ul></li><li>Cluster<ul><li>Container of Keyspace</li></ul></li><li>Node<ul><li>A computer system running an instance of Cassandra,</li><li>Can be a physical host, a machine instance in the cloud or even a docker container</li></ul></li></ul><h2 id="2-2-Data-Partitioning"><a href="#2-2-Data-Partitioning" class="headerlink" title="2.2 Data Partitioning"></a>2.2 Data Partitioning</h2><ul><li>Cassandra use consistent hashing as DynamoDB does</li></ul><h2 id="2-3-Primary-Key"><a href="#2-3-Primary-Key" class="headerlink" title="2.3 Primary Key"></a>2.3 Primary Key</h2><ul><li>The primary key consists of two parts:  E.G Primary Key as (city_id, employee_id)<ul><li>Partition Key<ul><li>Decides how data is distributed across nodes</li><li>city_id is the primary key, means the data will be partitioned by the city_id field, all rows with the same city_id will reside on the same node</li></ul></li><li>Clustering Key<ul><li>Decides how data is stored within a node</li><li>We could have multiple clustering keys, clustering columns specify the order that the data is arranged on a node.</li><li>employee_id is the clustering key. Within each node, the data is stored in sorted order according to the employee_id column.</li></ul></li></ul></li></ul><h2 id="2-4-Partitioner"><a href="#2-4-Partitioner" class="headerlink" title="2.4 Partitioner"></a>2.4 Partitioner</h2><p><img src="https://i.loli.net/2021/09/17/3NdkOaXUpbgnWq9.png" alt="Partitioner Flow"></p><ul><li>Responsible for determining how data is distributed on the consistent hash ring.</li><li>Cassandra use <strong>Murmur3 hashing function</strong> — which will always produce the same hash for a given partition key</li><li>All Cassandra nodes learn about the <strong>token assignments of other nodes</strong> through gossip. This means any node can handle a request for any other node’s range. The node receiving the request is called the <strong>coordinator</strong>, and any node can act in this role. If a key does not belong to the coordinator’s range, it <strong>forwards the request</strong> to the replicas responsible for that range.</li></ul><h2 id="2-5-Coordinator-Node"><a href="#2-5-Coordinator-Node" class="headerlink" title="2.5 Coordinator Node"></a>2.5 Coordinator Node</h2><ul><li>A client may connect to any node in the cluster to initiate a read or write query. This node is known as the coordinator node, the coordinator identifies the nodes responsible for the data that is being written or read    and forwards the queries to them</li></ul><h1 id="3-Low-Level-Architecture"><a href="#3-Low-Level-Architecture" class="headerlink" title="3. Low Level Architecture"></a>3. Low Level Architecture</h1><h2 id="3-1-Replication-Strategy"><a href="#3-1-Replication-Strategy" class="headerlink" title="3.1 Replication Strategy"></a>3.1 Replication Strategy</h2><ul><li><p>Each node in Cassandra serves as a replica for a different range of data.</p></li><li><p>It stores <strong>multiple copies of data</strong> and <strong>spreads them across various replicas</strong>.</p></li><li><p>The replication behavior is controlled by two factors</p><ul><li><p>Replication Factor</p><ul><li>Decides how many replicas the system will have</li><li>This represents the <strong>number of nodes that will receive the copy of the same data</strong></li><li>Each keyspace in cassandra can have a different replication factor</li></ul></li><li><p>Replication Strategy</p><ul><li><p>Decides which nodes will be responsible for the replicas</p></li><li><p>The node that owns the range in which the hash of the partition key falls will be the first replica</p></li><li><p>All the additional replicas are placed on the <strong>consecutive nodes</strong></p></li><li><p>Cassandra places the subsequent replicas on the next nodes in a clockwise manner</p></li><li><p>Two kinds of replication strategies</p><ul><li><p>Simple Replication Strategy</p><p>  <img src="https://i.loli.net/2021/09/17/cnz12lGFWEPw4fS.png" alt="Simple Replication Strategy"></p><ul><li>Used for a <strong>single data center cluster</strong></li><li>Cassandra places the first replica on a node determined by the partitioner and the subsequent replicas on the next node in a clockwise manner</li></ul></li><li><p>Network Topology Strategy</p><p>  <img src="https://i.loli.net/2021/09/17/TSAZbXKCYf9IsoN.png" alt="Network Topology Strategy"></p><ul><li>Used for multiple data centers</li><li>We can specify different replication factors for different data centers. We could then specify how many replicas will be placed in each data center</li><li>Additional replicas, in the same data center, are placed by <strong>walking the ring clockwise until reaching the 1st node in another rack</strong>. This is done to guard against a complete rack failure, as nodes in the same rack(or similar physical grouping) tend to fail together due to power, cooling or network issues.</li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="3-2-Consistency-Levels"><a href="#3-2-Consistency-Levels" class="headerlink" title="3.2 Consistency Levels"></a>3.2 Consistency Levels</h2><ul><li>Definition<ul><li><strong>Minimum number of nodes</strong> that must fulfill a read or write operation before the operation can be considered successful</li><li>It allows use to <strong>specify different consistency levels</strong> for read and write</li><li>It also has <strong>tunable consistency level</strong></li><li>Tradeoff between consistency and response time<ul><li>As a higher consistency level means more nodes need to respond to a read or write query, giving user more assurance that the values present on each replica are the same</li></ul></li></ul></li></ul><h3 id="3-2-1-Write-Consistency-Levels"><a href="#3-2-1-Write-Consistency-Levels" class="headerlink" title="3.2.1 Write Consistency Levels"></a>3.2.1 Write Consistency Levels</h3><ul><li>Consistency Levels specify how many replica nodes must respond for the write to be reported as successful to the client</li><li>Level is specified <strong>per query by the client</strong></li><li>Cassandra is eventually consistent, updates to other replica nodes may continue in the background</li><li>How does Cassandra perform a write operation?<ul><li>Coordinator node contacts all replicas, as determined by the <strong>replication factor</strong> , and consider the write successful when a number of replicas equal to the consistency level acknowledge the write</li></ul></li><li>Write Consistency Levels List:<ul><li>One/ Two/ Three<ul><li>The data must be written to at least the specified number of replica nodes before a write is considered successful</li></ul></li><li>Quorum<ul><li>Data must be written to at least a quorum of replica nodes</li><li>Quorum is defined as <code>floor(RF/2 + 1)</code>  RF represents replication factor</li></ul></li><li>All<ul><li>ensures the data is written to all replica nodes</li><li>provides the highest consistency but lowest availability as writes will fail if any replica is down</li></ul></li><li>Local Quorum<ul><li>Ensure that data is written to a quorum of nodes in the same datacenter as the coordinator</li><li>Does not wait for the response from the other data centers</li></ul></li><li>Each Quorum<ul><li>Ensures that the data is written to a quorum of nodes in each datacenter</li></ul></li><li>Any<ul><li>The data must be written to at least one node</li><li>In the extreme case, when all replica nodes for the given partition key are down, the write can still succeed after a hinted handoff (see 3.2.4 section) has been written.<ul><li>In this case, an any write could succeed with hinted handoff, but it will not be readable until the replica nodes for that partition has recovered and the latest data is written on them</li></ul></li></ul></li></ul></li></ul><h3 id="3-2-2-Read-Consistency-Levels"><a href="#3-2-2-Read-Consistency-Levels" class="headerlink" title="3.2.2 Read Consistency Levels"></a>3.2.2 Read Consistency Levels</h3><ul><li><p>Read Query Consistency Level specify how many replica nodes must respond to a read request before returning the data</p></li><li><p>It has the same consistency levels for read operations as that of write operations exception Each_Quorum cause it’s too expensive</p></li><li><p>To achieve strong consistency, we need to do <code>R + W &gt; RF</code> R represents read replica count, W represents write replication count, RF represents replication factor</p><ul><li>All client reads will see the most recent write in this scenario, and we will have strong consistency</li></ul></li><li><p>How does Cassandra perform a read operation?</p><ul><li><p>Coordinator always sends the read request to the fastest node</p><ul><li>E.G  for quorum=2, the coordinator sends the requests to the fastest node and the <strong>digest of the data</strong> from the second fastest node<ul><li>digest is the checksum of the data, we use this to save network bandwidth</li></ul></li></ul></li><li><p>if the digest doesn’t match, means some replica do not have the latest version of data</p><ul><li><p>Coordinator then <strong>reads the data from all the replicas</strong> to determine the latest data</p></li><li><p>Then coordinator <strong>returns the latest data to the client and initiates a read repair request</strong></p></li><li><p>The read repair request will help push the newer version of data to nodes with the older version</p><p>  <img src="https://i.loli.net/2021/09/17/sPM6HnKthBpT945.png" alt="Read Operation with Snitch"></p></li></ul></li><li><p>latest write timestamp is used as a mark for the correct version of data, read repair operation is performed only in a portion of the total reads to avoid performance degradation</p></li></ul></li></ul><h3 id="3-2-3-Snitch"><a href="#3-2-3-Snitch" class="headerlink" title="3.2.3 Snitch"></a>3.2.3 Snitch</h3><ul><li><p>Functions</p><ul><li>Application that determines the proximity of nodes within the ring, also tells which nodes are faster — monitor the read latencies</li><li>It keeps track of the network topology of Cassandra nodes, determines which <strong>data centers and racks</strong> nodes belong to</li><li>Replication strategy use this information provided by the Snitch to spread the replicas across the cluster intelligently. It could do its best by not having more than one replica on the same rack</li></ul></li><li><p>Cassandra nodes use this info to route read/ write requests efficiently</p><p>  <img src="https://i.loli.net/2021/09/17/ZnaOJqIvgdcMmAW.png" alt="Request when set consistency to one"></p></li></ul><h3 id="3-2-4-Hinted-Handoff"><a href="#3-2-4-Hinted-Handoff" class="headerlink" title="3.2.4 Hinted Handoff"></a>3.2.4 Hinted Handoff</h3><p><img src="https://i.loli.net/2021/09/17/QvSmb5wntE2AHJd.png" alt="Hinted Handoff"></p><ul><li>To let Cassandra still serve write requests even when nodes are down</li><li>When a node is down, the coordinator nodes <strong>writes a hint in a text file on local disk</strong><ul><li>Hint contains the data itself along with information about which node the data belongs to</li><li>Recover from gossiper — When the coordinator node discovers from the gossiper that a node for which it holds hints has recovered, it forwards the write request for each hint to the target</li><li>Recover from routine call — each node every ten minutes checks to see if the failing node, for which it is holding any hints, has recovered</li></ul></li><li>With consistency level ‘Any,’<ul><li>if all the replica nodes are down, the coordinator node will <strong>write the hints for all the nodes and report success to the client.</strong></li><li>However, this data will <strong>not reappear in any subsequent reads</strong> until one of the replica nodes comes back online, and the coordinator node successfully forwards the write requests to it.</li><li>This is assuming that the coordinator node is up when the replica node comes back.</li><li>This also means that we can lose our data if the coordinator node dies and never comes back. For this reason, we should avoid using the ‘Any’ consistency level</li></ul></li><li>For node offline for quite long<ul><li>Hints can build up considerably on other nodes</li><li>When it back online, other nodes tend to flood that node with write requests</li><li>It would cause issues on the node, as it is already trying to come back after a failure</li><li>To address this, Cassandra <strong>limits the storage of hints to a configurable time window</strong></li><li>By default, set the time window to 3 hours. Post that, older hints will be removed  — now the recovered nodes will have stale data<ul><li>The stale data would be fixed during the read path, it will issue a read repair when it sees the stale data</li></ul></li></ul></li><li>When the cluster cannot meet the consistency level specified by the client, Cassandra fails the write request and does not store a hint .</li></ul><h2 id="3-3-Gossiper"><a href="#3-3-Gossiper" class="headerlink" title="3.3 Gossiper"></a>3.3 Gossiper</h2><h3 id="3-3-1-How-does-Cassandra-use-Gossip-Protocol"><a href="#3-3-1-How-does-Cassandra-use-Gossip-Protocol" class="headerlink" title="3.3.1 How does Cassandra use Gossip Protocol?"></a>3.3.1 How does Cassandra use Gossip Protocol?</h3><ul><li>What’s for?<ul><li>Cassandra uses gossip protocol that allows each node to keep track of state information about the other nodes in the cluster.</li><li>It’s a Peer to Peer communication mechanism in which nodes <strong>periodically exchange state information about themselves and other nodes they know about</strong></li></ul></li><li>How it works?<ul><li>Each node initiates a gossip round every second to exchange state info about themselves with one to three other random nodes</li><li>Each gossip message has a version associated with it, so during a gossip exchange, older info is overwritten with the most current state for a particular node</li></ul></li><li>Generation number<ul><li>Each node stores a generation number which will be incremented every time a node restart</li><li>Node receiving the gossip message can compare the generation number it knows and the gossip message’s generation number</li><li>If the generation number in the gossip message is higher, it knows the node was restarted</li></ul></li><li>Seed nodes<ul><li>For node starting up for the first time</li><li>Assist in gossip convergence, thus guarantee schema/ state changes propagate regularly</li></ul></li></ul><h3 id="3-3-2-Node-Failure-Detection"><a href="#3-3-2-Node-Failure-Detection" class="headerlink" title="3.3.2 Node Failure Detection"></a>3.3.2 Node Failure Detection</h3><ul><li>Disadvantages for heartbeat<ul><li>outputs a boolean value telling us if the system is alive or not;</li><li>there is no middle ground.</li><li>Heartbeating uses a fixed timeout, and if there is no heartbeat from a server, the system, after the timeout, assumes that the server has crashed.</li><li>If we keep the timeout short, the system will be able to detect failures quickly but with many false positives due to slow machines or faulty networks.</li><li>On the other hand, if we keep the timeout long, the false positives will be reduced, but the system will not perform efficiently for being slow in detecting failures.</li></ul></li><li>Use adaptive failure detection mechanism  —— Phi Accrual Failure Detector<ul><li>Use historical heartbeat information to make the threshold adaptive</li><li>It outputs the suspicion level about a server</li><li>As a node’s suspicion level increases, the system can gradually decide to stop sending new requests to it</li><li>It makes the distributed system efficient as it takes into account fluctuations in the network env and other intermittent server issues before declaring a system completely dead</li></ul></li></ul><h2 id="3-4-Anatomy-of-Cassandra’s-Write-Operation"><a href="#3-4-Anatomy-of-Cassandra’s-Write-Operation" class="headerlink" title="3.4 Anatomy of Cassandra’s Write Operation"></a>3.4 Anatomy of Cassandra’s Write Operation</h2><p>Cassandra stores data both <strong>in memory and on disk</strong> to provide both high performance and durability. Every write includes a timestamp, write path involves a lot of components: </p><p><img src="https://i.loli.net/2021/09/17/LrMK7ckIS2zEsU1.png" alt="Write Path"></p><ul><li>Each write is appended to a commit log, which is stored on disk</li><li>It is then written to Memtable in memory</li><li>Periodically, MemTables are flushed to SSTables on the disk</li><li>Periodically, compaction runs to merge SSTables</li></ul><h3 id="3-4-1-Commit-Log"><a href="#3-4-1-Commit-Log" class="headerlink" title="3.4.1 Commit Log"></a>3.4.1 Commit Log</h3><ul><li>When a node receives a write request, it immediately writes data to a commit log</li><li>Commit log is a <strong>write ahead log</strong> stored on disk</li><li>Used as a crash recovery mechanism to support Cassandra’s durability goals</li><li>A write will not be considered successful on the node until it’s <strong>written to the commit log</strong><ul><li>This ensures if a write operation does not make it to the in-memory store, it will still be possible to recover the data</li></ul></li><li>If we shut down the node or it crashes unexpectedly, the commit log can ensure that data is not lost; that’s because if the node restart, the commit log gets replayed</li></ul><h3 id="3-4-2-MemTable"><a href="#3-4-2-MemTable" class="headerlink" title="3.4.2 MemTable"></a>3.4.2 MemTable</h3><ul><li>After written to the commit log, the data is written to a memory resident data structure called memTable<ul><li>Each node has a MemTable in memory for each Cassandra table</li><li>Each MemTable contains data for a specific Cassandra table, and it resembles that table in memory</li><li>Each MemTable accrues writes and <strong>provides reads for data not yet flushed to disk</strong></li><li>Commit log stores all the writes in sequential order, with each new write appended to the end; whereas MemTable stores data in the sorted order of partition key and clustering columns</li><li>After writing data to the commit log and MemTable, the node <strong>sends an acknowledgement to the coordinator</strong> that the data has been successfully written</li></ul></li></ul><h3 id="3-4-3-SStable"><a href="#3-4-3-SStable" class="headerlink" title="3.4.3 SStable"></a>3.4.3 SStable</h3><ul><li>When the number of objects stored in the MemTable reaches a threshold, the contents of the MemTable are <strong>flushed to disk</strong> in a file called <strong>SSTable</strong><ul><li>At this point, a new MemTable is created to store subsequent data</li><li>The flush is non blocking operation</li><li>Multiple Memtables may exist for a single table<ul><li>One current, and the rest waiting to be flushed</li></ul></li><li>When the MemTable is flushed to SStables, <strong>corresponding entries in the commit log</strong> are removed</li></ul></li><li>SStable —Sorted String Table<ul><li>Once a MemTable is flushed to disk as an SStable, it is immutable and cannot be changed later</li><li>Each delete or update is considered as a new write operation</li></ul></li><li>The current data state of a Cassandra table consists of its MemTables in memory and SSTables on the disk.<ul><li>Therefore, on reads, Cassandra will read both SSTables and MemTables to find data values, as the MemTable may contain values that have not yet been flushed to the disk.</li><li>The MemTable works like a write-back cache that Cassandra looks up by key</li></ul></li></ul><p><img src="https://i.loli.net/2021/09/17/Qd7x4M6HRrtuAoZ.png" alt="Whole Write Path"></p><h2 id="3-5-Anatomy-of-Cassandra’s-Read-Operation"><a href="#3-5-Anatomy-of-Cassandra’s-Read-Operation" class="headerlink" title="3.5 Anatomy of Cassandra’s Read Operation"></a>3.5 Anatomy of Cassandra’s Read Operation</h2><p><img src="https://i.loli.net/2021/09/17/wIZKE97YqVNAsrP.png" alt="Whole Read Path"></p><h3 id="3-5-1-Caching"><a href="#3-5-1-Caching" class="headerlink" title="3.5.1 Caching"></a>3.5.1 Caching</h3><ul><li>Row Cache<ul><li>Cache frequently read/ hot rows</li><li>Stores a complete data row, which can be returned directly to the client if requested by a read operation</li><li>Could significantly speed up read access for frequently accessed rows, at the cost of more memory usage</li></ul></li><li>Key Cache<ul><li>Stores a map of recently read partition keys to their <strong>SSTable offsets</strong></li><li>This facilitates faster read access into SSTables and improves the read performance</li><li>Use less memory comparing with row cache and provides a considerable improvement for read operations</li></ul></li><li>Chunk Cache<ul><li>Chunk Cache is used to store umcompressed chunks of data read from SSTable files that are accessed frequently</li></ul></li></ul><h3 id="3-5-2-Read-From-MemTable"><a href="#3-5-2-Read-From-MemTable" class="headerlink" title="3.5.2 Read From MemTable"></a>3.5.2 Read From MemTable</h3><ul><li>When a read request come in, node performs a binary search on the partition key to find the required partition and then return the row</li></ul><h3 id="3-5-3-Read-From-SSTable"><a href="#3-5-3-Read-From-SSTable" class="headerlink" title="3.5.3 Read From SSTable"></a>3.5.3 Read From SSTable</h3><ul><li><p>Bloom Filters</p><ul><li>Each SSTable has a Bloom Filter associated with it, which tells if a particular key is present in it or not</li><li>Used to boost performance of read operations</li><li>It’s a very fast, non deterministic algorithms for testing whether an element is a member of a set</li><li>It’s possible to get a false positive but never a false negative</li><li>Theory<ul><li>It works by <strong>mapping the values in a data set into a bit array</strong> and <strong>condensing a larger data set into a digest string</strong> with a hash function</li><li>Filters are stored in memory and are used to improve performance by reducing the need for disk access on key lookups</li></ul></li></ul></li><li><p>How are SSTables stored on the disk?</p><ul><li><p>Consists of two files</p><ul><li><p>Data File</p><ul><li>Actual data is stored here</li><li>It has partitions and rows associated with those partitions</li><li>Partitions are in sorted order</li></ul></li><li><p>Partition Index File</p><ul><li><p>Stored on disk, partition index file stores the sorted partition keys mapped to their SSTable offsets</p></li><li><p>Enable locating a partition exactly in an SSTable rather than scanning data</p><p><img src="https://i.loli.net/2021/09/17/9gUpTXZyLSksDdK.png" alt="Read via Partition Index File"></p></li></ul></li></ul></li></ul></li><li><p>Partition Index Summary File</p><ul><li><p>It’s stored in memory, stores the summary of the partition index file for performance improvement</p><ul><li><p>Two level index, e.g, search for key=19</p></li><li><p>in partition index summary file, it lays to key range 10 - 21</p></li><li><p>then we could go to byte offset 32,</p></li><li><p>in partition index file , we start from 32, to find partition key 19, and then we could go to 5450</p><p><img src="https://i.loli.net/2021/09/17/efsVEmvGAkIldF6.png" alt="Read via Partition Index Summary File"></p></li></ul></li></ul></li><li><p>Read from KeyCache</p><ul><li><p>As the Key Cache stores a map of recently read partition keys to their SSTable offset, it’s the fastest way to find the required row in the SSTable</p><p>  <img src="https://i.loli.net/2021/09/17/5KPTohGmWpecr1a.png" alt="Read From KeyCache"></p></li></ul></li><li><p>Overall workflow</p><p>  <img src="https://i.loli.net/2021/09/17/2zKlRtS48NQYkud.png" alt="Overall Workflow"></p></li></ul><h2 id="3-6-Compaction"><a href="#3-6-Compaction" class="headerlink" title="3.6 Compaction"></a>3.6 Compaction</h2><h3 id="3-6-1-Why-we-need-compaction-And-How-it-Works"><a href="#3-6-1-Why-we-need-compaction-And-How-it-Works" class="headerlink" title="3.6.1 Why we need compaction? And How it Works?"></a>3.6.1 Why we need compaction? And How it Works?</h3><p><img src="https://i.loli.net/2021/09/17/2DgirVjkeq6AI4T.png" alt="Compaction"></p><ul><li>SSTables are immutable, which helps Cassandra achieve high write speeds</li><li>And flushing from MemTable to SSTable is a continuous process, which means we could have a large number of SSTables lying on the disk</li><li>It’s tedious to scan all these SSTables while reading</li><li>We need compaction thus we could merge multiple related SSTables into a single one to improve reading speed</li><li>During compaction, the data in SSTables is merged, keys are merged, columns are combined, obsolete values are discarded, and a new index is created</li></ul><h3 id="3-6-2-Compaction-Strategies"><a href="#3-6-2-Compaction-Strategies" class="headerlink" title="3.6.2 Compaction Strategies"></a>3.6.2 Compaction Strategies</h3><ul><li>SizeTiered Compaction Strategy<ul><li>Suitable for insert-heavy and general workloads</li><li>Triggered when multiple SSTables of a similar size are present</li></ul></li><li>Leveled Compaction Strategy<ul><li>Optimize read performance</li><li>Groups SSTables into levels, each of which has a fixed size limit which is ten times larger than the previous level</li></ul></li><li>Time Window Compaction Strategy<ul><li>Work on time series data</li><li>Compact SSTables within a configured time window</li><li>Ideal for time series data which is immutable after a fixed time interval</li></ul></li></ul><h3 id="3-6-3-Sequential-Writes"><a href="#3-6-3-Sequential-Writes" class="headerlink" title="3.6.3 Sequential Writes"></a>3.6.3 Sequential Writes</h3><ul><li>Main reason that writes perform so well in Cassandra</li><li>No reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations</li><li>Compaction is intended to amortize the reorganization of data, but it uses sequential I/O to do so, which makes it efficient</li></ul><h2 id="3-7-Tombstones"><a href="#3-7-Tombstones" class="headerlink" title="3.7 Tombstones"></a>3.7 Tombstones</h2><h3 id="3-7-1-What-are-Tombstones"><a href="#3-7-1-What-are-Tombstones" class="headerlink" title="3.7.1 What are Tombstones?"></a>3.7.1 What are Tombstones?</h3><ul><li>Scenario<ul><li>We delete some data for a node that is down or unreachable, it would miss a delete</li><li>When the node com back online later and a repair occurs, the node could resurrect the data due to re-sharing it with other nodes</li><li>To prevent deleted data from being reintroduced, Cassandra used a concept of a Tombstone</li></ul></li><li>Tombstone<ul><li>Similar to the idea of soft delete from the relational database</li><li>When we delete, Cassandra does not delete it right away, instead, it associated a tombstone with it, with Time to Expiry</li><li>It’s a marker to indicate data that has been deleted</li><li>When we execute a delete operation, data is not immediately deleted</li><li>Instead, it’s treated as an update operation that places a tombstone on the value</li><li>Default Time to Expiry is set to 10 days<ul><li>If the node is down longer than this value, it should be treated as failed and replaced</li></ul></li><li>Tombstones are removed as part of compaction</li></ul></li></ul><h3 id="3-7-2-Common-problems-associated-with-Tombstones"><a href="#3-7-2-Common-problems-associated-with-Tombstones" class="headerlink" title="3.7.2 Common problems associated with Tombstones"></a>3.7.2 Common problems associated with Tombstones</h3><ul><li>Takes storage space</li><li>When a table accumulates many tombstones, read queries on that table could become slow and can cause serious performance problems like timeouts.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/09/17/IGdzKBJwD9TbZ6f.png&quot; alt=&quot;Cassandra MindMap.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introdu
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
      <category term="Cassandra" scheme="https://www.llchen60.com/tags/Cassandra/"/>
    
  </entry>
  
</feed>
