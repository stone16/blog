<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leilei&#39;s Blog | 磊磊的博客</title>
  
  <subtitle>Because it&#39;s there</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.llchen60.com/"/>
  <updated>2022-01-01T13:57:52.879Z</updated>
  <id>https://www.llchen60.com/</id>
  
  <author>
    <name>Leilei Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flyway</title>
    <link href="https://www.llchen60.com/Flyway/"/>
    <id>https://www.llchen60.com/Flyway/</id>
    <published>2022-01-01T13:57:09.000Z</published>
    <updated>2022-01-01T13:57:52.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>An open source database migration tool</li><li>Favors simplicity and convention over configuration</li><li>has 7 basic commands<ul><li>migrate</li><li>clean</li><li>info</li><li>validate</li><li>undo</li><li>baseline</li><li>repair</li></ul></li></ul><h2 id="1-1-Why-DB-migrations"><a href="#1-1-Why-DB-migrations" class="headerlink" title="1.1 Why DB migrations?"></a>1.1 Why DB migrations?</h2><ul><li>we need a way to version the table</li><li>we need to know what state is the db on this machine</li><li>database migration help us<ul><li>recreate a database from scratch</li><li>make it clear at all times what state a database is in</li><li>migrate in a deterministic way from your current version of the database to a newer one</li></ul></li></ul><h2 id="1-2-How-Flyway-works"><a href="#1-2-How-Flyway-works" class="headerlink" title="1.2 How Flyway works?"></a>1.2 How Flyway works?</h2><ul><li><p>Flyway first try to locate its schema history table</p><ul><li>if db empty, then flyway will create it instead</li><li>this default db named as <code>flyway_schema_history</code></li></ul></li><li><p>Then flyway will begin scanning the filesystem or the classpath of the application for migrations</p></li><li><p>The migrations are then sorted based on the version number and applied in order</p></li><li><p>The schema history table will be updated accordingly as each migration gets applied</p></li><li><p>we use <code>flyway migrate</code> to execute the migration</p></li></ul><h1 id="2-Flyway-Commands"><a href="#2-Flyway-Commands" class="headerlink" title="2. Flyway Commands"></a>2. Flyway Commands</h1><h2 id="2-1-migrate"><a href="#2-1-migrate" class="headerlink" title="2.1 migrate"></a>2.1 <code>migrate</code></h2><ul><li>help migrate the db</li></ul><h2 id="2-2-clean"><a href="#2-2-clean" class="headerlink" title="2.2 clean"></a>2.2 <code>clean</code></h2><ul><li>drop all objects in the confgured schemas</li></ul><h2 id="2-3-info"><a href="#2-3-info" class="headerlink" title="2.3 info"></a>2.3 <code>info</code></h2><ul><li>print the details and status information about all migrations</li></ul><h2 id="2-4-validate"><a href="#2-4-validate" class="headerlink" title="2.4 validate"></a>2.4 <code>validate</code></h2><ul><li>validates the applied migrations against the ones available on the classpath</li></ul><h2 id="2-5-undo"><a href="#2-5-undo" class="headerlink" title="2.5 undo"></a>2.5 <code>undo</code></h2><ul><li>undoes the most recently applied versioned migration</li></ul><h2 id="2-6-baseline"><a href="#2-6-baseline" class="headerlink" title="2.6 baseline"></a>2.6 <code>baseline</code></h2><ul><li>baselines an existing database, excluding all migrations up and including baseline version</li></ul><h2 id="2-7-repair"><a href="#2-7-repair" class="headerlink" title="2.7 repair"></a>2.7 <code>repair</code></h2><ul><li>repair the schema history table</li></ul><h1 id="3-Concepts"><a href="#3-Concepts" class="headerlink" title="3. Concepts"></a>3. Concepts</h1><h2 id="3-1-Migrations"><a href="#3-1-Migrations" class="headerlink" title="3.1 Migrations"></a>3.1 Migrations</h2><ul><li>all changes to the db are called migrations</li><li>migrations can be<ul><li>versioned<ul><li>types<ul><li>regular</li><li>undo<ul><li>the effect can be undone by supplying an undo migration</li></ul></li></ul></li><li>contains<ul><li><strong>version</strong><ul><li>must be unique</li></ul></li><li><strong>description</strong><ul><li>purely informative for you to be able to remember what each migration does</li></ul></li><li><strong>checksum</strong><ul><li>detect accidental changes</li></ul></li></ul></li></ul></li><li>repeatable<ul><li>contains<ul><li><strong>description</strong></li><li><strong>checksum</strong></li></ul></li><li>instead of being run just once, they are re-applied every time their checksum changes</li><li>Within a single migration run, repeatable migrations are always <strong>applied last</strong>, after all pending versioned migrations have been executed. Repeatable migrations are applied in the order of their description</li></ul></li></ul></li></ul><h3 id="3-1-1-Versioned-Migrations"><a href="#3-1-1-Versioned-Migrations" class="headerlink" title="3.1.1 Versioned Migrations"></a>3.1.1 Versioned Migrations</h3><ul><li>contains<ul><li>version<ul><li><strong>must be unique</strong></li><li>applied in order exactly once</li></ul></li><li>description<ul><li>purely informative for you to be able to remember what each migration does</li></ul></li><li>checksum<ul><li>detect accidental changes</li></ul></li></ul></li><li>used for<ul><li>creating/ altering/ dropping tables/ indexes/ foreign keys/ enums</li><li>reference data updates</li><li>user data corrections</li></ul></li></ul><h3 id="3-1-2-Undo-Migrations"><a href="#3-1-2-Undo-Migrations" class="headerlink" title="3.1.2 Undo Migrations"></a>3.1.2 Undo Migrations</h3><ul><li>A migration can fail at any point. If you have 10 statements, it is possible for the 1st, the 5th, the 7th or the 10th to fail. There is simply no way to know in advance. In contrast, undo migrations are written to undo an entire versioned migration and will not help under such conditions.</li><li>we should <strong>maintain backwards compatibility</strong> between the <strong>DB and all versions of the code</strong> currently deployed in production</li></ul><h3 id="3-1-3-Repeatable-Migrations"><a href="#3-1-3-Repeatable-Migrations" class="headerlink" title="3.1.3 Repeatable Migrations"></a>3.1.3 Repeatable Migrations</h3><ul><li><p>contains</p><ul><li>description and a checksum, but no version</li></ul></li><li><p>repeatable migrations are re-applied every time their checksum changes</p></li><li><p>Very useful for managing database objects whose definition can then simply be maintained in a single file in version control</p></li><li><p>Repeatable migrations are always applied last, after all pending versioned migrations have been executed; always applied in the order of their description</p></li></ul><h3 id="3-1-4-SQL-Based-Migrations"><a href="#3-1-4-SQL-Based-Migrations" class="headerlink" title="3.1.4 SQL Based Migrations"></a>3.1.4 SQL Based Migrations</h3><ul><li>used for<ul><li>DDL change — CREATE/ALTER/DROP statements for TABLES,VIEWS,TRIGGERS,SEQUENCES,…</li><li>Simple reference data changes</li><li>simple bulk data changes</li></ul></li><li>Naming  Patterns<ul><li>Prefix<ul><li>v for versioned</li><li>u for undo</li><li>r for repeatable migrations</li></ul></li><li>version<ul><li>with dots or underscores separate as many parts as you like</li></ul></li><li>Separator<ul><li>__ two underscores</li></ul></li><li>Suffix<ul><li><code>.sql</code></li></ul></li></ul></li><li>Discovery<ul><li>Flyway discover sql migrations from directories <strong>referenced by the location property</strong></li></ul></li></ul><h3 id="3-1-5-Script-Based-Migrations"><a href="#3-1-5-Script-Based-Migrations" class="headerlink" title="3.1.5 Script Based Migrations"></a>3.1.5 Script Based Migrations</h3><ul><li>name patten<ul><li>``V1__execute_batch_tool.sh`</li></ul></li><li>could be used for<ul><li>triggering execution of a 3rd party application as part of the migrations</li><li>cleaning up local files</li></ul></li></ul><h3 id="3-1-6-Transactions"><a href="#3-1-6-Transactions" class="headerlink" title="3.1.6 Transactions"></a>3.1.6 Transactions</h3><ul><li>By default, Flyway <strong>wraps the execution of an entire migration within a single transaction</strong></li></ul><h2 id="3-2-Callbacks"><a href="#3-2-Callbacks" class="headerlink" title="3.2 Callbacks"></a>3.2 Callbacks</h2><ul><li><p>For the case we need to execute same action over and over again</p></li><li><p>we could hook into its lifecycle</p></li><li><p>there are certain keywords we could use, and invoke them during the process</p></li></ul><p><a href="https://flywaydb.org/documentation/concepts/callbacks">https://flywaydb.org/documentation/concepts/callbacks</a> </p><h2 id="3-3-Error-Overrides"><a href="#3-3-Error-Overrides" class="headerlink" title="3.3 Error Overrides"></a>3.3 Error Overrides</h2><ul><li>By default, in case an error is returned, flyway displays it with all necessary details, marks the migration as failed and automatically rolls it back if possible</li><li>But we could change the behavior like<ul><li>treat an error as a waring</li><li>treat a waring as an error</li><li>perform an additional action</li></ul></li></ul><h2 id="3-4-Dry-Runs"><a href="#3-4-Dry-Runs" class="headerlink" title="3.4 Dry Runs"></a>3.4 Dry Runs</h2><ul><li>Used for<ul><li>preview changes Flyway will make to the db</li><li>submit the SQL statements for review</li><li>use Flyway to determine what needs updating,</li></ul></li><li>how it works<ul><li>flyway sets up a read only connection to the db,</li><li>assesses what migrations need to run and generates a single SQL file containing all statements it would have executed in case of a regular migration run</li></ul></li></ul><h2 id="3-5-Baseline-Migrations"><a href="#3-5-Baseline-Migrations" class="headerlink" title="3.5 Baseline Migrations"></a>3.5 Baseline Migrations</h2><ul><li><p>Over the lifetime of a project, there would be tons of db objects be created/ destroyed across many migrations</p><ul><li>we want to simplify with a single, cumulative migration that represents the state of db after all of those migrations have been applied without disrupting existing env</li></ul></li><li><p>How it works?</p><ul><li>Prefixed with B followed by the version of your db they represent</li><li>Only used when deploying to new env</li><li>If used in an env where some Flyway migrations have already been applied, <strong>baseline migrations will be ignored,</strong> <strong>new env will choose the latest baseline migration as the starting point</strong><ul><li>every migration with a version below the latest baseline migration’s version is marked as ignored</li></ul></li><li>baseline migration are executed during the migrate process</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://flywaydb.org/documentation/">https://flywaydb.org/documentation/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;An open source database migrat
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="database" scheme="https://www.llchen60.com/tags/database/"/>
    
      <category term="migration" scheme="https://www.llchen60.com/tags/migration/"/>
    
  </entry>
  
  <entry>
    <title>Protobuf Rampup</title>
    <link href="https://www.llchen60.com/Protobuf-Rampup/"/>
    <id>https://www.llchen60.com/Protobuf-Rampup/</id>
    <published>2021-12-25T02:09:49.000Z</published>
    <updated>2021-12-25T02:11:09.003Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Protobuf-Learning"><a href="#Protobuf-Learning" class="headerlink" title="Protobuf Learning"></a>Protobuf Learning</h1><h1 id="1-What-are-Protocol-Buffers"><a href="#1-What-are-Protocol-Buffers" class="headerlink" title="1. What are Protocol Buffers?"></a>1. What are Protocol Buffers?</h1><ul><li>Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler.</li><li>You <strong>define how you want your data to be structured</strong> once, then you <strong>use special generated source code to easily write and read your structured data</strong></li></ul><h1 id="2-Why-use-Protocol-Buffers"><a href="#2-Why-use-Protocol-Buffers" class="headerlink" title="2. Why use Protocol Buffers?"></a>2. Why use Protocol Buffers?</h1><ul><li>XML is human readable and wide language supports<ul><li>but is notoriously space intensive</li><li>encoding/ decoding can impose a huge performance penalty on applications</li></ul></li><li>With protocol buffers<ul><li>write a <code>.proto</code> description of the data structure</li><li>the <strong>protocol buffer compiler</strong> then <strong>creates a class</strong> that implements <strong>automatic encoding and parsing</strong> of the protocol buffer data with an <strong>efficient binary format</strong></li><li>the generated class <strong>provides getters and setters</strong> for the fields</li><li>take care of the details of reading and writing the protocol buffer <strong>as a unit</strong></li></ul></li></ul><h1 id="3-Java-Tutorial-In-Proto2"><a href="#3-Java-Tutorial-In-Proto2" class="headerlink" title="3. Java Tutorial (In Proto2)"></a>3. Java Tutorial (In Proto2)</h1><h2 id="3-1-Define-Protocol-Format"><a href="#3-1-Define-Protocol-Format" class="headerlink" title="3.1 Define Protocol Format"></a>3.1 Define Protocol Format</h2><pre><code class="protobuf">syntax = &quot;proto2&quot;;// starts with package delcaration // we should define this to get rid of name conflict package tutorial;// enable generating a separate .java file for each generated class option java_multiple_files = true;// specify in what java package name your generated classes should live// if not set here, it will simply match the pkg name given by the package declaration option java_package = &quot;com.example.tutorial.protos&quot;;// define the class name of the wrapper class which will represent this file // if not given, it will be auto generated by converting the file name to upper camel case option java_outer_classname = &quot;AddressBookProtos&quot;;/**Message Definition: An aggregate containing a set of typed fields Contain certain standard types    + boo1    + int32     + float     + double     + string we could also add further structure to msgs by using other msg types as field types + marker     + identify the unique tag field use in binary encoding     + try to use 1 - 15 as it neeeds one less byte+ modifier     + optional         + field may or may not be set         + if not, a default value will be used             + we could set our own default values             + or system will provide defaults                 + numeric types -- zero                 + strings -- empty string                 + bools -- false                 + embedded messages -- default instance or prototype of the message, which has none of its fields set     + repeated         + the field may be repeated any number of times [0, xxx)         + order will be preserved in the protocol buffer         + act like a dynamic sized array     + required         + a value for the field must be provided        + try to build an uninitialized msg will throw runtime exception         + parse an uninitialzied msg will throw IOException         + required is not favored as it cannot be backward compatible */message Person &#123;    // =1 marker identify the unique tag that field uses in the binary encoding   optional string name = 1;  optional int32 id = 2;  optional string email = 3;  enum PhoneType &#123;    MOBILE = 0;    HOME = 1;    WORK = 2;  &#125;  message PhoneNumber &#123;    optional string number = 1;    optional PhoneType type = 2 [default = HOME];  &#125;  repeated PhoneNumber phones = 4;&#125;message AddressBook &#123;  repeated Person people = 1;&#125;</code></pre><h2 id="3-2-Compiling-Protocol-Buffers"><a href="#3-2-Compiling-Protocol-Buffers" class="headerlink" title="3.2 Compiling Protocol Buffers"></a>3.2 Compiling Protocol Buffers</h2><ul><li>To generate the classes, we need to run the protocol buffer compiler</li><li>specify the source directory, the destination directory and the path to our <code>.proto</code></li></ul><pre><code class="protobuf">protoc -I=$SRC_DIR --java_out=$DST_DIR $SRC_DIR/addressbook.proto </code></pre><h2 id="3-3-Protocol-Buffer-API"><a href="#3-3-Protocol-Buffer-API" class="headerlink" title="3.3 Protocol Buffer API"></a>3.3 Protocol Buffer API</h2><ul><li>compiler helps auto generate source file<ul><li>getters and setters</li><li>each field also has <code>clear</code> method to set the field back to its empty state</li></ul></li><li>Builders vs Messages<ul><li>message classes are immutable</li><li>builder is used when you first construct a builder, then we could call the builder’s build() method</li></ul></li><li>standard message methods<ul><li><code>isInitialized</code> check if all the required fields have been set</li><li><code>toString</code> returns a human readable representation of the msg</li><li><code>mergeFrom(Message other)</code> merge the contents of other into this msg, overwrite singular scalar fields</li><li><code>clear</code> clear all the fields back to the empty state</li></ul></li><li>Parsing and Serialization<ul><li><code>byte[] toByteArray();</code><ul><li>serializes the msg and returns a byte array containing its raw bytes</li></ul></li><li><code>static xxx parseFrom(byte[] data);</code><ul><li>parse a msg from the given byte array</li></ul></li><li><code>void writeTo(OutputStream output);</code><ul><li>serialize the msg and writes to an OutputStream</li></ul></li><li><code>static xxx parseFrom(InputStream input);</code><ul><li>reads and parses a msg from an InputStream</li></ul></li></ul></li></ul><h2 id="3-4-How-to-extend-a-Protocol-Buffer"><a href="#3-4-How-to-extend-a-Protocol-Buffer" class="headerlink" title="3.4 How to extend a Protocol Buffer"></a>3.4 How to extend a Protocol Buffer</h2><ul><li>In the new version of the protocol buffer<ul><li>must not change the tag numbers of any existing fields</li><li>must not add or delete any required fields</li><li>may delete optional or repeated fields</li><li>may add new optional or repeated fields but must use fresh tag numbers</li></ul></li></ul><h1 id="4-Overall-Guide-In-Proto3"><a href="#4-Overall-Guide-In-Proto3" class="headerlink" title="4. Overall Guide (In Proto3)"></a>4. Overall Guide (In Proto3)</h1><h2 id="4-1-Define-message-type"><a href="#4-1-Define-message-type" class="headerlink" title="4.1 Define message type"></a>4.1 Define message type</h2><ul><li>Each field in the msg definition need to have a <strong>unique number</strong><ul><li>those numbers are used to identify fields in the message binary format</li><li>the number should never be changed</li></ul></li><li>specify field rules<ul><li>singular<ul><li>default field rule for proto3 syntax</li><li>can have <strong>zero or one of this field</strong></li></ul></li><li>repeated<ul><li>can be repeated any number of times (including zero)</li></ul></li></ul></li><li>reserved fields<ul><li>if you update a msg type by entirely removing a field or commenting it out, future users can reuse the field number but it would bring severe issues,</li><li>thus we could reserved the number for deleted fields and tag number</li></ul></li></ul><pre><code class="protobuf">message Foo &#123;  reserved 2, 15, 9 to 11;  reserved &quot;foo&quot;, &quot;bar&quot;;&#125;</code></pre><ul><li>Post compiler running<ul><li>Compiler generates a <code>.java</code> file with a class for each message type, as well as Builder classes for creating message class instances</li></ul></li><li>For enum values<ul><li>every enum definition must contain a constant that maps to zero as its first element</li><li>we can allow alias thus we could assign the same value to different enum constants</li></ul></li><li>import<ul><li>we could do import thus we could use definitions from other <code>.proto</code> file</li></ul></li></ul><h2 id="4-2-Scalar-Value-Types"><a href="#4-2-Scalar-Value-Types" class="headerlink" title="4.2 Scalar Value Types"></a>4.2 Scalar Value Types</h2><p><a href="https://developers.google.com/protocol-buffers/docs/proto3#scalar">Language Guide (proto3) | Protocol Buffers | Google Developers</a></p><h2 id="4-3-Nested-Types"><a href="#4-3-Nested-Types" class="headerlink" title="4.3 Nested Types"></a>4.3 Nested Types</h2><ul><li>we could define and use msg types inside other msg types</li></ul><pre><code class="protobuf">message SearchResponse &#123;  message Result &#123;    string url = 1;    string title = 2;    repeated string snippets = 3;  &#125;  repeated Result results = 1;&#125;// to use the msg type outside its parent message type message SomeOtherMessage &#123;  SearchResponse.Result result = 1;&#125;</code></pre><h2 id="4-4-Updating-a-Message-Type"><a href="#4-4-Updating-a-Message-Type" class="headerlink" title="4.4 Updating a Message Type"></a>4.4 Updating a Message Type</h2><ul><li>don’t change the field numbers for any existing fields</li><li>if you add new fields, any msg serialized by code using your old msg format can still be parsed by your new generated code<ul><li>keep in mind the default values for these elements so that new code can properly interact with msgs generated by old code</li></ul></li><li>to remove a field<ul><li>rename the field with prefix like <code>OBSOLETE_</code></li><li>or make the filed number reserved,</li></ul></li><li>int32, uint32, int64, uint64 and bool are all compatible</li><li>string and bytes are compatible as long as the bytes are valid UTF-8</li></ul><h2 id="4-5-Special-Keywords"><a href="#4-5-Special-Keywords" class="headerlink" title="4.5 Special Keywords"></a>4.5 Special Keywords</h2><h3 id="4-5-1-Any"><a href="#4-5-1-Any" class="headerlink" title="4.5.1 Any"></a>4.5.1 <code>Any</code></h3><ul><li>let you use messages as embedded types without having their .proto definition</li><li>it contains an aribitrary serialized messages as bytes</li></ul><h3 id="4-5-2-Oneof"><a href="#4-5-2-Oneof" class="headerlink" title="4.5.2 Oneof"></a>4.5.2 Oneof</h3><ul><li>if we have a msg with many fields and where at most one field will be set at the same time, we can enforce the behavior and save memory by using the oneof feature</li><li>at most one field can be set at the same time</li><li>setting any member of the oneof automatically clears all the other members</li></ul><h2 id="4-6-Maps"><a href="#4-6-Maps" class="headerlink" title="4.6 Maps"></a>4.6 Maps</h2><ul><li><code>map&lt;key_type, value_type&gt; map_field = N;</code></li></ul><h2 id="4-7-Define-Service"><a href="#4-7-Define-Service" class="headerlink" title="4.7 Define Service"></a>4.7 Define Service</h2><ul><li>If you want to use message types with an RPC system, we can define an RPC service interface in a <code>.proto</code> file</li><li>then the protocol buffer compiler will <strong>generate service interface code and stubs</strong> in chosen language</li></ul><h2 id="4-8-Options"><a href="#4-8-Options" class="headerlink" title="4.8 Options"></a>4.8 Options</h2><ul><li><p>Options do not change the overall meaning of a declaration, but may affect the way it is handled in a particular context.</p></li><li><p>java_package</p><ul><li>pkg you want to use for your generated Java classes</li></ul></li><li><p>java_outer_classname</p><ul><li>class name for the wrapper java class you want to generate</li></ul></li><li><p>java_multiple_files</p></li><li><p><code>optimize_for</code></p><ul><li><code>SPEED</code><ul><li>Compiler will generate code for serializing, parsing and performing other common operations on your msg types.</li><li>Code is highly optimized</li></ul></li><li><code>CODE_SIZE</code><ul><li>generate minimal classes</li><li>operations will be slower</li></ul></li><li><code>LITE_RUNTIME</code><ul><li>only depend on the lite runtime library</li><li>usefyl for apps running on constrained platform like mobile phones</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Overview <a href="https://developers.google.com/protocol-buffers">https://developers.google.com/protocol-buffers</a> </li><li>Language Guide <a href="https://developers.google.com/protocol-buffers/docs/overview">https://developers.google.com/protocol-buffers/docs/overview</a> </li><li>Java Tutorial <a href="https://developers.google.com/protocol-buffers/docs/javatutorial">https://developers.google.com/protocol-buffers/docs/javatutorial</a> </li><li>Java Generated Code <a href="https://developers.google.com/protocol-buffers/docs/reference/java-generated">https://developers.google.com/protocol-buffers/docs/reference/java-generated</a> </li><li>Java Encoding <a href="https://developers.google.com/protocol-buffers/docs/encoding">https://developers.google.com/protocol-buffers/docs/encoding</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Protobuf-Learning&quot;&gt;&lt;a href=&quot;#Protobuf-Learning&quot; class=&quot;headerlink&quot; title=&quot;Protobuf Learning&quot;&gt;&lt;/a&gt;Protobuf Learning&lt;/h1&gt;&lt;h1 id=&quot;1-Wha
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Protobuf" scheme="https://www.llchen60.com/tags/Protobuf/"/>
    
  </entry>
  
  <entry>
    <title>Bazel Intro</title>
    <link href="https://www.llchen60.com/Bazel-Intro/"/>
    <id>https://www.llchen60.com/Bazel-Intro/</id>
    <published>2021-12-22T02:05:15.000Z</published>
    <updated>2021-12-22T02:07:14.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-What-is-Bazel"><a href="#1-What-is-Bazel" class="headerlink" title="1. What is Bazel?"></a>1. What is Bazel?</h1><ul><li><p>Bazel is a build and test tool built that supports building and testing multiple projects for multiple languages and build outputs</p></li><li><p>What</p><ul><li>Build and Test tool similar to Make, Maven, Gradle</li><li>Caches all the previously done work, tests or builds faster everytime</li><li>Support multi languages, multi platforms,</li><li>Support large code base across multi repos</li><li>build, test, and query to trace dependencies in the code</li></ul></li><li><p>Why</p><ul><li>scales</li><li>multi platform</li></ul></li><li><p>How</p><ul><li>Need a BUILD file</li></ul></li></ul><h1 id="2-Concepts"><a href="#2-Concepts" class="headerlink" title="2. Concepts"></a>2. Concepts</h1><ul><li>Workspace - With WORKSPACE<ul><li>dir contains the source file</li><li>considered as root</li></ul></li><li>WORKSPACE<ul><li>a blank text file, which identifies the directory and its content as a Bazel workspace</li><li>at the root of the project’s directory structure</li></ul></li><li>Repos - With WORKSPACE<ul><li>External repos are defined in the WORKSPACE file using workspace rules</li></ul></li><li>Packages - With BUILD<ul><li>A package is defined as a directory containing a file named BUILD or BUILD.bazel</li><li>which reside beneath top level directory in the ws</li><li>This file has instructions on how to <strong>run or build or test</strong> the project</li></ul></li><li>Rules<ul><li>written using a DSL named Starlark</li><li>thus are built for certain language already like rules_java, etc.</li></ul></li><li>Targets<ul><li>Pkg is container, element of pkg —- target</li><li>Most targets are files or rules<ul><li>File<ul><li>source files - written by people</li><li>generated files — generated by build tool</li></ul></li><li>rule<ul><li>specify relationship between set of inputs and output</li><li>output are always generated files</li></ul></li></ul></li></ul></li></ul><h1 id="3-Best-Practices"><a href="#3-Best-Practices" class="headerlink" title="3. Best Practices"></a>3. Best Practices</h1><ul><li>A project should always be able to run <code>bazel build //...</code> and <code>bazel test //...</code></li><li>You may declare third party dependencies<ul><li>either declare them as remote repositories in the WORKSPACE file</li><li>or put them in a directory called third_party under workspace directory</li></ul></li><li>everything should be built from source whenever possible, instead of depending on a library so file, we should create a BUILD file and build so from its sources, then depend on that target</li><li>for project specific options, use the configuration file under <code>workspace/.bazelrc</code></li><li>every directory that contains buildable files should be a package</li></ul><h1 id="4-Build-a-Java-Project"><a href="#4-Build-a-Java-Project" class="headerlink" title="4. Build a Java Project"></a>4. Build a Java Project</h1><h2 id="4-1-Bazel-Jave-Basic"><a href="#4-1-Bazel-Jave-Basic" class="headerlink" title="4.1 Bazel Jave Basic"></a>4.1 Bazel Jave Basic</h2><ul><li><p>Refer <a href="https://docs.bazel.build/versions/1.2.0/tutorial/java.html">https://docs.bazel.build/versions/1.2.0/tutorial/java.html</a></p></li><li><p>build rule tells bazel how to build the desired outputs, executable binaries or libraries</p><ul><li>the java binary rule tells bazel to build a jar file and a wrapper shell script</li></ul></li><li><p><code>bazel build //:ProjectRunner</code></p><ul><li>the <code>//</code> part is the location of our BUILD file relative to the root of the workspace</li><li><code>ProjectRunner</code> is the target name we define in the BUILD file</li></ul></li><li><p>we could review our dependency graph by using</p><ul><li><code>bazel query --notool_deps --noimplicit_deps &quot;deps(//:ProjectRunner)&quot; --output graph</code></li></ul></li></ul><pre><code class="ruby">// generate graph for class in use, and output as a svg file bazel query  --notool_deps --noimplicit_deps &quot;deps(//booking)&quot; --output graph &gt; /Users/lchen1/Documents/bookingGraph.in dot -Tsvg &lt; bookingGraph.in &gt; graph.svg</code></pre><h2 id="4-2-Specify-multiple-build-targets"><a href="#4-2-Specify-multiple-build-targets" class="headerlink" title="4.2 Specify multiple build targets"></a>4.2 Specify multiple build targets</h2><ul><li><p>Package Splits</p><ul><li><p>for larger project, we may want to split into multiple targets and packages to allow for fast incremental builds, this could also speed up builds by building multiple parts of a project at once</p><pre><code class="json">java_binary(  name = &quot;ProjectRunner&quot;,  srcs = [&quot;src/main/java/com/example/ProjectRunner.java&quot;],  main_class = &quot;com.example.ProjectRunner&quot;,  deps = [&quot;:greeter&quot;],)java_library(  name = &quot;greeter&quot;,  srcs = [&quot;src/main/java/com/example/Greeting.java&quot;],)</code></pre></li></ul></li></ul><ul><li>with this configuration, bazel will first build greeter library, then the projectRunner binary<ul><li>deps attribute tells bazel the greeter library is required to build the projectRunner binary</li></ul></li></ul><h2 id="4-3-Use-multiple-packages"><a href="#4-3-Use-multiple-packages" class="headerlink" title="4.3 Use multiple packages"></a>4.3 Use multiple packages</h2><pre><code class="json">java_binary(    name = &quot;runner&quot;,    srcs = [&quot;Runner.java&quot;],    main_class = &quot;com.example.cmdline.Runner&quot;,    deps = [&quot;//:greeter&quot;])</code></pre><ul><li>To make sure above works, we need to let greeter be visible to cmdline.Runner<ul><li>Let the resource owner set the visibility attribute</li><li>we need to do this cause Bazel by default makes target only visible to other targets in the same BUILD file</li><li>bazel uses target visibility to prevent issues such as libraries containing implementation details leaking into public APIs</li></ul></li></ul><pre><code class="json">java_library(    name = &quot;greeter&quot;,    srcs = [&quot;src/main/java/com/example/Greeting.java&quot;],    visibility = [&quot;//src/main/java/com/example/cmdline:__pkg__&quot;],    )</code></pre><h2 id="4-4-Use-labels-to-reference-targets"><a href="#4-4-Use-labels-to-reference-targets" class="headerlink" title="4.4 Use labels to reference targets"></a>4.4 Use labels to reference targets</h2><ul><li>Bazel uses target labels to reference targets<ul><li><code>//:ProjectRunner</code></li><li>sync as follow:<ul><li><code>//path/to/package:target-name</code></li></ul></li></ul></li><li>when referencing targets within the same BUILD file, we can skip the <code>//</code> workspace root identifier and just use <code>:target_name</code></li></ul><h1 id="5-E-G"><a href="#5-E-G" class="headerlink" title="5. E.G"></a>5. E.G</h1><ul><li>java_binary<ul><li>pre defined rule telling bazel to create a binary when a target is invoked</li></ul></li></ul><pre><code class="json">java_binary(        // target name     name = &quot;mymain&quot;,        // all source files, passed as glob, inside the fully qualified directory names on classpath     srcs = glob([&quot;src/main/java/com/abhi/*.java&quot;]),        // main runner class     main_class = &quot;com.abhi.MyMain&quot;,        // dependent classes/ interfaces to be included, not part of srcs     deps = [&quot;//another-dir:animal&quot;])</code></pre><ul><li>java_library<ul><li>pre-defined to create library as the name suggests</li></ul></li></ul><pre><code class="json">java_library(    name = &quot;animal&quot;,    srcs = [&quot;src/main/java/com/abhi/Animal.java&quot;],        // if other class is implemented in a different pkg, it has to be visible to main-dir     visibility = [&quot;//main-dir:__pkg__&quot;])</code></pre><ul><li>CLI Reference<ul><li><code>bazel build //main-dir:mymain</code><ul><li>// means a valid package name</li><li>mymain is the target name</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Bazel best practice <a href="https://docs.bazel.build/versions/main/best-practices.html">https://docs.bazel.build/versions/main/best-practices.html</a> </li><li>Bazel Overview  <a href="https://docs.bazel.build/versions/1.2.0/bazel-overview.html">https://docs.bazel.build/versions/1.2.0/bazel-overview.html</a> </li><li>Java Tutorial <a href="https://docs.bazel.build/versions/1.2.0/tutorial/java.html">https://docs.bazel.build/versions/1.2.0/tutorial/java.html</a> </li><li>How to specify targets to build <a href="https://docs.bazel.build/versions/main/guide.html#target-patterns">https://docs.bazel.build/versions/main/guide.html#target-patterns</a> </li></ol><p><a href="https://docs.bazel.build/versions/4.2.1/command-line-reference.html">Command-Line Reference</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-What-is-Bazel&quot;&gt;&lt;a href=&quot;#1-What-is-Bazel&quot; class=&quot;headerlink&quot; title=&quot;1. What is Bazel?&quot;&gt;&lt;/a&gt;1. What is Bazel?&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bazel
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Bazel" scheme="https://www.llchen60.com/tags/Bazel/"/>
    
      <category term="Package Management" scheme="https://www.llchen60.com/tags/Package-Management/"/>
    
  </entry>
  
  <entry>
    <title>GraphQL Read</title>
    <link href="https://www.llchen60.com/GraphQL-Read/"/>
    <id>https://www.llchen60.com/GraphQL-Read/</id>
    <published>2021-12-04T09:29:50.000Z</published>
    <updated>2021-12-04T09:31:23.613Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GraphQL-Read"><a href="#GraphQL-Read" class="headerlink" title="GraphQL Read"></a>GraphQL Read</h1><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li>QraphQL is<ul><li>a query language</li><li>a server side runtime for executing queries using a type system you define</li></ul></li></ul><h1 id="2-Queries-and-Mutations"><a href="#2-Queries-and-Mutations" class="headerlink" title="2. Queries and Mutations"></a>2. Queries and Mutations</h1><ul><li>Fields<ul><li>GraphQL is about asking specific fields on objects</li><li>Query has the same shape as result<ul><li>server knows exactly what fields the client is asking for</li></ul></li></ul></li></ul><pre><code class="ruby">&#123;    hero &#123;        name    &#125;&#125;&#123;    &quot;data&quot;: &#123;        &quot;hero&quot;: &#123;            &quot;name&quot;: &quot;12test&quot;        &#125;    &#125;&#125;</code></pre><ul><li><p>Arguments</p><ul><li>we could pass arguments to fields</li><li>comparing with Restful, in GraphQL every field and nested object can get its own set of arguments, making GraphQL a complete replacement for making multiple APU fetches</li></ul></li><li><p>Fragments</p><ul><li>That’s the reusable units in GraphQL</li><li>Fragments let you construct sets of fields, and then include them in queries where you need to</li><li>It’s commonly used to split complicated application data requirements into smaller chunks</li></ul></li><li><p>Operation Name</p><ul><li>Operation Type<ul><li>Query</li><li>Mutation</li><li>Subscription</li></ul></li><li>Operation Name</li></ul></li><li><p>Variables</p><ul><li>It want to give dynamic power to graphql, as in most applications, the arguments to fields will be dynamic</li><li>Graphql supports this use case via variables</li><li>we could do:<ul><li>replace the static value in the query with <code>$variable</code></li><li>declare <code>$variable</code> as one of the variables accepted by the query</li><li>pass <code>variable: value</code> in the separate transport specific variables dictionary</li></ul></li><li>using variable could help us denote which arguments are expected to be dynamic</li><li>we should never do string interpolation to construct queries from user supplied values</li></ul></li></ul><pre><code class="ruby">query HeroNameAndFriends($episode: Episode) &#123;  hero(episode: $episode) &#123;    name    friends &#123;      name    &#125;  &#125;&#125;&#123;  &quot;episode&quot;: &quot;JEDI&quot;&#125; </code></pre><ul><li>Default variables</li></ul><pre><code class="ruby">query HeroNameAndFriends($episode: Episode = &quot;defaultOne&quot;) &#123;  hero(episode: $episode) &#123;    name    friends &#123;      name    &#125;  &#125;&#125;&#123;  &quot;episode&quot;: &quot;JEDI&quot;&#125; </code></pre><ul><li><p>Directives</p><ul><li>use this variable to dynamically change the structure and shape of our queries using variables</li><li><code>@include(if: Boolean)</code> only includes this field in the result if the argument is true</li><li><code>@skip(if: Boolean)</code> skip this field if the argument is true</li></ul></li><li><p>Mutations</p><ul><li>A way to modify server side data</li><li>A convention that any operations that cause writes should be sent explicitly via a mutation</li><li>!!! While query fields are executed in parallel, mutation fields run in series, one after the other<ul><li>means if we send two incrementCredits mutations in one request, the first is guranteed to finish before the second begins, ensuring that we don’t end up with a race condition with ourselves</li></ul></li></ul></li><li><p>Inline Fragments</p><ul><li><p>GraphQL schemas include the ability to define interfaces and union types</p></li><li><p>EG below, we need to return different attributes based on hero character</p><pre><code class="ruby">query HeroForEpisode($ep: Episode!) &#123;hero(episode: $ep) &#123;  name  ... on Droid &#123;    primaryFunction  &#125;  ... on Human &#123;    height  &#125;&#125;&#125;</code></pre></li></ul></li><li><p>Meta fields</p><ul><li>there are situations where you don’t know what type you’ll get back from the service</li><li>we need to determine how to handle that data on the client</li><li>we could use <code>__typename</code></li></ul></li></ul><h1 id="3-Schemas-and-Types"><a href="#3-Schemas-and-Types" class="headerlink" title="3. Schemas and Types"></a>3. Schemas and Types</h1><h2 id="3-1-how-the-schema-work"><a href="#3-1-how-the-schema-work" class="headerlink" title="3.1 how the schema work"></a>3.1 how the schema work</h2><ul><li><p>How does GraphQL work</p><ul><li><p>start with a <code>root</code> object</p></li><li><p>select the hero field on that</p></li><li><p>for the object returned by hero, we select the name and appearsIn fields</p><pre><code class="ruby">&#123;hero &#123;  name  appearsIn&#125;&#125;</code></pre></li></ul></li><li><p>we should know what we could query for</p><ul><li>an exact description of the data we can ask for</li><li>what kind of objects might they return</li><li>what fields are available on those sub objects</li></ul></li><li><p>Schema</p><ul><li>Each graphQL services defines a set of types which completely describe the set of possible data you can query on the service</li></ul></li></ul><h2 id="3-2-Type-Language"><a href="#3-2-Type-Language" class="headerlink" title="3.2 Type Language"></a>3.2 Type Language</h2><ul><li>GraphQL use its won Schema Language</li></ul><h3 id="3-2-1-Object-Types-and-Fields"><a href="#3-2-1-Object-Types-and-Fields" class="headerlink" title="3.2.1 Object Types and Fields"></a>3.2.1 Object Types and Fields</h3><ul><li>Object types<ul><li>represent a kind of object you can fetch from your service, and what fields it has</li></ul></li></ul><pre><code class="ruby">type Character &#123;  name: String!// means an array of Episode objects, notnull, 0 or more items, and each item would be an episode object   appearsIn: [Episode!]!&#125;</code></pre><h3 id="3-2-2-Query-and-Mutation-types"><a href="#3-2-2-Query-and-Mutation-types" class="headerlink" title="3.2.2 Query and Mutation types"></a>3.2.2 Query and Mutation types</h3><ul><li>Entry points into the schema</li></ul><h3 id="3-2-3-Scalar-Types"><a href="#3-2-3-Scalar-Types" class="headerlink" title="3.2.3 Scalar Types"></a>3.2.3 Scalar Types</h3><ul><li>Scalar types represent the leaves of the query</li><li>default scalar types<ul><li>Int</li><li>Float</li><li>String</li><li>Boolean</li><li>ID<ul><li>it represents a unique identifier</li><li>The ID type is serialized in the same way as a String, but it means it’s not intended to be human readable</li></ul></li></ul></li><li>We could also define our own Scalar type in this way<ul><li><code>scalar Date</code></li></ul></li></ul><h3 id="3-2-4-Enumeration-Types"><a href="#3-2-4-Enumeration-Types" class="headerlink" title="3.2.4 Enumeration Types"></a>3.2.4 Enumeration Types</h3><ul><li>Restricted to a particular set of allowed values</li><li>Allow you to<ul><li>validate that any arguments of this type are one of the allowed values</li><li>communicate through the type system that a field will always be one of a finite set of values</li></ul></li></ul><h3 id="3-2-5-Lists-and-Non-Null"><a href="#3-2-5-Lists-and-Non-Null" class="headerlink" title="3.2.5 Lists and Non-Null"></a>3.2.5 Lists and Non-Null</h3><pre><code>type Character &#123;  name: String!  appearsIn: [Episode]!&#125;</code></pre><ul><li>We could use <code>!</code> to indicate it should never return null</li><li>We could use <code>[]</code> to indicate that should be an array</li></ul><h3 id="3-2-6-Interfaces"><a href="#3-2-6-Interfaces" class="headerlink" title="3.2.6 Interfaces"></a>3.2.6 Interfaces</h3><ul><li>An abstract type that includes a certain set of fields that a type must include to implement the interface</li></ul><pre><code class="ruby">interface Character &#123;    id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!&#125;type Human implements Character &#123;  id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!  starships: [Starship]  totalCredits: Int&#125;type Droid implements Character &#123;  id: ID!  name: String!  friends: [Character]  appearsIn: [Episode]!  primaryFunction: String&#125;</code></pre><ul><li>Type implement the interface need to have all those fields, but they could also have their own fields</li></ul><h3 id="3-2-7-Union-Types"><a href="#3-2-7-Union-Types" class="headerlink" title="3.2.7 Union Types"></a>3.2.7 Union Types</h3><p><code>union SearchResult = Human | Droid | Starship</code></p><h3 id="3-2-8-Input-Types"><a href="#3-2-8-Input-Types" class="headerlink" title="3.2.8 Input Types"></a>3.2.8 Input Types</h3><ul><li>we need to pass complex objects especially when we are using mutations, where we want to pass in a whole object to be created</li></ul><pre><code class="ruby">input ReviewInput &#123;  stars: Int!  commentary: String&#125;mutation CreateReviewForEpisode($ep: Episode!, $review: ReviewInput!) &#123;  createReview(episode: $ep, review: $review) &#123;    stars    commentary  &#125;&#125;</code></pre><h1 id="4-Validation"><a href="#4-Validation" class="headerlink" title="4. Validation"></a>4. Validation</h1><ul><li>Graph ql has validation module to fulfill the validation phase of fulfilling a graphQL result</li></ul><h1 id="5-Execution"><a href="#5-Execution" class="headerlink" title="5. Execution"></a>5. Execution</h1><h2 id="5-1-Resolvers"><a href="#5-1-Resolvers" class="headerlink" title="5.1 Resolvers"></a>5.1 Resolvers</h2><ul><li>Each field in a GraphQL query is a function or method of the previous type which returns the next type</li><li>Each field is backed by a function called the resolver. When a field is executed, the corresponding resolver is called to produce the next value</li><li>The resolver continue to work until reach scalar values</li></ul><h2 id="5-2-Root-fields"><a href="#5-2-Root-fields" class="headerlink" title="5.2 Root fields"></a>5.2 Root fields</h2><pre><code class="ruby">Query: &#123;  human(obj, args, context, info) &#123;    return context.db.loadHumanByID(args.id).then(      userData =&gt; new Human(userData)    )  &#125;&#125;</code></pre><ul><li>obj<ul><li>previous object</li></ul></li><li>args<ul><li>arguments provided to the field in the graphQL query</li></ul></li><li>context<ul><li>a value which is provided to every resolver and holds important contextual information like the currently logged in user, or access to a database</li></ul></li><li>info<ul><li>a value which holds field specific information relevant to the current query as well as the schema details</li></ul></li></ul><h1 id="6-Best-Practices"><a href="#6-Best-Practices" class="headerlink" title="6. Best Practices"></a>6. Best Practices</h1><h2 id="6-1-HTTP"><a href="#6-1-HTTP" class="headerlink" title="6.1 HTTP"></a>6.1 HTTP</h2><ul><li><p>Mostly use HTTP with graphQL</p></li><li><p>Normally web frameworks use a pipeline model where requests are passed through a stack of middle ware</p></li><li><p>Requests could be inspected, transformed, modified or terminated with a response</p></li><li><p>GraphQL should be placed after all authentication middleware— thus you have access to the same session and user info you would in your HTTP endpoints handler</p></li><li><p>GraphQL server operates on a single URL/ endpoint, usually <code>graphql</code> , and all graphql requests for a given service should be directed at this endpoint</p></li></ul><h2 id="6-2-JSON-with-GZIP"><a href="#6-2-JSON-with-GZIP" class="headerlink" title="6.2 JSON with GZIP"></a>6.2 JSON with GZIP</h2><ul><li>typically respond using JSON, and we compress it with GZIP</li></ul><h2 id="6-3-Versioning"><a href="#6-3-Versioning" class="headerlink" title="6.3 Versioning"></a>6.3 Versioning</h2><ul><li>No need to do versioning for graphql api</li><li>Why do most APIs version? When there’s limited control over the data that’s returned from an API endpoint, <em>any change</em> can be considered a breaking change, and breaking changes require a new version. If adding new features to an API requires a new version, then a tradeoff emerges between releasing often and having many incremental versions versus the understandability and maintainability of the API.</li><li>In contrast, GraphQL only returns the data that’s explicitly requested, so new capabilities can be added via new types and new fields on those types without creating a breaking change. This has led to a common practice of always avoiding breaking changes and serving a versionless API.</li></ul><h2 id="6-4-Nullability"><a href="#6-4-Nullability" class="headerlink" title="6.4 Nullability"></a>6.4 Nullability</h2><ul><li>GraphQL default to nullable unless you specifically declare nonnull</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.graphql-java-kickstart.com/">https://www.graphql-java-kickstart.com/</a>  </li><li><a href="https://graphql.org/learn/">https://graphql.org/learn/</a>  </li><li><a href="https://www.apollographql.com/docs/federation/">https://www.apollographql.com/docs/federation/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;GraphQL-Read&quot;&gt;&lt;a href=&quot;#GraphQL-Read&quot; class=&quot;headerlink&quot; title=&quot;GraphQL Read&quot;&gt;&lt;/a&gt;GraphQL Read&lt;/h1&gt;&lt;h1 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="Web" scheme="https://www.llchen60.com/categories/Web/"/>
    
    
  </entry>
  
  <entry>
    <title>如何缓解疲劳</title>
    <link href="https://www.llchen60.com/%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3%E7%96%B2%E5%8A%B3/"/>
    <id>https://www.llchen60.com/%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3%E7%96%B2%E5%8A%B3/</id>
    <published>2021-11-02T13:31:04.000Z</published>
    <updated>2021-11-02T13:34:58.934Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/11/02/zEvGV8uDnJxoITr.png" alt="如何抵抗缓解疲劳.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/11/02/zEvGV8uDnJxoITr.png&quot; alt=&quot;如何抵抗缓解疲劳.png&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>碳达峰与碳中和</title>
    <link href="https://www.llchen60.com/%E7%A2%B3%E8%BE%BE%E5%B3%B0%E4%B8%8E%E7%A2%B3%E4%B8%AD%E5%92%8C/"/>
    <id>https://www.llchen60.com/%E7%A2%B3%E8%BE%BE%E5%B3%B0%E4%B8%8E%E7%A2%B3%E4%B8%AD%E5%92%8C/</id>
    <published>2021-09-30T02:21:14.000Z</published>
    <updated>2021-09-30T02:22:44.858Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/09/30/XGS6Qey4qHki2FC.png" alt="碳达峰与碳中和"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/09/30/XGS6Qey4qHki2FC.png&quot; alt=&quot;碳达峰与碳中和&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Cassandra</title>
    <link href="https://www.llchen60.com/Cassandra/"/>
    <id>https://www.llchen60.com/Cassandra/</id>
    <published>2021-09-17T13:05:55.000Z</published>
    <updated>2021-09-17T13:41:43.051Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/09/17/IGdzKBJwD9TbZ6f.png" alt="Cassandra MindMap.png"></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><h2 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h2><ul><li>We wanna have a distributed and scalable system that can store a <strong>huge amount of structured data</strong>, which is indexed by a row key where each row can have an <strong>unbounded</strong> number of columns.</li><li>Cassandra was originally developed at Facebook in 2007 for index search feature. It’s designed to provide scalability, availability, and reliability to store large amounts of data.</li><li>It combines nature of Dynamo which is a <strong>key value store</strong> and the data model of Bigtable which is a <strong>column based</strong> data store</li><li>Cassandra is in favor of availability and partition tolerance, it could be tuned with <strong>replication factor</strong> and <strong>consistency levels</strong> to meet <strong>strong consistency</strong> requirements, and of course with a performance cost.</li><li>It uses peer to peer architecture, with each node connected to all other nodes</li><li>Each Cassandra node performs all database operations and can serve client requests without the need for any leader node.</li></ul><h2 id="1-2-Use-cases"><a href="#1-2-Use-cases" class="headerlink" title="1.2 Use cases"></a>1.2 Use cases</h2><ul><li>Store key value data with high availability</li><li>Time series data model<ul><li>Due to its data model and log structured storage engine, cassandra benefits from high performing write operations, This also make it well suited for storing and analyzing sequentially captured metrics</li></ul></li><li>Write Heavy Applications<ul><li>Suited for write intensive applications such as time series streaming services, sensor logs, and IoT applications</li></ul></li></ul><h1 id="2-High-Level-Architecture"><a href="#2-High-Level-Architecture" class="headerlink" title="2. High Level Architecture"></a>2. High Level Architecture</h1><h2 id="2-1-Common-Terms"><a href="#2-1-Common-Terms" class="headerlink" title="2.1 Common Terms"></a>2.1 Common Terms</h2><p><img src="https://i.loli.net/2021/09/17/IrfBD5HFqAX76NJ.png" alt="Primary and Clustering Keys"></p><ul><li>Column<ul><li>A key value pair and is the most basic unit of data structure</li><li>Column Key: Uniquely identifies a column in a row</li><li>Column Value: Store a value or a collection of values</li></ul></li><li>Row<ul><li>A container for columns referenced by primary key. Cassandra does not store a column that has a null value, this saves a lot of space</li></ul></li><li>Table<ul><li>A container of rows</li></ul></li><li>Keyspace<ul><li>A container for tables that span over one or more Cassandra nodes</li></ul></li><li>Cluster<ul><li>Container of Keyspace</li></ul></li><li>Node<ul><li>A computer system running an instance of Cassandra,</li><li>Can be a physical host, a machine instance in the cloud or even a docker container</li></ul></li></ul><h2 id="2-2-Data-Partitioning"><a href="#2-2-Data-Partitioning" class="headerlink" title="2.2 Data Partitioning"></a>2.2 Data Partitioning</h2><ul><li>Cassandra use consistent hashing as DynamoDB does</li></ul><h2 id="2-3-Primary-Key"><a href="#2-3-Primary-Key" class="headerlink" title="2.3 Primary Key"></a>2.3 Primary Key</h2><ul><li>The primary key consists of two parts:  E.G Primary Key as (city_id, employee_id)<ul><li>Partition Key<ul><li>Decides how data is distributed across nodes</li><li>city_id is the primary key, means the data will be partitioned by the city_id field, all rows with the same city_id will reside on the same node</li></ul></li><li>Clustering Key<ul><li>Decides how data is stored within a node</li><li>We could have multiple clustering keys, clustering columns specify the order that the data is arranged on a node.</li><li>employee_id is the clustering key. Within each node, the data is stored in sorted order according to the employee_id column.</li></ul></li></ul></li></ul><h2 id="2-4-Partitioner"><a href="#2-4-Partitioner" class="headerlink" title="2.4 Partitioner"></a>2.4 Partitioner</h2><p><img src="https://i.loli.net/2021/09/17/3NdkOaXUpbgnWq9.png" alt="Partitioner Flow"></p><ul><li>Responsible for determining how data is distributed on the consistent hash ring.</li><li>Cassandra use <strong>Murmur3 hashing function</strong> — which will always produce the same hash for a given partition key</li><li>All Cassandra nodes learn about the <strong>token assignments of other nodes</strong> through gossip. This means any node can handle a request for any other node’s range. The node receiving the request is called the <strong>coordinator</strong>, and any node can act in this role. If a key does not belong to the coordinator’s range, it <strong>forwards the request</strong> to the replicas responsible for that range.</li></ul><h2 id="2-5-Coordinator-Node"><a href="#2-5-Coordinator-Node" class="headerlink" title="2.5 Coordinator Node"></a>2.5 Coordinator Node</h2><ul><li>A client may connect to any node in the cluster to initiate a read or write query. This node is known as the coordinator node, the coordinator identifies the nodes responsible for the data that is being written or read    and forwards the queries to them</li></ul><h1 id="3-Low-Level-Architecture"><a href="#3-Low-Level-Architecture" class="headerlink" title="3. Low Level Architecture"></a>3. Low Level Architecture</h1><h2 id="3-1-Replication-Strategy"><a href="#3-1-Replication-Strategy" class="headerlink" title="3.1 Replication Strategy"></a>3.1 Replication Strategy</h2><ul><li><p>Each node in Cassandra serves as a replica for a different range of data.</p></li><li><p>It stores <strong>multiple copies of data</strong> and <strong>spreads them across various replicas</strong>.</p></li><li><p>The replication behavior is controlled by two factors</p><ul><li><p>Replication Factor</p><ul><li>Decides how many replicas the system will have</li><li>This represents the <strong>number of nodes that will receive the copy of the same data</strong></li><li>Each keyspace in cassandra can have a different replication factor</li></ul></li><li><p>Replication Strategy</p><ul><li><p>Decides which nodes will be responsible for the replicas</p></li><li><p>The node that owns the range in which the hash of the partition key falls will be the first replica</p></li><li><p>All the additional replicas are placed on the <strong>consecutive nodes</strong></p></li><li><p>Cassandra places the subsequent replicas on the next nodes in a clockwise manner</p></li><li><p>Two kinds of replication strategies</p><ul><li><p>Simple Replication Strategy</p><p>  <img src="https://i.loli.net/2021/09/17/cnz12lGFWEPw4fS.png" alt="Simple Replication Strategy"></p><ul><li>Used for a <strong>single data center cluster</strong></li><li>Cassandra places the first replica on a node determined by the partitioner and the subsequent replicas on the next node in a clockwise manner</li></ul></li><li><p>Network Topology Strategy</p><p>  <img src="https://i.loli.net/2021/09/17/TSAZbXKCYf9IsoN.png" alt="Network Topology Strategy"></p><ul><li>Used for multiple data centers</li><li>We can specify different replication factors for different data centers. We could then specify how many replicas will be placed in each data center</li><li>Additional replicas, in the same data center, are placed by <strong>walking the ring clockwise until reaching the 1st node in another rack</strong>. This is done to guard against a complete rack failure, as nodes in the same rack(or similar physical grouping) tend to fail together due to power, cooling or network issues.</li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="3-2-Consistency-Levels"><a href="#3-2-Consistency-Levels" class="headerlink" title="3.2 Consistency Levels"></a>3.2 Consistency Levels</h2><ul><li>Definition<ul><li><strong>Minimum number of nodes</strong> that must fulfill a read or write operation before the operation can be considered successful</li><li>It allows use to <strong>specify different consistency levels</strong> for read and write</li><li>It also has <strong>tunable consistency level</strong></li><li>Tradeoff between consistency and response time<ul><li>As a higher consistency level means more nodes need to respond to a read or write query, giving user more assurance that the values present on each replica are the same</li></ul></li></ul></li></ul><h3 id="3-2-1-Write-Consistency-Levels"><a href="#3-2-1-Write-Consistency-Levels" class="headerlink" title="3.2.1 Write Consistency Levels"></a>3.2.1 Write Consistency Levels</h3><ul><li>Consistency Levels specify how many replica nodes must respond for the write to be reported as successful to the client</li><li>Level is specified <strong>per query by the client</strong></li><li>Cassandra is eventually consistent, updates to other replica nodes may continue in the background</li><li>How does Cassandra perform a write operation?<ul><li>Coordinator node contacts all replicas, as determined by the <strong>replication factor</strong> , and consider the write successful when a number of replicas equal to the consistency level acknowledge the write</li></ul></li><li>Write Consistency Levels List:<ul><li>One/ Two/ Three<ul><li>The data must be written to at least the specified number of replica nodes before a write is considered successful</li></ul></li><li>Quorum<ul><li>Data must be written to at least a quorum of replica nodes</li><li>Quorum is defined as <code>floor(RF/2 + 1)</code>  RF represents replication factor</li></ul></li><li>All<ul><li>ensures the data is written to all replica nodes</li><li>provides the highest consistency but lowest availability as writes will fail if any replica is down</li></ul></li><li>Local Quorum<ul><li>Ensure that data is written to a quorum of nodes in the same datacenter as the coordinator</li><li>Does not wait for the response from the other data centers</li></ul></li><li>Each Quorum<ul><li>Ensures that the data is written to a quorum of nodes in each datacenter</li></ul></li><li>Any<ul><li>The data must be written to at least one node</li><li>In the extreme case, when all replica nodes for the given partition key are down, the write can still succeed after a hinted handoff (see 3.2.4 section) has been written.<ul><li>In this case, an any write could succeed with hinted handoff, but it will not be readable until the replica nodes for that partition has recovered and the latest data is written on them</li></ul></li></ul></li></ul></li></ul><h3 id="3-2-2-Read-Consistency-Levels"><a href="#3-2-2-Read-Consistency-Levels" class="headerlink" title="3.2.2 Read Consistency Levels"></a>3.2.2 Read Consistency Levels</h3><ul><li><p>Read Query Consistency Level specify how many replica nodes must respond to a read request before returning the data</p></li><li><p>It has the same consistency levels for read operations as that of write operations exception Each_Quorum cause it’s too expensive</p></li><li><p>To achieve strong consistency, we need to do <code>R + W &gt; RF</code> R represents read replica count, W represents write replication count, RF represents replication factor</p><ul><li>All client reads will see the most recent write in this scenario, and we will have strong consistency</li></ul></li><li><p>How does Cassandra perform a read operation?</p><ul><li><p>Coordinator always sends the read request to the fastest node</p><ul><li>E.G  for quorum=2, the coordinator sends the requests to the fastest node and the <strong>digest of the data</strong> from the second fastest node<ul><li>digest is the checksum of the data, we use this to save network bandwidth</li></ul></li></ul></li><li><p>if the digest doesn’t match, means some replica do not have the latest version of data</p><ul><li><p>Coordinator then <strong>reads the data from all the replicas</strong> to determine the latest data</p></li><li><p>Then coordinator <strong>returns the latest data to the client and initiates a read repair request</strong></p></li><li><p>The read repair request will help push the newer version of data to nodes with the older version</p><p>  <img src="https://i.loli.net/2021/09/17/sPM6HnKthBpT945.png" alt="Read Operation with Snitch"></p></li></ul></li><li><p>latest write timestamp is used as a mark for the correct version of data, read repair operation is performed only in a portion of the total reads to avoid performance degradation</p></li></ul></li></ul><h3 id="3-2-3-Snitch"><a href="#3-2-3-Snitch" class="headerlink" title="3.2.3 Snitch"></a>3.2.3 Snitch</h3><ul><li><p>Functions</p><ul><li>Application that determines the proximity of nodes within the ring, also tells which nodes are faster — monitor the read latencies</li><li>It keeps track of the network topology of Cassandra nodes, determines which <strong>data centers and racks</strong> nodes belong to</li><li>Replication strategy use this information provided by the Snitch to spread the replicas across the cluster intelligently. It could do its best by not having more than one replica on the same rack</li></ul></li><li><p>Cassandra nodes use this info to route read/ write requests efficiently</p><p>  <img src="https://i.loli.net/2021/09/17/ZnaOJqIvgdcMmAW.png" alt="Request when set consistency to one"></p></li></ul><h3 id="3-2-4-Hinted-Handoff"><a href="#3-2-4-Hinted-Handoff" class="headerlink" title="3.2.4 Hinted Handoff"></a>3.2.4 Hinted Handoff</h3><p><img src="https://i.loli.net/2021/09/17/QvSmb5wntE2AHJd.png" alt="Hinted Handoff"></p><ul><li>To let Cassandra still serve write requests even when nodes are down</li><li>When a node is down, the coordinator nodes <strong>writes a hint in a text file on local disk</strong><ul><li>Hint contains the data itself along with information about which node the data belongs to</li><li>Recover from gossiper — When the coordinator node discovers from the gossiper that a node for which it holds hints has recovered, it forwards the write request for each hint to the target</li><li>Recover from routine call — each node every ten minutes checks to see if the failing node, for which it is holding any hints, has recovered</li></ul></li><li>With consistency level ‘Any,’<ul><li>if all the replica nodes are down, the coordinator node will <strong>write the hints for all the nodes and report success to the client.</strong></li><li>However, this data will <strong>not reappear in any subsequent reads</strong> until one of the replica nodes comes back online, and the coordinator node successfully forwards the write requests to it.</li><li>This is assuming that the coordinator node is up when the replica node comes back.</li><li>This also means that we can lose our data if the coordinator node dies and never comes back. For this reason, we should avoid using the ‘Any’ consistency level</li></ul></li><li>For node offline for quite long<ul><li>Hints can build up considerably on other nodes</li><li>When it back online, other nodes tend to flood that node with write requests</li><li>It would cause issues on the node, as it is already trying to come back after a failure</li><li>To address this, Cassandra <strong>limits the storage of hints to a configurable time window</strong></li><li>By default, set the time window to 3 hours. Post that, older hints will be removed  — now the recovered nodes will have stale data<ul><li>The stale data would be fixed during the read path, it will issue a read repair when it sees the stale data</li></ul></li></ul></li><li>When the cluster cannot meet the consistency level specified by the client, Cassandra fails the write request and does not store a hint .</li></ul><h2 id="3-3-Gossiper"><a href="#3-3-Gossiper" class="headerlink" title="3.3 Gossiper"></a>3.3 Gossiper</h2><h3 id="3-3-1-How-does-Cassandra-use-Gossip-Protocol"><a href="#3-3-1-How-does-Cassandra-use-Gossip-Protocol" class="headerlink" title="3.3.1 How does Cassandra use Gossip Protocol?"></a>3.3.1 How does Cassandra use Gossip Protocol?</h3><ul><li>What’s for?<ul><li>Cassandra uses gossip protocol that allows each node to keep track of state information about the other nodes in the cluster.</li><li>It’s a Peer to Peer communication mechanism in which nodes <strong>periodically exchange state information about themselves and other nodes they know about</strong></li></ul></li><li>How it works?<ul><li>Each node initiates a gossip round every second to exchange state info about themselves with one to three other random nodes</li><li>Each gossip message has a version associated with it, so during a gossip exchange, older info is overwritten with the most current state for a particular node</li></ul></li><li>Generation number<ul><li>Each node stores a generation number which will be incremented every time a node restart</li><li>Node receiving the gossip message can compare the generation number it knows and the gossip message’s generation number</li><li>If the generation number in the gossip message is higher, it knows the node was restarted</li></ul></li><li>Seed nodes<ul><li>For node starting up for the first time</li><li>Assist in gossip convergence, thus guarantee schema/ state changes propagate regularly</li></ul></li></ul><h3 id="3-3-2-Node-Failure-Detection"><a href="#3-3-2-Node-Failure-Detection" class="headerlink" title="3.3.2 Node Failure Detection"></a>3.3.2 Node Failure Detection</h3><ul><li>Disadvantages for heartbeat<ul><li>outputs a boolean value telling us if the system is alive or not;</li><li>there is no middle ground.</li><li>Heartbeating uses a fixed timeout, and if there is no heartbeat from a server, the system, after the timeout, assumes that the server has crashed.</li><li>If we keep the timeout short, the system will be able to detect failures quickly but with many false positives due to slow machines or faulty networks.</li><li>On the other hand, if we keep the timeout long, the false positives will be reduced, but the system will not perform efficiently for being slow in detecting failures.</li></ul></li><li>Use adaptive failure detection mechanism  —— Phi Accrual Failure Detector<ul><li>Use historical heartbeat information to make the threshold adaptive</li><li>It outputs the suspicion level about a server</li><li>As a node’s suspicion level increases, the system can gradually decide to stop sending new requests to it</li><li>It makes the distributed system efficient as it takes into account fluctuations in the network env and other intermittent server issues before declaring a system completely dead</li></ul></li></ul><h2 id="3-4-Anatomy-of-Cassandra’s-Write-Operation"><a href="#3-4-Anatomy-of-Cassandra’s-Write-Operation" class="headerlink" title="3.4 Anatomy of Cassandra’s Write Operation"></a>3.4 Anatomy of Cassandra’s Write Operation</h2><p>Cassandra stores data both <strong>in memory and on disk</strong> to provide both high performance and durability. Every write includes a timestamp, write path involves a lot of components: </p><p><img src="https://i.loli.net/2021/09/17/LrMK7ckIS2zEsU1.png" alt="Write Path"></p><ul><li>Each write is appended to a commit log, which is stored on disk</li><li>It is then written to Memtable in memory</li><li>Periodically, MemTables are flushed to SSTables on the disk</li><li>Periodically, compaction runs to merge SSTables</li></ul><h3 id="3-4-1-Commit-Log"><a href="#3-4-1-Commit-Log" class="headerlink" title="3.4.1 Commit Log"></a>3.4.1 Commit Log</h3><ul><li>When a node receives a write request, it immediately writes data to a commit log</li><li>Commit log is a <strong>write ahead log</strong> stored on disk</li><li>Used as a crash recovery mechanism to support Cassandra’s durability goals</li><li>A write will not be considered successful on the node until it’s <strong>written to the commit log</strong><ul><li>This ensures if a write operation does not make it to the in-memory store, it will still be possible to recover the data</li></ul></li><li>If we shut down the node or it crashes unexpectedly, the commit log can ensure that data is not lost; that’s because if the node restart, the commit log gets replayed</li></ul><h3 id="3-4-2-MemTable"><a href="#3-4-2-MemTable" class="headerlink" title="3.4.2 MemTable"></a>3.4.2 MemTable</h3><ul><li>After written to the commit log, the data is written to a memory resident data structure called memTable<ul><li>Each node has a MemTable in memory for each Cassandra table</li><li>Each MemTable contains data for a specific Cassandra table, and it resembles that table in memory</li><li>Each MemTable accrues writes and <strong>provides reads for data not yet flushed to disk</strong></li><li>Commit log stores all the writes in sequential order, with each new write appended to the end; whereas MemTable stores data in the sorted order of partition key and clustering columns</li><li>After writing data to the commit log and MemTable, the node <strong>sends an acknowledgement to the coordinator</strong> that the data has been successfully written</li></ul></li></ul><h3 id="3-4-3-SStable"><a href="#3-4-3-SStable" class="headerlink" title="3.4.3 SStable"></a>3.4.3 SStable</h3><ul><li>When the number of objects stored in the MemTable reaches a threshold, the contents of the MemTable are <strong>flushed to disk</strong> in a file called <strong>SSTable</strong><ul><li>At this point, a new MemTable is created to store subsequent data</li><li>The flush is non blocking operation</li><li>Multiple Memtables may exist for a single table<ul><li>One current, and the rest waiting to be flushed</li></ul></li><li>When the MemTable is flushed to SStables, <strong>corresponding entries in the commit log</strong> are removed</li></ul></li><li>SStable —Sorted String Table<ul><li>Once a MemTable is flushed to disk as an SStable, it is immutable and cannot be changed later</li><li>Each delete or update is considered as a new write operation</li></ul></li><li>The current data state of a Cassandra table consists of its MemTables in memory and SSTables on the disk.<ul><li>Therefore, on reads, Cassandra will read both SSTables and MemTables to find data values, as the MemTable may contain values that have not yet been flushed to the disk.</li><li>The MemTable works like a write-back cache that Cassandra looks up by key</li></ul></li></ul><p><img src="https://i.loli.net/2021/09/17/Qd7x4M6HRrtuAoZ.png" alt="Whole Write Path"></p><h2 id="3-5-Anatomy-of-Cassandra’s-Read-Operation"><a href="#3-5-Anatomy-of-Cassandra’s-Read-Operation" class="headerlink" title="3.5 Anatomy of Cassandra’s Read Operation"></a>3.5 Anatomy of Cassandra’s Read Operation</h2><p><img src="https://i.loli.net/2021/09/17/wIZKE97YqVNAsrP.png" alt="Whole Read Path"></p><h3 id="3-5-1-Caching"><a href="#3-5-1-Caching" class="headerlink" title="3.5.1 Caching"></a>3.5.1 Caching</h3><ul><li>Row Cache<ul><li>Cache frequently read/ hot rows</li><li>Stores a complete data row, which can be returned directly to the client if requested by a read operation</li><li>Could significantly speed up read access for frequently accessed rows, at the cost of more memory usage</li></ul></li><li>Key Cache<ul><li>Stores a map of recently read partition keys to their <strong>SSTable offsets</strong></li><li>This facilitates faster read access into SSTables and improves the read performance</li><li>Use less memory comparing with row cache and provides a considerable improvement for read operations</li></ul></li><li>Chunk Cache<ul><li>Chunk Cache is used to store umcompressed chunks of data read from SSTable files that are accessed frequently</li></ul></li></ul><h3 id="3-5-2-Read-From-MemTable"><a href="#3-5-2-Read-From-MemTable" class="headerlink" title="3.5.2 Read From MemTable"></a>3.5.2 Read From MemTable</h3><ul><li>When a read request come in, node performs a binary search on the partition key to find the required partition and then return the row</li></ul><h3 id="3-5-3-Read-From-SSTable"><a href="#3-5-3-Read-From-SSTable" class="headerlink" title="3.5.3 Read From SSTable"></a>3.5.3 Read From SSTable</h3><ul><li><p>Bloom Filters</p><ul><li>Each SSTable has a Bloom Filter associated with it, which tells if a particular key is present in it or not</li><li>Used to boost performance of read operations</li><li>It’s a very fast, non deterministic algorithms for testing whether an element is a member of a set</li><li>It’s possible to get a false positive but never a false negative</li><li>Theory<ul><li>It works by <strong>mapping the values in a data set into a bit array</strong> and <strong>condensing a larger data set into a digest string</strong> with a hash function</li><li>Filters are stored in memory and are used to improve performance by reducing the need for disk access on key lookups</li></ul></li></ul></li><li><p>How are SSTables stored on the disk?</p><ul><li><p>Consists of two files</p><ul><li><p>Data File</p><ul><li>Actual data is stored here</li><li>It has partitions and rows associated with those partitions</li><li>Partitions are in sorted order</li></ul></li><li><p>Partition Index File</p><ul><li><p>Stored on disk, partition index file stores the sorted partition keys mapped to their SSTable offsets</p></li><li><p>Enable locating a partition exactly in an SSTable rather than scanning data</p><p><img src="https://i.loli.net/2021/09/17/9gUpTXZyLSksDdK.png" alt="Read via Partition Index File"></p></li></ul></li></ul></li></ul></li><li><p>Partition Index Summary File</p><ul><li><p>It’s stored in memory, stores the summary of the partition index file for performance improvement</p><ul><li><p>Two level index, e.g, search for key=19</p></li><li><p>in partition index summary file, it lays to key range 10 - 21</p></li><li><p>then we could go to byte offset 32,</p></li><li><p>in partition index file , we start from 32, to find partition key 19, and then we could go to 5450</p><p><img src="https://i.loli.net/2021/09/17/efsVEmvGAkIldF6.png" alt="Read via Partition Index Summary File"></p></li></ul></li></ul></li><li><p>Read from KeyCache</p><ul><li><p>As the Key Cache stores a map of recently read partition keys to their SSTable offset, it’s the fastest way to find the required row in the SSTable</p><p>  <img src="https://i.loli.net/2021/09/17/5KPTohGmWpecr1a.png" alt="Read From KeyCache"></p></li></ul></li><li><p>Overall workflow</p><p>  <img src="https://i.loli.net/2021/09/17/2zKlRtS48NQYkud.png" alt="Overall Workflow"></p></li></ul><h2 id="3-6-Compaction"><a href="#3-6-Compaction" class="headerlink" title="3.6 Compaction"></a>3.6 Compaction</h2><h3 id="3-6-1-Why-we-need-compaction-And-How-it-Works"><a href="#3-6-1-Why-we-need-compaction-And-How-it-Works" class="headerlink" title="3.6.1 Why we need compaction? And How it Works?"></a>3.6.1 Why we need compaction? And How it Works?</h3><p><img src="https://i.loli.net/2021/09/17/2DgirVjkeq6AI4T.png" alt="Compaction"></p><ul><li>SSTables are immutable, which helps Cassandra achieve high write speeds</li><li>And flushing from MemTable to SSTable is a continuous process, which means we could have a large number of SSTables lying on the disk</li><li>It’s tedious to scan all these SSTables while reading</li><li>We need compaction thus we could merge multiple related SSTables into a single one to improve reading speed</li><li>During compaction, the data in SSTables is merged, keys are merged, columns are combined, obsolete values are discarded, and a new index is created</li></ul><h3 id="3-6-2-Compaction-Strategies"><a href="#3-6-2-Compaction-Strategies" class="headerlink" title="3.6.2 Compaction Strategies"></a>3.6.2 Compaction Strategies</h3><ul><li>SizeTiered Compaction Strategy<ul><li>Suitable for insert-heavy and general workloads</li><li>Triggered when multiple SSTables of a similar size are present</li></ul></li><li>Leveled Compaction Strategy<ul><li>Optimize read performance</li><li>Groups SSTables into levels, each of which has a fixed size limit which is ten times larger than the previous level</li></ul></li><li>Time Window Compaction Strategy<ul><li>Work on time series data</li><li>Compact SSTables within a configured time window</li><li>Ideal for time series data which is immutable after a fixed time interval</li></ul></li></ul><h3 id="3-6-3-Sequential-Writes"><a href="#3-6-3-Sequential-Writes" class="headerlink" title="3.6.3 Sequential Writes"></a>3.6.3 Sequential Writes</h3><ul><li>Main reason that writes perform so well in Cassandra</li><li>No reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations</li><li>Compaction is intended to amortize the reorganization of data, but it uses sequential I/O to do so, which makes it efficient</li></ul><h2 id="3-7-Tombstones"><a href="#3-7-Tombstones" class="headerlink" title="3.7 Tombstones"></a>3.7 Tombstones</h2><h3 id="3-7-1-What-are-Tombstones"><a href="#3-7-1-What-are-Tombstones" class="headerlink" title="3.7.1 What are Tombstones?"></a>3.7.1 What are Tombstones?</h3><ul><li>Scenario<ul><li>We delete some data for a node that is down or unreachable, it would miss a delete</li><li>When the node com back online later and a repair occurs, the node could resurrect the data due to re-sharing it with other nodes</li><li>To prevent deleted data from being reintroduced, Cassandra used a concept of a Tombstone</li></ul></li><li>Tombstone<ul><li>Similar to the idea of soft delete from the relational database</li><li>When we delete, Cassandra does not delete it right away, instead, it associated a tombstone with it, with Time to Expiry</li><li>It’s a marker to indicate data that has been deleted</li><li>When we execute a delete operation, data is not immediately deleted</li><li>Instead, it’s treated as an update operation that places a tombstone on the value</li><li>Default Time to Expiry is set to 10 days<ul><li>If the node is down longer than this value, it should be treated as failed and replaced</li></ul></li><li>Tombstones are removed as part of compaction</li></ul></li></ul><h3 id="3-7-2-Common-problems-associated-with-Tombstones"><a href="#3-7-2-Common-problems-associated-with-Tombstones" class="headerlink" title="3.7.2 Common problems associated with Tombstones"></a>3.7.2 Common problems associated with Tombstones</h3><ul><li>Takes storage space</li><li>When a table accumulates many tombstones, read queries on that table could become slow and can cause serious performance problems like timeouts.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/09/17/IGdzKBJwD9TbZ6f.png&quot; alt=&quot;Cassandra MindMap.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introdu
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
      <category term="Cassandra" scheme="https://www.llchen60.com/tags/Cassandra/"/>
    
  </entry>
  
  <entry>
    <title>Logical Fallacies</title>
    <link href="https://www.llchen60.com/Logical-Fallacies/"/>
    <id>https://www.llchen60.com/Logical-Fallacies/</id>
    <published>2021-09-12T01:23:39.000Z</published>
    <updated>2021-09-12T01:50:35.560Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Logical-Fallacies"><a href="#Logical-Fallacies" class="headerlink" title="Logical Fallacies"></a>Logical Fallacies</h1><h2 id="Overconfidence"><a href="#Overconfidence" class="headerlink" title="Overconfidence"></a>Overconfidence</h2><ul><li>Overconfidence — wishful thinking bias<ul><li>most people think they are above avg</li><li>overestimate possibilities that they want to happen</li><li><strong>this could explain the trade in financial market</strong></li><li>overconfidence in friends and leaders</li></ul></li></ul><h2 id="Cognitive-Dissonance"><a href="#Cognitive-Dissonance" class="headerlink" title="Cognitive Dissonance"></a>Cognitive Dissonance</h2><ul><li>Cognitive Dissonance  认知失调<ul><li>this concept used to describe the mental discomfort that results from holding two conflicting beliefs, values or attitudes</li><li>People tend to seek consistency in their attitudes and perceptions, so this conflict causes feelings of unease or discomfort</li><li>This inconsistency between <strong>what people believe and how they behave</strong> motivates people to <strong>engage in actions</strong> that will help minimize feelings of discomfort</li><li>when we made decision, most people will still look for info about it, to self prove hisself right… in a lot different aspects… to make themselves happy, and to prove they are make right decision</li><li>disposition effect — gonna avoid that</li><li>what’s the causes for that?<ul><li>Forced Compliance<ul><li>Engaging in behaviors that are opposed to your own beliefs due to external expectations, often for work, school, or a social situation</li></ul></li><li>New Information</li><li>Decisions<ul><li>People make decisions both large and small, on a daily basis</li><li>When faced with two similar choices, people often are left with feelings of dissonance because both options are equally appealing</li><li>Once they make decisions, people need to find a way to <strong>reduce feelings of discomfort</strong></li><li>Accomplish by justifying why their choice was the best option so that they can believe they made the right decision</li></ul></li></ul></li></ul></li></ul><h2 id="Mental-Compartments"><a href="#Mental-Compartments" class="headerlink" title="Mental Compartments"></a>Mental Compartments</h2><ul><li>Mental compartments<ul><li>people don’t look at whole portfolio, in fact, people has two or more portfolio<ul><li>usually they have a safe part and a risky part</li></ul></li></ul></li></ul><h2 id="Attention-Anomalies"><a href="#Attention-Anomalies" class="headerlink" title="Attention Anomalies"></a>Attention Anomalies</h2><ul><li>Attention Anomalies<ul><li>We cannot pay attention to anything</li><li>Attention is fundamental aspect of human intelligence and its limits</li><li>Social Basis for attention<ul><li>We incline to pay more attention to what other s pay attention to</li></ul></li></ul></li></ul><h2 id="Anchoring"><a href="#Anchoring" class="headerlink" title="Anchoring"></a>Anchoring</h2><ul><li>Anchoring<ul><li>A tendency in ambiguous situations to allow one’s decisions to be affected by some anchor</li><li>Our subconscious will do anchoring for us, lol</li><li>subjects unaware of their own anchoring behavior</li><li>stock prices anchored to past values, or to other stock in same market</li></ul></li></ul><h2 id="Representativeness-Heuristic"><a href="#Representativeness-Heuristic" class="headerlink" title="Representativeness Heuristic"></a>Representativeness Heuristic</h2><ul><li>Representativeness Heuristic<ul><li>People judge by similarity to familiar types, without regard to <strong>base rate probabilities</strong><ul><li>For example, we describe a person as artist, and skeptical, then what’s the highest possible occupation of him/ her?<ul><li>two choice: banker, and sculptress</li><li>should be banker, cause there are so many more bank tellers than sculptresses</li></ul></li></ul></li><li>Tendency to see patterns in what is really random walk</li><li>Stock price manipulators try to create patterns to fool investors</li></ul></li></ul><h2 id="Disjunction-Effect"><a href="#Disjunction-Effect" class="headerlink" title="Disjunction Effect"></a>Disjunction Effect</h2><ul><li>inability to make decisions in advance in anticipation of future information</li></ul><h2 id="Magical-Thinking-amp-Quasi-Magical-Thinking"><a href="#Magical-Thinking-amp-Quasi-Magical-Thinking" class="headerlink" title="Magical Thinking  &amp; Quasi Magical Thinking"></a>Magical Thinking  &amp; Quasi Magical Thinking</h2><ul><li>Some coincidence lead you to build superstitious, but there are actually no karma (cause and effect)</li><li>Belief that unrelated events are causally connected despite the absence of any plausible causal link between them, particularly as a result of supernatural effects.</li><li>E.G<ul><li>For voting, though our vote actually has basically 0 possibility to influence president election, but a lot people do it</li><li>For lottery, we somehow put more money if we select the number</li></ul></li></ul><h2 id="Personality-Disorders"><a href="#Personality-Disorders" class="headerlink" title="Personality Disorders"></a>Personality Disorders</h2><ul><li>culture and social contagion — collective memory<ul><li>same effect, same memory, then similar decisions</li></ul></li><li>Antisocial Personality Disorder</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Logical-Fallacies&quot;&gt;&lt;a href=&quot;#Logical-Fallacies&quot; class=&quot;headerlink&quot; title=&quot;Logical Fallacies&quot;&gt;&lt;/a&gt;Logical Fallacies&lt;/h1&gt;&lt;h2 id=&quot;Overc
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>抗逆力</title>
    <link href="https://www.llchen60.com/%E6%8A%97%E9%80%86%E5%8A%9B/"/>
    <id>https://www.llchen60.com/%E6%8A%97%E9%80%86%E5%8A%9B/</id>
    <published>2021-08-28T19:28:59.000Z</published>
    <updated>2021-08-28T19:30:58.113Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>很庆幸听了这场讲座，回想起原先做的一些决定，会囿于固定性思维，因为害怕失败，害怕被证实不够聪明，失去了很多很多成长的机会。感觉无论是工作还是生活，在度过了刚刚开始的熟悉了解阶段以后，后期除了专业技能上的提升之外，更加重要的是mindset的提升 — 关于你是怎么看待自己的，怎么看待你经历的事情。不知道最终答案是什么，但是这篇里面说的东西至少告诉了我想要达到理想的状态，你需要每天做些什么 :)  值得过一段时间回来再看看各种action items 呀</p></blockquote><h1 id="1-心理韧性"><a href="#1-心理韧性" class="headerlink" title="1. 心理韧性"></a>1. 心理韧性</h1><ul><li>高心理韧性是成功者的共性<ul><li>因为对于任何一个成功者来说，磨难是必不可少的一部分</li><li>成功者 坚韧不拔的精神</li><li>心理学研究<ul><li>预测成功的概率 和 坚韧不拔的特质有很明显的正相关，和智商的关系反倒并不是很强</li><li>生存下来的不是最强大的生物，也不是最聪明的生物，而是最能够适应变化的生物</li></ul></li></ul></li></ul><h2 id="1-1-心理学定义"><a href="#1-1-心理学定义" class="headerlink" title="1.1 心理学定义"></a>1.1 心理学定义</h2><ul><li>复原力 resilence<ul><li>人从逆境，冲突，痛楚，失败，压力当中迅速恢复的心理能力</li></ul></li><li>坚毅力 grip</li><li>创伤后的成长 PTG — Post Traumatic Growth<ul><li>不消沉，奋进</li></ul></li></ul><h2 id="1-2-高心理韧性人的特质"><a href="#1-2-高心理韧性人的特质" class="headerlink" title="1.2 高心理韧性人的特质"></a>1.2 高心理韧性人的特质</h2><ul><li>能力<ul><li>适应力</li><li>成长力</li><li>抗挫力</li><li>积极力 — 情绪的调节的方法</li><li>关系力 — 如何建立关系</li><li>控制力 — 淡定从容，自我控制</li></ul></li><li>特质<ul><li>有积极的认知方式  — the power of positive thinking<ul><li>决定我们的幸福指数的不是事情本身，而是我们如何看待这个事情</li></ul></li><li>乐观的情绪调节</li><li>健康的身心状态</li><li>强大的自我效能感  — 感觉自己能成，感觉自己有用  lol<ul><li>结婚能让男性长寿7年 lol</li></ul></li><li>解决问题的行动精神</li><li>良好的人际关系</li></ul></li></ul><h1 id="2-如何提升心理韧性"><a href="#2-如何提升心理韧性" class="headerlink" title="2. 如何提升心理韧性"></a>2. 如何提升心理韧性</h1><h2 id="2-1-自我效能感的提升"><a href="#2-1-自我效能感的提升" class="headerlink" title="2.1 自我效能感的提升"></a>2.1 自我效能感的提升</h2><ul><li>自我效能感的提升 Self Efficacy<ul><li>定义 — 是个人对自己完成某方面工作能力的主观评估，通过两条路径体现出来</li><li>高自我效能感的人，甚至会把压力 挫折 打击当做一种证明自己的能力的机遇</li><li>体现路径<ul><li>结果预期<ul><li>相信自己，认为我可以做到，是一种自我实现的预言</li></ul></li><li>效能预期<ul><li>我认为我能做到不是因为运气好或者环境好，而是因为我的能力</li><li>因此我要施展我的能力，为结果做足准备</li></ul></li></ul></li></ul></li><li>如何去做<ul><li>做出成功的模样<ul><li>装积极，是会变成真积极的</li><li>步伐更快</li><li>说话更多</li><li>做事主动</li><li>穿衣更正式些</li><li>锻炼更频繁些</li></ul></li><li>被成功者接纳<ul><li>与积极的人同行</li><li>替代性强化<ul><li>观察者看到榜样或者他人收到强化，成功了; 从而使得自己也倾向于做出榜样的行为</li></ul></li></ul></li><li>社会支持<ul><li>进化选择的是合作者</li><li>社会网络面积越大，更容易产生优势效应</li><li><strong>弱联系的强势效应</strong><ul><li>弱联系有着很快的低成本和高效能的传播效率</li><li>在六度分隔试验当中，正是层层叠加的弱联系将世界上原本毫不相关的人联系到了一起</li></ul></li></ul></li><li>模拟实战<ul><li>预见</li><li>大脑休闲的时候处于默认模式状态  hh<ul><li>会畅想未来，是一种竞争优势的~</li><li>对于事情进行遇见，是对我们帮助很大的</li></ul></li><li>Visualization  预见想象<ul><li>将自己将要做的事情去提前想象一下</li><li>过一遍自己需要怎么做</li><li>训练越多，意向越清晰</li><li>设想遇到打击，困难的时候你要怎么做</li></ul></li></ul></li><li>不断积累成功<ul><li>人最可怕的是发现自己一成不变</li><li>要去做</li></ul></li></ul></li></ul><h2 id="2-2-培养成长性思维"><a href="#2-2-培养成长性思维" class="headerlink" title="2.2 培养成长性思维"></a>2.2 培养成长性思维</h2><ul><li><p>人的思维模式</p><ul><li>成长性思维  Growth Mindset<ul><li>天赋只是起点</li><li>态度和努力可以决定一切</li><li>可以学会任何我想学会的东西</li><li>喜欢自我挑战</li><li>当我失败的时候，我学到了很多东西</li><li>我希望你表扬我很努力</li><li>如果别人成功了，我会收到别人的启发</li></ul></li><li>固定性思维 — 卓越的包袱<ul><li>我的聪明才智决定了一切</li><li>我擅长某些事，不擅长另外一些事</li><li>我不想尝试我可能不擅长的东西</li><li>如果我失败了，我就无地自容了</li><li>我希望你表扬我很聪明</li><li>如果别人成功了，他会威胁到我</li></ul></li></ul></li><li><p>固定性思维对于人的影响很大</p><ul><li>你会因为认为自己聪明，不敢做更大的挑战，因为一旦失败，你会害怕别人认为你不聪明了 会越来越难达到别人的预期的</li><li>被表扬努力的往往会选择更加困难的任务，也会更愿意通过学习，去尝试解决方案</li><li>卓越的包袱<ul><li>装酷的孩子的包袱</li><li>不愿意去冒险，不愿意去奋斗</li><li>努力愚蠢，装聪明</li><li>精英父母的过高的期望造成的心理压力和心理阴影</li><li>优秀女孩的诅咒，这种包袱往往对女孩的打击更大，她们往往更在意外在的评价，不敢冒险和努力</li><li>we are supposed to be dumb all the way, hhh</li></ul></li></ul></li><li><p>如何培养成长性思维</p><ul><li>改变考核的标准<ul><li>关注进步，而不是结果</li></ul></li><li>改变沟通的方式<ul><li>在评价表现的时候，用暂时不行代替就是不行</li><li>短暂 局部 可以改的</li><li>不要把事情说成稳定的长期的不可改变的</li><li>not yet instead of failed</li></ul></li><li>改变认知的习惯 — Albert Ellis 的认知治疗ABC</li><li>发挥辩证思维的优势 — 从负面体验中吸取成功的经验<ul><li>当一个人出于自我保护而抗拒内心的地狱的时候，他一并切断了通往内在天堂的道路。<ul><li>不承认自己内心的阴暗龌龊，那么就无从改进了</li></ul></li></ul></li></ul></li></ul><ul><li>认知治疗ABC<ul><li>构成<ul><li>A — Activating Events  诱发刺激</li><li>B — Beliefs  信念反应</li><li>C — Consequences  行为后果</li></ul></li><li>原理<ul><li>我们是改不了A的，但是我们可以改B，然后C就会发生变化！！</li><li>关键是你怎么看待A的 ！ 改变认知</li><li>真正困扰我们的并不是发生在我们身上的事情，而是我们围绕这个事情对它编织的故事，和由此引起的身心反应</li></ul></li></ul></li><li>情绪的ABCD理论 — 对于孩子而言<ul><li>出现了ABC以后，给一个机会让其反驳</li><li>让孩子去反驳他当时的念头</li><li>干预B  从而干预C</li></ul></li></ul><h2 id="2-3-提高自我调控的能力"><a href="#2-3-提高自我调控的能力" class="headerlink" title="2.3 提高自我调控的能力"></a>2.3 提高自我调控的能力</h2><ul><li>延迟满足， 自我控制</li><li>自我调控能力是可以锻炼从而获得提升的</li><li>如何进行训练<ul><li>体育锻炼</li><li>正念冥想 — 做事情沉浸其中就好啊！！</li><li>自我挑战</li><li>目标想象</li><li>有效休息</li><li>积极心态</li></ul></li></ul><h1 id="3-组织韧性"><a href="#3-组织韧性" class="headerlink" title="3. 组织韧性"></a>3. 组织韧性</h1><ul><li><p>复原力</p><ul><li>企业遇到困难后，如何回归正常</li></ul></li><li><p>复原后的发展能力</p></li><li><p>影响组织韧性的维度</p><ul><li>组织资本<ul><li>人力资源的保障<ul><li>什么政策</li></ul></li></ul></li><li>组织承诺<ul><li>员工对于组织的感情</li><li>信任</li></ul></li><li>组织领导<ul><li>leader本身的态度，思考，韧性</li></ul></li><li>组织学习</li><li>组织文化<ul><li>组织的传统和信仰</li></ul></li><li>社会网络</li></ul></li><li><p>提升组织韧性的方式</p><ul><li>Staff  选择心理韧性高的人才，锻炼心理韧性<ul><li>积极的自我认识</li><li>提倡积极的思维</li><li>加强关系建设</li><li>未来导向</li><li>乐观主义精神<ul><li>对于路径的乐观</li><li>对于结果的乐观</li></ul></li></ul></li><li>System 创造积极的心理健康环境</li><li>Skill</li></ul></li></ul><h1 id="4-压力的应对技巧"><a href="#4-压力的应对技巧" class="headerlink" title="4. 压力的应对技巧"></a>4. 压力的应对技巧</h1><h2 id="4-1-压力的应激反应"><a href="#4-1-压力的应激反应" class="headerlink" title="4.1 压力的应激反应"></a>4.1 压力的应激反应</h2><ul><li>应激反应的三轴心<ul><li>下丘脑</li><li>垂体</li><li>肾上腺</li></ul></li><li>三个器官会释放压力激素，使得我们的反应是fight or flight lol</li><li>而后激素水平下降</li><li>各种情绪<ul><li>焦虑  为未来的恐慌</li><li>抑郁  为过去伤心</li><li>自残</li></ul></li></ul><h2 id="4-2-与情绪有关的脑区"><a href="#4-2-与情绪有关的脑区" class="headerlink" title="4.2 与情绪有关的脑区"></a>4.2 与情绪有关的脑区</h2><ul><li><p>杏仁核  Amygdala</p><ul><li>会影响我们的情绪</li><li>情绪不好的时候会让杏仁核充血，然后杏仁核温度升高</li></ul></li><li><p>大脑皮层 Cerebral Cortex</p></li><li><p>如何应对压力 — <strong>抑制</strong>杏仁核的活动</p><ul><li>吸入凉气，降低杏仁核的温度  hhh</li><li>香气  让我们产生愉悦的感觉</li><li>写写日记</li></ul></li><li><p>激活大脑的奖励中枢</p><ul><li><p>神经元之间的间隙 靠神经递质连接</p></li><li><p>当奖赏中枢释放神经递质的时候，会释放积极的情绪</p></li><li><p>！！心理活动不是一个一个点，而是一片一片的产生的</p></li><li><p>多巴胺</p><ul><li>庆祝自己的成功 — 让自己的成功和快乐的体验延续一段时间<ul><li>将快乐的体验延续4分钟，就可以在大脑中形成记忆，从而形成一个快乐的神经网络</li></ul></li><li>做自己喜欢做的事情</li><li>享受艺术的美妙</li></ul></li><li><p>血清素</p><ul><li>能够振奋人的心情</li><li>什么时候会分泌<ul><li>体验到自我的价值</li><li>帮助别人的时候</li><li>自尊心的呵护<ul><li>保护自尊心  体现其价值</li></ul></li></ul></li><li>一些行为<ul><li>晒太阳~</li></ul></li></ul></li><li><p>内啡肽</p><ul><li>只有我们身心痛苦的时候，才会释放</li><li>行为<ul><li>有规律的运动</li><li>先苦后甜的体验</li><li>看喜剧，相声~ 烧脑的幽默</li></ul></li></ul></li><li><p>催产素</p><ul><li>男人也有催产素</li><li>主要作用是增加人的爱的感受，而不是为了怀孕</li><li>行为<ul><li>夸奖，赞美</li><li>陪伴</li></ul></li></ul><h2 id="4-3-应对压力的长期策略"><a href="#4-3-应对压力的长期策略" class="headerlink" title="4.3 应对压力的长期策略"></a>4.3 应对压力的长期策略</h2></li><li><p>压力容易让人失控失常</p></li><li><p>培养应对压力的积极习惯</p><ul><li>strength based approach  发挥自己的长处优势  找到自己的优势，然后充分在工作生活当中使用  使用自己的优势！！！</li><li>找到自己的流  find your flow<ul><li>喜欢做的事情，进入心流状态</li></ul></li><li>借助一些科学方法，自修，同修，专修<ul><li>reading &amp; learning</li></ul></li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>彭凯平 演讲  — 抗逆力— 重压下的心理韧性与成功</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;很庆幸听了这场讲座，回想起原先做的一些决定，会囿于固定性思维，因为害怕失败，害怕被证实不够聪明，失去了很多很多成长的机会。感觉无论是工作还是生活，在度过了刚刚开始的熟悉了解阶段以后，后期除了专业技能上的提升之外，更加重要的是mindset的提升 —
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="积极心理学" scheme="https://www.llchen60.com/tags/%E7%A7%AF%E6%9E%81%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Make Body Language Your Super Power</title>
    <link href="https://www.llchen60.com/Make-Body-Language-Your-Super-Power/"/>
    <id>https://www.llchen60.com/Make-Body-Language-Your-Super-Power/</id>
    <published>2021-08-28T02:24:14.000Z</published>
    <updated>2021-08-28T02:25:19.176Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Stand Strong</li><li>Gesture Effectively</li><li>Mind Your Audience</li></ul><h1 id="1-Posture"><a href="#1-Posture" class="headerlink" title="1. Posture"></a>1. Posture</h1><ul><li>Communication begins before you open your mouth</li><li>How to stand<ul><li>no<ul><li>hands in the pocket  — cannot convey strong msg</li><li>hands in the hip — tend to look overwhelming and powerful</li><li>hands in front of family jewels lol</li></ul></li><li>yes<ul><li>base posture — feet should be shoulder width apart<ul><li>that’s the first impression</li></ul></li><li>movement<ul><li>give</li><li>show</li><li>chop — strong msg</li></ul></li><li>palms up has better impact! comparing with palm down and pointing</li></ul></li></ul></li><li>Where to stand<ul><li>face your audience</li><li>move around in the center box</li><li>get rid of potential distraction<ul><li>like window, by nature we are attracted by moving thing, will break the concentration</li></ul></li></ul></li></ul><h1 id="2-Audience"><a href="#2-Audience" class="headerlink" title="2. Audience"></a>2. Audience</h1><ul><li>Speaker need to understand what audience is doing , make sure we are all in the journey</li><li>How to engage with audience more<ul><li>gesture</li><li>notice<ul><li>how your audience sitting</li><li>eye contact</li></ul></li><li>surprise<ul><li>cold call, lol</li></ul></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.youtube.com/watch?v=cFLjudWTuGQ&ab_channel=StanfordGraduateSchoolofBusiness">https://www.youtube.com/watch?v=cFLjudWTuGQ&amp;ab_channel=StanfordGraduateSchoolofBusiness</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Stand Strong&lt;/li&gt;
&lt;li&gt;Gesture Effectively&lt;/li&gt;
&lt;li&gt;Mind Your Audience&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;1-Posture&quot;&gt;&lt;a href=&quot;#1-Posture&quot; class=&quot;hea
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Distributed Messaging System: Kafka</title>
    <link href="https://www.llchen60.com/Distributed-Messaging-System-Kafka/"/>
    <id>https://www.llchen60.com/Distributed-Messaging-System-Kafka/</id>
    <published>2021-08-24T17:07:33.000Z</published>
    <updated>2021-08-24T17:10:35.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview-of-Messaging-Systems"><a href="#1-Overview-of-Messaging-Systems" class="headerlink" title="1. Overview of Messaging Systems"></a>1. Overview of Messaging Systems</h1><h2 id="1-1-Why-we-need-a-messaging-system"><a href="#1-1-Why-we-need-a-messaging-system" class="headerlink" title="1.1 Why we need a messaging system"></a>1.1 Why we need a messaging system</h2><ul><li><p>Aim:</p><ul><li>Reliably transfer a high throughput of messages between different entities</li></ul></li><li><p>Challenges</p><ul><li>how we handle a spike of messages</li><li>how we divide the work among a set of instances</li><li>how could we receive messages from different types of sources</li><li>what will happen if the service is down?</li></ul></li><li><p>We need messaging systems in distributed architecture due to challenges above</p></li></ul><h2 id="1-2-What-is-a-messaging-system"><a href="#1-2-What-is-a-messaging-system" class="headerlink" title="1.2 What is a messaging system?"></a>1.2 What is a messaging system?</h2><ul><li><p>responsible for transferring data among services /applications/ processes/ servers</p></li><li><p>help decouple different parts of a distributed system by providing an asynchronous way of transferring messaging between the sender and the receiver</p></li><li><p>Two common ways to handle messages</p><ul><li>Queuing<ul><li>msgs are stored sequentially in a queue</li><li>producers push msg to the rear of the queue</li><li>consumers extract the msgs from the front of the queue</li><li>a particular msg can be consumed by a <strong>max of one consumer</strong> only</li></ul></li><li>Publish - Subscribe<ul><li>messages are divided into topics</li><li>a publisher sends a message to a topic</li><li>subscribers subscribe to a topic to receive every message published to that topic</li><li>msg system that stores and maintains the msg named as <strong>message broker</strong></li></ul></li></ul></li></ul><h1 id="2-Kafka"><a href="#2-Kafka" class="headerlink" title="2. Kafka"></a>2. Kafka</h1><h2 id="2-1-General"><a href="#2-1-General" class="headerlink" title="2.1 General"></a>2.1 General</h2><ul><li><strong>publish subscribe based</strong> messaging system</li><li>takes streams of messages from applications known as producers, stores them reliably on a central cluster, and allows those messages to be received by applications that process the messages</li><li>kafka is mainly used for<ul><li>reliably storing a huge amount of data</li><li>enabling high throughput of message transfer between different entities</li><li>streaming real time data</li></ul></li><li>kafka is a distributed commit log — write ahead log<ul><li>append-only data structure that can <strong>persistently store a sequence of records</strong></li><li>all messages are stored <strong>on disk</strong></li><li>since all reads and writes happen <strong>in sequence</strong>, Kafka takes advantage of <strong>sequential disk reads</strong></li></ul></li></ul><h2 id="2-2-Use-Cases"><a href="#2-2-Use-Cases" class="headerlink" title="2.2 Use Cases"></a>2.2 Use Cases</h2><ul><li>Metrics<ul><li>collect and aggregate monitoring data</li></ul></li><li>Log Aggregation<ul><li>collect logs from multiple sources and make them available in a standard format to multiple consumers</li></ul></li><li>Stream Processing<ul><li>the raw data consumed from a topic is transformed, enriched, or aggregated and pushed to a <strong>new topic</strong> for further consumption. This way of data processing is known as stream processing.</li></ul></li><li>Commit Log<ul><li>can be used as an external commit log for any distributed system</li><li>Distributed services can log their transactions to Kafka to keep track of what is happening. This transaction data can be used for replication between nodes and also becomes very useful for disaster recovery, for example, to help failed nodes to recover their states.</li></ul></li><li>Website activity tracking<ul><li>Build a user activity tracking pipeline</li><li>User activities like page clicks, searches, etc., are published to Kafka into separate topics. These topics are available for subscription for a range of use cases, including real-time processing, real-time monitoring, or loading into Hadoop or data warehousing systems for offline processing and reporting</li></ul></li><li>Product Suggestion</li></ul><h1 id="3-High-Level-Architecture"><a href="#3-High-Level-Architecture" class="headerlink" title="3. High Level Architecture"></a>3. High Level Architecture</h1><h2 id="3-1-Common-Terms"><a href="#3-1-Common-Terms" class="headerlink" title="3.1 Common Terms"></a>3.1 Common Terms</h2><ul><li>Brokers<ul><li>A Kafka server</li><li>responsible for reliably storing data provided by the producers and making it available to the consumers</li></ul></li><li>Records<ul><li>A message or an event that get stored in Kafka</li><li>A record contains<ul><li>key</li><li>value</li><li>timestamp</li><li>optional metadata headers</li></ul></li></ul></li><li>Topics<ul><li>Messages are divided into categories called topics</li><li>Each msg that Kafka receives from a producer is associated with a topic</li><li>consumers can subscribe to a topic to get notified when new messages are added to the topic</li><li>a topic can have multiple subscribers that read messages from it</li><li>a topic is identified by its name and must be unique</li><li>mes in a topic can be read as often as needed — message are not deleted after consumption, instead, Kafka <strong>retains messages for a configurable amount of time or until a storage size is exceeded</strong></li></ul></li><li>Producers<ul><li>Applications that publish or write records to Kafka</li></ul></li><li>Consumers<ul><li>Applications that subscribe to read and process data from Kafka topics</li><li>Consumers subscribe to one or more topics and consume published messages by pulling data from the brokers</li><li>In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for</li></ul></li></ul><h2 id="3-2-Architecture"><a href="#3-2-Architecture" class="headerlink" title="3.2 Architecture"></a>3.2 Architecture</h2><p><img src="https://i.loli.net/2021/08/25/Chuwtvg4mNfFU58.png" alt="Overall Architecture"></p><ul><li>Kafka cluster<ul><li>Kafka is run as a cluster of one or more servers, where each server is responsible for running one Kafka broker</li></ul></li><li>ZooKeeper<ul><li>Distributed key value store</li><li>Used for coordination and storing configurations</li><li>Kafka uses ZooKeeper to coordinate between Kafka brokers; ZooKeeper maintains metadata information about the Kafka cluster</li></ul></li></ul><h2 id="3-3-Performance-concern"><a href="#3-3-Performance-concern" class="headerlink" title="3.3 Performance concern"></a>3.3 Performance concern</h2><h3 id="3-3-1-Storing-messages-to-disks"><a href="#3-3-1-Storing-messages-to-disks" class="headerlink" title="3.3.1 Storing messages to disks"></a>3.3.1 Storing messages to disks</h3><ul><li>there is a huge difference in disk performance between <strong>random block access and sequential access</strong>. Random block access is slower because of <strong>numerous disk seeks</strong>, whereas the sequential nature of writing or reading, enables disk operations to be <strong>thousands of times faster</strong> than random access.</li><li>OS level optimization<ul><li>Read Ahead — prefetch large block multiples</li><li>Write Behind — group small logical writes into big physical writes</li><li>PageCache — cache the disk in free RAM</li></ul></li><li>Zero Copy optimization<ul><li>OS copy data from the pageCache directly to a socket, effectively bypassing the kafka broker application entirely</li></ul></li><li>Kafka protocol to group msg together<ul><li>reduce network overhead</li></ul></li></ul><h1 id="4-Dive-Deep-in-Kafka-Cluster"><a href="#4-Dive-Deep-in-Kafka-Cluster" class="headerlink" title="4. Dive Deep in Kafka Cluster"></a>4. Dive Deep in Kafka Cluster</h1><h2 id="4-1-Topic-Partitions"><a href="#4-1-Topic-Partitions" class="headerlink" title="4.1 Topic Partitions"></a>4.1 Topic Partitions</h2><ul><li><p>Topics are partitioned, spread over a number of fragments</p></li><li><p>Each partition can be placed on a separate Kafka broker</p></li><li><p>A new message get appended to one of the topic’s partition</p><ul><li>producer controls which partition it publishes to based on the data</li></ul></li><li><p>One partition is an <strong>ordered sequence</strong> of messages</p><ul><li>producers continually append new messages to partition</li><li>ordering of messages is <strong>maintained at the partition level, not across the topic</strong></li></ul></li><li><p>Unique sequence ID — offset</p><ul><li>It will get assigned to every message that enters a partition</li><li>used to identify every message’s sequential position within a topic’s partition</li><li>offset sequences are unique only to each partition</li><li>to locate a specific message<ul><li>topic</li><li>partition</li><li>offset number</li></ul></li><li>producers can choose to publish a message to any partition<ul><li>if ordering within a partition is not needed, a round robin partition strategy can be used</li><li>Placing each partition on separate Kafka brokers enables multiple consumers to read from a topic in parallel. That means, different consumers can concurrently read different partitions present on separate brokers</li></ul></li></ul></li><li><p>Messages once written to partitions are immutable and cannot be updated.</p></li><li><p>Kafka guarantees that messages with the same key are written to the same partition.</p></li></ul><h2 id="4-2-Dumb-Broker-and-Smart-Consumer"><a href="#4-2-Dumb-Broker-and-Smart-Consumer" class="headerlink" title="4.2 Dumb Broker and Smart Consumer"></a>4.2 Dumb Broker and Smart Consumer</h2><ul><li>Kafka does not keep track of what records are read by the consumer</li><li>Consumers themselves poll kafka for new messages and say what records they want to read<ul><li>this allow them to increment/ decrement the offset they are as they wish</li></ul></li></ul><h2 id="4-3-Leader-and-Follower"><a href="#4-3-Leader-and-Follower" class="headerlink" title="4.3 Leader and Follower"></a>4.3 Leader and Follower</h2><p>Every topic can be replicated to multiple Kafka brokers to make the data fault-tolerant and highly available. Each topic partition has one leader broker and multiple replica (follower) brokers. </p><ul><li>Structure<ul><li>the broker cluster could have multiple brokers, each broker could have multiple partitions which belong to different topics</li><li>Each topic partition would have one lead broker and multiple replica brokers</li></ul></li></ul><h3 id="4-3-1-Leader"><a href="#4-3-1-Leader" class="headerlink" title="4.3.1 Leader"></a>4.3.1 Leader</h3><ul><li>A leader is the node responsible for all reads and writes for the given partition</li><li>Each partition has one kafka broker acting as a leader</li></ul><h3 id="4-3-2-Follower"><a href="#4-3-2-Follower" class="headerlink" title="4.3.2 Follower"></a>4.3.2 Follower</h3><ul><li><p>To handle single point of failure, Kafka replicate partitions and distribute them across multiple broker servers called followers.</p></li><li><p>Each follower’s responsibility is to replicate the leader’s data to serve as a backup partition</p><ul><li><p>any follower can take over the leadership if the leader goes down</p><ul><li><p>from the image below, you could see only the leader take read and write requests, follower acts as replica but not take any read and write reqeusts</p><p><img src="https://i.loli.net/2021/08/25/4t7kVJfv3jliZyK.png" alt="Leader and Follower"></p></li></ul></li></ul></li></ul><h3 id="4-3-3-In-Sync-Replicas"><a href="#4-3-3-In-Sync-Replicas" class="headerlink" title="4.3.3 In Sync Replicas"></a>4.3.3 In Sync Replicas</h3><ul><li>In Sync Replicas means the broker has the latest data for a given partition</li><li>A leader is always an in sync replica</li><li>A follower is an in sync replica only if it has fully caught up to the partition it is following</li><li>Only ISRs are eligible to become partition leaders.</li><li>Kafka can choose the minimum number of ISRs required before the data becomes available for consumers to read</li></ul><h3 id="4-3-4-High-Water-mark"><a href="#4-3-4-High-Water-mark" class="headerlink" title="4.3.4 High Water mark"></a>4.3.4 High Water mark</h3><ul><li><p>To ensure data consistency, the leader broker never returns (or exposes) messages which have not been replicated to a minimum set of ISRs</p></li><li><p>For this, brokers keep track of the high-water mark, which is the highest offset that all ISRs of a particular partition share</p></li><li><p>The leader exposes data only up to the high-water mark offset and propagates the high-water mark offset to all followers</p><p>  <img src="https://i.loli.net/2021/08/25/pyDKzGCRow6O2Wu.png" alt="High Water Mark"></p></li></ul><h1 id="5-Consumer-Group"><a href="#5-Consumer-Group" class="headerlink" title="5. Consumer Group"></a>5. Consumer Group</h1><ul><li>A set of one or more consumers working together in parallel to consume messages from topic partitions, messages are equally divided among all the consumers of a group. with no two consumers receiving the same message</li></ul><h2 id="5-1-How-to-distribute-a-specific-message-to-only-a-single-consumer"><a href="#5-1-How-to-distribute-a-specific-message-to-only-a-single-consumer" class="headerlink" title="5.1 How to distribute a specific message to only a single consumer"></a>5.1 How to distribute a specific message to only a single consumer</h2><ul><li><p>only a single consumer reads messages from any partition within a consumer group</p><ul><li>means only one consumer can work on a partition in a consumer group at a time</li><li>every time a consumer is added to or removed from a group, the consumption is rebalanced within the group</li></ul></li><li><p>with consumer groups, consumers can be parallelized so that multiple consumers can read from multiple partitions on a topic, allowing a very high message processing throughput</p></li><li><p>number of partitions impacts consumers’ maximum parallelism. as there cannot be more consumers than partitions</p></li><li><p>Kafka stores the <strong>current offset per consumer group per topic per partition</strong>, as it would for a single consumer. This means that unique messages are only sent to a single consumer in a consumer group, and the load is balanced across consumers as equally as possible</p></li><li><p><strong>Number of consumers in a group = number of partitions:</strong> each consumer consumes one partition.</p></li><li><p><strong>Number of consumers in a group &gt; number of partitions:</strong> some consumers will be idle.</p></li><li><p><strong>Number of consumers in a group &lt; number of partitions:</strong> some consumers will consume more partitions than others.</p></li></ul><h1 id="6-Kafka-Workflow"><a href="#6-Kafka-Workflow" class="headerlink" title="6. Kafka Workflow"></a>6. Kafka Workflow</h1><h2 id="6-1-Pub-sub-messaging"><a href="#6-1-Pub-sub-messaging" class="headerlink" title="6.1 Pub sub messaging"></a>6.1 Pub sub messaging</h2><ul><li>Producer publish messages on a topic</li><li>Kafka broker stores messages in the partitions configured for that particular topic.<ul><li>If the producer did not specify the partition in which the msg should be stored, the broker ensures that the msg are equally shared between partitions</li><li>If the producer sends two msgs and there are two partitions, Kafka will store those two in two partitions separately.</li></ul></li><li>consumer subscribe to a specific topic</li><li>Kafka will provide the current offset of the topic to the consumer and also saves that offset in the zookeeper</li><li>consumer request kafka at regular intervals for new msgs</li><li>once kafka receives msg from producers, it forward these messages to the consumer</li><li>consumer will receive msg and process it</li><li>once processed, consumer will send an acknowledgement to the kafka broker</li><li>upon receiving the acknowledgement, kafka <strong>increments the offset and updates it in the zooKeeper</strong><ul><li>this info is stored in zooKeeper, thus consumer could read the next msg correctly even during broker outages</li></ul></li><li>consumers can rewind/ skip to the desired offset of a topic at any time and read all the subsequent messages</li></ul><h2 id="6-2-Kafka-workflow-for-consumer-group"><a href="#6-2-Kafka-workflow-for-consumer-group" class="headerlink" title="6.2 Kafka workflow for consumer group"></a>6.2 Kafka workflow for consumer group</h2><ul><li>Producers publish messages on a topic.</li><li>Kafka stores all messages in the partitions configured for that particular topic, similar to the earlier scenario.</li><li>A single consumer subscribes to a specific topic, assume <code>Topic-01</code> with Group ID as <code>Group-1</code>.</li><li>Kafka interacts with the consumer in the same way as pub-sub messaging until a new consumer subscribes to the same topic, <code>Topic-01</code>, with the same Group ID as <code>Group-1</code>.</li><li>Once the new consumer arrives, Kafka switches its operation to share mode, such that each message is passed to only one of the subscribers<br>of the consumer group <code>Group-1</code>. This message transfer is<br>similar to queue-based messaging, as only one consumer of the group<br>consumes a message. Contrary to queue-based messaging, messages are not<br>removed after consumption.</li><li>This message transfer can go on until the number of consumers<br>reaches the number of partitions configured for that particular topic.</li><li>Once the number of consumers exceeds the number of partitions, the<br>new consumer will not receive any message until an existing consumer<br>unsubscribes. This scenario arises because each consumer in Kafka will<br>be assigned a minimum of one partition. Once all the partitions are<br>assigned to the existing consumers, the new consumers will have to wait.</li></ul><h1 id="7-ZooKeeper"><a href="#7-ZooKeeper" class="headerlink" title="7. ZooKeeper"></a>7. ZooKeeper</h1><h2 id="7-1-What-is-ZooKeeper"><a href="#7-1-What-is-ZooKeeper" class="headerlink" title="7.1 What is ZooKeeper"></a>7.1 What is ZooKeeper</h2><ul><li>A distributed configuration and synchronization service</li><li>In Kafka case, help to store basic metadata<ul><li>information about brokers</li><li>topics</li><li>partitions</li><li>partition leader/ followers</li><li>consumer offsets</li></ul></li></ul><h2 id="7-2-Act-as-central-coordinator"><a href="#7-2-Act-as-central-coordinator" class="headerlink" title="7.2 Act as central coordinator"></a>7.2 Act as central coordinator</h2><p>ZooKeeper is used for storing all sorts of metadata about the Kafka cluster:</p><ul><li>It maintains the <strong>last offset position</strong> of each consumer group per partition, so that consumers can quickly recover from the last position in case of a failure (although modern clients store offsets in a<br>separate Kafka topic).</li><li>It tracks the topics, number of partitions assigned to those topics, and leaders’/followers’ location in each partition.</li><li>It also manages the access control lists (ACLs) to different topics in the cluster. ACLs are used to enforce access or authorization.</li></ul><h2 id="7-3-How-to-find-leaders"><a href="#7-3-How-to-find-leaders" class="headerlink" title="7.3 How to find leaders"></a>7.3 How to find leaders</h2><ul><li>The producer connects to any broker and asks for the leader of Partition 1<ul><li>each broker contains metadata</li><li>each brokers will talk to zooKeeper to get the latest metadata</li></ul></li><li>The broker responds with the identification of the leader broker responsible for partition 1</li><li>The producer connects to the leader broker to publish the message</li></ul><h1 id="8-Controller-Broker"><a href="#8-Controller-Broker" class="headerlink" title="8. Controller Broker"></a>8. Controller Broker</h1><ul><li>Within the Kafka cluster, one broker will be elected as the Controller</li><li>Responsibility<ul><li>admin operations<ul><li>creating/ deleting a topic</li><li>adding partitions</li><li>assigning leaders to partitions</li><li>monitoring broker failures</li></ul></li><li>check the health of other brokers in the system periodically</li><li>communicates the result of the partition leader election to other brokers in the system</li></ul></li></ul><h2 id="8-1-Split-brain-issue"><a href="#8-1-Split-brain-issue" class="headerlink" title="8.1 Split brain issue"></a>8.1 Split brain issue</h2><ul><li>some controller has temporary issue, during the period, we assign a new controller, but the previous one auto recover, so we have two controllers and it could bring inconsistency easily.</li><li>Solution:<ul><li>Generation Clock<ul><li>simply a monotonically increasing number to indicate a server’s generation</li><li>If the old leader had an epoch number of ‘1’, the new one would have ‘2’.</li><li>This epoch is included in every request that is sent from the Controller to other brokers.</li><li>This way, brokers can now easily differentiate the real Controller by simply trusting the Controller with the highest number.</li><li>The Controller with the highest number is undoubtedly the latest one, since the epoch number is always increasing.</li><li>This epoch number is stored in ZooKeeper.</li></ul></li></ul></li></ul><h1 id="9-Delivery-Semantics"><a href="#9-Delivery-Semantics" class="headerlink" title="9. Delivery Semantics"></a>9. Delivery Semantics</h1><h2 id="9-1-Producer-Delivery-Semantics"><a href="#9-1-Producer-Delivery-Semantics" class="headerlink" title="9.1 Producer Delivery Semantics"></a>9.1 Producer Delivery Semantics</h2><ul><li>How can a producer know that the data is successfully stored at the leader or that the followers are keeping up with the leader</li><li>Kafka offers three options to denote the <strong>number of brokers</strong> that <strong>must receive the record</strong> before the <strong>producer considers the write as successful</strong><ul><li>Async<ul><li>Producer sends a msg to kafka and does not wait for acknowledgement from the server</li><li>fire-and-forget approach gives the best performance as we can write data to Kafka at network speed, but <strong>no guarantee can be made</strong> that the server has received the record in this case.</li></ul></li><li>Committed to Leader<ul><li>Producer waits for an acknowledgment from the leader.</li><li>This ensures that the data is committed at the leader; it will be slower than the ‘Async’ option, as the data has to be written on disk on the leader.</li><li>Under this scenario, the leader will respond without waiting for acknowledgments from the followers.</li><li>In this case, the record <strong>will be lost if the leader crashes immediately after acknowledging the producer but before the followers have replicated it</strong>.</li></ul></li><li>Committed to Leader and Quorum<ul><li>Producer waits for an acknowledgment from the leader and the quorum. This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This will be the slowest write but guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee.</li></ul></li></ul></li></ul><h2 id="9-2-Consumer-Delivery-Semantics"><a href="#9-2-Consumer-Delivery-Semantics" class="headerlink" title="9.2 Consumer Delivery Semantics"></a>9.2 Consumer Delivery Semantics</h2><ul><li>Ways to provide consistency to the consumer<ul><li>At most once<ul><li>Message may be lost but are never redelivered</li><li>Under this option, the consumer upon receiving a message, commit (or increment) the offset to the broker. Now, if the consumer crashes before fully consuming the message, that message will be lost, as when the consumer restarts, it will receive the next message from the last committed offset.</li></ul></li><li>At least once<ul><li>Messages are never lost but maybe redelivered</li><li>This scenario occurs when the consumer receives a message from Kafka, and it does not immediately commit the offset.</li><li>Instead, it waits till it completes the processing.</li><li>So, if the consumer crashes after processing the message but before committing the offset, it has to reread the message upon restart.</li><li>Since, in this case, the consumer never committed the offset to the broker, the broker will redeliver the same message. Thus, duplicate message delivery could happen in such a scenario.</li></ul></li><li>Exactly once<ul><li>It is very hard to achieve this unless the consumer is working with a transactional system.</li><li>Under this option, the consumer puts the message processing and the offset increment in one transaction.</li><li>This will ensure that the offset increment will happen only if the whole transaction is complete.</li><li>If the consumer crashes while processing, the transaction will be rolled back, and the offset will not be incremented. When the consumer restarts, it can reread the message as it failed to process it last time. This option leads to no data duplication and no data loss but can lead to decreased throughput.</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview-of-Messaging-Systems&quot;&gt;&lt;a href=&quot;#1-Overview-of-Messaging-Systems&quot; class=&quot;headerlink&quot; title=&quot;1. Overview of Messaging Syste
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>货运代理法律风险</title>
    <link href="https://www.llchen60.com/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E6%B3%95%E5%BE%8B%E9%A3%8E%E9%99%A9/"/>
    <id>https://www.llchen60.com/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E6%B3%95%E5%BE%8B%E9%A3%8E%E9%99%A9/</id>
    <published>2021-08-17T04:29:17.000Z</published>
    <updated>2021-08-19T23:01:27.883Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-货运代理法律风险基本内容"><a href="#1-货运代理法律风险基本内容" class="headerlink" title="1. 货运代理法律风险基本内容"></a>1. 货运代理法律风险基本内容</h1><h2 id="1-1-国际货运代理人定义"><a href="#1-1-国际货运代理人定义" class="headerlink" title="1.1 国际货运代理人定义"></a>1.1 国际货运代理人定义</h2><ul><li>历史视角上来看<ul><li>国际货运代理经历了由显明代理人到隐名代理人再到<strong>运输合同当事人</strong>的过程  Fright Agency —→ Freight Forwarder</li><li>研究内容从单纯的代理法律关系向运输法律关系扩展</li><li>现代货运代理的定义 1992 《货运代理》<ul><li>提供并安排货物运输以取得报酬，</li><li>或者为货物合并拼箱并承担将这些货物由收货地运至目的地的<strong>运输责任</strong></li></ul></li><li>2002 《货运代理法》  Legal Classification of Fright Forwarders<ul><li>奠定了国际货运代理制度的FIATA的立法模式下，以货物运输合同的当事人 (as principal) 和非当事人 (except as principal) 来区分国际货物代理企业作为<strong>契约承运人和纯粹代理人</strong>的情况</li></ul></li><li>FIATA (International Federation of Freight Forwarders Association)《国际货运代理业示范法》<ul><li>国际货运代理人是指与客户达成货运代理协定，为其提供各类运输相关服务及其他辅助和咨询服务，或者在前述服务之外还以使用自有运输工具或者签发自己的运输单据的方式为客户承运货物的人</li></ul></li></ul></li><li>无船承运人<ul><li>对于实际货主而言，作为公共承运人与之订立海上货物运输合同</li><li>对于实际承运人而言，又承担着托运人的义务</li></ul></li><li>国际货运代理人定义<ul><li>International Freight Forwarder</li></ul></li><li>对货代的两类界定<ul><li>代理人说<ul><li>将国际货运代理人规定为受委托人的指示为其货物在国际间的运输及其他有关事务提供合理审慎服务的自然人或经济组织，本身业务不涉及货物的承运</li><li>与货主或委托人之间是纯粹的代理关系</li></ul></li><li>双重身份说<ul><li>规定了国际货运代理人是为委托方代办国际货运事务的代理人</li><li>规定了在一定条件下可以<strong>成为运输合同的当事人并对外承担承运人的责任</strong></li></ul></li></ul></li></ul><h2 id="1-2-货运代理法律关系辨析"><a href="#1-2-货运代理法律关系辨析" class="headerlink" title="1.2 货运代理法律关系辨析"></a>1.2 货运代理法律关系辨析</h2><ul><li>法律关系<ul><li>指相关海运国际公约，各国国内法以及行业规范等在调整国际海上货运代理行为的过程中形成的各有关主体间的权利和义务关系</li><li>国际海上货运代理法律关系包括<ul><li>国际货运代理企业作为海运代理人的法律关系</li><li>作为无船承运人的法律关系</li></ul></li></ul></li><li>作为海运代理人的法律关系<ul><li>在提供海上货物运输的相关代理业务的时候，呈现出的关系</li><li>内部委托法律关系<ul><li>国际货运代理企业与托运人之间的委托代理合同法律关系</li></ul></li><li>外部代理法律关系<ul><li>国际货运代理企业为托运人的利益和承运人签订海上货物运输合同而产生的运输合同法律关系</li><li>货代企业往往会以自己的名义代替多笔散货托运人与承运人签订一个总的运输合同，自己再分别同各托运人签订货运代理协议</li></ul></li><li>我国合同法对于包括货运代理合同在内的委托合同采用过错责任原则<ul><li>货代仅在自身确实受托事项存在过错并造成托运人损失的情况下承担违约损害赔偿责任</li><li>且只要在第三方选任上能够证明已经履行了合理和谨慎的义务，对由于第三方造成的托运人损失，可以免于承担责任</li></ul></li></ul></li><li>作为无船承运人的法律关系<ul><li>契约承运人的一种，即不拥有或者不经营船舶，不进行实际的货物运输活动，以签发无船承运人提单(Non Vessel Operating Common Carrier Bill of Loading)的方式明示或者默示对运输负有责任的人，承运责任来源于和托运人签订的货物运输契约，而不是实际的运输行为</li><li>法律关系<ul><li>无船承运人和托运人之间的海上货物运输合同关系</li><li>无船承运人和船公司之间的海上货物运输合同关系<ul><li>货代以托运人的身份和船公司 — 实际承运人签订运输合同并获得海运提单的 (Master Bill of Loading MBL)</li><li><strong>根据海商法， 无船承运人所需要承担的法律责任和实际承运人的责任是一样的</strong></li><li>无船承运人不享有不完全责任制<ul><li>当免责事由发生的时候，船公司可以免于对无船承运人承担责任，而无船承运人无法以相同的事由对托运人免责</li></ul></li><li>当承运人和实际承运人需要对货损货差需要进行赔偿的时候，二者在责任范围内承担连带责任</li><li>对于除了海上运输之外实际托运人提供的其他运输服务，无船承运人均承担严格责任</li></ul></li><li>无船承运人和船公司代理人之间的海上货物运输合同关系</li></ul></li></ul></li></ul><h1 id="2-风险状况分析"><a href="#2-风险状况分析" class="headerlink" title="2. 风险状况分析"></a>2. 风险状况分析</h1><ul><li><p>风险的定义</p><ul><li>由于国际货运代理企业在开展海上业务过程中受到客观法律环境，包括自身在内的各海洋运输相关主体所实施的法律行为的影响，导致其权利义务状态发生改变，从而产生的可能由该企业承担的法律上的不利后果</li><li>法律风险由环境诱因 — 客观法律环境，行为诱因 — 实施的法律行为及二者所引发的不利后果构成<ul><li>法律的不完善和不确定性<ul><li>不完善 — 制定法因为无法避免地滞后于社会的发展而必然存在的漏洞和空白</li><li>不确定性 — 由于法律条文意义晦涩之处需要法官来阐释说明，法官对于法律的理解会有所不同，从而导致了裁判结果不总是一致的</li></ul></li><li>行为诱因<ul><li>货代的一些不规范的法律行为，</li></ul></li></ul></li></ul></li><li><p>货代纠纷案件研究 争议焦点主要在：</p><ul><li>涉案主体间货运代理合同关系的认定<ul><li>因为转委托而引发的对双方间是否存在直接法律关系的异议</li><li>因为货运代理人和无船承运人身份识别而引发的对合同关系性质是货物运输合同法律关系还是货物运输代理合同法律关系的异议</li></ul></li></ul></li><li><p>主要风险分类</p><ul><li>身份认定上的风险<ul><li>指对国际货运代理企业是涉案货物的海运代理人还是无船承运人的认定结果和企业自身对此的认识不同造成的问题</li></ul></li><li>转委托和双方代理上的风险</li><li>涉外法律适用上的风险<ul><li>譬如美国统一商法典，允许以记名提单的方式放货，如果双方在法律适用条款当中约定适用美国法，那么海外代理提单无单放货的行为就不属于过错行为</li></ul></li></ul></li></ul><h1 id="3-风险成因分析"><a href="#3-风险成因分析" class="headerlink" title="3. 风险成因分析"></a>3. 风险成因分析</h1><h2 id="3-1-身份认定上的法律风险成因"><a href="#3-1-身份认定上的法律风险成因" class="headerlink" title="3.1 身份认定上的法律风险成因"></a>3.1 身份认定上的法律风险成因</h2><ul><li>国际货运代理企业是涉案货物的海运代理人还是无船承运人的认定结果的不同，会导致企业需要承担的法律责任和风险完全不同<ul><li>立法上的小混乱<ul><li>因为很有可能货运代理企业同时担当着无船承运人还有代理人的角色</li></ul></li><li>司法裁判的不确定<ul><li>《海上货代纠纷规定》的一些判断逻辑<ul><li>双方之间是否订立了海上货运代理合同，反映出来的是代理协议还是运输协议</li><li>受托人向委托人是否签发了提单，是承运提单还是代理人的分提单</li><li>受托人收取费用的名义是佣金还是运费</li><li>双方以往的交易历史和交易习惯</li></ul></li></ul></li><li>从业者自身行为的不规范<ul><li>是否保留了重要的往来文件，发票和提单</li></ul></li></ul></li></ul><h2 id="3-2-转委托与双方代理上的法律风险成因"><a href="#3-2-转委托与双方代理上的法律风险成因" class="headerlink" title="3.2 转委托与双方代理上的法律风险成因"></a>3.2 转委托与双方代理上的法律风险成因</h2><h3 id="3-2-1-转委托行为的法律风险成因"><a href="#3-2-1-转委托行为的法律风险成因" class="headerlink" title="3.2.1 转委托行为的法律风险成因"></a>3.2.1 转委托行为的法律风险成因</h3><ul><li>什么是转委托<ul><li>受托人将委托人委托其代为处理的事务转交给第三人处理的行为</li></ul></li><li>转委托行为的法律风险成因<ul><li>合同法规定，对于委托事务，除了经过委托人同意或者出现紧急状况可以转委托之外，受托人均应当亲自处理，否则就要为第三人的行为承担责任</li><li>但是对于国际货运代理行业来说，转委托是一个常规方式<ul><li>原因在于海运货代委托人更为看重成本和效率，让货代企业去完全处理每一件委托事务是不经济也不现实的</li></ul></li><li>造成货代企业转委托风险的是是否取得了委托人的同意</li><li>当前《海上货代纠纷规定》明确排除了推定托运人默示同意货运代理人转委托的可能<ul><li>转委托具体权限约定不明的时候，委托人将负有就不明权限想委托人报告的义务</li><li>委托人在受托人指示下与第三人的通常接触行为不能认定为委托人以该行为对转委托的明确同意</li></ul></li></ul></li></ul><h3 id="3-2-2-双方代理行为上的法律风险成因"><a href="#3-2-2-双方代理行为上的法律风险成因" class="headerlink" title="3.2.2 双方代理行为上的法律风险成因"></a>3.2.2 双方代理行为上的法律风险成因</h3><ul><li>什么是双方代理？<ul><li>指在同一法律关系内，一方当事人的代理人同时又接受另一方当事人委托，并为其代理的行为</li><li>由于合同关系中双方是相对的，双方代理会使得本是冲突的合同双方意思表示被代理的个人意志予以替代，偏离了合同的本质属性</li></ul></li><li>海运货代行业需要这样做，因为效率上的提升。但会有法律上的风险</li></ul><h2 id="3-3-涉外法律适用上的风险成因"><a href="#3-3-涉外法律适用上的风险成因" class="headerlink" title="3.3 涉外法律适用上的风险成因"></a>3.3 涉外法律适用上的风险成因</h2><ul><li>作为法院裁判依据的国外法律规定可能会让国际货运代理企业在诉讼中处于不利地位</li><li>涉外商事代理法律关系，会受到途经国家的法律管辖</li><li>如果双方未约定准据法，或涉诉的国际货代企业没有准确理解和把握已约定的准据法，就会大大增加诉讼中的不稳定因素</li><li>英美法系，判例法； 成文法国家，有专门的货运代理法律</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>《国际海上货运代理法律风险研究》  邓大鸣</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-货运代理法律风险基本内容&quot;&gt;&lt;a href=&quot;#1-货运代理法律风险基本内容&quot; class=&quot;headerlink&quot; title=&quot;1. 货运代理法律风险基本内容&quot;&gt;&lt;/a&gt;1. 货运代理法律风险基本内容&lt;/h1&gt;&lt;h2 id=&quot;1-1-国际货运代理人定义&quot;&gt;
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="货运代理" scheme="https://www.llchen60.com/tags/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>国际货运代理行业初探</title>
    <link href="https://www.llchen60.com/%E5%9B%BD%E9%99%85%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E8%A1%8C%E4%B8%9A%E5%88%9D%E6%8E%A2/"/>
    <id>https://www.llchen60.com/%E5%9B%BD%E9%99%85%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86%E8%A1%8C%E4%B8%9A%E5%88%9D%E6%8E%A2/</id>
    <published>2021-08-15T14:52:04.000Z</published>
    <updated>2021-08-19T23:01:51.975Z</updated>
    
    <content type="html"><![CDATA[<h1 id="国际货运代理行业初探"><a href="#国际货运代理行业初探" class="headerlink" title="国际货运代理行业初探"></a>国际货运代理行业初探</h1><h1 id="1-货运公司业务与工作内容"><a href="#1-货运公司业务与工作内容" class="headerlink" title="1. 货运公司业务与工作内容"></a>1. 货运公司业务与工作内容</h1><h2 id="1-1-主要业务"><a href="#1-1-主要业务" class="headerlink" title="1.1 主要业务"></a>1.1 主要业务</h2><h3 id="1-1-1-Overview"><a href="#1-1-1-Overview" class="headerlink" title="1.1.1 Overview"></a>1.1.1 Overview</h3><blockquote><p>客户委托货运代理运输货物，客户本人并不承担承运责任；货代接受发货人委托后，为发货人提供相关服务，满足发货人的具体要求，同时要求发货人根据服务来支付一定报酬的行业</p></blockquote><h3 id="1-1-2-主营业务"><a href="#1-1-2-主营业务" class="headerlink" title="1.1.2 主营业务"></a>1.1.2 主营业务</h3><ul><li>纯粹代理人的业务<ul><li>取得委托人授权</li><li>负责货物离开港口以及到达港口的公务处理工作<ul><li>报关</li><li>报检</li><li>保险</li></ul></li><li>这个清关环节主要是配合当地海关进行文件准备工作</li></ul></li><li>国际多式联运业务<ul><li>多式联运，发展自集装箱运输</li></ul></li><li>无船承运业务<ul><li>具有无船公共承运人NVOCC (Non Vessel Operating Common Carrier)资质的货代企业可以以承运人的身份接受货载委托</li><li>货物清关的时候用货代企业的提单，承担承运人的责任，完成货物离港到岸的国际海运经营业务</li><li>无船承运人只是契约承运人，而实际完成运输的承运人是货代企业所委托的其他国际船舶运输经营者</li></ul></li><li>物流业务<ul><li>可以针对不同客户提供定制化的供应链解决方案，囊括了货物从生产，出厂，转运，清仓报关，到达目标市场等各个环节</li></ul></li><li>国际快递业务</li></ul><h2 id="1-2-基本工作流程"><a href="#1-2-基本工作流程" class="headerlink" title="1.2 基本工作流程"></a>1.2 基本工作流程</h2><ul><li>海运流程<ul><li>他国买主和中国企业就贸易行为签订贸易合同</li><li>中国工厂确定货运代理企业，并与之商讨出货</li><li>中国货运代理企业和船运公司商讨订舱事宜</li><li>拿到承载货物的船的名字和达到目的地的准确信息</li><li>国内外货运代理企业共同确认货物运输信息，以及到达目的地的时间信息，并将具体信息传递给国外买家</li></ul></li><li>location / key points<ul><li>卖方，出口商地点</li><li>出口单证手续</li><li>边境/ 机场/ 码头交货</li><li>装运港</li><li>船舷</li><li>船上</li><li>船上</li><li>船舷</li><li>到达卸货港</li><li>指定目的地交货; 边境/ 机场/ 码头</li><li>进口单证手续</li><li>卖方/ 进口商地点</li></ul></li></ul><h2 id="1-3-货代企业等级"><a href="#1-3-货代企业等级" class="headerlink" title="1.3 货代企业等级"></a>1.3 货代企业等级</h2><ul><li>一级货代企业<ul><li>需要获得国际货代资格证书</li><li>商务部颁发的，一级货代企业有权从中国不同港口订舱</li></ul></li><li>小型货代公司<ul><li>依托于一级货代企业，来进行订舱的公司</li></ul></li></ul><h1 id="2-国际货运行业发展的外部环境"><a href="#2-国际货运行业发展的外部环境" class="headerlink" title="2. 国际货运行业发展的外部环境"></a>2. 国际货运行业发展的外部环境</h1><ul><li><p>PEST分析方式</p><ul><li>维度<ul><li>政治<ul><li>2010年货代标准化委员会</li><li>2015年《推动共建丝绸之路经济带和21世纪海上丝绸之路的愿景与行动》</li><li>2016年《关于加快国际货运代理物流业健康发展的指导意见》</li><li>《中华人民共和国海商法》</li></ul></li><li>经济</li><li>技术</li><li>社会文化</li></ul></li><li>外部的变化是组织无法控制的，脱离于组织内部，却环绕在组织四周，从不同方面作用在组织内部</li></ul></li><li><p>波特五力竞争理论</p><ul><li>维度<ul><li>替代者<ul><li>货代的供应商</li></ul></li><li>客户</li><li>供应商<ul><li>供应商 尤其是船舶公司航空公司有着比较强的议价能力，尤其对于中小型货代而言</li></ul></li><li>潜在竞争者<ul><li>政策壁垒整体在降低</li><li>传统业务 — 报关报检，码头物流，短途专线运输</li><li>新兴业务 — 供应链管理，非常规货物托运，多式联运</li></ul></li><li>同行业竞争者<ul><li>船舶公司旗下自己设立的货代物流公司<ul><li>有直接客户源</li><li>底价海运费和订舱费减免的优势</li><li>可以通过船公司完善的海外网络指定国内的出口供应商，可以提供更多种的贸易方式<ul><li>贸易方式<ul><li>Exxxx<ul><li>EXW<ul><li>卖方仅在自己的地点为买房备妥货物交付，出工厂以后就没有费用和安全责任了</li></ul></li></ul></li><li>Fxxx  — 卖方需要将货物交到制定的承运人处<ul><li>FCA  货交承运人</li><li>FAS  装运港 船边  交货 — 到码头</li><li>FOB 装运港 船上 交货  — 到船舷<ul><li>价格计算  = (产品含税成本 + 利润 + 国内运输费用 - 出口退税)/ 汇率</li><li>卖方的义务<ul><li>将合同规定的货物交到买房所指派的船上并及时通知买方</li><li>承担货物越过装运港船舷之前的一切风险</li><li>办理货物的出口手续</li><li>提交商业发票等所需的凭证</li></ul></li><li>买方的义务<ul><li>租船订舱，支付运费</li><li>将船名，装货地点和要求交货的时间及时通知卖方</li><li>受领货物，支付货款</li><li>承担货物越过装运港船舷之后的一切风险</li><li>办理货物的进口手续</li></ul></li></ul></li></ul></li><li>Cxxx — 卖方需要订立运输合同，但是对于货物损失的风险以及装船和启运之后发生的意外所产生的额外费用，卖方不承担责任<ul><li>CFR  成本+运费<ul><li>CFR = FOB + 海运费</li></ul></li><li>CIF  成本运费保险<ul><li>CIF = FOB + 海运费 + 海运保险费</li></ul></li><li>CIP  运费/ 保险费付到目的地</li></ul></li><li>Dxxx — 卖方承担将货物交到出口国边境或者目的国所需的全部费用和风险<ul><li>DAF  出口国边境交货</li><li>DES  目的港船上交货</li><li>DEQ  目的港码头交货</li><li>DDU  未完税交货</li><li>DDP  完税后交货</li></ul></li><li>FOB</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>SWOT 战略管理分析</p><ul><li>S - Strength<ul><li>企业内部优势</li></ul></li><li>W - Weakness<ul><li>企业面临的竞争弱势</li></ul></li><li>O - Opportunity<ul><li>发展过程当中的有利外部环境</li></ul></li><li>T  - Threat<ul><li>企业自身业务的外部威胁</li><li>当外部环境不利于企业的发展，企业为了避免外部环境带来的威胁，应当及时调整经营战略</li></ul></li></ul></li></ul><h1 id="3-内部环境分析"><a href="#3-内部环境分析" class="headerlink" title="3. 内部环境分析"></a>3. 内部环境分析</h1><ul><li>价值链分析法<ul><li>将活动分为基础活动以及辅助活动两大类<ul><li>基础活动<ul><li>内部物流活动<ul><li>各个部门和环节之间的合作，如何能直观了解到项目的进展此类问题</li></ul></li><li>生产经营活动<ul><li>生产 销售  供应 财务</li></ul></li><li>外部物流活动<ul><li>货物的周转</li><li>报关 报检 查验 清关 缴税等</li></ul></li><li>服务性活动</li></ul></li><li>辅助活动<ul><li>人力资源管理活动</li><li>改善基础设施条件的活动</li><li>原材料采购活动</li><li>新产品研发活动等</li></ul></li></ul></li><li>只有在一些特定环节，才会真的去创造价值，这些是战略环节，需要在此构建战略优势</li></ul></li></ul><h1 id="4-Other-Notes"><a href="#4-Other-Notes" class="headerlink" title="4. Other Notes"></a>4. Other Notes</h1><ul><li><p>企业战略</p><ul><li>用于整合与重新优化配置的措施及可行性方案</li><li>企业组织结构要跟随企业战略进行调整</li></ul></li><li><p>如何选择合适的竞争战略</p><ul><li>对有吸引力，高潜力的产业的正确选择</li><li>在选择的行业当中确立自己的竞争优势地位</li><li>除此以外还需要从企业内部环境，研究价值链</li></ul></li><li><p>企业内部性的深入研究 — 企业核心能力</p><ul><li>让企业构成其他企业并不具备的能力，同时资源无法在不同的企业之间进行流通</li><li>企业内部的资源的独特性，直接关系到企业利润的获得以及竞争优势的保持</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>1.<a href="https://www.52by.com/article/2584">https://www.52by.com/article/2584</a><br>2. 《小型国际货运代理X公司的发展战略研究》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;国际货运代理行业初探&quot;&gt;&lt;a href=&quot;#国际货运代理行业初探&quot; class=&quot;headerlink&quot; title=&quot;国际货运代理行业初探&quot;&gt;&lt;/a&gt;国际货运代理行业初探&lt;/h1&gt;&lt;h1 id=&quot;1-货运公司业务与工作内容&quot;&gt;&lt;a href=&quot;#1-货运公司业务
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://www.llchen60.com/categories/Notes/"/>
    
    
      <category term="货运代理" scheme="https://www.llchen60.com/tags/%E8%B4%A7%E8%BF%90%E4%BB%A3%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>System Design Patterns - Quorum</title>
    <link href="https://www.llchen60.com/System-Design-Patterns-Quorum/"/>
    <id>https://www.llchen60.com/System-Design-Patterns-Quorum/</id>
    <published>2021-08-11T04:26:01.000Z</published>
    <updated>2021-08-11T04:26:42.996Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h1><p>In distributed system, data is replicated across multiple servers for fault tolerance and high availability.</p><p>Once system decides to maintain multiple copies of data, another problem arises: how to make sure that all replicas are consistent?? </p><h1 id="2-Dive-Deep"><a href="#2-Dive-Deep" class="headerlink" title="2. Dive Deep"></a>2. Dive Deep</h1><h2 id="2-1-Definition"><a href="#2-1-Definition" class="headerlink" title="2.1 Definition"></a>2.1 Definition</h2><ul><li>Quorum<ul><li>Minimum number of servers on which a distributed operation needs to be performed successfully before declaring the operation’s overall success</li></ul></li></ul><h2 id="2-2-How-it-works"><a href="#2-2-How-it-works" class="headerlink" title="2.2 How it works?"></a>2.2 How it works?</h2><ul><li>Suppose a database is replicated on 5 machines, then quorum refers to the minimum number of machines that perform the same action for a given transaction in order to decide the final operation for that transaction</li><li>So in a set of 5, three machines form the majority quorum, quorum <strong>enforces the consistency requirement</strong> needed for distributed operations</li><li>Quorum Number<ul><li>N / 2 + 1</li></ul></li><li>Quorum is achieved when nodes follow the below protocol R + W &gt; N<ul><li>R  minimum read nodes</li><li>W minimum write nodes</li><li>N  nodes in the quorum group</li></ul></li></ul><h2 id="2-3-Where-is-it-used"><a href="#2-3-Where-is-it-used" class="headerlink" title="2.3 Where is it used?"></a>2.3 Where is it used?</h2><ul><li>Chubby<ul><li>Use paxos for leader election, which use quorum to ensure strong consistency</li></ul></li><li>Cassandra<ul><li>Ensure data consistency, each write request can be configured to be successful only if the data has been written to at least a quorum of replica nodes</li></ul></li><li>Dynamodb<ul><li>Writes to a sloppy quorum of other nodes in the system</li><li>All read/ write operations are performed on the first N healthy nodes from the preference list, which may not always be the first N nodes encountered walking the consistent hashing ring</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Background&quot;&gt;&lt;a href=&quot;#1-Background&quot; class=&quot;headerlink&quot; title=&quot;1. Background&quot;&gt;&lt;/a&gt;1. Background&lt;/h1&gt;&lt;p&gt;In distributed system, data 
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>System Design Patterns - Bloom Filters</title>
    <link href="https://www.llchen60.com/System-Design-Patterns-Bloom-Filters/"/>
    <id>https://www.llchen60.com/System-Design-Patterns-Bloom-Filters/</id>
    <published>2021-08-10T02:33:45.000Z</published>
    <updated>2021-08-10T02:34:11.571Z</updated>
    
    <content type="html"><![CDATA[<h1 id="System-Design-Patterns-Bloom-Filters"><a href="#System-Design-Patterns-Bloom-Filters" class="headerlink" title="System Design Patterns - Bloom Filters"></a>System Design Patterns - Bloom Filters</h1><p>Created: August 8, 2021 10:06 PM<br>Status: Finished<br>Tags: System Design<br>Type: Tech Resource</p><h1 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h1><ul><li>Suppose we have a large set of structured data(identified by record IDs) stored in a set of data files, and we want to know which file might contain our required data<ul><li>we don’t want to read each file, as it’s slow and we have to read a lot of data from the disk</li></ul></li><li>Solution 1: Build an index on each data file and store it in a separate index file, to map each record ID to its offset in the data file; each index file will be sorted on the record ID. Then we could do a binary search in index file</li><li>Solution 2: We could use Bloom Filters</li></ul><h1 id="2-How-does-Bloom-Filter-work"><a href="#2-How-does-Bloom-Filter-work" class="headerlink" title="2. How does Bloom Filter work?"></a>2. How does Bloom Filter work?</h1><ul><li>The Bloom filter data structure tells whether an element <strong>may be in a set, or definitely is not</strong><ul><li>which means the only possible errors are false positives</li></ul></li><li>How it looks<ul><li>An empty bloom filter is a bit array of m bits, all set to 0</li><li>There are also k different hash functions, each of which maps a set element to one of the m bit positions</li></ul></li><li>Workflow<ul><li>To add an element, feed it to the hash functions to get k bit positions, and set the bits at these positions to 1</li><li>To test if an element is in the set, feed it to the hash functions to get k bit positions<ul><li>if any of the bits at these positions is 0, the element is definitely not in the set</li><li>if all are 1, then the element may be in the set</li></ul></li></ul></li></ul><h1 id="3-When-will-we-use-Bloom-Filter"><a href="#3-When-will-we-use-Bloom-Filter" class="headerlink" title="3. When will we use Bloom Filter?"></a>3. When will we use Bloom Filter?</h1><ul><li>In BigTable, any read operation has to read from all SSTables that make up a tablet<ul><li>if these SSTables are not in memory, the read operation may end up doing many disk accesses</li><li>BigTable uses bloom filters to reduce the number of disk accesses</li><li>Store of BloomFilter could drastically reduces the number of disk seeks, thereby improving read performance</li></ul></li></ul><h1 id="4-How-to-use-Bloom-Filter-in-Java"><a href="#4-How-to-use-Bloom-Filter-in-Java" class="headerlink" title="4. How to use Bloom Filter in Java?"></a>4. How to use Bloom Filter in Java?</h1><ul><li>We could use BloomFilter class from the Guava library to achieve this</li></ul><pre><code class="jsx">BloomFilter&lt;Integer&gt; filter = BloomFilter.create(    Funnels.integerFunnel(),     500,     0.01);</code></pre><ul><li>How to implement from scratch   <a href="https://www.inlighting.org/archives/java-implement-bloom-filter/">https://www.inlighting.org/archives/java-implement-bloom-filter/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;System-Design-Patterns-Bloom-Filters&quot;&gt;&lt;a href=&quot;#System-Design-Patterns-Bloom-Filters&quot; class=&quot;headerlink&quot; title=&quot;System Design Patter
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis集合统计模式</title>
    <link href="https://www.llchen60.com/Redis%E9%9B%86%E5%90%88%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    <id>https://www.llchen60.com/Redis%E9%9B%86%E5%90%88%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F/</id>
    <published>2021-08-08T23:32:13.000Z</published>
    <updated>2021-08-08T23:32:56.696Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis集合统计模式"><a href="#Redis集合统计模式" class="headerlink" title="Redis集合统计模式"></a>Redis集合统计模式</h1><h1 id="1-聚合统计"><a href="#1-聚合统计" class="headerlink" title="1. 聚合统计"></a>1. 聚合统计</h1><h2 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h2><ul><li>统计多个集合元素的聚合结果<ul><li>交集统计 — 统计多个集合的共有元素</li><li>差集统计 — 统计其中一个集合独有的元素</li><li>并集统计 — 统计多个集合的所有元素</li></ul></li><li>聚合统计可以使用Set类型来做</li><li>但是Set的差集，并集，交集的计算复杂度比较高，在数据量比较大的情况下，直接执行可能会导致Redis实例阻塞。</li><li>可以从主从集群当中选择一个从库，使其专门负责聚合计算，或者将数据读取到客户端，在客户端完成聚合统计</li></ul><h2 id="1-2-案例分析"><a href="#1-2-案例分析" class="headerlink" title="1.2 案例分析"></a>1.2 案例分析</h2><ul><li>统计一个手机App的每天的新增用户数和第二天的留存用户数</li><li>用一个集合记录所有登陆过App的用户Id<ul><li>key — user:id</li><li>value — set类型 记录用户id</li></ul></li><li>另外一个集合记录每天用户set<ul><li>key — user:id:20210808</li><li>value — set类型  记录用户id</li></ul></li></ul><pre><code class="jsx">// 差值统计SUNIONSTORE user:id user:id user:id:20210808// 计算新用户SDIFFSTORE user:new user:id:20210808 user:id // 计算留存用户SINTERSTOPRE user:id:rem user:id:20210808 user:id:20210809</code></pre><h1 id="2-排序统计"><a href="#2-排序统计" class="headerlink" title="2. 排序统计"></a>2. 排序统计</h1><h2 id="2-1-概念"><a href="#2-1-概念" class="headerlink" title="2.1 概念"></a>2.1 概念</h2><ul><li>需要能对输出进行排序，Redis常用的4个集合类型当中 (List, Hash, Set, Sorted Set)， List和Sorted Set属于有序集合</li><li>List按照元素进入List的顺序排序，而Sorted Set可以根据元素的权重来排序</li></ul><h2 id="2-2-案例"><a href="#2-2-案例" class="headerlink" title="2.2 案例"></a>2.2 案例</h2><ul><li>电商网站上提供最新评论列表的场景</li><li>List在这个场景里面的问题<ul><li>因为根据位置排序，当有新的评价加进来，那么可能会有一些评价会在不同页面重复出现</li></ul></li><li>Sorted Set不存在这个问题，因为它是根据元素的实际权重来排序和获取数据的<ul><li>我们可以按照评论时间的先后给每条评论设置一个权重值，然后将评论保存到Sorted Set当中</li><li>ZRANGEBYSCORE命令就可以按照权重排序以后返回元素</li></ul></li></ul><h1 id="3-二值状态统计"><a href="#3-二值状态统计" class="headerlink" title="3. 二值状态统计"></a>3. 二值状态统计</h1><h2 id="3-1-概念"><a href="#3-1-概念" class="headerlink" title="3.1 概念"></a>3.1 概念</h2><ul><li>指集合元素的取值只有0和1两种</li><li>计算海量二值状态数据的时候，bitmap可以有效减少所需的内存空间</li></ul><h2 id="3-2-案例"><a href="#3-2-案例" class="headerlink" title="3.2 案例"></a>3.2 案例</h2><ul><li>签到统计<ul><li>每个用户一天的签到用一个bit位就能表示</li><li>因此并不需要非常复杂的数据类型，使用bitmap就可以了</li></ul></li><li>Redis提供了Bitmap类型<ul><li>GETBIT</li><li>SETBIT<ul><li>将某一位设置为1</li></ul></li><li>BITCOUNT<ul><li>用来统计所有1的个数</li></ul></li></ul></li></ul><pre><code class="jsx">// 记录用户8 3 签到了SETBIT uid:sign:3000:202008 2 1 // 检查是否8 3 签到了GETBIT uid:sign:3000:202008 2 // 统计该用户8月份的签到次数BITCOUNT uid:sign:3000:202008 </code></pre><ul><li>如何统计一亿个用户连续10天的签到情况<ul><li>将每天日期作为key，每个key对应一个1亿位的bitmap  每一个bit对应一个用户当天的签到情况</li><li>对10个bitmap做与操作</li><li>然后用BITCOUNT统计下最终生成的Bitmap当中1的个数</li></ul></li></ul><h1 id="4-基数统计"><a href="#4-基数统计" class="headerlink" title="4. 基数统计"></a>4. 基数统计</h1><h2 id="4-1-概念"><a href="#4-1-概念" class="headerlink" title="4.1 概念"></a>4.1 概念</h2><ul><li>基数统计指统计一个集合中不重复的元素的个数</li></ul><h2 id="4-2-案例"><a href="#4-2-案例" class="headerlink" title="4.2 案例"></a>4.2 案例</h2><ul><li>统计一个网页的UV<ul><li>需要去重，一个用户一天内的多次访问只能算一次</li></ul></li><li>可以使用SET或者HASH类型来进行记录，但是会消耗很大的内存空间</li><li>可以使用HyperLogLog<ul><li>用于统计基数的数据集合类型</li><li>优势在于当集合元素数量非常多的时候，计算基数所需的空间总是固定的，而且还很小</li><li>Redis中每个HyperLogLog只需要使用12KB内存，就可以计算接近2^64个元素的基数</li></ul></li><li>HyperLogLog的统计规则是基于概率完成的，因此其给出的统计结果是有一定误差的，标准误算率为0.81% ; 如果应用场景是必须非常精确，那就还需要使用Set或者Hash类型</li></ul><pre><code class="jsx">PFADD page1:uv  user1 user2 user3 user4 PFCOUNT page1:uv </code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis集合统计模式&quot;&gt;&lt;a href=&quot;#Redis集合统计模式&quot; class=&quot;headerlink&quot; title=&quot;Redis集合统计模式&quot;&gt;&lt;/a&gt;Redis集合统计模式&lt;/h1&gt;&lt;h1 id=&quot;1-聚合统计&quot;&gt;&lt;a href=&quot;#1-聚合统计&quot; cla
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>DynamoDB Architecture (Paper Reading)</title>
    <link href="https://www.llchen60.com/DynamoDB-Architecture-Paper-Reading/"/>
    <id>https://www.llchen60.com/DynamoDB-Architecture-Paper-Reading/</id>
    <published>2021-08-07T20:14:38.000Z</published>
    <updated>2021-08-08T18:51:22.531Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>Designed to be always on</li><li>Dynamo falls within the category of AP systems (available and partition tolerant) and is designed for high availability and partition tolerance at the expense of strong consistency</li></ul><h2 id="1-1-Design-Goals"><a href="#1-1-Design-Goals" class="headerlink" title="1.1 Design Goals"></a>1.1 Design Goals</h2><ul><li>Scalable<ul><li>System need to be highly scalable. We should be able to throw a machine into the system to see proportional improvement</li></ul></li><li>Decentralized<ul><li>To avoid single points of failures and performance bottlenecks, there should not be any central/ leader process</li></ul></li><li>Eventually Consistent<ul><li>Data can be optimistically replicated to become eventually consistent</li></ul></li></ul><h2 id="1-2-Use-cases"><a href="#1-2-Use-cases" class="headerlink" title="1.2 Use cases"></a>1.2 Use cases</h2><ul><li>eventually consistent database</li></ul><h2 id="1-3-System-APIs"><a href="#1-3-System-APIs" class="headerlink" title="1.3 System APIs"></a>1.3 System APIs</h2><ul><li>put(key, context, object)<ul><li>find the nodes where the object associated with the given key should locate</li><li>context is a value that is returned with a get operation and then sent back with the put operation</li><li>context is always stored along with the object</li><li>used like a cookie to verify the validity of the object supplied in the put request</li></ul></li><li>get(key)<ul><li>find the nodes where the object associated with the given key is located</li><li>return a single object or a list of objects with conflicting versions along with a context</li><li>context contains encoded metadata about the object, and <strong>version of the object</strong></li></ul></li></ul><h1 id="2-High-Level-Design"><a href="#2-High-Level-Design" class="headerlink" title="2. High Level Design"></a>2. High Level Design</h1><h2 id="2-1-Data-Distribution"><a href="#2-1-Data-Distribution" class="headerlink" title="2.1 Data Distribution"></a>2.1 Data Distribution</h2><h3 id="2-1-1-What-is-it"><a href="#2-1-1-What-is-it" class="headerlink" title="2.1.1 What is it"></a>2.1.1 What is it</h3><ul><li>Consistent hashing to distribute its data among nodes</li><li>Also make it easy to add/ remove nodes from a dynamo cluster</li></ul><h3 id="2-1-2-Challenge"><a href="#2-1-2-Challenge" class="headerlink" title="2.1.2 Challenge"></a>2.1.2 Challenge</h3><ul><li>how do we know on which node a particular piece of data will be stored?</li><li>when we add/ remove nodes, how do we know what data will be moved from existing nodes to the new nodes?</li><li>how can we minimize data movement when nodes join or leave?</li></ul><h3 id="2-1-3-Consistent-Hashing"><a href="#2-1-3-Consistent-Hashing" class="headerlink" title="2.1.3 Consistent Hashing"></a>2.1.3 Consistent Hashing</h3><ul><li><p>Represents the data managed by a cluster as a ring</p></li><li><p>Each node in the ring is assigned a range of data</p></li><li><p>Token</p><ul><li><p>The start of the range is called a token</p></li><li><p>each node will be assigned with one token</p><p>  <img src="https://i.loli.net/2021/08/08/8jFPMEDNmn4cXaf.png" alt=""></p></li></ul></li><li><p>Process for a put or get request</p><ul><li>DDB performs a MD5 hashing algorithm to the key</li><li>Output determines within which range the data lies —→ which node the data will be stored</li></ul></li><li><p>Problems for only use physical nodes</p><ul><li>for adding or removing nodes, it only influence the next node, but it would cause uneven distribution of traffic</li><li>recomputing the tokens causing a significant administrative overhead for a large cluster</li><li>Since each node is assigned one large range, if the data is not evenly distributed, some nodes can become hotspots</li><li>Since each node’s data is replicated on a fixed number of nodes (discussed later), when we need to rebuild a node, only its replica nodes can provide the data. This puts a lot of pressure on the replica nodes and can lead to service degradation</li></ul></li></ul><h3 id="2-1-4-Virtual-Nodes"><a href="#2-1-4-Virtual-Nodes" class="headerlink" title="2.1.4 Virtual Nodes"></a>2.1.4 Virtual Nodes</h3><p><img src="https://i.loli.net/2021/08/08/YWwxt9eMmToQ3qn.png" alt=""></p><ul><li><p>Hash range is divided into multiple smaller ranges, and each physical node is assigned multiple of these smaller ranges</p></li><li><p>Each of these subranges is called a Vnode</p></li><li><p>Vnodes are randomly distributed across the cluster and are generally non contiguous (不连续的)</p><p>  <img src="https://i.loli.net/2021/08/08/RVgrPm8T19nbKZB.png" alt=""></p></li><li><p>Benefits of Vnodes</p><ul><li>Help spread the load more evenly across the physical nodes on the cluster by dividing the hash range into smaller subranges<ul><li>speeds up the rebalancing process after adding or removing nodes</li><li>When a new node is added, it receives many Vnodes from the existing nodes to maintain a balanced cluster. Similarly, when a node needs to be rebuilt, instead of getting data from a fixed number of replicas, many nodes participate in the rebuild process.</li></ul></li><li>Vnodes make it easier to maintain a cluster containing heterogeneous machines. This means, with Vnodes, we can assign a high number of ranges to a powerful server and a lower number of ranges to a less powerful server</li><li>Since Vnodes help assign smaller ranges to each physical node, the probability of hotspots is much less than the basic Consistent Hashing scheme which uses one big range per node</li></ul></li></ul><h2 id="2-2-Data-Replication-and-Consistency"><a href="#2-2-Data-Replication-and-Consistency" class="headerlink" title="2.2 Data Replication and Consistency"></a>2.2 Data Replication and Consistency</h2><ul><li>Data is replicated optimistically</li><li>Dynamo provides eventual consistency</li></ul><h3 id="2-2-1-Optimistic-Replication"><a href="#2-2-1-Optimistic-Replication" class="headerlink" title="2.2.1 Optimistic Replication"></a>2.2.1 Optimistic Replication</h3><ul><li><p>To ensure high availability and durability, Dynamo replicates each data item on multiple N nodes in the system where the value N is equivalent to the <strong>replication factor</strong>, also is configured per instance of Dynamo</p></li><li><p>Each key is assigned to a coordinator node, which first stores the data locally and then replicates it to N-1 clockwise successor nodes on the ring</p><ul><li>Thus each node owns the region on the ring between it and its Nth predecessor</li></ul></li><li><p>Replication is done asynchronously and Dynamo provides an eventually consistent model</p></li><li><p>It’s called optimistic replication, as the replicas are not guaranteed to be identical at all times</p><p>  <img src="https://i.loli.net/2021/08/08/QaHhmPTI6XcYfwu.png" alt=""></p></li><li><p>Preference List</p><ul><li>List of nodes responsible for storing a particular key</li><li>Dynamo is designed so that every node in the system can determine which nodes should be in the list for any specific key</li><li>The list contains more than N nodes to account for failures and skip virtual nodes on the ring so that the list only contains distinct physical nodes</li></ul></li></ul><h2 id="2-3-Handling-Temporary-Failures"><a href="#2-3-Handling-Temporary-Failures" class="headerlink" title="2.3 Handling Temporary Failures"></a>2.3 Handling Temporary Failures</h2><ul><li>To handle temporary failures, dynamo replicates data to a <strong>sloppy quorum</strong> of other nodes in the system instead of a strict majority quorum</li></ul><h3 id="2-3-1-Quorum-Approach"><a href="#2-3-1-Quorum-Approach" class="headerlink" title="2.3.1 Quorum Approach"></a>2.3.1 Quorum Approach</h3><ul><li>Traditional quorum approach<ul><li>any distributed system becomes unavailable during server failures or network partitions and would have reduced availability even under simple failure conditions</li></ul></li><li>Sloppy quorum<ul><li>all read/ write operations are performed on the first N healthy nodes from the preference list. may not always be the first N nodes encountered while moving clockwise on the consistent hashing ring</li></ul></li></ul><h3 id="2-3-2-Hinted-Handoff"><a href="#2-3-2-Hinted-Handoff" class="headerlink" title="2.3.2 Hinted Handoff"></a>2.3.2 Hinted Handoff</h3><ul><li>When a node is unreachable, another node can accept writes on its behalf</li><li>Write is then kept in a local buffer and sent out once the destination node is reachable again</li><li>Problem<ul><li>Sloppy quorum is not a strict majority, the data can and will diverge</li><li>It is possible for two concurrent writes to the same key to be accepted by non-overlapping sets of nodes. This means that multiple conflicting values against the same key can exist in the system, and we can get stale or conflicting data while reading. Dynamo allows this and resolves these conflicts using Vector Clocks.</li></ul></li></ul><h2 id="2-4-Inter-node-communication-and-failure-detection"><a href="#2-4-Inter-node-communication-and-failure-detection" class="headerlink" title="2.4 Inter node communication and failure detection"></a>2.4 Inter node communication and failure detection</h2><ul><li>Use gossip protocol to keep track of the cluster state</li></ul><h3 id="2-4-1-Gossip-Protocol"><a href="#2-4-1-Gossip-Protocol" class="headerlink" title="2.4.1 Gossip Protocol"></a>2.4.1 Gossip Protocol</h3><ul><li><p>Enable each node to keep track of state information about the other nodes in the cluster</p><ul><li>which nodes are reachable</li><li>what key ranges they are responsible for</li></ul></li><li><p>Gossip Protocol</p><ul><li><p>Peer to peer communication mechanism</p></li><li><p>nodes periodically exchange state information about themselves and other nodes they know about</p></li><li><p>each node initiate a gossip round every second with a random node</p><p>  <img src="https://i.loli.net/2021/08/08/hgBPER5e2kuxvMw.png" alt=""></p></li></ul></li><li><p>External discovery through seed nodes</p><ul><li>An administrator joins node A to the ring and then joins node B to the ring. Nodes A and B consider themselves part of the ring, yet neither would be immediately aware of each other. To prevent these logical partitions, Dynamo introduced the concept of seed nodes. Seed nodes are fully functional nodes and can be obtained either from a static configuration or a configuration service. This way, all nodes are aware of seed nodes. Each node communicates with seed nodes through gossip protocol to reconcile membership changes; therefore, logical partitions are highly unlikely.</li></ul></li></ul><h2 id="2-5-Conflict-Resolution-and-Handling-permanent-failures"><a href="#2-5-Conflict-Resolution-and-Handling-permanent-failures" class="headerlink" title="2.5 Conflict Resolution and Handling permanent failures"></a>2.5 Conflict Resolution and Handling permanent failures</h2><h3 id="2-5-1-Clock-Skew"><a href="#2-5-1-Clock-Skew" class="headerlink" title="2.5.1 Clock Skew"></a>2.5.1 Clock Skew</h3><ul><li>Dynamo resolves potential conflicts using below mechanisms<ul><li>use vector clocks to keep track of value history and reconcile divergent histories at read time</li><li>in the background, dynamo use an <strong>anti entropy mechanism</strong> like <strong>Merkle trees</strong> to handle permanent failures</li></ul></li></ul><h3 id="2-5-2-Vector-Clock"><a href="#2-5-2-Vector-Clock" class="headerlink" title="2.5.2 Vector Clock"></a>2.5.2 Vector Clock</h3><ul><li><p>Used to capture causality between different versions of the same object</p></li><li><p>A vector clock is a <code>node, counter</code> pair</p></li><li><p>Each version of every object associate with a vector clock</p><ul><li>one can determine whether two versions of an object are on parallel branches or have a causal ordering by examining vector clocks</li><li>If the counters on the first object’s clock are less-than-or-equal to all of the nodes in the second clock, then the first is an ancestor of the second and can be forgotten. Otherwise, the two changes are considered to be in conflict and require reconciliation.</li></ul></li><li><p>  <img src="https://i.loli.net/2021/08/08/9fKB418IrMezTun.png" alt=""></p></li><li><p>Issue occur when there are network partition, that same data cannot be shared / communicated via different servers</p></li><li><p>In this case, DynamoDB will return it back and let client reads and reconciles</p></li></ul><h3 id="2-5-3-Conflict-free-replicated-data-types-CRDTs"><a href="#2-5-3-Conflict-free-replicated-data-types-CRDTs" class="headerlink" title="2.5.3 Conflict free replicated data types (CRDTs)"></a>2.5.3 Conflict free replicated data types (CRDTs)</h3><ul><li>we need to model our data in such a way that concurrent changes can be applied to the data in any order and will produce the same end result</li></ul><h2 id="2-6-put-and-get-Operations"><a href="#2-6-put-and-get-Operations" class="headerlink" title="2.6 put() and get() Operations"></a>2.6 put() and get() Operations</h2><h3 id="2-6-1-Strategies-for-choosing-the-coordinator-node"><a href="#2-6-1-Strategies-for-choosing-the-coordinator-node" class="headerlink" title="2.6.1 Strategies for choosing the coordinator node"></a>2.6.1 Strategies for choosing the coordinator node</h3><ul><li><p>Strategies</p><p>  <img src="https://i.loli.net/2021/08/08/MB8qFbQGdwhoZAJ.png" alt=""></p><ul><li>Clients can route their requests through a generic load balancer<ul><li>client is unaware of the dynamo ring<ul><li>helps scalability</li><li>make ddb architecture loosely coupled</li></ul></li><li>it’s possible node it select is not part of the perference list, this will result in an extra hop</li></ul></li><li>Clients can use a partition aware client library that routes the request to the appropriate coordinator nodes with lower latency<ul><li>helps to achieve lower latency  — achieve zero hop</li><li>DDB doesn’t have much control over the load distribution and request handling</li></ul></li></ul></li></ul><h3 id="2-6-2-Consistency-Protocol"><a href="#2-6-2-Consistency-Protocol" class="headerlink" title="2.6.2 Consistency Protocol"></a>2.6.2 Consistency Protocol</h3><ul><li>R W is the min number of nodes that must participate in a successful read/ write operation</li><li>R + W &gt; N yields a quorun like system</li><li>A Common (N,R,WN, R, WN,R,W) configuration used by Dynamo is (3, 2, 2)</li><li>In general, low values of WWW and RRR increase the risk of inconsistency, as write requests are deemed successful and returned to the clients even if a majority of replicas have not processed them. This also introduces a vulnerability window for durability when a write request is successfully returned to the client even though it has been persisted at only a small number of nodes</li></ul><h3 id="2-6-3-put-process"><a href="#2-6-3-put-process" class="headerlink" title="2.6.3  put() process"></a>2.6.3  <code>put()</code> process</h3><ul><li>the coordinator generates a new data version and vector clock component</li><li>saves new data locally</li><li>sends the write request to N-1 highest ranked healthy nodes from the preference list</li><li>the put() operation is considered successful after receiving W - 1 confirmation</li></ul><h3 id="2-6-4-get-process"><a href="#2-6-4-get-process" class="headerlink" title="2.6.4 get() process"></a>2.6.4 <code>get()</code> process</h3><ul><li>coordinator requests the data version from N - 1 highest ranked healthy nodes from the preference list</li><li>waits until R - 1 replies</li><li>coordinator handles causal data versions through a vector clock</li><li>returns all relevant data versions to the caller</li></ul><h3 id="2-6-5-Request-handling-through-state-machine"><a href="#2-6-5-Request-handling-through-state-machine" class="headerlink" title="2.6.5 Request handling through state machine"></a>2.6.5 Request handling through state machine</h3><ul><li>Each client request results in creating a state machine on the node that received the client request</li><li>A read operation workflow would be:<ul><li>send read requests to the nodes</li><li>wait for the minimum number of required responses</li><li>if too few replies were received within a given time limit, fail the request</li><li>otherwise, gather all the data versions and determine the ones to be returned</li><li>if versioning is enabled, perform syntactic reconciliation and generate an opaque write context that contains the vector clock that subsumes all the remaining versions</li><li>At this point, read response has been returned to the caller</li><li>the state machine waits for a short period to receive any outstanding responces</li><li>if stale versions were returned in any of the responses, the coordinator updates those nodes with the latest version — READ REPAIR</li></ul></li></ul><h2 id="2-7-Anti-entropy-Through-Merkle-Trees"><a href="#2-7-Anti-entropy-Through-Merkle-Trees" class="headerlink" title="2.7 Anti-entropy Through Merkle Trees"></a>2.7 Anti-entropy Through Merkle Trees</h2><ul><li>Vector clocks are useful to remove conflicts while serving read requests</li><li>But if a replica falls significantly behind others, it might take a very long time to resolve conflicts using just vector clocks</li></ul><hr><p>—&gt; we need to quickly compare two copies of a range of data residing on different replicas and figure out exactly which parts are different </p><h3 id="2-7-1-What-are-MerkleTrees"><a href="#2-7-1-What-are-MerkleTrees" class="headerlink" title="2.7.1 What are MerkleTrees?"></a>2.7.1 What are MerkleTrees?</h3><ul><li><p>Dynamo use Merkel Trees to compare replicas of a range</p></li><li><p>A merkle tree is a binary tree of hashes, where each internal node is the hash of its two children, each leaf node is a hash of a portion of the original data</p></li><li><p>Then compare the merkle tree come to be super easy, just compare the root hashes of both trees, if equal, then stop; else, recurse on the left and right children</p></li><li><p>The principal advantage of using a Merkle tree is that each branch of the tree can be checked independently without requiring nodes to download the entire tree or the whole data set. Hence, Merkle trees minimize the amount of data that needs to be transferred for synchronization and reduce the number of disk reads performed during the anti-entropy process.</p></li><li><p>The disadvantage of using Merkle trees is that many key ranges can change when a node joins or leaves, and as a result, the trees need to be recalculated.</p></li></ul><h2 id="2-8-Dynamo-Characteristics"><a href="#2-8-Dynamo-Characteristics" class="headerlink" title="2.8 Dynamo Characteristics"></a>2.8 Dynamo Characteristics</h2><h3 id="2-8-1-Dynamo’s-Node-Responsibilities"><a href="#2-8-1-Dynamo’s-Node-Responsibilities" class="headerlink" title="2.8.1 Dynamo’s Node Responsibilities"></a>2.8.1 Dynamo’s Node Responsibilities</h3><ul><li>Each node serves three functions:<ul><li>Managing <code>get()</code> and <code>put()</code> requests<ul><li>A node may act as a coordinator and manage all operations for a particular key</li><li>A node also could forward the request to the appropriate node</li></ul></li></ul></li><li>Keep track of membership and detecting failures<ul><li>Every node uses gossip protocol to keep track of other nodes in the system and their associated hash ranges</li></ul></li><li>Local persistent storage<ul><li>Each node is responsible for being either the primary or replica store for keys that hash to a specific range of values</li><li>These pairs are stored within that node using various storage systems depending on application needs</li><li>E.G<ul><li>BerkeleyDB Transactional Data Store</li><li>MySQL</li><li>In memory buffer backed by persistent storage</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Designed to be always on&lt;/li&gt;

      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
      <category term="DynamoDB" scheme="https://www.llchen60.com/tags/DynamoDB/"/>
    
  </entry>
  
  <entry>
    <title>数据库索引</title>
    <link href="https://www.llchen60.com/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/"/>
    <id>https://www.llchen60.com/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/</id>
    <published>2021-07-11T04:56:10.000Z</published>
    <updated>2021-07-11T04:56:48.749Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>索引的实现，底层一般会依赖于哪些数据结构？</p></blockquote><h1 id="1-为什么需要索引"><a href="#1-为什么需要索引" class="headerlink" title="1. 为什么需要索引"></a>1. 为什么需要索引</h1><ul><li>为了能够尽快得到想要的数据 — 提高性能</li><li>选取高效的数据结构，在针对访问特性有比较好的访问速度的同时，减少空间上的占用<ul><li>如何节省存储空间</li><li>如何提高数据增删改查的执行效率</li></ul></li></ul><h1 id="2-功能性需求"><a href="#2-功能性需求" class="headerlink" title="2. 功能性需求"></a>2. 功能性需求</h1><ul><li>格式化数据还是非格式化数据？<ul><li>结构化数据<ul><li>比如MySQL数据</li></ul></li><li>非结构化数据<ul><li>比如搜索引擎当中的网页</li><li>一般需要进行预处理，提取出查询关键词，对关键词建立索引</li></ul></li></ul></li><li>静态数据还是动态数据？<ul><li>静态数据<ul><li>意味着没有增删改查</li><li>只需要考虑查询效率即可</li></ul></li><li>动态数据<ul><li>当原始数据更新的同时，我们还需要动态的更新索引</li></ul></li></ul></li><li>索引存储在内存还是硬盘？<ul><li>存储在内存<ul><li>访问速度快</li><li>原始数据量大的前提下，对应的索引也会非常大</li><li>因为内存有限，我们将不得不存放到内存当中</li></ul></li><li>存储在硬盘</li><li>部分存储在内存，部分在硬盘</li></ul></li><li>单值查找还是区间查找<ul><li>单值查找<ul><li>查询关键词等于某个值的数据</li></ul></li><li>区间查找<ul><li>查找关键词处于某个区间值的数据</li></ul></li></ul></li><li>单关键词还是多关键词组合的查找<ul><li>多关键词查询<ul><li>对于结构化数据的，<ul><li>实现针对多个关键词的组合建立索引</li></ul></li><li>对于非结构化数据的<ul><li>建立针对单个关键词的索引</li><li>然后通过集合操作，计算出查询结果</li></ul></li></ul></li></ul></li></ul><h1 id="3-非功能性需求"><a href="#3-非功能性需求" class="headerlink" title="3. 非功能性需求"></a>3. 非功能性需求</h1><ul><li>无论存放在内存还是磁盘当中，对于存储空间的消耗不能过大</li><li>在考虑索引查询效率的同时，还要考虑索引的维护成本</li></ul><h1 id="4-构建索引常用的数据结构"><a href="#4-构建索引常用的数据结构" class="headerlink" title="4. 构建索引常用的数据结构"></a>4. 构建索引常用的数据结构</h1><ul><li>散列表<ul><li>构建内存索引</li><li>如Redis Memcache</li></ul></li><li>红黑树<ul><li>构建内存索引</li><li>Ext文件系统中对磁盘块的索引</li></ul></li><li>B+树<ul><li>比起红黑树，更适合在磁盘当中构建索引</li><li>多叉树，比起红黑树，高度更低，IO更少</li><li>大部分关系型数据库的索引，会使用B+树来实现<ul><li>MySQL</li><li>Oracle</li></ul></li></ul></li><li>跳表<ul><li>Redis中的有序集合</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;索引的实现，底层一般会依赖于哪些数据结构？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;1-为什么需要索引&quot;&gt;&lt;a href=&quot;#1-为什么需要索引&quot; class=&quot;headerlink&quot; title=&quot;1. 为什么需要索引&quot;&gt;&lt;/a&gt;1.
      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>B+树</title>
    <link href="https://www.llchen60.com/B-%E6%A0%91/"/>
    <id>https://www.llchen60.com/B-%E6%A0%91/</id>
    <published>2021-07-11T04:25:25.000Z</published>
    <updated>2021-07-11T04:27:41.539Z</updated>
    
    <content type="html"><![CDATA[<h1 id="B-树"><a href="#B-树" class="headerlink" title="B+树"></a>B+树</h1><h1 id="1-什么时候会用到？"><a href="#1-什么时候会用到？" class="headerlink" title="1. 什么时候会用到？"></a>1. 什么时候会用到？</h1><h2 id="1-1-数据库索引"><a href="#1-1-数据库索引" class="headerlink" title="1.1 数据库索引"></a>1.1 数据库索引</h2><p>工作中为了加快数据库中数据的查找速度，我们常用的处理思路是对表中数据创建索引，而数据库的索引底层就使用了B+树的结构</p><p>对于数据库，我们希望通过索引实现查询数据的效率的提升；同时不要消耗太多的内存空间。而数据库索引查找的过程当中，其特点是：</p><ul><li>需要进行直接查找</li><li>需要支持按照一定区间的快速查找</li></ul><h2 id="1-2-Overview"><a href="#1-2-Overview" class="headerlink" title="1.2 Overview"></a>1.2 Overview</h2><ul><li>B + 树特征<ul><li>每个节点中子节点的个数不能超过 m，也不能小于 m/2；</li><li>根节点的子节点个数可以不超过 m/2，这是一个例外；</li><li>m 叉树只存储索引，并不真正存储数据，这个有点儿类似跳表；</li><li>通过链表将叶子节点串联在一起，这样可以方便按区间查找；</li><li>一般情况下，根节点会被存储在内存中，其他节点存储在磁盘中</li></ul></li><li>B树<ul><li>节点中存储数据</li><li>叶子节点并没有使用链表来串联</li><li>是一个每个节点的子节点个数不少于m/2的m叉树</li></ul></li></ul><h1 id="2-为什么在数据库索引的时候会使用到B-树呢？"><a href="#2-为什么在数据库索引的时候会使用到B-树呢？" class="headerlink" title="2. 为什么在数据库索引的时候会使用到B+树呢？"></a>2. 为什么在数据库索引的时候会使用到B+树呢？</h1><h2 id="2-1-使用二叉查找树来实现索引？"><a href="#2-1-使用二叉查找树来实现索引？" class="headerlink" title="2.1 使用二叉查找树来实现索引？"></a>2.1 使用二叉查找树来实现索引？</h2><ul><li>从二叉查找树开始说起 我们可以改一下<ul><li>树中的节点不存储数据本身，只作为索引</li><li>每个叶子节点串在一条链表上</li><li>链表当中的数据从小到大有序</li></ul></li><li>对于区间查找的支持程度<ul><li>先在树中遍历，然后到叶子节点以后再顺着链表来往后遍历</li><li>直到链表当中的结点数据值大于区间的终止值为止</li></ul></li><li>问题<ul><li>占用的内存空间非常大<ul><li>解决方案<ul><li>放到硬盘上，但是访问速度会变慢很多了</li><li>内存访问速度在纳秒级别</li><li>硬盘访问速度在毫秒级别</li></ul></li><li>解决方案的问题在于<ul><li>磁盘IO操作的次数等于树的高度</li><li>我们需要尽可能的减少树的高度来减少磁盘IO的次数</li></ul></li></ul></li></ul></li></ul><h2 id="2-2-使用m叉树来实现"><a href="#2-2-使用m叉树来实现" class="headerlink" title="2.2 使用m叉树来实现"></a>2.2 使用m叉树来实现</h2><ul><li>使用m叉树的好处<ul><li>减少了访问磁盘的IO次数</li></ul></li></ul><pre><code class="jsx">/** * 这是B+树非叶子节点的定义。 * * 假设keywords=[3, 5, 8, 10] * 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF) * 5个区间分别对应：children[0]...children[4] * * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = (m-1)*4[keywordss大小]+m*8[children大小] */public class BPlusTreeNode &#123;  public static int m = 5; // 5叉树  public int[] keywords = new int[m-1]; // 键值，用来划分数据区间  public BPlusTreeNode[] children = new BPlusTreeNode[m];//保存子节点指针&#125;/** * 这是B+树中叶子节点的定义。 * * B+树中的叶子节点跟内部节点是不一样的, * 叶子节点存储的是值，而非区间。 * 这个定义里，每个叶子节点存储3个数据行的键值及地址信息。 * * k值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = k*4[keyw..大小]+k*8[dataAd..大小]+8[prev大小]+8[next大小] */public class BPlusTreeLeafNode &#123;  public static int k = 3;  public int[] keywords = new int[k]; // 数据的键值  public long[] dataAddress = new long[k]; // 数据地址  public BPlusTreeLeafNode prev; // 这个结点在链表中的前驱结点  public BPlusTreeLeafNode next; // 这个结点在链表中的后继结点&#125;</code></pre><ul><li>m值的设定<ul><li>操作系统是按页来读取数据的，一页通常大小为4KB</li><li>一次会读取一页的数据</li><li>如果要读取的数据量超过一页的大小，就会触发多次IO操作了</li><li>因此我们选定m大小的时候尽量让每个节点的大小等于一个页的大小</li><li>这样的话读取一个节点只需要一次磁盘IO操作即可</li></ul></li></ul><p><img src="https://i.loli.net/2021/07/11/148A67p9Y3dre2g.png" alt="B+树数据存储"></p><h2 id="2-3-索引的问题-导致写入效率下降"><a href="#2-3-索引的问题-导致写入效率下降" class="headerlink" title="2.3 索引的问题 - 导致写入效率下降"></a>2.3 索引的问题 - 导致写入效率下降</h2><ul><li>数据写入过程，会涉及到索引的更新，这是导致索引写入变慢的主要原因</li><li>场景描述<ul><li>m值是提前计算好的</li><li>向数据库写入过程当中，有可能会使得索引当中某些节点的子节点的个数超过m</li><li>这个节点的大小超过一个页的大小，那么读取这样一个节点的时候，就会导致多次磁盘IO操作，这是我们极力避免的</li></ul></li><li>解决方案<ul><li>将这个节点分裂成两个节点</li><li>这个问题会向上传导，即上层父节点的子节点个数有可能超过m个了</li><li>因此我们就需要向上递归来重构这棵B+树了</li><li>正是因为我们需要保证B+树索引是一个m叉树，所以索引的存在会导致数据库写入速度降低</li></ul></li></ul><p><img src="https://i.loli.net/2021/07/11/9qAHTSmnlUeVPjb.png" alt="B+树索引修改"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;B-树&quot;&gt;&lt;a href=&quot;#B-树&quot; class=&quot;headerlink&quot; title=&quot;B+树&quot;&gt;&lt;/a&gt;B+树&lt;/h1&gt;&lt;h1 id=&quot;1-什么时候会用到？&quot;&gt;&lt;a href=&quot;#1-什么时候会用到？&quot; class=&quot;headerlink&quot; title=&quot;1
      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis哨兵机制</title>
    <link href="https://www.llchen60.com/Redis%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/"/>
    <id>https://www.llchen60.com/Redis%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/</id>
    <published>2021-07-06T03:07:59.000Z</published>
    <updated>2021-07-05T19:22:25.605Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis多机数据库-哨兵机制"><a href="#Redis多机数据库-哨兵机制" class="headerlink" title="Redis多机数据库-哨兵机制"></a>Redis多机数据库-哨兵机制</h1><p>主从库的集群模式使得当从库发生故障以后，客户端可以继续向主库或者其他从库发送请求，进行相关的操作；但是如果主库发生了故障，那会直接影响到从库的同步。无论是写中断还是从库无法进行数据同步都是Redis所不能接受的。因此我们需要一些机制，来能够<strong>将一个从库切换为主库</strong>，这就涉及到了Redis的哨兵机制。</p><h1 id="1-哨兵机制的基本流程"><a href="#1-哨兵机制的基本流程" class="headerlink" title="1. 哨兵机制的基本流程"></a>1. 哨兵机制的基本流程</h1><ul><li>哨兵可以理解为一个运行在特殊模式下的<strong>Redis进程</strong>，其在主从库实例运行的同时也在运行</li><li>哨兵主要的三个任务为：<ul><li>监控 — 决策：判断主库是否处于下线状态<ul><li>周期性的ping主库，检测其是否仍然在线运行</li><li>如果从库没有在规定时间内响应哨兵的Ping命令，哨兵就会将其标记为下线状态</li><li>对主库来说同理，在判定主库下线以后会开始一个自动切换主库的流程</li></ul></li><li>选主 — 决策：决定选择哪个从库实例作为主库<ul><li>主库挂了以后，哨兵就需要从很多从库里按照一定的规则选择一个从库实例，将其作为新的主库</li></ul></li><li>通知<ul><li>将新主库的连接信息发给其他从库，让它们执行replicaof命令，与新主库建立连接，并进行数据复制</li><li>哨兵会将新主库的连接信息通知给客户端，让它们将请求操作发到新主库当中</li></ul></li></ul></li></ul><p><img src="https://i.loli.net/2021/01/07/z7o6Kkdfp2IaPsx.png" alt="哨兵机制任务"></p><p>哨兵三大任务</p><h1 id="2-判断主库的下线状态"><a href="#2-判断主库的下线状态" class="headerlink" title="2. 判断主库的下线状态"></a>2. 判断主库的下线状态</h1><h2 id="2-1-哨兵集群使用原因"><a href="#2-1-哨兵集群使用原因" class="headerlink" title="2.1 哨兵集群使用原因"></a>2.1 哨兵集群使用原因</h2><h3 id="2-1-1-为什么需要哨兵集群？"><a href="#2-1-1-为什么需要哨兵集群？" class="headerlink" title="2.1.1 为什么需要哨兵集群？"></a>2.1.1 为什么需要哨兵集群？</h3><ul><li>如果哨兵发生误判，后续的选主和通知操作都会带来额外的计算和通信的开销</li><li>误判通常发生在<ul><li>集群网络压力较大</li><li>网络拥塞</li><li>主库本身压力较大的情况</li></ul></li><li>哨兵机制也是类似的，它通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。</li></ul><h3 id="2-1-2-如何使用哨兵集群？"><a href="#2-1-2-如何使用哨兵集群？" class="headerlink" title="2.1.2 如何使用哨兵集群？"></a>2.1.2 如何使用哨兵集群？</h3><ul><li>简单来说，“客观下线”的标准就是，当有 N 个哨兵实例时，最好要有 N/2 + 1 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来，就可以减少误判的概率，也能避免误判带来的无谓的主从库切换。（当然，有多少个实例做出“主观下线”的判断才可以，可以由 Redis 管理员自行设定）。</li></ul><h2 id="2-2-哨兵集群原理-—-基于PubSub机制"><a href="#2-2-哨兵集群原理-—-基于PubSub机制" class="headerlink" title="2.2 哨兵集群原理 — 基于PubSub机制"></a>2.2 哨兵集群原理 — 基于PubSub机制</h2><h3 id="2-2-1-pubsub机制"><a href="#2-2-1-pubsub机制" class="headerlink" title="2.2.1 pubsub机制"></a>2.2.1 pubsub机制</h3><p>哨兵实例之间的相互发现是基于Redis提供的pubsub机制的，<strong>哨兵只要和主库建立起连接</strong>，就可以在主库上发布消息了</p><ul><li>可以选择发布自己的连接信息到主库上</li><li>也可以从主库上订阅消息，获得其他哨兵发布的连接信息</li><li>当多个哨兵实例都在主库上做了发布和订阅操作之后，他们之间就能知道彼此的IP地址和端口</li></ul><h3 id="2-2-2-频道"><a href="#2-2-2-频道" class="headerlink" title="2.2.2 频道"></a>2.2.2 频道</h3><ul><li>Redis通过频道来区分不同应用的消息，对这些消息进行分门别类的管理。频道就是指消息的类别，当消息类别相同时，就会属于同一个频道，否则属于不同的频道。</li><li>主库会构建一个叫做 <code>__sentinel__:hello</code> 的频道，来让各个哨兵互相发现</li><li>只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换</li></ul><p><img src="https://i.loli.net/2021/01/11/LIhj62iuDBbPEve.png" alt="pubsub机制"></p><p>频道订阅机制</p><ul><li>哨兵1 向频道hello发送信息，因为哨兵2 哨兵3 subscribe了hello频道，他们就能从这个频道获取到哨兵1的IP地址和端口号信息</li></ul><h3 id="2-2-3-哨兵和从库的连接沟通"><a href="#2-2-3-哨兵和从库的连接沟通" class="headerlink" title="2.2.3 哨兵和从库的连接沟通"></a>2.2.3 哨兵和从库的连接沟通</h3><ul><li><p>哨兵向主库发出INFO命令</p></li><li><p>主库收到命令后，就会将从库列表返回给哨兵</p></li><li><p>接着哨兵就可以根据从库列表中的信息，和每个从库建立连接，并在这个连接上持续对从库进行监控</p><p>  <img src="https://i.loli.net/2021/01/11/sPTRS1mkhU2lLQn.png" alt="哨兵与从库之间的连接"></p><p>  哨兵和从库的连接</p></li><li><p>哨兵除了上述的和主库之间的连接，获取从库列表，并和从库们建立连接之外，还承担着在发生主库更换以后，将新主库的信息告诉客户端这个任务</p></li></ul><h2 id="2-3-客户端事件通知机制"><a href="#2-3-客户端事件通知机制" class="headerlink" title="2.3 客户端事件通知机制"></a>2.3 客户端事件通知机制</h2><ul><li><p>哨兵是一个运行在特定模式下的Redis实例，只是它不服务请求操作，只是完成监控，选主和通知的任务</p></li><li><p>因此每个哨兵实例也提供pubsub机制，客户端可以从哨兵订阅消息</p><ul><li><p>哨兵提供了很多的消息订阅频道，不同频道包含了主从库切换过程中的不同关键事件</p><p>  <img src="https://i.loli.net/2021/07/06/lBP5xJYieohTZXw.png" alt="哨兵常用的消息订阅频道"></p></li><li><p>客户端可以执行订阅命令，来订阅不同的频道，然后来获取不同的事件信息</p><ul><li>当哨兵将新的主库选出来以后，客户端会看到switch-master事件，事件中会包含新的主库的IP地址还有端口信息</li><li>此时客户端就会使用新主库地址和端口来进行通信了</li></ul></li></ul></li></ul><h1 id="3-如何选定新主库？"><a href="#3-如何选定新主库？" class="headerlink" title="3. 如何选定新主库？"></a>3. 如何选定新主库？</h1><blockquote><p>哨兵筛选新主库的过程称为筛选+打分</p></blockquote><ul><li>筛选 — 按照一定条件筛选，将不符合条件的从库去掉<ul><li>确保从库仍然在线运行</li><li>判断其之前的网络状态 看该从库和主库之间是否经常断联，出现网络相关的问题<ul><li>通过使用down-after-milliseconds属性，这是我们认为的最大主从间的连接超时，如果超过这个时间我们就认为断联了，超过一定次数就认为从库网络状况不好</li></ul></li></ul></li><li>打分 — <strong>只要有得分最高的，那么就在当前轮停止并且认定其为主库</strong><ul><li>从库优先级 — 手动设置的<ul><li>用户可以通过slave-priority配置项，给不同的从库设置不同的优先级<ul><li>譬如：两个从库内存大小不一样，我们就可以手动给内存大的实例设置一个高优先级</li></ul></li></ul></li><li>从库复制进度<ul><li>选择和旧主库<strong>同步最为接近</strong>的那个从库作为主库</li><li>如何判断从库和旧主库的同步进度？<ul><li>主从库之间命令传播机制里面的master_repl_offset 和slave_repl_offset</li><li>看二者的接近程度</li></ul></li></ul></li><li>从库ID号<ul><li>当优先级和复制进度都相同的情况下，ID号最小的从库得分最高，被选为新主库</li></ul></li></ul></li></ul><h1 id="4-由哪个哨兵来执行主从切换？"><a href="#4-由哪个哨兵来执行主从切换？" class="headerlink" title="4. 由哪个哨兵来执行主从切换？"></a>4. 由哪个哨兵来执行主从切换？</h1><ul><li><h2 id="任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应"><a href="#任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应" class="headerlink" title="任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应"></a>任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应</h2><pre><code>  ![哨兵沟通，确定主库是否下线](https://i.loli.net/2021/01/11/HpT5MAdKX9fmo2S.png)  is master down by addr</code></pre><ul><li><p>一个哨兵获得了<strong>仲裁所需的赞成票数后，就可以标记主库为客观下线</strong></p><ul><li>这个所需的赞成票数是通过哨兵配置文件中的quorum配置项设定的</li></ul></li><li><p>当获得了所需赞成票数以后，这个哨兵会再给其他哨兵发送命令，希望由自己来执行主从切换，并让所有其他哨兵进行投票，这个过程称为<strong>Leader选举</strong>。</p></li><li><p>在投票过程中，任何一个想成为 Leader 的哨兵，要满足两个条件：</p><ul><li>第一，拿到半数以上的赞成票；</li><li>第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。以 3 个哨兵为例，假设此时的 quorum 设置为 2，那么，任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以了。</li></ul><p><img src="https://i.loli.net/2021/01/11/RAaq7KZr2LSUdVx.png" alt="哨兵投票，选举leader"></p><p>票选执行主从切换哨兵的过程</p></li></ul><ol><li>在 T1 时刻，S1 判断主库为“客观下线”，它想成为 Leader，就先给自己投一张赞成票，然后分别向 S2 和 S3 发送命令，表示要成为 Leader。</li><li>在 T2 时刻，S3 判断主库为“客观下线”，它也想成为 Leader，所以也先给自己投一张赞成票，再分别向 S1 和 S2 发送命令，表示要成为 Leader。</li><li>在 T3 时刻，S1 收到了 S3 的 Leader 投票请求。<strong>因为 S1 已经给自己投了一票 Y，所以它不能再给其他哨兵投赞成票了，所以 S1 回复 N 表示不同意</strong>。同时，S2 收到了 T2 时 S3 发送的 Leader 投票请求。因为 S2 之前没有投过票，它会给第一个向它发送投票请求的哨兵回复 Y，给后续再发送投票请求的哨兵回复 N，所以，在 T3 时，S2 回复 S3，同意 S3 成为 Leader。</li><li>在 T4 时刻，S2 才收到 T1 时 S1 发送的投票命令。因为 S2 已经在 T3 时同意了 S3 的投票请求，此时，S2 给 S1 回复 N，表示不同意 S1 成为 Leader。发生这种情况，是因为 S3 和 S2 之间的网络传输正常，而 S1 和 S2 之间的网络传输可能正好拥塞了，导致投票请求传输慢了。</li><li>在 T5 时刻，S1 得到的票数是来自它自己的一票 Y 和来自 S2 的一票 N。而 S3 除了自己的赞成票 Y 以外，还收到了来自 S2 的一票 Y。此时，S3 不仅获得了半数以上的 Leader 赞成票，也达到预设的 quorum 值（quorum 为 2），所以它最终成为了 Leader。接着，S3 会开始执行选主操作，而且在选定新主库后，会给其他从库和客户端通知新主库的信息。</li></ol><ul><li><p>如果 S3 没有拿到 2 票 Y，那么这轮投票就不会产生 Leader。<strong>哨兵集群会等待一段时间（也就是哨兵故障转移超时时间的 2 倍）</strong>，再重新选举。这是因为，哨兵集群能够进行成功投票，很大程度上依赖于选举命令的正常网络传播。<strong>如果网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票</strong>。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。需要注意的是，如果哨兵集群只有 2 个实例，此时，一个哨兵要想成为 Leader，必须获得 2 票，而不是 1 票。所以，如果有个哨兵挂掉了，那么，此时的集群是无法进行主从库切换的。因此，通常我们至少会配置 3 个哨兵实例。</p><h1 id="5-FAQs"><a href="#5-FAQs" class="headerlink" title="5. FAQs"></a>5. FAQs</h1><h2 id="5-1-哨兵在操作主从切换的过程当中，客户端能否正常进行请求操作？"><a href="#5-1-哨兵在操作主从切换的过程当中，客户端能否正常进行请求操作？" class="headerlink" title="5.1 哨兵在操作主从切换的过程当中，客户端能否正常进行请求操作？"></a>5.1 哨兵在操作主从切换的过程当中，客户端能否正常进行请求操作？</h2></li><li><p>如果客户端使用了读写分离，那么<strong>读请求</strong>可以在从库上正常执行，不会受到影响。但是由于此时主库已经挂了，而且哨兵还没有选出新的主库，所以在这期间<strong>写请求会失败</strong>，失败持续的时间 = 哨兵切换主从的时间 + 客户端感知到新主库 的时间。</p></li><li><p>如果不想让业务感知到异常，客户端只能把写失败的请求先缓存起来或<strong>写入消息队列中间件</strong>中，等哨兵切换完主从后，再把这些写请求发给新的主库，但这种场景只适合对写入请求返回值不敏感的业务，而且还需要业务层做适配，另外主从切换时间过长，也会导致客户端或消息队列中间件缓存写请求过多，切换完成之后重放这些请求的时间变长。</p></li><li><p>哨兵检测主库多久没有响应就提升从库为新的主库，这个时间是可以配置的（down-after-milliseconds参数）。配置的时间越短，哨兵越敏感，哨兵集群认为主库在短时间内连不上就会发起主从切换，这种配置很可能因为网络拥塞但主库正常而发生不必要的切换，当然，当主库真正故障时，因为切换得及时，对业务的影响最小。如果配置的时间比较长，哨兵越保守，这种情况可以减少哨兵误判的概率，但是主库故障发生时，业务写失败的时间也会比较久，缓存写请求数据量越多。</p></li><li><p>应用程序不感知服务的中断，还需要哨兵和客户端做些什么？当哨兵完成主从切换后，客户端需要及时感知到主库发生了变更，然后把缓存的写请求写入到新库中，保证后续写请求不会再受到影响，具体做法如下：</p><ul><li>哨兵提升一个从库为新主库后，哨兵会把新主库的地址写入自己实例的pubsub（switch-master）中。客户端需要订阅这个pubsub，当这个pubsub有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。</li><li>如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见，客户端也需要支持主动去获取最新主从的地址进行访问。</li><li>所以，客户端需要访问主从库时，不能直接写死主从库的地址了，而是需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name命令），这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。</li><li>一般Redis的SDK都提供了通过哨兵拿到实例地址，再访问实例的方式，我们直接使用即可，不需要自己实现这些逻辑。当然，对于只有主从实例的情况，客户端需要和哨兵配合使用，而在分片集群模式下，这些逻辑都可以做在proxy层，这样客户端也不需要关心这些逻辑了，Codis就是这么做的</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1></li></ul><ol><li>极客时间Redis课程</li><li>Redis设计与实现</li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis多机数据库-哨兵机制&quot;&gt;&lt;a href=&quot;#Redis多机数据库-哨兵机制&quot; class=&quot;headerlink&quot; title=&quot;Redis多机数据库-哨兵机制&quot;&gt;&lt;/a&gt;Redis多机数据库-哨兵机制&lt;/h1&gt;&lt;p&gt;主从库的集群模式使得当从库发生故障以
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
</feed>
