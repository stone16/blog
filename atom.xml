<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leilei&#39;s Blog | 磊磊的博客</title>
  
  <subtitle>Because it&#39;s there</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.llchen60.com/"/>
  <updated>2021-01-15T03:53:09.305Z</updated>
  <id>https://www.llchen60.com/</id>
  
  <author>
    <name>Leilei Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Distributed Locks with the DynamoDB Lock Client </title>
    <link href="https://www.llchen60.com/Distributed-Locks-with-the-DynamoDB-Lock-Client/"/>
    <id>https://www.llchen60.com/Distributed-Locks-with-the-DynamoDB-Lock-Client/</id>
    <published>2021-01-15T03:51:14.000Z</published>
    <updated>2021-01-15T03:53:09.305Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Distributed-Locks-with-the-DynamoDB-Lock-Client"><a href="#Distributed-Locks-with-the-DynamoDB-Lock-Client" class="headerlink" title="Distributed Locks with the DynamoDB Lock Client"></a>Distributed Locks with the DynamoDB Lock Client</h1><h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li><p>DynamoDB Lock Client</p><ul><li>enable you to solve distributed computing problems like leader election and distributed locking with client only code and a DDB table</li></ul></li><li><p>Why we need it</p><ul><li>Distributed Locking is complicated<ul><li>you need to <strong>atomically ensure</strong> only one actor is modifying a <strong>stateful resource</strong> at any given time</li></ul></li></ul></li></ul><h1 id="2-Practical-Example"><a href="#2-Practical-Example" class="headerlink" title="2. Practical Example"></a>2. Practical Example</h1><ul><li>Background<ul><li>A retail bank that want to ensure at most one customer service representative change customer details at a time</li><li>solution<ul><li>temporarily lock customer records during an update</li><li>suppose there are bunch different tables to contain all customer information, as the tables are independent, so we cannot just wrap the changes we need in a relational transaction</li><li>we need to lock customer id at a high level</li><li>You’d do so with a locking API action for a certain duration in your application before making any changes.</li></ul></li></ul></li></ul><h2 id="2-1-Locking-Protocol"><a href="#2-1-Locking-Protocol" class="headerlink" title="2.1 Locking Protocol"></a>2.1 Locking Protocol</h2><ul><li>For a new lock, the lock clients store a lock item in the lock table<ul><li>it stores<ul><li>the host name of the owner</li><li>the lease duration in milliseconds</li><li>a UUID unique to the host</li><li>the host system clock time when the lock was initially created</li></ul></li></ul></li></ul><p><img src="https://i.loli.net/2021/01/15/gk6qico4zUw1YXK.png" alt="Whole Workflow"></p><ol><li>Host A acquires a lock on Moe by writing an item to the lock table on the condition that no item keyed at “Moe” exists yet. Host A<br>acquires the lock with a revision version number (RVN) of UUID.</li><li>Host B tries to get a lock on Moe with a RVN UUID.</li><li>Host B checks to see if a lock already exists with a GetItem call.</li><li>In this case, host B finds that host A holds a lock on Moe with a record version number (RVN) of UUID. The same application runs on hosts A and B. That being so, host B<br>expects host A to heartbeat and renew the lock on Moe in less than 10 seconds, if host A intends to keep the lock on Moe. Host A heartbeats once, and uses a conditional update on the lock keyed at Moe to update the RVN of the lock to UUID.</li><li>Host B checks 10 seconds after the first AcquireLock call to see if the RVN in A’s lock on Moe changed with a conditional UpdateItem call and a RVN of UUID.</li><li>Host A successfully updates the lock. Thus, host B finds the new RVN equal to UUID and waited 10 more seconds. Host A died after the first heartbeat, so it never changes the RVN past UUID. When host B calls tries to acquire a lock on Moe for the third time, it finds that the RVN was still UUID, the same RVN retrieved on the second lock attempt.</li><li>In this case, hosts A and B run the same application. Because host B expects host A to heartbeat if host A is healthy and intends to keep the lock, host B considers the lock on Moe expired. Host B’s conditional update to acquire the lock on Moe succeeds, and your application makes progress!</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://aws.amazon.com/blogs/database/building-distributed-locks-with-the-dynamodb-lock-client/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/database/building-distributed-locks-with-the-dynamodb-lock-client/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Distributed-Locks-with-the-DynamoDB-Lock-Client&quot;&gt;&lt;a href=&quot;#Distributed-Locks-with-the-DynamoDB-Lock-Client&quot; class=&quot;headerlink&quot; title
      
    
    </summary>
    
    
      <category term="Cloud" scheme="https://www.llchen60.com/categories/Cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>序列化和反序列化</title>
    <link href="https://www.llchen60.com/%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/"/>
    <id>https://www.llchen60.com/%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/</id>
    <published>2021-01-14T06:03:10.000Z</published>
    <updated>2021-01-14T06:04:54.739Z</updated>
    
    <content type="html"><![CDATA[<h1 id="序列化和反序列化"><a href="#序列化和反序列化" class="headerlink" title="序列化和反序列化"></a>序列化和反序列化</h1><h1 id="1-定义"><a href="#1-定义" class="headerlink" title="1. 定义"></a>1. 定义</h1><ul><li><p>序列化</p><ul><li>将对象转化为可传输的字节序列的过程<ul><li>是属于TCP/ IP协议应用层的一部分</li><li>常用的你可能看到的名字<ul><li>serialization</li><li>marshalling</li><li>flatteing</li></ul></li></ul></li><li>常见序列化方式<ul><li>JSON</li><li>XML</li></ul></li></ul></li><li><p>反序列化</p><ul><li>将字节序列还原为对象的过程称为反序列化</li></ul></li><li><p>二进制串</p><ul><li>二进制串：序列化所生成的二进制串指的是存储在内存中的一块数据。</li><li>C++语言具有内存操作符，所以二进制串的概念容易理解，例如，C++语言的字符串可以直接被传输层使用，因为其本质上就是以’\0’结尾的存储在内存中的二进制串。</li><li>在Java语言里面，二进制串的概念容易和String混淆。实际上String 是Java的一等公民，是一种特殊对象（Object）。对于跨语言间的通讯，序列化后的数据当然不能是某种语言的特殊数据类型。二进制串在Java里面所指的是<strong>byte[]</strong>，byte是Java的8中原生数据类型之一（Primitive data types）</li></ul></li></ul><h1 id="2-为什么要序列化"><a href="#2-为什么要序列化" class="headerlink" title="2. 为什么要序列化"></a>2. 为什么要序列化</h1><ul><li>为了使得对象可以跨平台进行存储，进行网络传输</li><li>本质上存储和网络传输都需要将一个对象状态保存成一种跨平台识别的字节格式，然后其他的平台才可以通过字节信息解析还原对象的信息</li><li>而且序列化后可以存在文件当中，永久性的存到硬盘上</li></ul><h1 id="3-序列化技术选择的metrics"><a href="#3-序列化技术选择的metrics" class="headerlink" title="3. 序列化技术选择的metrics"></a>3. 序列化技术选择的metrics</h1><h2 id="3-1-跨平台-amp-跨语言"><a href="#3-1-跨平台-amp-跨语言" class="headerlink" title="3.1 跨平台 &amp; 跨语言"></a>3.1 跨平台 &amp; 跨语言</h2><p>是否需要支持多种语言，应该选择没有语言局限性的序列化协议</p><ul><li>Json会是一个很好的选择，因为Json表示出来就是一个字符串，可以被所有语言读取，也可以方便地存储到磁盘或者通过网络传输</li><li>JSON不仅是标准格式，并且比XML更快，而且可以直接在Web页面当中读取</li></ul><h2 id="3-2-性能"><a href="#3-2-性能" class="headerlink" title="3.2 性能"></a>3.2 性能</h2><h3 id="3-2-1-速度"><a href="#3-2-1-速度" class="headerlink" title="3.2.1 速度"></a>3.2.1 速度</h3><p>如果序列化的频率非常高，那么选择序列化速度快的协议会为你的系统性能提升不少</p><h3 id="3-2-3-序列化后的大小"><a href="#3-2-3-序列化后的大小" class="headerlink" title="3.2.3 序列化后的大小"></a>3.2.3 序列化后的大小</h3><p>数据量小对于网络的压力小，传输也会快，能够提升整体的性能</p><p>序列化需要在原有的数据上加上描述字段，以为反序列化解析之用。如果序列化过程引入的额外开销过高，可能会导致过大的网络，磁盘等各方面的压力。对于海量分布式存储系统，数据量往往以TB为单位，巨大的的额外空间开销意味着高昂的成本。</p><h2 id="3-3-鲁棒性"><a href="#3-3-鲁棒性" class="headerlink" title="3.3 鲁棒性"></a>3.3 鲁棒性</h2><p>从两方面来进行考虑</p><ul><li>成熟度<ul><li>一个协议从制定，实施到成熟是一个很漫长的阶段。协议的强健性依赖于大量而全面的测试。对于致力于提供高质量服务的系统，采用处于测试阶段的序列化协议会带来很高的风险。</li></ul></li><li>语言/ 平台的公平性<ul><li>当所支持的语言或者平台之间存在难以调和的特性的时候，协议制定者需要做权衡<ul><li>做支持更多人使用的语言/平台  vs 为了支持更多的语言/ 平台而放弃某个特性</li></ul></li></ul></li></ul><h2 id="3-4-可调试性-可读性"><a href="#3-4-可调试性-可读性" class="headerlink" title="3.4 可调试性/ 可读性"></a>3.4 可调试性/ 可读性</h2><ul><li>序列化和反序列化的数据正确性和业务正确性的调试往往需要很长的时间，良好的调试机制会大大提高开发效率。</li><li>序列化后的二进制串往往不具备人眼可读性，为了验证序列化结果的正确性，写入方不得同时撰写反序列化程序，或提供一个查询平台–这比较费时；</li><li>另一方面，如果读取方未能成功实现反序列化，这将给问题查找带来了很大的挑战–难以定位是由于自身的反序列化程序的bug所导致还是由于写入方序列化后的错误数据所导致</li></ul><h2 id="3-5-可扩展性-兼容性"><a href="#3-5-可扩展性-兼容性" class="headerlink" title="3.5 可扩展性/ 兼容性"></a>3.5 可扩展性/ 兼容性</h2><p>移动互联时代，业务系统需求的更新周期变得更快，新的需求不断涌现，而老的系统还是需要继续维护。如果序列化协议具有良好的可扩展性，支持自动增加新的业务字段，而不影响老的服务，这将大大提供系统的灵活度</p><h2 id="3-6-安全性-访问限制"><a href="#3-6-安全性-访问限制" class="headerlink" title="3.6 安全性/ 访问限制"></a>3.6 安全性/ 访问限制</h2><p>在序列化选型的过程中，安全性的考虑往往发生在<strong>跨局域网访问的场景</strong>。当通讯发生在公司之间或者跨机房的时候，出于安全的考虑，对于跨局域网的访问往往被限制为基于HTTP/HTTPS的80和443端口。如果使用的序列化协议没有兼容成熟的HTTP传输层框架支持，可能会导致以下三种结果之一：</p><p>第一、因为访问限制而降低服务可用性。<br>第二、被迫重新实现安全协议而导致实施成本大大提高。<br>第三、开放更多的防火墙端口和协议访问，而牺牲安全性。</p><h1 id="4-序列化和反序列化的组件"><a href="#4-序列化和反序列化的组件" class="headerlink" title="4. 序列化和反序列化的组件"></a>4. 序列化和反序列化的组件</h1><h2 id="4-1-IDL-—-Interface-Description-Language"><a href="#4-1-IDL-—-Interface-Description-Language" class="headerlink" title="4.1 IDL — Interface Description Language"></a>4.1 IDL — Interface Description Language</h2><ul><li>参与通讯的各方需要对通讯的内容做相关的约定 — specifications</li><li>这个约定需要和语言以及平台无关</li><li>被称为接口描述语言</li></ul><h2 id="4-2-IDL-Compiler"><a href="#4-2-IDL-Compiler" class="headerlink" title="4.2 IDL Compiler"></a>4.2 IDL Compiler</h2><ul><li>IDL文件中约定的内容为了在各个语言和平台可见，需要有一个编译器，将IDL文件转换成各个语言对应的动态库</li></ul><h2 id="4-3-Stub-Skeleton-Lib"><a href="#4-3-Stub-Skeleton-Lib" class="headerlink" title="4.3 Stub/ Skeleton Lib"></a>4.3 Stub/ Skeleton Lib</h2><ul><li>负责序列化和反序列化的工作代码</li><li>Stub是一段部署在分布式系统客户端的代码，一方面接收应用层的参数，并对其序列化后通过底层协议栈发送到服务端，另一方面接收服务端序列化后的结果数据，反序列化后交给客户端应用层；</li><li>Skeleton部署在服务端，其功能与Stub相反，从传输层接收序列化参数，反序列化后交给服务端应用层，并将应用层的执行结果序列化后最终传送给客户端Stub。</li></ul><p><img src="https://i.loli.net/2021/01/14/l1jSYEnHLxdzD92.png" alt="序列化反序列化过程"></p><h1 id="5-常见的序列化和反序列化协议"><a href="#5-常见的序列化和反序列化协议" class="headerlink" title="5. 常见的序列化和反序列化协议"></a>5. 常见的序列化和反序列化协议</h1><p>下面以这两个类的序列化反序列化为例</p><pre><code class="java">class Address{    private String city;    private String postcode;    private String street;}public class UserInfo{    private Integer userid;    private String name;    private List&lt;Address&gt; address;}</code></pre><h2 id="5-1-XML-amp-SOAP"><a href="#5-1-XML-amp-SOAP" class="headerlink" title="5.1 XML &amp; SOAP"></a>5.1 XML &amp; SOAP</h2><h3 id="5-1-1-XML"><a href="#5-1-1-XML" class="headerlink" title="5.1.1 XML"></a>5.1.1 XML</h3><ul><li>特点<ul><li>描述语言，self-describing</li><li>XML自身可以被用于XML序列化的IDL</li><li>标准的XML描述格式有<ul><li>DTD - Document Type Definition</li><li>XSD - XML Schema Definition</li></ul></li></ul></li><li>优点<ul><li>跨机器，跨语言</li><li>可读性<ul><li>因为其最初的目标是对互联网文档Document进行标记，所以其设计理念当中就包含了对于人和机器都具备可读性</li></ul></li></ul></li><li>缺点<ul><li>冗长，复杂</li></ul></li></ul><h3 id="5-1-2-SOAP-Simple-Object-Access-Protocol"><a href="#5-1-2-SOAP-Simple-Object-Access-Protocol" class="headerlink" title="5.1.2 SOAP - Simple Object Access Protocol"></a>5.1.2 SOAP - Simple Object Access Protocol</h3><ul><li><p>概念</p><ul><li>基于XML为序列化和反序列化协议的结构化的消息传递协议</li><li>SOAP支持多种传输层协议，最常见的使用方式是XML + HTTP</li></ul></li><li><p>IDL</p><ul><li>SOAP协议的主要接口描述语言IDL是WSDL — Web Service Description Language</li></ul></li><li><p>SOAP具有安全、可扩展、跨语言、跨平台并支持多种传输层协议。如果不考虑跨平台和跨语言的需求，XML的在某些语言里面具有非常简单易用的序列化使用方法，无需IDL文件和第三方编译器， 例如Java＋XStream</p></li><li><p>自我描述和递归</p><ul><li><p>SOAP是一种采用XML进行序列化和反序列化的协议，它的IDL是WSDL. 而WSDL的描述文件是XSD，而XSD自身是一种XML文件。 这里产生了一种有趣的在数学上称之为“递归”的问题，这种现象往往发生在一些具有自我属性（Self-description）的事物上</p><pre><code class="java">&lt;xsd:complexType name=&#39;Address&#39;&gt;   &lt;xsd:attribute name=&#39;city&#39; type=&#39;xsd:string&#39; /&gt;   &lt;xsd:attribute name=&#39;postcode&#39; type=&#39;xsd:string&#39; /&gt;   &lt;xsd:attribute name=&#39;street&#39; type=&#39;xsd:string&#39; /&gt;&lt;/xsd:complexType&gt;&lt;xsd:complexType name=&#39;UserInfo&#39;&gt;   &lt;xsd:sequence&gt;   &lt;xsd:element name=&#39;address&#39; type=&#39;tns:Address&#39;/&gt;   &lt;xsd:element name=&#39;address1&#39; type=&#39;tns:Address&#39;/&gt;    &lt;/xsd:sequence&gt;   &lt;xsd:attribute name=&#39;userid&#39; type=&#39;xsd:int&#39; /&gt;   &lt;xsd:attribute name=&#39;name&#39; type=&#39;xsd:string&#39; /&gt; &lt;/xsd:complexType&gt;</code></pre></li></ul></li><li><p>优点</p><ul><li>安全性</li><li>XML肉眼可读，可调试性好</li></ul></li><li><p>缺点</p><ul><li>XML空间开销会大很多，序列化后的数据量剧增，这意味着巨大的内存和磁盘开销</li></ul></li><li><p>适用场景</p><ul><li>对于公司之间传输数据量小或者实时性要求相对低的服务是一个很好的选择</li></ul></li></ul><h2 id="5-2-JSON-Javascript-Object-Notation"><a href="#5-2-JSON-Javascript-Object-Notation" class="headerlink" title="5.2 JSON - Javascript Object Notation"></a>5.2 JSON - Javascript Object Notation</h2><ul><li><p>概念</p><ul><li>出自Javascript，产生来自于Associative array的概念，本质是采用Attribute-value的方法来描述对象</li></ul></li><li><p>优点</p><ul><li>这种Associative array格式非常符合工程师对对象的理解。</li><li>它保持了XML的人眼可读（Human-readable）的优点。</li><li>相对于XML而言，序列化后的数据更加简洁。 来自于的以下链接的研究表明：XML所产生序列化之后文件的大小接近JSON的两倍。<a href="http://www.codeproject.com/Articles/604720/JSON-vs-XML-Some-hard-numbers-about-verbosity" target="_blank" rel="noopener">http://www.codeproject.com/Articles/604720/JSON-vs-XML-Some-hard-numbers-about-verbosity</a></li><li>它具备Javascript的先天性支持，所以被广泛应用于Web browser的应用常景中，是Ajax的事实标准协议。</li><li>与XML相比，其协议比较简单，解析速度比较快。</li><li>松散的Associative array使得其具有良好的可扩展性和兼容性。</li></ul></li><li><p>应用场景</p><ul><li>公司之间传输数据量<strong>相对小</strong>，实时性要求相对低（<strong>例如秒级别）</strong>的服务。</li><li>基于Web browser的Ajax请求。</li><li>由于JSON具有非常强的前后兼容性，对于接口经常发生变化，并对可调式性要求高的场景，例如Mobile app与服务端的通讯。</li><li>由于JSON的典型应用场景是JSON＋HTTP，适合跨防火墙访问</li></ul></li><li><p>缺点</p><ul><li>采用JSON进行序列化的额外空间开销比较大，对于大数据量服务或持久化，这意味着巨大的内存和磁盘开销，这种场景不适合。</li><li>没有统一可用的IDL降低了对参与方的约束，实际操作中往往只能<strong>采用文档方式来进行约定</strong>，这可能会给调试带来一些不便，延长开发周期。</li><li>由于JSON在一些语言中的序列化和反序列化需要采用反射机制，所以在性能要求为ms级别，不建议使用</li></ul></li></ul><pre><code class="java">{&quot;userid&quot;:1,&quot;name&quot;:&quot;messi&quot;,&quot;address&quot;:[{&quot;city&quot;:&quot;北京&quot;,&quot;postcode&quot;:&quot;1000000&quot;,&quot;street&quot;:&quot;wangjingdonglu&quot;}]}</code></pre><h2 id="5-3-Thrift"><a href="#5-3-Thrift" class="headerlink" title="5.3 Thrift"></a>5.3 Thrift</h2><ul><li>简介<ul><li>FB开源的一个高性能轻量级的RPC框架</li><li>为了满足当前大数据量、分布式、跨语言、跨平台数据通讯的需求</li><li>，Thrift并不仅仅是序列化协议，而是一个RPC框架。相对于JSON和XML而言，Thrift在空间开销和解析性能上有了比较大的提升，对于对性能要求比较高的分布式系统，它是一个优秀的RPC解决方案；但是由于Thrift的序列化被嵌入到Thrift框架里面，Thrift框架本身并没有透出序列化和反序列化接口，这导致其很难和其他传输层协议共同使用</li></ul></li></ul><pre><code class="java">// IDL wenstruct Address{     1: required string city;    2: optional string postcode;    3: optional string street;} struct UserInfo{     1: required string userid;    2: required i32 name;    3: optional list&lt;Address&gt; address;}</code></pre><h2 id="5-4-Protobuf"><a href="#5-4-Protobuf" class="headerlink" title="5.4 Protobuf"></a>5.4 Protobuf</h2><ul><li>特征<ul><li>标准的IDL和IDL编译器，这使得其对工程师非常友好。</li><li>序列化数据非常简洁，紧凑，与XML相比，其序列化之后的数据量约为1/3到1/10。</li><li>解析速度非常快，比对应的XML快约20-100倍。</li><li>提供了非常友好的动态库，使用非常简介，反序列化只需要一行代码。</li><li>Protobuf是一个纯粹的展示层协议，可以和各种传输层协议一起使用；Protobuf的文档也非常完善。 但是由于Protobuf产生于Google，所以目前其仅仅支持Java、C++、Python三种语言。另外Protobuf支持的数据类型相对较少，不支持常量类型。由于其设计的理念是纯粹的展现层协议（Presentation Layer），目前并没有一个专门支持Protobuf的RPC框架</li></ul></li><li>应用场景<ul><li>Protobuf具有广泛的用户基础，空间开销小以及高解析性能是其亮点，非常适合于公司内部的对性能要求高的RPC调用。</li><li>由于Protobuf提供了标<strong>准的IDL以及对应的编译器，其IDL文件是参与各方的非常强的业务约束</strong>，</li><li>另外，Protobuf与传输层无关，采用HTTP具有良好的跨防火墙的访问属性，所以Protobuf也适用于公司间对性能要求比较高的场景</li><li>由于其解析性能高，序列化后数据量相对少，非常适合应用层对象的持久化场景。</li></ul></li></ul><pre><code class="java">// IDL File E.Gmessage Address{    required string city=1;        optional string postcode=2;        optional string street=3;}message UserInfo{    required string userid=1;    required string name=2;    repeated Address address=3;}</code></pre><h2 id="5-5-Avro"><a href="#5-5-Avro" class="headerlink" title="5.5 Avro"></a>5.5 Avro</h2><ul><li>介绍<ul><li>Apache Hadoop的子项目</li><li>提供两种序列化方式<ul><li>Json<ul><li>为了方便测试的调试过程</li></ul></li><li>Binary<ul><li>空间开销和解析性能可以和Protobuf媲美</li></ul></li></ul></li></ul></li><li>优点<ul><li>Avro支持JSON格式的IDL和类似于Thrift和Protobuf的IDL（实验阶段），这两者之间可以互转。</li><li>Schema可以在传输数据的同时发送，加上JSON的自我描述属性，这使得Avro非常适合动态类型语言。</li><li>Avro在做文件持久化的时候，一般会和Schema一起存储，所以Avro序列化文件自身具有自我描述属性，所以非常适合于做Hive、Pig和MapReduce的持久化数据格式。</li><li>对于不同版本的Schema，在进行RPC调用的时候，服务端和客户端可以在握手阶段对Schema进行互相确认，大大提高了最终的数据解析速度。</li></ul></li></ul><h1 id="6-Benchmark"><a href="#6-Benchmark" class="headerlink" title="6. Benchmark"></a>6. Benchmark</h1><ul><li>通过下面链接可以发现Protobuf和Avro在两个方面都表现很优越</li></ul><p><a href="https://code.google.com/archive/p/thrift-protobuf-compare/wikis/Benchmarking.wiki" target="_blank" rel="noopener">https://code.google.com/archive/p/thrift-protobuf-compare/wikis/Benchmarking.wiki</a> </p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/40462507" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40462507</a></li><li><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017624706151424" target="_blank" rel="noopener">https://www.liaoxuefeng.com/wiki/1016959663602400/1017624706151424</a> </li><li><a href="https://tech.meituan.com/2015/02/26/serialization-vs-deserialization.html" target="_blank" rel="noopener">https://tech.meituan.com/2015/02/26/serialization-vs-deserialization.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;序列化和反序列化&quot;&gt;&lt;a href=&quot;#序列化和反序列化&quot; class=&quot;headerlink&quot; title=&quot;序列化和反序列化&quot;&gt;&lt;/a&gt;序列化和反序列化&lt;/h1&gt;&lt;h1 id=&quot;1-定义&quot;&gt;&lt;a href=&quot;#1-定义&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="Web" scheme="https://www.llchen60.com/categories/Web/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis哨兵机制</title>
    <link href="https://www.llchen60.com/Redis%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/"/>
    <id>https://www.llchen60.com/Redis%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/</id>
    <published>2021-01-07T04:07:59.000Z</published>
    <updated>2021-01-10T18:04:18.357Z</updated>
    
    <content type="html"><![CDATA[<h1 id="哨兵机制"><a href="#哨兵机制" class="headerlink" title="哨兵机制"></a>哨兵机制</h1><p>主从库的集群模式使得当从库发生故障以后，客户端可以继续向主库或者其他从库发送请求，进行相关的操作；但是如果主库发生了故障，那会直接影响到从库的同步。无论是写中断还是从库无法进行数据同步都是Redis所不能接受的。因此我们需要一些机制，来能够将一个从库切换为主库，这就涉及到了Redis的哨兵机制。</p><h1 id="1-哨兵机制的基本流程"><a href="#1-哨兵机制的基本流程" class="headerlink" title="1. 哨兵机制的基本流程"></a>1. 哨兵机制的基本流程</h1><ul><li>哨兵可以理解为一个运行在特殊模式下的Redis进程，其在主从库实例运行的同时也在运行</li><li>哨兵主要的三个任务为：<ul><li>监控 — 决策：判断主库是否处于下线状态<ul><li>周期性的ping主库，检测其是否仍然在线运行</li><li>如果从库没有在规定时间内响应哨兵的Ping命令，哨兵就会将其标记为下线状态</li><li>对主库来说同理，在判定主库下线以后会开始一个自动切换主库的流程</li></ul></li><li>选主 — 决策：决定选择哪个从库实例作为主库<ul><li>主库挂了以后，哨兵就需要从很多歌从库里按照一定的规则选择一个从库实例，将其作为新的主库</li></ul></li><li>通知<ul><li>将新主库的连接信息发给其他从库，让它们执行replicaof命令，与新主库建立连接，并进行数据复制</li><li>哨兵会将新主库的连接信息通知给客户端，让它们将请求操作发到新主库当中</li></ul></li></ul></li></ul><p><img src="https://i.loli.net/2021/01/07/z7o6Kkdfp2IaPsx.png" alt="哨兵三大任务"></p><h1 id="2-判断主库的下线状态"><a href="#2-判断主库的下线状态" class="headerlink" title="2. 判断主库的下线状态"></a>2. 判断主库的下线状态</h1><h2 id="2-1-哨兵集群使用原因"><a href="#2-1-哨兵集群使用原因" class="headerlink" title="2.1 哨兵集群使用原因"></a>2.1 哨兵集群使用原因</h2><h3 id="2-1-1-为什么需要哨兵集群？"><a href="#2-1-1-为什么需要哨兵集群？" class="headerlink" title="2.1.1 为什么需要哨兵集群？"></a>2.1.1 为什么需要哨兵集群？</h3><ul><li>如果哨兵发生误判，后续的选主和通知操作都会带来额外的计算和通信的开销</li><li>误判通常发生在<ul><li>集群网络压力较大</li><li>网络拥塞</li><li>主库本身压力较大的情况</li></ul></li><li>哨兵机制也是类似的，它通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。</li></ul><h3 id="2-1-2-如何使用哨兵集群？"><a href="#2-1-2-如何使用哨兵集群？" class="headerlink" title="2.1.2 如何使用哨兵集群？"></a>2.1.2 如何使用哨兵集群？</h3><ul><li>简单来说，“客观下线”的标准就是，当有 N 个哨兵实例时，最好要有 N/2 + 1 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来，就可以减少误判的概率，也能避免误判带来的无谓的主从库切换。（当然，有多少个实例做出“主观下线”的判断才可以，可以由 Redis 管理员自行设定）。</li></ul><h2 id="2-2-哨兵集群原理-—-基于PubSub机制"><a href="#2-2-哨兵集群原理-—-基于PubSub机制" class="headerlink" title="2.2 哨兵集群原理 — 基于PubSub机制"></a>2.2 哨兵集群原理 — 基于PubSub机制</h2><h3 id="2-2-1-pubsub机制"><a href="#2-2-1-pubsub机制" class="headerlink" title="2.2.1  pubsub机制"></a>2.2.1  pubsub机制</h3><p>哨兵实例之间的相互发现是基于Redis提供的pubsub机制的，哨兵只要和主库建立起连接，就可以在主库上发布消息了</p><ul><li>可以选择发布自己的连接信息到主库上</li><li>也可以从主库上订阅消息，获得其他哨兵发布的连接信息</li><li>当多个哨兵实例都在主库上做了发布和订阅操作之后，他们之间就能知道彼此的IP地址和端口</li></ul><h3 id="2-2-2-频道"><a href="#2-2-2-频道" class="headerlink" title="2.2.2 频道"></a>2.2.2 频道</h3><ul><li>Redis通过频道来区分不同应用的消息，对这些消息进行分门别类的管理。频道就是指消息的类别，当消息类别相同时，就会属于同一个频道，否则属于不同的频道。</li><li>只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换</li></ul><p><img src="https://i.loli.net/2021/01/11/LIhj62iuDBbPEve.png" alt="频道订阅机制"></p><ul><li>哨兵1 想频道hello发送信息，因为哨兵2 哨兵3 subscribe了hello频道，他们就能从这个频道获取到哨兵1的IP地址和端口号信息</li></ul><h3 id="2-2-3-哨兵和从库的连接沟通"><a href="#2-2-3-哨兵和从库的连接沟通" class="headerlink" title="2.2.3 哨兵和从库的连接沟通"></a>2.2.3 哨兵和从库的连接沟通</h3><ul><li><p>哨兵向主库发出INFO命令</p></li><li><p>主库收到命令后，就会将从库列表返回给哨兵</p></li><li><p>接着哨兵就可以根据从库列表中的信息，和每个从库建立连接，并在这个连接上持续对从库进行监控</p><p>  <img src="https://i.loli.net/2021/01/11/sPTRS1mkhU2lLQn.png" alt="哨兵和从库的连接"></p></li><li><p>哨兵除了上述的和主库之间的连接，获取从库列表，并和从库们建立连接之外，还承担着在发生主库更换以后，将新主库的信息告诉客户端这个任务</p></li></ul><h2 id="2-3-客户端事件通知机制"><a href="#2-3-客户端事件通知机制" class="headerlink" title="2.3 客户端事件通知机制"></a>2.3 客户端事件通知机制</h2><ul><li><p>哨兵是一个运行在特定模式下的Redis实例，只是它不服务请求操作，只是完成监控，选主和通知的任务</p></li><li><p>因此每个哨兵实例也提供pubsub机制，客户端可以从哨兵订阅消息</p><ul><li><p>哨兵提供了很多的消息订阅频道，不同频道包含了主从库切换过程中的不同关键事件</p><p>  <img src="https://i.loli.net/2021/01/11/RAaq7KZr2LSUdVx.png" alt="常用的关键时间列表"></p></li><li><p>客户端可以执行订阅命令，来订阅不同的频道，然后来获取不同的事件信息</p></li></ul></li></ul><h1 id="3-如何选定新主库？"><a href="#3-如何选定新主库？" class="headerlink" title="3. 如何选定新主库？"></a>3. 如何选定新主库？</h1><ul><li>筛选<ul><li>确保从库仍然在线运行</li><li>判断其之前的网络状态 看该从库和主库之间是否经常断联，出现网络相关的问题</li></ul></li><li>打分 — 只要有得分最高的，那么就在当前轮停止并且认定其为主库<ul><li>从库优先级<ul><li>用户可以通过slave-priority配置项，给不同的从库设置不同的优先级<ul><li>譬如：两个从库内存大小不一样，我们就可以手动给内存大的实例设置一个高优先级</li></ul></li></ul></li><li>从库复制进度<ul><li>选择和旧主库同步最为接近的那个从库作为主库</li><li>如何判断从库和旧主库的同步进度？<ul><li>主从库之间命令传播机制里面的master_repl_offset 和slave_repl_offset</li><li>看二者的接近程度</li></ul></li></ul></li><li>从库ID号<ul><li>当优先级和复制进度都相同的情况下，ID号最小的从库得分最高，被选为新主库</li></ul></li></ul></li></ul><h1 id="4-由哪个哨兵来执行主从切换？"><a href="#4-由哪个哨兵来执行主从切换？" class="headerlink" title="4. 由哪个哨兵来执行主从切换？"></a>4. 由哪个哨兵来执行主从切换？</h1><ul><li><h2 id="任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应"><a href="#任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应" class="headerlink" title="任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应"></a>任何一个实例只要自身判断主库主观下线之后，就会给其他势力发送is-master-down-by-addr命令。接着其他实例会根据自己和主库的连接情况，做出Y或N的响应</h2><pre><code>  ![is master down by addr](https://i.loli.net/2021/01/11/HpT5MAdKX9fmo2S.png)</code></pre><ul><li><p>一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为客观下线</p><ul><li>这个所需的赞成票数是通过哨兵配置文件中的quorum配置项设定的</li></ul></li><li><p>当获得了所需赞成票数以后，这个哨兵会再给其他哨兵发送命令，希望由自己来执行主从切换，并让所有其他哨兵进行投票，这个过程称为Leader选举。</p></li><li><p>在投票过程中，任何一个想成为 Leader 的哨兵，要满足两个条件：第一，拿到半数以上的赞成票；第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。以 3 个哨兵为例，假设此时的 quorum 设置为 2，那么，任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以了。</p></li><li><p><img src="https://i.loli.net/2021/01/11/RAaq7KZr2LSUdVx.png" alt="票选执行主从切换哨兵的过程"></p></li></ul><ol><li>在 T1 时刻，S1 判断主库为“客观下线”，它想成为 Leader，就先给自己投一张赞成票，然后分别向 S2 和 S3 发送命令，表示要成为 Leader。</li><li>在 T2 时刻，S3 判断主库为“客观下线”，它也想成为 Leader，所以也先给自己投一张赞成票，再分别向 S1 和 S2 发送命令，表示要成为 Leader。</li><li>在 T3 时刻，S1 收到了 S3 的 Leader 投票请求。<strong>因为 S1 已经给自己投了一票 Y，所以它不能再给其他哨兵投赞成票了，所以 S1 回复 N 表示不同意</strong>。同时，S2 收到了 T2 时 S3 发送的 Leader 投票请求。因为 S2 之前没有投过票，它会给第一个向它发送投票请求的哨兵回复 Y，给后续再发送投票请求的哨兵回复 N，所以，在 T3 时，S2 回复 S3，同意 S3 成为 Leader。</li><li>在 T4 时刻，S2 才收到 T1 时 S1 发送的投票命令。因为 S2 已经在 T3 时同意了 S3 的投票请求，此时，S2 给 S1 回复 N，表示不同意 S1 成为 Leader。发生这种情况，是因为 S3 和 S2 之间的网络传输正常，而 S1 和 S2 之间的网络传输可能正好拥塞了，导致投票请求传输慢了。</li><li>在 T5 时刻，S1 得到的票数是来自它自己的一票 Y 和来自 S2 的一票 N。而 S3 除了自己的赞成票 Y 以外，还收到了来自 S2 的一票 Y。此时，S3 不仅获得了半数以上的 Leader 赞成票，也达到预设的 quorum 值（quorum 为 2），所以它最终成为了 Leader。接着，S3 会开始执行选主操作，而且在选定新主库后，会给其他从库和客户端通知新主库的信息。</li></ol><ul><li>如果 S3 没有拿到 2 票 Y，那么这轮投票就不会产生 Leader。<strong>哨兵集群会等待一段时间（也就是哨兵故障转移超时时间的 2 倍）</strong>，再重新选举。这是因为，哨兵集群能够进行成功投票，很大程度上依赖于选举命令的正常网络传播。<strong>如果网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票</strong>。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。需要注意的是，如果哨兵集群只有 2 个实例，此时，一个哨兵要想成为 Leader，必须获得 2 票，而不是 1 票。所以，如果有个哨兵挂掉了，那么，此时的集群是无法进行主从库切换的。因此，通常我们至少会配置 3 个哨兵实例。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;哨兵机制&quot;&gt;&lt;a href=&quot;#哨兵机制&quot; class=&quot;headerlink&quot; title=&quot;哨兵机制&quot;&gt;&lt;/a&gt;哨兵机制&lt;/h1&gt;&lt;p&gt;主从库的集群模式使得当从库发生故障以后，客户端可以继续向主库或者其他从库发送请求，进行相关的操作；但是如果主库发生了故障，那
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis数据同步</title>
    <link href="https://www.llchen60.com/Redis%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/"/>
    <id>https://www.llchen60.com/Redis%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/</id>
    <published>2021-01-04T03:09:48.000Z</published>
    <updated>2021-01-04T03:18:02.081Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis数据同步"><a href="#Redis数据同步" class="headerlink" title="Redis数据同步"></a>Redis数据同步</h1><h1 id="1-Redis的高可靠性"><a href="#1-Redis的高可靠性" class="headerlink" title="1. Redis的高可靠性"></a>1. Redis的高可靠性</h1><p>Redis的高可靠性体现在两个方面： </p><ul><li>数据尽量少丢失<ul><li>AOF</li><li>RDB</li></ul></li><li>服务尽量少中断<ul><li>增加副本冗余量 — 将一份数据同时保存在多个实例上</li></ul></li></ul><h1 id="2-数据同步-—-主从库模式"><a href="#2-数据同步-—-主从库模式" class="headerlink" title="2. 数据同步 — 主从库模式"></a>2. 数据同步 — 主从库模式</h1><ul><li><p>主从库之间采用的是读写分离的方式</p><ul><li><p>读操作</p><ul><li>主库，从库都可以接收</li></ul></li><li><p>写操作</p><ul><li><p>首先到主库执行</p></li><li><p>然后主库将写操作同步给从库</p><p>  <img src="https://i.loli.net/2021/01/04/8wE4dPDgFxqRX6r.png" alt="主从读写分离"></p></li></ul></li></ul></li><li><p>主从库的好处是修改操作都只会在一个库实现</p><ul><li>可以减少加锁，实例间协商这类开销</li></ul></li></ul><h2 id="2-1-主从库之间如何进行第一个同步？"><a href="#2-1-主从库之间如何进行第一个同步？" class="headerlink" title="2.1 主从库之间如何进行第一个同步？"></a>2.1 主从库之间如何进行第一个同步？</h2><ul><li><p>多个Redis实例之间通过replicaof命令形成主库和从库的关系，然后按照三个阶段完成数据的第一次同步：</p><p>  <img src="https://i.loli.net/2021/01/04/KZemoVB6CFjNlJI.png" alt="主从首次同步过程"></p></li><li><p>第一阶段</p><ul><li>主从库之间建立连接，协商同步</li><li>为全量复制做准备</li><li>从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复以后，主从库间就可以开始同步了<ul><li>从库给主库发送psync命令，表示要进行数据同步</li><li>主库根据这个命令的参数来启动复制<ul><li>psync命令包含主库的runId和复制进度的offset两个参数<ul><li>runID — Redis实例启动的时候自动随机生成的ID，用来唯一标识当前实例</li><li>offset 此时设为-1，表示第一次复制</li></ul></li></ul></li><li>主库收到psync命令后，使用FULLRESYNC响应命令，包括了主库的runID还有主库目前的复制进度offset，返回给从库<ul><li>从库记录下两个参数</li></ul></li><li>FULLRESYNC表示第一次复制使用的是全量复制</li></ul></li></ul></li><li><p>第二阶段</p><ul><li>主库将所有数据同步给从库</li><li>从库收到数据后，在本地完成数据加载 — 依赖于内存快照生成的RDB文件<ul><li>主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。</li><li>从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。<ul><li>这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空</li></ul></li></ul></li><li>在做数据同步的过程中，主库不会被阻塞。对于这个过程中接收到的正常请求，写操作会记录在主库的Replication Buffer当中</li></ul></li><li><p>第三阶段</p><ul><li>主库会将第二阶段新收到的修改命令，再发给从库</li><li>当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了</li></ul></li></ul><h2 id="2-2-主从级联方式分担全量复制时的主库压力"><a href="#2-2-主从级联方式分担全量复制时的主库压力" class="headerlink" title="2.2 主从级联方式分担全量复制时的主库压力"></a>2.2 主从级联方式分担全量复制时的主库压力</h2><ul><li><p>现状/ 问题</p><ul><li>一次全量复制主库需要完成两个耗时操作<ul><li>生成RDB文件和传输RDB文件</li></ul></li><li>如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于 fork 子进程生成 RDB 文件，进行数据全量同步。fork 这个操作会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。</li><li>传输 RDB 文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力</li></ul></li><li><p>解决方案 — 主从从模式</p><ul><li><p>我们在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。然后，我们可以再选择一些从库（例如三分之一的从库），在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系。replicaof 所选从库的IP 6379</p><p><img src="https://i.loli.net/2021/01/04/eihQpmN6FJdRxLy.png" alt="级联主从库"></p></li></ul></li></ul><h2 id="2-3-突发情况下的增量复制"><a href="#2-3-突发情况下的增量复制" class="headerlink" title="2.3 突发情况下的增量复制"></a>2.3 突发情况下的增量复制</h2><ul><li><p>网络断了以后我们需要一种开销相对合理的复制方式，即增量复制</p><ul><li>将主从库断联期间主库收到的命令，同步给从库</li></ul></li><li><p>增量复制的时候，主从库之间依靠repl_backlog_buffer这个缓冲区来做同步</p></li><li><p>整个过程如下：</p><ul><li><p>当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。</p></li><li><p>repl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置。</p></li><li><p>刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这个偏移距离的大小，对主库来说，对应的偏移量就是 master_repl_offset。主库接收的新写操作越多，这个值就会越大。</p></li><li><p>同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量 slave_repl_offset 也在不断增加。正常情况下，这两个偏移量基本相等。</p></li><li><p>主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的 slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset 之间的差距</p></li><li><p>在网络断连阶段，主库可能会收到新的写操作命令，所以，一般来说，master_repl_offset 会大于 slave_repl_offset。此时，主库只用把 master_repl_offset 和 slave_repl_offset 之间的命令操作同步给从库就行。就像刚刚示意图的中间部分，主库和从库之间相差了 put d e 和 put d f 两个操作，在增量复制时，主库只需要把它们同步给从库，就行了。</p><p><img src="https://i.loli.net/2021/01/04/w3TLhzRgOH2A65d.png" alt="增量复制过程"></p></li></ul></li></ul><blockquote><p>因为 repl_backlog_buffer 是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致。</p></blockquote><p>我们要想办法避免这一情况，一般而言，我们可以调整 repl_backlog_size 这个参数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：<strong>缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小</strong>。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 repl_backlog_size = 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值</p><ul><li>repl_backlog_buffer<ul><li>是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销</li><li>如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量同步，所以repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量同步的概率</li><li>而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer</li></ul></li><li>replication_buffer<ul><li>Redis和客户端通信也好，和从库通信也好，Redis都需要给分配一个 内存buffer进行数据交互</li><li>客户端是一个client，从库也是一个client，我们每个client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的</li><li>Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互。</li><li>所以主从在增量同步时，从库作为一个client，也会分配一个buffer，只不过这个buffer专门用来传播用户的写命令到从库，保证主从数据一致，我们通常把它叫做replication buffer</li><li>这个buffer需要做大小的限制<ul><li>如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个buffer就会持续增长，消耗大量的内存资源，甚至OOM</li><li>所以Redis提供了<strong>client-output-buffer-limit</strong>参数限制这个buffer的大小，如果超过限制，主库会强制断开这个client的连接，也就是说从库处理慢导致主库内存buffer的积压达到限制后，主库会强制断开从库的连接，此时主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴，这种情况需要格外注意。</li></ul></li></ul></li></ul><h2 id="2-4-主从全量同步-RDB-vs-AOF"><a href="#2-4-主从全量同步-RDB-vs-AOF" class="headerlink" title="2.4 主从全量同步 RDB vs AOF"></a>2.4 主从全量同步 RDB vs AOF</h2><p>1、RDB文件内容是<strong>经过压缩的二进制数据（不同数据类型数据做了针对性优化）</strong>，文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。在主从全量数据同步时，传输RDB文件可以尽量降低对主库机器网络带宽的消耗，从库在加载RDB文件时，一是文件小，读取整个文件的速度会很快，二是因为<strong>RDB文件存储的都是二进制数据</strong>，从库直接按照RDB协议解析还原数据即可，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量同步的成本最低。</p><p>2、假设要使用AOF做全量同步，意味着必须打开AOF功能，打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能。而RDB只有在需要定时备份和主从全量同步数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis数据同步&quot;&gt;&lt;a href=&quot;#Redis数据同步&quot; class=&quot;headerlink&quot; title=&quot;Redis数据同步&quot;&gt;&lt;/a&gt;Redis数据同步&lt;/h1&gt;&lt;h1 id=&quot;1-Redis的高可靠性&quot;&gt;&lt;a href=&quot;#1-Redis的高可靠性&quot;
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis内存快照</title>
    <link href="https://www.llchen60.com/Redis%E5%86%85%E5%AD%98%E5%BF%AB%E7%85%A7/"/>
    <id>https://www.llchen60.com/Redis%E5%86%85%E5%AD%98%E5%BF%AB%E7%85%A7/</id>
    <published>2021-01-02T21:27:09.000Z</published>
    <updated>2021-01-02T21:29:47.029Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis内存快照"><a href="#Redis内存快照" class="headerlink" title="Redis内存快照"></a>Redis内存快照</h1><h1 id="1-AOF数据恢复存在的问题"><a href="#1-AOF数据恢复存在的问题" class="headerlink" title="1. AOF数据恢复存在的问题"></a>1. AOF数据恢复存在的问题</h1><ul><li>AOF方法每次执行记录的是操作命令，需要持久化的数据量不大</li><li>但是也因为记录的是操作命令，而不是实际数据，所以用AOF方法进行故障恢复的时候，需要逐一把操作日志都执行一遍<ul><li>如果操作日志很多，Redis的恢复就会很缓慢，可能影响到正常</li></ul></li></ul><h1 id="2-内存快照Overview"><a href="#2-内存快照Overview" class="headerlink" title="2. 内存快照Overview"></a>2. 内存快照Overview</h1><ul><li><p>内存快照可以解决上述的问题</p><ul><li>内存快照指的是记录下内存中的数据在某一时刻的状态</li><li>将某一时刻的状态以文件的形式写到磁盘上  这样即使宕机，快照文件也不会丢失，数据的可靠性也就有了保证</li><li>快照文件成为RDB文件，RDB — Redis DataBase</li></ul></li><li><p>RDB特征</p><ul><li>记录的是某一个时刻的数据，并不是操作</li><li>因此在数据恢复的时候，我们可以将RDB文件直接读入内存，很快完成恢复</li></ul></li></ul><h2 id="2-1-给哪些数据做快照？"><a href="#2-1-给哪些数据做快照？" class="headerlink" title="2.1 给哪些数据做快照？"></a>2.1 给哪些数据做快照？</h2><ul><li>Redis的数据都在内存当中，为了提供所有数据的可靠性保证，其执行的是<strong>全量快照</strong><ul><li>即将内存中的所有数据都记录到磁盘当中</li><li>与之一起来的问题就是，当需要对内存的全量数据做快照的时候，将其全部写入磁盘会花费很多时间</li><li>而且全量数据越多，RDB文件就越大，往磁盘上写数据的时间开销就越大</li><li>而Redis的单线程模型决定了我们要尽量避免阻塞主线程的操作</li></ul></li><li>Redis生成RDB文件的命令<ul><li>save<ul><li>在主线程中执行，会导致阻塞</li></ul></li><li>bgsave<ul><li>创建一个子进程，专门用于写入RDB文件，可以避免对于主线程的阻塞</li></ul></li></ul></li></ul><h2 id="2-2-做快照的时候数据是否能够被增删改？"><a href="#2-2-做快照的时候数据是否能够被增删改？" class="headerlink" title="2.2 做快照的时候数据是否能够被增删改？"></a>2.2 做快照的时候数据是否能够被增删改？</h2><ul><li><p>我们需要使系统在进行快照的时候仍然能够接受修改请求，要不然会严重影响系统的执行效率</p></li><li><p>Redis会借助操作系统提供的写时复制技术 — copy on write，在执行快照的同时，正常处理写操作</p><ul><li><p>copy on write</p><ul><li><p>copy operation is deferred until the first write,</p></li><li><p>could significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations</p><p><a href="https://en.wikipedia.org/wiki/Copy-on-write" target="_blank" rel="noopener">Copy-on-write</a></p><p><img src="https://i.loli.net/2021/01/03/I8kwNqF41KlezWL.png" alt="Copy on Write实现"></p></li></ul></li></ul></li></ul><h2 id="2-3-多久做一次快照？"><a href="#2-3-多久做一次快照？" class="headerlink" title="2.3 多久做一次快照？"></a>2.3 多久做一次快照？</h2><ul><li>尽管bgsave执行时不阻塞主线程，但是频繁的执行全量快照，会带来两方面的开销<ul><li>磁盘带宽压力<ul><li>频繁将全量数据写入磁盘，会给磁盘带来很大的压力</li><li>多个快照竞争有限的磁盘贷款，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环</li></ul></li><li>fork操作的阻塞<ul><li>bgsave子进程需要通过fork操作从主线程创建出来</li><li>fork创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间就越长</li></ul></li></ul></li></ul><h1 id="3-AOF和RDB混用模式"><a href="#3-AOF和RDB混用模式" class="headerlink" title="3.  AOF和RDB混用模式"></a>3.  AOF和RDB混用模式</h1><ul><li>为什么要混用<ul><li>AOF执行速度会比较慢</li><li>RDB的全量复制频率难以把控，太低，会容易丢失数据；太高，系统开销会很大</li></ul></li><li>如何实现的<ul><li>RDB以一定的频率来执行</li><li>在两次快照之间，使用AOF日志记录这期间所有的命令操作</li></ul></li></ul><p><img src="https://i.loli.net/2021/01/03/C98RNZ7PanDWyUr.png" alt="AOF &amp; RDB Mix"></p><ul><li>如上图所示，到了第二次做全量快照的时候，就可以清空AOF日志，因为所有的操作都已经保存到了第二次的全量快照当中了</li></ul><h1 id="4-实际场景探究"><a href="#4-实际场景探究" class="headerlink" title="4. 实际场景探究"></a>4. 实际场景探究</h1><blockquote><p>我们使用一个 2 核 CPU、4GB 内存、500GB 磁盘的云主机运行 Redis，Redis 数据库的数据量大小差不多是 2GB，我们使用了 RDB 做持久化保证。当时 Redis 的运行负载以修改操作为主，写读比例差不多在 8:2 左右，也就是说，如果有 100 个请求，80 个请求执行的是修改操作。你觉得，在这个场景下，用 RDB 做持久化有什么风险吗？</p></blockquote><ul><li><p>内存资源风险</p><ul><li><p>Redis fork子进程做RDB持久化，由于写的比例为80%，那么在持久化过程中，“写实复制”会重新分配整个实例80%的内存副本，大约需要重新分配1.6GB内存空间，这样整个系统的内存使用接近饱和，</p></li><li><p>如果此时父进程又有大量新key写入，很快机器内存就会被吃光，如果机器开启了Swap机制，那么Redis会有一部分数据被换到磁盘上，当Redis访问这部分在磁盘上的数据时，性能会急剧下降，已经达不到高性能的标准（可以理解为武功被废）。如果机器没有开启Swap，会直接触发OOM，父子进程会面临被系统kill掉的风险。</p><ul><li><p>swap 机制</p><ul><li><p>将一块磁盘或者一个本地文件当做内存来使用</p><ul><li><p>换入</p><ul><li>当进程再次访问内存的时候，从磁盘读取数据到内存当中</li></ul></li><li><p>换出</p><ul><li><p>将进程暂时不用的内存数据保存到磁盘上，再释放内存给其他进程使用</p></li><li><p>当进程再次访问内存的时候，从磁盘读取数据到内存中</p><p><a href="https://blog.csdn.net/qq_24436765/article/details/103822548" target="_blank" rel="noopener">Linux系统的swap机制_囚牢-峰子的博客-CSDN博客</a></p></li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>CPU资源风险</p><ul><li>虽然子进程在做RDB持久化，但生成RDB快照过程会消耗大量的CPU资源，</li><li>虽然Redis处理处理请求是单线程的，但Redis Server还有其他线程在后台工作，例如AOF每秒刷盘、异步关闭文件描述符这些操作。</li><li>由于机器只有2核CPU，这也就意味着父进程占用了超过一半的CPU资源，此时子进程做RDB持久化，可能会产生CPU竞争，导致的结果就是父进程处理请求延迟增大，子进程生成RDB快照的时间也会变长，整个Redis Server性能下降。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis内存快照&quot;&gt;&lt;a href=&quot;#Redis内存快照&quot; class=&quot;headerlink&quot; title=&quot;Redis内存快照&quot;&gt;&lt;/a&gt;Redis内存快照&lt;/h1&gt;&lt;h1 id=&quot;1-AOF数据恢复存在的问题&quot;&gt;&lt;a href=&quot;#1-AOF数据恢复存在
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis AOF 日志</title>
    <link href="https://www.llchen60.com/Redis-AOF-%E6%97%A5%E5%BF%97/"/>
    <id>https://www.llchen60.com/Redis-AOF-%E6%97%A5%E5%BF%97/</id>
    <published>2020-12-28T18:32:59.000Z</published>
    <updated>2021-01-02T23:08:05.383Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis-AOF-日志"><a href="#Redis-AOF-日志" class="headerlink" title="Redis AOF 日志"></a>Redis AOF 日志</h1><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><p>Redis很大的一个应用场景就是缓存，因为速度很快，通过将后端数据库中的数据存储在内存当中，然后直接从内存中读取数据。</p><p>但是这样做的一个问题，是如果服务器宕机，内存中的数据将会全部丢失掉。对于恢复数据，我们可能的解决方案是：</p><ul><li>从后端数据库访问<ul><li>对数据库的频繁访问会给数据库造成巨大的压力</li><li>会导致应用程序响应速度变慢</li></ul></li><li>理念 - 不从后端数据库读取，实现数据的持久化<ul><li>AOF 日志</li><li>RDB快照</li></ul></li></ul><h1 id="2-AOF日志的实现"><a href="#2-AOF日志的实现" class="headerlink" title="2. AOF日志的实现"></a>2. AOF日志的实现</h1><h2 id="2-1-什么是AOF"><a href="#2-1-什么是AOF" class="headerlink" title="2.1 什么是AOF"></a>2.1 什么是AOF</h2><ul><li><p>AOF - Append Only File</p></li><li><p>写后日志</p><ul><li><p>Redis先执行命令，将数据写入内存，然后才记录日志</p><p>  ![写后日志]](<a href="https://i.loli.net/2020/12/29/bNUOftoVI19G8Wj.png" target="_blank" rel="noopener">https://i.loli.net/2020/12/29/bNUOftoVI19G8Wj.png</a>)</p></li></ul></li></ul><h2 id="2-2-AOF记录了什么"><a href="#2-2-AOF记录了什么" class="headerlink" title="2.2 AOF记录了什么"></a>2.2 AOF记录了什么</h2><ul><li><p>传统数据库日志</p><ul><li>记录修改后的数据</li></ul></li><li><p>AOF</p><ul><li><p>写后日志</p></li><li><p>记录Redis收到的每一条指令，这些命令以文本形式保存</p></li><li><p>AOF记录日志的时候，不会进行语法检查的！ 因此，如果先记录日志，再做执行的话，日志当中就有可能记录错误的命令，在使用日志恢复数据的时候，就有可能出错</p><p>  <img src="https://i.loli.net/2020/12/29/qn9adxRcv2ZSJiD.png" alt="AOF日志范例"></p></li><li><p>写后日志可以避免出现记录错误命令的情况</p></li><li><p>而且因为是在命令执行后才记录日志，所以不会阻塞当前的写操作</p></li></ul></li></ul><h3 id="2-2-1-写后日志的风险"><a href="#2-2-1-写后日志的风险" class="headerlink" title="2.2.1 写后日志的风险"></a>2.2.1 写后日志的风险</h3><ul><li>如果刚执行完一个命令，还没有记录日志就宕机了，那么命令和相应的数据都有丢失的风险。</li><li>AOF虽然避免了对当前命令的阻塞，但是可能会给下一个操作带来阻塞风险<ul><li>因为AOF日志也是在主线程中执行，如果将日志文件写入磁盘的时候，磁盘写压力大，会导致写盘非常慢</li></ul></li></ul><p>解决方案： 需要控制写命令执行完成后AOF日志写回磁盘的时机</p><h1 id="3-单点研究"><a href="#3-单点研究" class="headerlink" title="3. 单点研究"></a>3. 单点研究</h1><h2 id="3-1-写回策略"><a href="#3-1-写回策略" class="headerlink" title="3.1 写回策略"></a>3.1 写回策略</h2><ul><li><p>可用的写回策略 - AOF当中的appendfsync的三个可选值</p><ul><li><p>Always 同步写回</p><ul><li>每个写命令执行完，立刻同步将日志写回磁盘</li></ul></li><li><p>EverySec 每秒写回</p><ul><li>每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，每隔一秒将缓冲区中的内容写入磁盘</li></ul></li><li><p>No 操作系统控制的写回</p><ul><li>每个写命令执行完，只是将日志写到AOF文件的缓冲区，由操作系统决定何时将缓冲区内容写回磁盘</li></ul><p><img src="https://i.loli.net/2020/12/29/RkfhClbVDv5JKzp.png" alt="写回策略对比"></p></li></ul></li><li><p>写回策略的选择 — 根据对于性能和可靠性的要求，来选择选用哪一种写回策略</p><ul><li>想要获得高性能，选用No策略</li><li>想要高可靠性的保证，选用Always策略</li><li>如果允许数据有一点丢失，又希望性能不受太大的影响，选用EverySec策略</li></ul></li></ul><h2 id="3-2-如何处理过大的日志文件-—-AOF重写机制"><a href="#3-2-如何处理过大的日志文件-—-AOF重写机制" class="headerlink" title="3.2 如何处理过大的日志文件 — AOF重写机制"></a>3.2 如何处理过大的日志文件 — AOF重写机制</h2><p>日志过大会产生性能问题，主要在以下三个方面：</p><ol><li>文件系统本身对文件大小的限制，无法保存过大的文件</li><li>如果文件太大，再向里面追加命令记录，效率会降低</li><li>如果发生宕机，AOF中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程会非常缓慢，这就会影响到Redis的正常使用</li></ol><h3 id="3-2-1-重写可以优化日志大小的原理"><a href="#3-2-1-重写可以优化日志大小的原理" class="headerlink" title="3.2.1 重写可以优化日志大小的原理"></a>3.2.1 重写可以优化日志大小的原理</h3><ul><li><p>AOF重写机制</p><ul><li><p>重写的时候，根据数据库现状创建一个新的AOF文件</p><ul><li>读取数据库所有的键值对</li><li>针对每一个键值对用一条命令记录它的写入</li><li>读取了键值对testkey:testvalue之后，重写机制就会记录set testkey testvalue这条命令</li><li>需要回复的时候，直接执行这条命令</li></ul></li><li><p>重写可以使得日志文件变小，因为可以压缩多条指令到一条</p><ul><li><p>即AOF日志是用来做恢复的，我不需要记录每一步的中间状态，只要知道最终对应的key的value是多少就好</p><p>  <img src="https://i.loli.net/2020/12/29/R7fgD2tVBvZUk8z.png" alt="重写原理"></p></li></ul></li></ul></li></ul><h3 id="3-2-2-重写如何避免阻塞？"><a href="#3-2-2-重写如何避免阻塞？" class="headerlink" title="3.2.2  重写如何避免阻塞？"></a>3.2.2  重写如何避免阻塞？</h3><ul><li><p>AOF日志由主线程写回，而重写过程是由后台子进程bgrewriteaof来完成的，是为了避免阻塞主线程，导致数据库性能的下降</p></li><li><p>重写的整个流程</p><ul><li><p>一处拷贝</p><ul><li>每次执行重写的时候，主线程fork到bgrewriteaof子进程</li><li>主线程的内存会被拷贝一份到bgrewriteaof子进程当中，其中会包含数据库的最新数据</li><li>然后该子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志</li></ul></li><li><p>两处日志</p><ul><li><p>主线程当中的AOF日志</p><ul><li>但有新的操作进入，Redis会将该操作写到AOF日志缓冲区</li><li>这样即使宕机，AOF日志的操作仍齐全，可以用来做恢复</li></ul></li><li><p>AOF重写日志</p><ul><li><p>该操作同时也会被写入到重写日志的缓冲区</p></li><li><p>等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以<strong>用新的 AOF 文件替代旧文件</strong>了。</p><p><img src="https://i.loli.net/2020/12/29/6vco3pLJNDBwU52.png" alt="重写流程"></p></li></ul></li></ul></li></ul></li></ul><h2 id="3-3-AOF-日志重写过程当中的阻塞风险"><a href="#3-3-AOF-日志重写过程当中的阻塞风险" class="headerlink" title="3.3 AOF 日志重写过程当中的阻塞风险"></a>3.3 AOF 日志重写过程当中的阻塞风险</h2><ul><li>Fork子进程的过程<ul><li>fork并不会一次性拷贝所有内存数据给子进程，采用的是操作系统提供的copy on write机制<ul><li>copy on write机制就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞的问题</li></ul></li><li>fork子进程需要先拷贝进程必要的数据结构<ul><li>拷贝内存页表 — 即虚拟内存和物理内存的映射索引表</li><li>这个拷贝过程会消耗大量的CPU资源，并且拷贝完成之前整个进程是会阻塞的</li><li>阻塞时间取决于整个实例的内存大小<ul><li>实例越大，内存页表也越大，fork阻塞时间就会越久</li></ul></li></ul></li><li>在完成了拷贝内存页表之后，子进程和父进程指向的是相同的内存地址空间<ul><li>这个时候虽然产生了子进程，但是并没有申请和父进程相同的内存大小</li><li>真正的内存分离是<strong>在写发生的时候，这个时候才会真正拷贝内存的数据</strong></li></ul></li></ul></li><li>AOF重写过程中父进程产生写入的过程<ul><li>Fork出的子进程当前状态是指向了和父进程相同的内存地址空间，这个时候子进程就可以执行AOF重写，将内存中的所有数据写入到AOF文件里</li><li>但是同时父进程仍然会有流量写入<ul><li>如果父进程操作的是一个已经存在的key，那么这个时候父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间，这样逐渐地，父子进程内存数据开始分离</li><li>父子进程逐渐拥有各自独立的内存空间。因为内存分配是以页为单位进行分配的，默认4k，如果父进程此时操作的是一个bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险</li><li>如果操作系统开启了内存大页机制(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在Redis机器上需要关闭Huge Page机制。Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间</li></ul></li></ul></li></ul><h2 id="3-4-AOF重写日志为什么不共享AOF本身的日志？"><a href="#3-4-AOF重写日志为什么不共享AOF本身的日志？" class="headerlink" title="3.4 AOF重写日志为什么不共享AOF本身的日志？"></a>3.4 AOF重写日志为什么不共享AOF本身的日志？</h2><p>AOF重写不复用AOF本身的日志，一个原因是父子进程写同一个文件必然会产生竞争问题，控制竞争就意味着会影响父进程的性能。二是如果AOF重写过程中失败了，那么原本的AOF文件相当于被污染了，无法做恢复使用。所以Redis AOF重写一个新文件，重写失败的话，直接删除这个文件就好了，不会对原先的AOF文件产生影响。等重写完成之后，直接替换旧文件即可</p><h2 id="3-5-如何触发AOF重写？"><a href="#3-5-如何触发AOF重写？" class="headerlink" title="3.5 如何触发AOF重写？"></a>3.5 如何触发AOF重写？</h2><p>有两个配置项在控制AOF重写的触发时机：</p><ol><li><p>auto-aof-rewrite-min-size: 表示运行AOF重写时文件的最小大小，默认为64MB</p></li><li><p>auto-aof-rewrite-percentage: 这个值的计算方法是：当前AOF文件大小和上一次重写后AOF文件大小的差值，再除以上一次重写后AOF文件大小。也就是当前AOF文件比上一次重写后AOF文件的增量大小，和上一次重写后AOF文件大小的比值。</p></li></ol><p>AOF文件大小同时超出上面这两个配置项时，会触发AOF重写。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://time.geekbang.org/column/article/271754" target="_blank" rel="noopener">https://time.geekbang.org/column/article/271754</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis-AOF-日志&quot;&gt;&lt;a href=&quot;#Redis-AOF-日志&quot; class=&quot;headerlink&quot; title=&quot;Redis AOF 日志&quot;&gt;&lt;/a&gt;Redis AOF 日志&lt;/h1&gt;&lt;h1 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; cla
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>使用AWS EC2搭建Halo博客</title>
    <link href="https://www.llchen60.com/%E4%BD%BF%E7%94%A8AWS-EC2%E6%90%AD%E5%BB%BAHalo%E5%8D%9A%E5%AE%A2/"/>
    <id>https://www.llchen60.com/%E4%BD%BF%E7%94%A8AWS-EC2%E6%90%AD%E5%BB%BAHalo%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-12-26T17:45:40.000Z</published>
    <updated>2020-12-28T18:54:52.980Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-EC2-设置"><a href="#1-EC2-设置" class="headerlink" title="1. EC2 设置"></a>1. EC2 设置</h1><p>在完成了AWS注册之后，登录后台，在EC2的看板左侧点击Instances，选择Launch Instances，这时候会带你进入到选择AMI的界面，按照Halo的推荐是选择CentOS比较合适，不过亲测了下RHEL，CentOS都没有什么问题，按照自己的需要 (如果新账号的话，会有eligible free tier)，可以免费使用一年，使用其即可。</p><p>对于运行的EC2实例，我们还需要对VPC, Security Group. Elastic IP做配置，目的是为了能够在VPC之外(公网)能够访问HTTP, HTTPS端口，一般来说就是80,还有443. 这里的整个过程(troubleshooting)可以根据这篇官方博客来做。</p><p><a href="https://aws.amazon.com/cn/premiumsupport/knowledge-center/ec2-connect-internet-gateway/" target="_blank" rel="noopener">排查 EC2 实例的互联网网关连接问题</a></p><h1 id="2-Halo基本设置"><a href="#2-Halo基本设置" class="headerlink" title="2. Halo基本设置"></a>2. Halo基本设置</h1><p>在根据第一部分的说明设置好服务器之后，我们可以ssh上服务器，然后开始做Halo的基本设置</p><p>详情可以看Halo安装的官方教程 — <a href="https://halo.run/archives/install-with-linux.html" target="_blank" rel="noopener">在Linux服务器部署Hal</a>o</p><ul><li>几个值得注意的地方<ul><li>JVM启动内存的分配</li><li>halo版本的更新</li><li>端口的设置</li></ul></li></ul><h1 id="3-反向代理"><a href="#3-反向代理" class="headerlink" title="3. 反向代理"></a>3. 反向代理</h1><p>使用Catty或者Nginx来做反向代理，完成https证书的申请，在你自己域名的服务商下设置dns，开始访问你自己的博客。</p><p>Halo域名的配置与访问</p><h1 id="4-Troubleshooting"><a href="#4-Troubleshooting" class="headerlink" title="4. Troubleshooting"></a>4. Troubleshooting</h1><ol><li>服务器上启动了服务，port开了但是Public Ip还是无法访问到</li></ol><p>这的错误很可能不在开启的服务(halo) 方面，而在于EC2防火墙 VPC等的配置，检查下端口是否都正常开启，根据第一部分的排查EC2互联网网关连接问题的文章一步步排查，基本上可以解决。</p><ol start="2"><li>在服务器上查看开启的端口，发现服务只开在IPV6上而没有在IPV4上开启</li></ol><p>发现这个问题是发现在使用telnet -tlnp 指令的时候，发现Halo的进程确实开启了，但是是监听在tcp6 下，在Ipv4下没有端口监听。查询资料发现Java 网络模块现在是默认先检察当前操作系统是否支持IPv6， 如果支持，就会直接使用IPv6， 否则才会使用ipv4. <a href="https://stackoverflow.com/questions/44718174/spring-boot-application-listens-over-ipv6-without-djava-net-preferipv4stack-tru" target="_blank" rel="noopener">StackOverflow 上的问答</a> 如果想要设置先监听ipv4的话，我们可以在指令上加上</p><pre><code class="jsx">-Djava.net.preferIPv4Stack=true-Djava.net.preferIPv4Addresses// 整个语句如下所示 (是在/etc/systemd/system/halo.service这个文件里做配置) ExecStart=/usr/bin/java -server -Xms256m -Xmx256m -jar YOUR_JAR_PATH -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses</code></pre><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/connect-http-https-ec2/" target="_blank" rel="noopener">Connect to an Amazon EC2 instance on HTTP or HTTPS ports</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-EC2-设置&quot;&gt;&lt;a href=&quot;#1-EC2-设置&quot; class=&quot;headerlink&quot; title=&quot;1. EC2 设置&quot;&gt;&lt;/a&gt;1. EC2 设置&lt;/h1&gt;&lt;p&gt;在完成了AWS注册之后，登录后台，在EC2的看板左侧点击Instances，选择Laun
      
    
    </summary>
    
    
      <category term="Web" scheme="https://www.llchen60.com/categories/Web/"/>
    
    
      <category term="AWS" scheme="https://www.llchen60.com/tags/AWS/"/>
    
      <category term="EC2" scheme="https://www.llchen60.com/tags/EC2/"/>
    
  </entry>
  
  <entry>
    <title>Redis基础</title>
    <link href="https://www.llchen60.com/Redis%E5%9F%BA%E7%A1%80/"/>
    <id>https://www.llchen60.com/Redis%E5%9F%BA%E7%A1%80/</id>
    <published>2020-12-20T19:39:32.000Z</published>
    <updated>2021-01-02T23:07:46.143Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis基础"><a href="#Redis基础" class="headerlink" title="Redis基础"></a>Redis基础</h1><h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li><p>为什么需要Redis</p><ul><li>key value内存数据库</li><li>支持丰富的数据结构，</li><li>性能非常高，可以支持很高的TPS</li></ul></li><li><p>在使用Redis过程中可能遇到的一些问题</p><ul><li>CPU使用方面的问题<ul><li>数据结构的复杂度</li><li>跨CPU核的访问</li></ul></li><li>内存使用方面<ul><li>主从同步和AOF的内存竞争</li></ul></li><li>存储持久化方面<ul><li>SSD上做快照的性能抖动</li></ul></li><li>网络通信方面<ul><li>多实例时的异常网络丢包</li></ul></li></ul></li><li><p>如何进行学习 — 需要系统化</p><ul><li><p>从应用维度和系统维度进行研究</p></li><li><p>分别看其在以下三个方面的表现</p><ul><li><p>高性能</p><ul><li>线程模型</li><li>数据结构</li><li>持久化</li><li>网络框架</li></ul></li><li><p>高可靠</p><ul><li>主从复制</li><li>哨兵机制</li></ul></li><li><p>高可扩展性</p><ul><li><p>数据分片</p></li><li><p>负载均衡</p><p>  <img src="https://i.loli.net/2020/12/20/vKdkl934yUw7pOe.png" alt="Redis框架"></p></li></ul></li></ul></li></ul></li></ul><h1 id="2-Redis数据结构"><a href="#2-Redis数据结构" class="headerlink" title="2. Redis数据结构"></a>2. Redis数据结构</h1><h2 id="2-1-如何构建一个键值数据库"><a href="#2-1-如何构建一个键值数据库" class="headerlink" title="2.1 如何构建一个键值数据库"></a>2.1 如何构建一个键值数据库</h2><ul><li>目标<ul><li>创建一个叫做SimpleKV的数据库</li></ul></li><li>几个需要思考的问题<ul><li>问题<ul><li>里面会存什么样的数据 (数据模型)</li><li>需要对数据做什么样的操作 (操作接口)</li></ul></li><li>为什么需要思考这种问题？<ul><li>这影响到你认为这个数据库到底能做什么</li><li>譬如如果支持集合，那么对于存储用户信息的一个关系型数据库，我们也可以将用户Id作为Key，剩余信息作为一个集合存储到我们的键值数据库当中</li><li>接口的定义确定了我们希望使用这个数据库做什么，是简单的get, put操作，还是说相对复杂的聚合型的操作</li></ul></li></ul></li></ul><hr><h3 id="2-1-1-可以存哪些数据？"><a href="#2-1-1-可以存哪些数据？" class="headerlink" title="2.1.1 可以存哪些数据？"></a>2.1.1 可以存哪些数据？</h3><ul><li>基本数据类型 Key - Value</li><li>希望Value能够支持复杂类型<ul><li>memcache只支持String</li><li>Redis支持String, HashMap, 列表，集合等<ul><li>值得注意的点是不同的数据结构在实际使用的时候会有在性能，空间效率等方面的差异，从而导致不同的value操作之间也会存在差异</li></ul></li></ul></li></ul><h3 id="2-1-2-可以对数据做什么操作？"><a href="#2-1-2-可以对数据做什么操作？" class="headerlink" title="2.1.2 可以对数据做什么操作？"></a>2.1.2 可以对数据做什么操作？</h3><ul><li>PUT/ SET<ul><li>新写入或者更新一个KV对</li></ul></li><li>GET<ul><li>根据KEY读取相应的VALUE值</li></ul></li><li>DELETE<ul><li>根据KEY删除整个KV对</li></ul></li><li>SCAN<ul><li>根据一段Key的范围返回相应的value值</li></ul></li><li>Tips<ul><li>当一个键值数据库的value类型多样的时候，也需要包含相应的操作接口的</li></ul></li></ul><h3 id="2-1-3-数据库存储位置"><a href="#2-1-3-数据库存储位置" class="headerlink" title="2.1.3 数据库存储位置"></a>2.1.3 数据库存储位置</h3><ul><li>可选方案<ul><li>内存<ul><li>读写非常快</li><li>访问速度在百ns级别</li><li>潜在风险是一旦断电，所有的数据都会丢失</li></ul></li><li>外存<ul><li>可以避免数据的丢失，但是受限于磁盘的慢速读写（几个ms）键值数据库的整体性能会被拉低</li></ul></li></ul></li><li>考量的因素<ul><li>主要应用场景<ul><li>缓存场景<ul><li>需要能够快速访问但允许丢失 — 可以采用内存保存键值数据</li><li>memcache 和Redis都属于内存键值数据库</li></ul></li></ul></li></ul></li></ul><h3 id="2-1-4-数据库基本组件"><a href="#2-1-4-数据库基本组件" class="headerlink" title="2.1.4 数据库基本组件"></a>2.1.4 数据库基本组件</h3><ul><li>一个基本的内部结构需要包括<ul><li>访问框架<ul><li>动态库访问</li><li>网络访问框架</li></ul></li><li>操作模块<ul><li>上述的一系列操作 DELETE/PUT/SCAN etc</li></ul></li><li>索引模块</li><li>存储模块</li></ul></li></ul><p><img src="https://i.loli.net/2020/12/20/ILR4uFc73ZVmevP.png" alt="SimpleKV 内部架构"></p><ul><li><p>采用什么访问模式？</p><ul><li>通过函数库调用的方式供外部应用使用<ul><li>比如图片当中的<code>libsimplekv.so</code> 就是通过动态链接库的形式链接到我们的程序当中，来提供键值存储功能</li></ul></li><li>通过网络框架以Socket通信的形式对外提供键值对操作<ul><li>系统设计上的问题 — 单线程，多线程还是多个进程来进行交互？<strong>IO模型的选择</strong><ul><li>网络连接的处理</li><li>网络请求的解析</li><li>数据存取的处理</li></ul></li></ul></li></ul></li><li><p>如何定位键值对的位置？</p><ul><li>需要依赖于键值数据库的索引模块<ul><li>让键值数据库能够根据key找到相应value的存储位置，进而执行操作<ul><li>索引类型<ul><li>哈希表</li><li>B+树</li><li>字典树</li></ul></li><li>Redis选用的是哈希表，是因为保存在内存中，内存的高性能随机访问特性可以很好地与哈希表O(1)的操作复杂度匹配<ul><li>关于Redis值得注意的是它的value支持多种类型，当我们通过索引找到一个key对应的value后，仍然需要从value的复杂结构中进一步找到我们实际需要的数据</li><li>Redis采用一些高效索引结构作为某些value类型的底层数据结构，可以为Redis实现高性能访问提供良好的支撑</li></ul></li></ul></li></ul></li></ul></li><li><p>不同操作的具体逻辑是？</p><ul><li>对于GET/SCAN 操作而言，根据value的存储位置返回value的值即可</li><li>对于PUT操作，需要为新的键值对分配内存空间</li><li>对于DELETE操作，需要删除键值对，并释放相应的内存空间，这个过程由分配器完成</li></ul></li><li><p>如何实现重启后快速提供服务？</p><ul><li>采用内存分配器glibc的malloc和free<ul><li>但是键值对因为通常大小不一，glibc分配器在处理随机大小的内存块分配时表现会不太好。一旦保存的键值对数据规模过大，就可能造成较为严重的内存碎片的问题</li></ul></li><li>持久化功能<ul><li>采用文件形式，将键值数据通过调用本地文件系统的操作接口保存在磁盘上<ul><li>需要考虑什么时候，什么间隔来做从内存到文件的键值数据的保存工作</li></ul></li><li>也可以每一个键值对都进行持久化<ul><li>坏处是因为每次都要写到磁盘里面，性能会受到很大影响</li></ul></li><li>可以周期性的将内存中的键值数据保存到文件当中，这样就可以避免频繁写盘操作的性能影响<ul><li>潜在的风险就是数据仍然有可能丢失</li></ul></li></ul></li></ul></li></ul><p><img src="https://i.loli.net/2020/12/20/52Hgk6apwXFtzSU.png" alt="SimpleKV vs Redis"></p><ul><li>SimpleKV和Redis的对比<ul><li>Redis通过网络访问，可以作为一个基础性的网络服务来进行访问</li><li>value类型丰富，就带来了更多的操作接口<ul><li>面向列表的LPush/ LPop</li><li>面向集合的SADD</li></ul></li><li>Redis持久化模块支持日志(AOF)和快照(RDB)两种模式</li><li>Redis支持高可靠集群和高可扩展集群</li></ul></li></ul><h2 id="2-2-Redis的慢操作们"><a href="#2-2-Redis的慢操作们" class="headerlink" title="2.2 Redis的慢操作们"></a>2.2 Redis的慢操作们</h2><ul><li>Redis的快<ul><li>Redis在接收到一个键值对操作后，能够以微秒级别的速度找到数据，并且快速完成操作</li><li>快速的原因<ul><li>内存数据库<ul><li>所有操作都在内存上完成</li><li>内存本身访问速度非常快</li></ul></li><li>数据结构<ul><li>键值对是按照一定的数据结构来组织的</li><li>这是Redis实现高速的基础</li></ul></li></ul></li></ul></li></ul><h3 id="2-2-1-Redis数据类型和底层数据结构"><a href="#2-2-1-Redis数据类型和底层数据结构" class="headerlink" title="2.2.1 Redis数据类型和底层数据结构"></a>2.2.1 Redis数据类型和底层数据结构</h3><ul><li>Redis 键值对中值的数据类型以及对应的底层实现<ul><li>String<ul><li>简单动态字符串</li></ul></li><li>List<ul><li>双向链表</li><li>压缩列表</li></ul></li><li>Hash<ul><li>压缩列表</li><li>哈希表</li></ul></li><li>Set<ul><li>整数数组</li><li>压缩列表</li></ul></li><li>Sorted Set<ul><li>压缩列表</li><li>跳表</li></ul></li></ul></li></ul><h3 id="2-2-2-键和值用什么结构来组织？"><a href="#2-2-2-键和值用什么结构来组织？" class="headerlink" title="2.2.2 键和值用什么结构来组织？"></a>2.2.2 键和值用什么结构来组织？</h3><ul><li>使用一个哈希表来保存所有的键值对</li><li>一个哈希表即为一个数组</li><li>数组的每个元素称为一个哈希桶</li><li>每个哈希桶中保存了键值对的数据</li><li>哈希桶中的元素保存的并不是值本身，而是指向具体值的指针</li><li>哈希表的好处是我们可以用O(1)的时间复杂度来快速查找键值对<ul><li>只需要计算键的哈希值，就可以知道它所对应的哈希桶的位置，然后就可以访问相应的entry原色</li><li>而且查找过程主要依赖于哈希计算，和数据量的多少并没有直接关系</li></ul></li></ul><h3 id="2-2-3-哈希表操作会变慢？"><a href="#2-2-3-哈希表操作会变慢？" class="headerlink" title="2.2.3 哈希表操作会变慢？"></a>2.2.3 哈希表操作会变慢？</h3><ul><li><p>因为哈希表的冲突问题和rehash可能带来的操作阻塞</p></li><li><p>哈希冲突在写入了大量数据之后，是不可避免的</p><ul><li>两个key的哈希值和哈希桶计算对应关系的时候，正好落到了同一个哈希桶当中</li></ul></li><li><p>Redis的解决方案</p><ul><li><p>链式哈希</p><ul><li><p>同一个哈希桶的多个元素用一个链表来保存，它们之间依次使用指针连接</p><p>  <img src="https://i.loli.net/2020/12/20/vWzTF9AUehcsyiQ.png" alt="哈希冲突的解决"></p></li><li><p>存在的问题</p><ul><li>哈希冲突链上的元素只能通过指针逐一查找再操作</li><li>如果冲突链太长，会导致这个链上的元素查找耗时变长，效率降低</li></ul></li><li><p>解决方案</p><ul><li>对哈希表做rehash操作<ul><li>增加现有的哈希桶数量</li><li>让逐渐增多的entry元素能在更多的桶之间分散保存</li><li>减少单个桶的元素数量</li><li>从而减少单个桶中的冲突</li></ul></li></ul></li><li><p>解决方案的具体实施</p><ul><li><p>Redis默认使用两个全局哈希表(原理)</p><ul><li>刚插入数据的时候，默认使用哈希表1</li><li>随着数据增多，触发并开始执行rehash<ul><li>给哈希表2分配更大的空间，一般为当前哈希表1的两倍大小</li><li>将哈希表1中的数据重新映射并拷贝到哈希表2当中</li><li>释放哈希表1的空间</li></ul></li></ul></li><li><p>渐进式rehash</p><ul><li><p>原因</p><ul><li>上述的表之间数据的重新映射会涉及到大量的数据拷贝</li><li>一次性将哈希表1中的数据都迁移完，会造成Redis线程阻塞，无法服务其他请求</li></ul></li><li><p>具体方式 — 渐进式rehash</p><ul><li><p>在进行拷贝数据的时候，仍然正常处理客户端请求</p></li><li><p>每处理一个请求，就从哈希表1中的第一个索引位置开始，将这个索引位置上的所有entries拷贝到哈希表2当中</p></li><li><p>同理，处理下一个请求的时候，再顺带着拷贝哈希表1中的下一个索引位置的entries</p></li><li><p>渐进式哈希</p><p><img src="https://i.loli.net/2020/12/20/UYCiuyTthdRDOVB.png" alt="渐进式哈希"></p></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><h3 id="2-2-4-集合数据的操作效率"><a href="#2-2-4-集合数据的操作效率" class="headerlink" title="2.2.4 集合数据的操作效率"></a>2.2.4 集合数据的操作效率</h3><ul><li><p>和String类型不同，一个集合类型的值，第一步是通过全局哈希表找到对应的哈希桶位置，第二步是在集合当中再做增删改查</p></li><li><p>底层数据结构的分析</p><ul><li><p>哈希表</p></li><li><p>整数数组</p><ul><li>插入删除效率比较低 O(N)</li><li>访问效率高 O(1)</li></ul></li><li><p>双向链表</p><ul><li>插入删除O(1)</li><li>访问O(N)</li></ul></li><li><p>压缩列表</p><ul><li>类似一个数组<ul><li>每一个元素保存一个数据</li><li>表头有三个字段<ul><li>zlbytes<ul><li>列表长度</li></ul></li><li>zltail<ul><li>列表尾的偏移量</li></ul></li><li>zllen<ul><li>列表中entry的个数</li></ul></li></ul></li><li>表尾<ul><li>zlend<ul><li>表示列表的结束</li></ul></li></ul></li></ul></li></ul></li><li><p>跳表</p><ul><li><p>在链表上加多级索引来加快查询速度</p><p>  <img src="https://i.loli.net/2020/12/20/gziY58rpVRQUZnw.png" alt="跳表"></p></li><li></li></ul></li></ul></li></ul><h1 id="3-Redis的IO模型"><a href="#3-Redis的IO模型" class="headerlink" title="3. Redis的IO模型"></a>3. Redis的IO模型</h1><p><strong>为什么单线程的Redis那么快？</strong></p><p>这里的单线程主要指Redis的网络IO和键值对读写是由一个线程来完成的，而Redis的其他功能，比如持久化，异步删除，集群数据同步等，是由额外的线程执行的。</p><h2 id="3-1-为什么要使用单线程？"><a href="#3-1-为什么要使用单线程？" class="headerlink" title="3.1 为什么要使用单线程？"></a>3.1 为什么要使用单线程？</h2><ul><li><p>使用多线程的开销</p><ul><li><p>使用多线程，一定程度上可以增加系统的吞吐率/ 拓展性</p></li><li><p>但是值得注意的是多线程本身有开销，并不是线程增多吞吐率会线性增长的。达到了某个线程数之后，系统吞吐率的增长就会开始迟缓了，有时甚至会出现下降的情况</p><p><img src="https://i.loli.net/2020/12/24/UNVLxyIRokWmjTQ.png" alt="吞吐率随着线程数增长的变化"></p></li><li><p>出现这种情况的原因在于</p><ul><li>系统中通常会存在被多线程同时访问的共享资源 — 比如一个共享的数据结构</li><li>当有多个线程要修改这个共享资源的时候，为了保证共享资源的正确性，就需要有额外的机制进行保证。这会带来额外的开销</li></ul></li></ul></li><li><p>Redis采用多线程就是希望能够避免这种共享资源，放锁的情况</p><ul><li>而且CPU往往不是Redis的瓶颈，瓶颈很可能是机器内存或者网络带宽</li></ul></li></ul><h2 id="3-2-单线程Redis快的原因"><a href="#3-2-单线程Redis快的原因" class="headerlink" title="3.2 单线程Redis快的原因"></a>3.2 单线程Redis快的原因</h2><h3 id="3-2-1-基本IO模型和阻塞点"><a href="#3-2-1-基本IO模型和阻塞点" class="headerlink" title="3.2.1 基本IO模型和阻塞点"></a>3.2.1 基本IO模型和阻塞点</h3><p>以前面的SimpleKV为例，为了处理一个Get请求，数据库需要：</p><ol><li>监听客户端请求(bind/ listen) </li><li>和客户端建立连接 (accept)</li><li>从socket中读取请求(recv)</li><li>解析客户端发送请求(parse)</li><li>根据请求类型读取键值数据(get)</li><li>从客户端返回结果，即向socket中写回数据(send)</li></ol><p><img src="https://i.loli.net/2020/12/24/1pFsDaMO452fGkQ.png" alt="Get请求处理示意图"></p><p>在上述的整个过程当中，如果Redis监听到客户端请求，但没有成功建立连接的时候，会阻塞在accept函数上，导致其他的客户端无法建立连接。这种基本IO模型效率会非常低，因为是阻塞式的，任何一个请求出现了任何一个问题，都会导致其他的请求无法成功完成。</p><h3 id="3-2-2-非阻塞模式"><a href="#3-2-2-非阻塞模式" class="headerlink" title="3.2.2 非阻塞模式"></a>3.2.2 非阻塞模式</h3><p>Socket网络模型的非阻塞模式体现在不同操作调用后会返回不同的套接字类型。socket() 方法会返回主动套接字，然后调用 listen() 方法，将主动套接字转化为监听套接字，此时，可以监听来自客户端的连接请求。最后，调用 accept() 方法接收到达的客户端连接，并返回已连接套接字。</p><p>这样子可以实现非阻塞，值得注意的是我们需要一些机制来监听套接字，有数据到达的时候再通知数据库线程</p><h3 id="3-2-3-基于多路复用的高性能I-O模型"><a href="#3-2-3-基于多路复用的高性能I-O模型" class="headerlink" title="3.2.3 基于多路复用的高性能I/O模型"></a>3.2.3 基于多路复用的高性能I/O模型</h3><ul><li><p>为什么使用I/O多路复用这种技术</p><ul><li>解决单线程下阻塞操作的问题</li></ul></li><li><p>如何实现的</p><ul><li><p>select epoll方法同时监控多个文件描述符FD的读写情况，当某些FD可读/ 可写的时候，该方法就会返回可读/ 写的FD个数</p><ul><li><p>将用户Socket对应的FD注册进epoll，然后epoll告诉那些需要进行读写操作的socket，只处理那些活跃的，有变化的socket FD</p><p><a href="https://cloud.tencent.com/developer/article/1639569" target="_blank" rel="noopener">IO多路复用：Redis中经典的Reactor设计模式</a></p></li></ul></li></ul></li></ul><p><a href="https://draveness.me/redis-io-multiplexing/" target="_blank" rel="noopener">https://draveness.me/redis-io-multiplexing/</a>  </p><p><a href="https://cloud.tencent.com/developer/article/1639569" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1639569</a></p><p><a href="https://blog.csdn.net/u014590757/article/details/79860766" target="_blank" rel="noopener">https://blog.csdn.net/u014590757/article/details/79860766</a></p><ul><li><p>一个线程处理多个IO流 — select / epoll机制</p><p>  <img src="https://i.loli.net/2020/12/24/QUKfj9ExTgy4tMN.png" alt="epoll机制"></p><ul><li><p>允许内核中，同时存在多个监听套接字和已连接套接字</p></li><li><p>内核会一直监听这些套接字上的连接请求或数据请求</p></li><li><p>一旦有请求到达，就会交给Redis线程处理</p><p><img src="https://i.loli.net/2020/12/24/4Cp7TQMZ3csAUIm.png" alt="多路复用全程"></p></li></ul></li><li><p>select/ epoll 一旦检测到FD上有请求到达，就会触发相应的事件</p><ul><li>事件会被放到一个事件队列，Redis单线程对该事件队列不断进行处理</li></ul></li></ul><h2 id="3-3-单线程处理的性能瓶颈"><a href="#3-3-单线程处理的性能瓶颈" class="headerlink" title="3.3 单线程处理的性能瓶颈"></a>3.3 单线程处理的性能瓶颈</h2><p>1、任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。耗时的操作包括以下几种：</p><p>a、操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时；</p><p>b、使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据；</p><p>c、大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长；</p><p>d、淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长；</p><p>e、AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能；</p><p>f、主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久；</p><p>2、并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。</p><p>针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。</p><p>针对问题2，Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.usenix.org/conference/atc17/technical-sessions/presentation/xia" target="_blank" rel="noopener">HiKV: A Hybrid Index Key-Value Store for DRAM-NVM Memory Systems</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis基础&quot;&gt;&lt;a href=&quot;#Redis基础&quot; class=&quot;headerlink&quot; title=&quot;Redis基础&quot;&gt;&lt;/a&gt;Redis基础&lt;/h1&gt;&lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="数据存储" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>API Rate Limiter Design</title>
    <link href="https://www.llchen60.com/API-Rate-Limiter-Design/"/>
    <id>https://www.llchen60.com/API-Rate-Limiter-Design/</id>
    <published>2020-12-07T21:35:04.000Z</published>
    <updated>2020-12-07T21:35:30.246Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li><p>What is a rate limiter? </p><ul><li>due to limited resources, also get rid of some abusive action, we need some kind of throttling or rate limiting mechanism thus only a certain number of requests will go to our service, and we are able to respond all of them </li><li>a rate limiter limits the number of events an entity can perform in a particular time window, then block requests once the cap is reached </li></ul></li><li><p>why we need to do rate limiting? </p><ul><li><p>Protect services against abusive behaviors targeting the application layer like </p><ul><li>DOS attacks </li><li>Brute Force credit card transactions </li></ul></li><li><p>why we need such protection?</p><ul><li>these attacks are a barrage of HTTP/S requests which may look like they are coming from real users, but are actually generated by machines </li><li>thus such attachs are harder to detect and can more easily bring down a service, application or an API </li></ul></li><li><p>could make a service and APIs more reliable</p><ul><li><p>misbehaving clients/ scripts </p></li><li><p>security </p><ul><li>second factor attempts </li></ul></li><li><p>prevent abusive behavior and bad design practices </p></li><li><p>keep costs and resource usage under control </p></li><li><p>revenue </p><ul><li>revenue model based on rate limiting </li></ul></li><li><p>eliminate spikiness in traffic </p></li></ul></li></ul></li></ul><h1 id="2-Requirement-and-Goal"><a href="#2-Requirement-and-Goal" class="headerlink" title="2. Requirement and Goal"></a>2. Requirement and Goal</h1><ul><li>functional <ul><li>limit the number of requests an entity can send to an API within a time window </li><li>APIs are accessible through a cluster, so the rate limit should be considered across different servers <ul><li>users should get an error message whenever the defined threshold is crossed within a single server or across a combination of servers </li></ul></li></ul></li></ul><h1 id="3-Thoughts"><a href="#3-Thoughts" class="headerlink" title="3. Thoughts"></a>3. Thoughts</h1><ul><li><p>table to store the request information, </p><ul><li><p>every entry will look like </p><ul><li>userId</li><li>api name </li><li>accessTime</li><li>api parameters</li></ul></li><li><p>and then we could query and sort by the accesstime to get related info </p></li></ul></li><li><p>there should have a caching layer to store the related info, most important one is for a specific user and specific api, based on the throttle limit(suppose n), what’s the time when they made the recent nth request, thus we could take a notes on this info </p></li></ul><h1 id="4-Design"><a href="#4-Design" class="headerlink" title="4. Design"></a>4. Design</h1><h2 id="4-1-High-Level"><a href="#4-1-High-Level" class="headerlink" title="4.1 High Level"></a>4.1 High Level</h2><ul><li>Clients make call to our web server </li><li>when request come, first sync with rate limiter server to decide if it will be served or throttled</li><li>web server then sync with API servers if rate limiter says it should not be blocked </li></ul><h2 id="4-2-Basic-System-Design-and-Algo"><a href="#4-2-Basic-System-Design-and-Algo" class="headerlink" title="4.2 Basic System Design and Algo"></a>4.2 Basic System Design and Algo</h2><ul><li><p>target </p><ul><li>limit the number of requests per user <ul><li>keep a count representing how many requestss the user has made </li><li>a timestamp when we started counting the requests </li></ul></li></ul></li><li><p>use a hashtable to store the info</p><ul><li>key - userId</li><li>value - count + startTime <ul><li>count would be enough if we don’t need detail about the metrics or we say there are some other stuff controlling it </li><li>so based on the count and requirement, we could either increase the count, reset the count, or reset the start time. </li></ul></li><li>one issue for only store the starttime is it’s possible to allow twice the configured number during a period. End of the previous window, with full capacity; and start at the next window, with full capacity. </li></ul></li></ul><ul><li>then we need to store timestamp of each request thus we could keep a sliding window <ul><li>use redis sorted set </li><li>steps when new request comes in<ul><li>remove all timestamps from the sorted set that are older than currentTime - 1 min (suppose that’s the configured window)</li><li>count the total number of elements in the sorted set </li><li>reject the request if count is greater than throttling limit </li><li>insert the current time in the sorted set and accept the reuqest </li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What is a rate limiter? &lt;/p
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
      <category term="rate limiter" scheme="https://www.llchen60.com/tags/rate-limiter/"/>
    
  </entry>
  
  <entry>
    <title>Dropbox Design Scratch</title>
    <link href="https://www.llchen60.com/Dropbox-Design-Scratch/"/>
    <id>https://www.llchen60.com/Dropbox-Design-Scratch/</id>
    <published>2020-12-01T02:54:17.000Z</published>
    <updated>2020-12-01T02:54:37.861Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Intro-and-requirement"><a href="#1-Intro-and-requirement" class="headerlink" title="1. Intro and requirement"></a>1. Intro and requirement</h1><ul><li><p>cloud file storage services </p><ul><li>simplify the storage and exchange of digital resources among multiple devices </li><li>benefits <ul><li>availability </li><li>reliability and durability </li><li>scalability </li></ul></li></ul></li><li><p>requirements and goals </p><ul><li>users should be able to upload and download their files/ photos from any device </li><li>users should be able to share files or folders with other users </li><li>service should support automatic synchronization between devices </li><li>support storing large files </li><li>ACID support </li><li>Offline editing, CURD offline, and get synced when online </li><li>snapshotting of the data </li></ul></li></ul><h1 id="2-Thoughts"><a href="#2-Thoughts" class="headerlink" title="2. Thoughts"></a>2. Thoughts</h1><ul><li><p>size, limit , pattern </p><ul><li>read heavy </li><li>write huge file </li></ul></li><li><p>synchronize among different devices</p><ul><li>all devices make call to backend to grab newest status </li></ul></li><li><p>file/ folder share </p><ul><li>permission management </li></ul></li><li><p>Offline editing </p><ul><li>Use queue to store the data locally? </li></ul></li></ul><h1 id="3-Design"><a href="#3-Design" class="headerlink" title="3. Design"></a>3. Design</h1><h2 id="3-1-considerations"><a href="#3-1-considerations" class="headerlink" title="3.1  considerations"></a>3.1  considerations</h2><pre><code>+ huge read and write volumes    + read write almost equal + files can be stored in &lt;u&gt;**small parts or chunks (4MB)**&lt;/u&gt;, when fails, only the failed chunk should be retried + reduce the amount of data exchange by transfering updated chunks only + keep a local copy of metadata (file name, size, etc.) with the client can save us a lot of round trips to the server </code></pre><ul><li>high level design <ul><li>need to store files and metadata information like File name, file size, directory, who this file is shared with </li><li>need some servers that can help the clients to upload/ download files to cloud storage and some servers that can facilitate updating metadata about files and users</li><li>need some mechanism to notify all clients whenever an updates happens so they can synchronize their files </li></ul></li></ul><h2 id="3-2-high-level-design"><a href="#3-2-high-level-design" class="headerlink" title="3.2 high level design"></a>3.2 high level design</h2><pre><code>+ user specify folder as the workspace on the device     + file in the folder will be uploaded to the cloud     + updater/ delete will be reflected in the same way + modification on one device should be freely synced across others + systems we need based on requirements     + a storage server to store all real files     + a metadata service store file metadata, thus we could quickly get info about it     + a synchronizer service, to notify all clients whenever an update happens so they can synchronize files </code></pre><h2 id="3-3-Component-Design"><a href="#3-3-Component-Design" class="headerlink" title="3.3 Component Design"></a>3.3 Component Design</h2><h3 id="3-3-1-Client"><a href="#3-3-1-Client" class="headerlink" title="3.3.1 Client"></a>3.3.1 Client</h3><ul><li><p>responsibility </p><ul><li>need to monitor the workspace folder on user’s machine </li><li>sync files/ folders with remote cloud storage </li><li>interact with the remote synchronization service to handle file metadata updates </li></ul></li><li><p>how to handle file transfer efficiently</p><ul><li><p>break each file into smaller chunks so that we transfer only those chunks that are modified and not the whole file </p></li><li><p>calculate chunk size based on:</p><ul><li>storage devices we use in the cloud </li><li>network bandwidth </li><li>average file size in the storage </li></ul></li><li><p>need to keep a record of each file and the chunks in metadata </p></li></ul></li><li><p>shall we keep a copy of metadata with client? </p><ul><li>yes, it then allows us to do offline updates </li></ul></li><li><p>how can clients efficiently listen to changes happening with other clients? </p><ul><li><p>Solution 1</p><ul><li>clients periodically check with the server if there are any changes </li><li>issues <ul><li>have a delay in reflecting changes locally as clients will be checking for changes periodically compared to server notifying whever there is some change </li></ul></li></ul></li><li><p>Solution 2</p><ul><li>HTTP long polling </li><li>client requests information from the server with the expectation that the server may not respond immediately </li><li>servers hold the request open and waits for response information to become available </li></ul></li></ul></li><li><p>components </p><ul><li><p>inernal metadata db </p><ul><li>keep track of all the files, chunks, versions and location </li></ul></li><li><p>chunker </p><ul><li>split files into smaller pieces </li><li>reconstruct a file from its chunks </li></ul></li><li><p>watcher </p><ul><li>monitor the local workspace folders and notify the indexer of any action performed by the users </li></ul></li><li><p>indexer </p><ul><li>process the events received from the watcher and update the internal metadata database with information about the chunks of the modified files </li><li>once confirm chunks are successfully uploaded to the cloud storage, the indexer will communicate with the remote synchronization service to broadcast changes to other clients and update remote metadata database </li></ul></li></ul></li><li><p>for phone users, probably should sync on demand to save bandwidth and capacity </p></li></ul><h3 id="3-3-2-Metadata-Database"><a href="#3-3-2-Metadata-Database" class="headerlink" title="3.3.2 Metadata Database"></a>3.3.2 Metadata Database</h3><ul><li>responsible for maintaing the versioning and metadata information about files/ chunks, users and workspaces </li><li>store info like <ul><li>chunks </li><li>files </li><li>users</li><li>devices</li><li>workspace </li></ul></li></ul><h3 id="3-3-3-Synchronization-Service"><a href="#3-3-3-Synchronization-Service" class="headerlink" title="3.3.3 Synchronization Service"></a>3.3.3 Synchronization Service</h3><ul><li><p>component that processes file updates made by a client and apply these changes to other subscribed clients </p></li><li><p>also synchronizes clients local databases with the information stored in the remote metadata db </p></li><li><p>Implement a differenciation algo to reduce the amount of the data that needs to be synchronized </p><ul><li>just transmit the difference between two versions instead of the whole file </li></ul></li><li><p>to support a scalable synchronization protocol</p><ul><li>use a communication middleware </li><li>messaging system <ul><li>push or pull strategies </li></ul></li></ul></li></ul><h3 id="3-3-4-Message-Queuing-Service"><a href="#3-3-4-Message-Queuing-Service" class="headerlink" title="3.3.4 Message Queuing Service"></a>3.3.4 Message Queuing Service</h3><ul><li>Message queue service <ul><li>supports asynchronous message based communication between clients and the synchronization service </li></ul></li></ul><h3 id="3-3-5-Cloud-Storage"><a href="#3-3-5-Cloud-Storage" class="headerlink" title="3.3.5 Cloud Storage"></a>3.3.5 Cloud Storage</h3><p>Cloud/Block Storage stores chunks of files uploaded by the users. Clients directly interact with the storage to send and receive objects from it. Separation of the metadata from storage enables us to use any storage either in the cloud or in-house.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.educative.io/courses/grokking-the-system-design-interview/m22Gymjp4mG" target="_blank" rel="noopener">Grokking the system design</a></li><li><a href="https://www.youtube.com/watch?v=U0xTu6E2CT8&t=9s&ab_channel=TechDummiesNarendraL" target="_blank" rel="noopener">Tech Dummies</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Intro-and-requirement&quot;&gt;&lt;a href=&quot;#1-Intro-and-requirement&quot; class=&quot;headerlink&quot; title=&quot;1. Intro and requirement&quot;&gt;&lt;/a&gt;1. Intro and req
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>LC-Dynamic Programming</title>
    <link href="https://www.llchen60.com/LC-Dynamic-Programming/"/>
    <id>https://www.llchen60.com/LC-Dynamic-Programming/</id>
    <published>2020-11-25T05:53:50.000Z</published>
    <updated>2020-12-17T05:46:42.917Z</updated>
    
    <content type="html"><![CDATA[<h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><h1 id="1-动态规划问题的一些思路"><a href="#1-动态规划问题的一些思路" class="headerlink" title="1. 动态规划问题的一些思路"></a>1. 动态规划问题的一些思路</h1><h2 id="1-1-原理"><a href="#1-1-原理" class="headerlink" title="1.1 原理"></a>1.1 原理</h2><ul><li>一般形式 –&gt; 求最值<ul><li>核心思路： 穷举</li><li>特殊性： 因为其存在重叠子问题<ul><li>暴力穷举的时间复杂度就会非常高</li><li>我们就需要备忘录/ DP table来优化穷举的过程，避免不必要的计算</li><li>一般会具备最优子结构，才能通过子问题的最值得到原问题的最值</li><li>关于如何穷举所有的可能解，就需要分析得出状态转移方程，以上也就是动态规划的三要素</li></ul></li></ul></li><li>三要素<ul><li>重叠子问题<ul><li>子问题之间需要互相独立</li><li>重叠子问题 需要做某种记录，使用dp table 或者备忘录<ul><li>可以尝试通过状态压缩，来缩小DP table的大小，只记录必要的数据</li></ul></li></ul></li><li>最优子结构</li><li>状态转移方程<ul><li>就是n 和 n-1, n-2之间的关系</li><li>如何思考<ul><li>明确base case</li><li>明确状态</li><li>明确选择</li><li>定义dp数组/ 函数的含义</li></ul></li></ul></li></ul></li><li>和递归的对比<ul><li>递归往往是自上而下，分解出子问题，再利用栈的逻辑，反向拿到结果</li><li>而动态规划是自下而上的，直接定义出了子问题，然后我们来定义状态是如何转移的</li></ul></li></ul><h2 id="1-2-基本方法论"><a href="#1-2-基本方法论" class="headerlink" title="1.2 基本方法论"></a>1.2 基本方法论</h2><ul><li>逻辑是需要找到一些规律来指导我们解决动态规划问题<ul><li>寻找子问题</li><li>递归求解</li><li>重叠子问题</li><li>无后效性</li><li>状态存储</li></ul></li></ul><h2 id="1-3-递归"><a href="#1-3-递归" class="headerlink" title="1.3 递归"></a>1.3 递归</h2><ul><li>使用贪心算法是有可能失效导致无法生成全局最优解的</li><li>最优化问题本身指的是一种求极值的方法，即在一组约束为等式或不等式的条件下，使系统的目标函数达到极值，即最大值或最小值的过程</li><li>我们需要做到的是在满足条件的组合里找出最优解的组合<ul><li>枚举<ul><li>求出所有满足条件的组合，然后看看这些组合能否得到最大值或者最小值</li><li>关键问题 – 如何得到满足条件的所有组合<ul><li>使用递归来获得满足约束条件的所有组合</li><li>–》 递归是一种枚举的手法，满足限定条件下，做递归</li><li>在递归的过程中，每一步都对需要求解的问题来进行判断</li></ul></li><li>使用递归的好处<ul><li>是在使用堆栈的时候实际上自然的就保存了每一层的局部变量们</li><li>递归的形式也给了回溯的能力，通过堆栈保存了上一步的状态的</li></ul></li></ul></li></ul></li><li>直接使用递归的问题<ul><li>实际上是穷举了所有的组合，导致效率很低</li><li>构成的递归树会随着数据规模的增长而指数级别的增长</li><li>直接使用递归调试相对比较困难，因为需要思考每一层的表现</li></ul></li><li>递归的优化<ul><li>剪枝<ul><li>减少搜索的分支数量</li><li>重叠子问题，可以通过提前存储，来减少重复的运算</li></ul></li></ul></li></ul><h2 id="1-4-备忘录"><a href="#1-4-备忘录" class="headerlink" title="1.4 备忘录"></a>1.4 备忘录</h2><ul><li>前面说过我们可以通过枚举来获得满足条件的所有组合，然后再来求需要的极值，但是在这中间是很有可能有重叠子问题的</li><li>可用的前提条件<ul><li>无后效性<ul><li>即在通过 A 阶段的子问题推导 B 阶段的子问题的时候，我们不需要回过头去再根据 B 阶段的子问题重新推导 A 阶段的子问题</li><li>即子问题之间的依赖是单向性的</li></ul></li></ul></li><li>以斐波那契数列为例<ul><li>我们在疯狂的重复计算子函数，大部分的递归树都是重复的</li><li>举个例子<ul><li>f(9) = f(8) + f(7)</li><li>在计算f(8)的时候，又需要计算一遍f(7)</li><li>如此反复</li></ul></li></ul></li><li>因此我们需要使用备忘录来解决重复计算的问题<ul><li>在每次计算出一个子问题的答案以后，将这个临时的中间结果记录到备忘录当中，然后再返回</li></ul></li><li>常见的可用数据结构<ul><li>数组</li><li>哈希表</li></ul></li></ul><h2 id="1-5-动归的应用场景"><a href="#1-5-动归的应用场景" class="headerlink" title="1.5 动归的应用场景"></a>1.5 动归的应用场景</h2><h3 id="1-5-1-0-1背包问题"><a href="#1-5-1-0-1背包问题" class="headerlink" title="1.5.1 0-1背包问题"></a>1.5.1 0-1背包问题</h3><ul><li><p>问题描述</p><ul><li>总重量为W的背包和N个物品，对每个物品，有重量w和价值v两个属性，即第i个物品重量为w[i],价值为v[i]</li></ul></li><li><p>算法问题分析</p><ul><li><p>首先看是什么类型的问题</p><ul><li><p>求最优解的问题（max / min）</p><ul><li>考虑下使用贪心算法的可能性</li><li>暴力递归做穷举</li><li>动态规划</li></ul></li><li><p>求可行性的问题</p></li><li><p>求方案总数的问题</p></li></ul></li><li><p>进一步确认是否为动态规划问题</p><ul><li>数据需要不可排序</li><li>数据不可交换</li></ul></li></ul></li><li><p>状态转移方程</p><ul><li>确定终止条件<ul><li>背包容量为0了或者物品数量为0要终止执行</li><li>背包中的物品数量和背包还能装下的重量是这个问题的状态参数</li></ul></li></ul></li></ul><pre><code class="jsx">int dp(int[] w, int[] v, int N, int W) {    // 创建备忘录    int[][] dp = new int[N+1][W+1];    // 初始化状态    for (int i = 0; i &lt; N + 1; i++) { dp[i][0] = 0; }    for (int j = 0; j &lt; W + 1; j++) { dp[0][j] = 0; }    for (int tn = 1; tn &lt; N + 1; tn++) { // 遍历每一件物品    for (int rw = 1; rw &lt; W + 1; rw++) { // 背包容量有多大就还要计算多少次        if (rw &lt; w[tn]) {          // 当背包容量小于第tn件物品重量时，只能放入前tn-1件          dp[tn][rw] = dp[tn-1][rw];        } else {                // 当背包容量还大于第tn件物品重量时，进一步作出决策          dp[tn][rw] = Math.max(dp[tn-1][rw], dp[tn-1][rw-w[tn]] + v[tn]);        }      }    }  return dp[N][W];}int solveDP() {  int N = 3, W = 5; // 物品的总数，背包能容纳的总重量  int[] w = {0, 3, 2, 1}; // 物品的重量  int[] v = {0, 5, 2, 3}; // 物品的价值  return dp(w, v, N, W); // 输出答案}</code></pre><h1 id="2-动态规划的写法分析"><a href="#2-动态规划的写法分析" class="headerlink" title="2. 动态规划的写法分析"></a>2. 动态规划的写法分析</h1><ul><li>动态规划问题描述<ul><li>重叠子问题<ul><li>穷举过程中存在重复计算的现象</li></ul></li><li>无后效性<ul><li>子问题之间的依赖是单向性的</li><li>某阶段状态一旦确定，就不受后续决策的影响</li></ul></li><li>最优子结构<ul><li>子问题之间必须相互独立，或者说后续的计算可以通过前面的状态推导出来</li></ul></li></ul></li></ul><h1 id="3-案例分析"><a href="#3-案例分析" class="headerlink" title="3. 案例分析"></a>3. 案例分析</h1><h2 id="3-1-硬币找零问题"><a href="#3-1-硬币找零问题" class="headerlink" title="3.1 硬币找零问题"></a>3.1 硬币找零问题</h2><ul><li>问题描述<ul><li>给定n种不同面值的硬币 分别记为c[0], c[1], c[2]…c[n]</li><li>总数额k</li><li>编写一个函数计算出最少需要几枚硬币凑出这个金额K</li><li>若无可能的组合，返回-1</li></ul></li><li>思路<ul><li>这是个求最值的问题</li><li>求最值问题的核心原理就是穷举，将所有可能的凑硬币的方法做穷举，看看最少需要多少枚硬币</li></ul></li><li>贪心算法<ul><li>每一步计算做出的都是在当前看起来最好的选择<ul><li>即局部最优解，并不从整体来考虑</li></ul></li><li>基本思路<ul><li>建立数学模型</li><li>将待求解的问题划分为若干子问题，对每个子问题进行求解，得到子问题的局部最优解</li><li>将子问题的局部最优解进行合并，最终得到基于局部最优解的一个解</li></ul></li></ul></li><li>用贪心算法来解决上述问题的时候，会遇到“过于贪心”导致最终无解的问题，所以需要引入回溯来解决过于贪心的问题</li><li>贪心算法的实现</li></ul><pre><code>int getMinCoinCountHelper(int total, int[] values, int valueCount) {    int rest = total;    int count = 0;    // 从大到小遍历所有面值    for (int i = 0; i &lt; valueCount; ++ i) {        int currentCount = rest / values[i]; // 计算当前面值最多能用多少个        rest -= currentCount * values[i]; // 计算使用完当前面值后的余额        count += currentCount; // 增加当前面额用量        if (rest == 0) {            return count;        }    }    return -1; // 如果到这里说明无法凑出总价，返回-1}int getMinCoinCount() {    int[] values = { 5, 3 }; // 硬币面值    int total = 11; // 总价    return getMinCoinCountHelper(total, values, 2); // 输出结果}</code></pre><ul><li>贪心算法 + 回溯的实现</li></ul><pre><code>int getMinCoinCountOfValue(int total, int[] values, int valueIndex) {    int valueCount = values.length;    if (valueIndex == valueCount) { return Integer.MAX_VALUE; }    int minResult = Integer.MAX_VALUE;    int currentValue = values[valueIndex];    int maxCount = total / currentValue;    for (int count = maxCount; count &gt;= 0; count --) {        int rest = total - count * currentValue;        // 如果rest为0，表示余额已除尽，组合完成        if (rest == 0) {            minResult = Math.min(minResult, count);            break;        }        // 否则尝试用剩余面值求当前余额的硬币总数        int restCount = getMinCoinCountOfValue(rest, values, valueIndex + 1);        // 如果后续没有可用组合        if (restCount == Integer.MAX_VALUE) {            // 如果当前面值已经为0，返回-1表示尝试失败            if (count == 0) { break; }            // 否则尝试把当前面值-1            continue;        }        minResult = Math.min(minResult, count + restCount);    }    return minResult;}int getMinCoinCountLoop(int total, int[] values, int k) {    int minCount = Integer.MAX_VALUE;    int valueCount = values.length;    if (k == valueCount) {        return Math.min(minCount, getMinCoinCountOfValue(total, values, 0));    }    for (int i = k; i &lt;= valueCount - 1; i++) {        // k位置已经排列好        int t = values[k];        values[k] = values[i];        values[i]=t;        minCount = Math.min(minCount, getMinCoinCountLoop(total, values, k + 1)); // 考虑后一位        // 回溯        t = values[k];        values[k] = values[i];        values[i]=t;    }    return minCount;}int getMinCoinCountOfValue() {    int[] values = { 5, 3 }; // 硬币面值    int total = 11; // 总价    int minCoin = getMinCoinCountLoop(total, values, 0);    return (minCoin == Integer.MAX_VALUE) ? -1 : minCoin;  // 输出答案}</code></pre><ul><li><p>使用动态规划来求解</p><ul><li>初始化状态<ul><li>终止条件</li></ul></li><li>状态参数<ul><li>子问题与原问题之间会发生变化的变量</li><li>变量是目标兑换金额K</li></ul></li><li>整体思路<ul><li>确定初始化状态</li><li>确定状态参数</li><li>设计决策</li></ul></li></ul></li><li><p>递归与动态规划的对比</p><ul><li><p>递归是自上而下的，从目标问题开始，不断将大问题拆解成子问题，直到子问题不可拆借为止</p></li><li><p>如果要自底向上，那就应该首先求出所有的子问题，然后通过底层的子问题向上求解更大的问题</p><pre><code>// 伪代码DP(values, k) {res = MAXfor c in values// 作出决策，找到需要硬币最少的那个结果res = min(res, 1 + DP(values, k-c)) // 递归调用if res == MAXreturn -1return res}</code></pre></li></ul></li></ul><pre><code>int getMinCounts(int k, int[] values) {   int[] memo = new int[k + 1]; // 创建备忘录   memo[0] = 0; // 初始化状态   for (int i = 1; i &lt; k + 1; i++) { memo[i] = k + 1; }   for (int i = 1; i &lt; k + 1; i++) {       for (int coin : values) {           if (i - coin &lt; 0) { continue; }           memo[i] = Math.min(memo[i], memo[i - coin] + 1); // 作出决策       }   }   return memo[k] == k + 1 ? -1 : memo[k];}int getMinCountsDPSolAdvance() {   int[] values = { 3, 5 }; // 硬币面值   int total = 22; // 总值   return getMinCounts(total, values); // 输出答案}</code></pre><h1 id="62-Unique-Paths"><a href="#62-Unique-Paths" class="headerlink" title="62. Unique Paths"></a>62. Unique Paths</h1><h2 id="Solution-1-Recursion"><a href="#Solution-1-Recursion" class="headerlink" title="Solution 1: Recursion"></a>Solution 1: Recursion</h2><pre><code>class Solution {    int result = 0;    public int uniquePaths(int m, int n) {        // direction is either right or down         helper(1, 1, m, n);        return result;    }    private void helper(int x, int y, int m, int n) {        if (x &lt; 1 || x &gt; m || y &lt; 1 || y &gt; n) {            return;        }        if (x == m &amp;&amp; y == n) {            result ++;        }        helper(x + 1, y, m, n);        helper(x, y + 1, m, n);    }}</code></pre><h2 id="Solution-2-DP"><a href="#Solution-2-DP" class="headerlink" title="Solution 2: DP"></a>Solution 2: DP</h2><ul><li>注意的点<ul><li>状态转移方程<ul><li><code>dp[m][n] = dp[m-1][n] + dp[m][n-1];</code></li></ul></li><li>因为只有两个方向，实际上第一行第一列能到达的方法都只有一种，所以我们在做初始化的时候可以都初始化为1，具体的迭代从col = 1, row = 1开始，即第二行和第二列 这样来做就okay了</li></ul></li></ul><pre><code>class Solution {     public int uniquePaths(int m, int n) {        // 二维DP           // dp[m][n] = dp[m-1][n] + dp[m][n-1];        int[][] dp = new int[m][n];        for (int[] arr: dp) {            Arrays.fill(arr, 1);        }        for (int col = 1; col &lt; n; col ++) {            for(int row = 1; row &lt; m; row ++) {                dp[row][col] = dp[row-1][col] + dp[row][col-1];            }        }        return dp[m-1][n-1];    }}</code></pre><h1 id="70-Climbing-Stairs"><a href="#70-Climbing-Stairs" class="headerlink" title="70. Climbing Stairs"></a>70. Climbing Stairs</h1><h2 id="Solution-1-DP"><a href="#Solution-1-DP" class="headerlink" title="Solution 1: DP"></a>Solution 1: DP</h2><ul><li><code>dp[n] = dp[n-1] + dp[n-2]</code></li></ul><pre><code>class Solution {    public int climbStairs(int n) {        // dp[n] = dp[n-1] + dp[n-2]        if (n &lt;= 0) {            return 0;        }        int[] dp = new int[n + 1];        dp[0] = 1;        dp[1] = 1;        for (int i = 2; i &lt;= n; i++) {            dp[i] = dp[i-1] + dp[i-2];        }        return dp[n];    }}</code></pre><h1 id="91-Decode-Ways"><a href="#91-Decode-Ways" class="headerlink" title="91. Decode Ways"></a>91. Decode Ways</h1><h2 id="Solution-1-DP-1"><a href="#Solution-1-DP-1" class="headerlink" title="Solution 1: DP"></a>Solution 1: DP</h2><ul><li>每次可以选择往前走一步或者两步</li></ul><pre><code>class Solution {    public int numDecodings(String s) {        if (s == null || s.length() == 0) {            return 0;        }        int[] dp = new int[s.length() + 1];        // could go by 1 step or 2 step         // dp[n] = dp[n-1] + dp[n-2]        //     s.charAt(n) 1 --9        //     s.charAt(n-1) 1 -- 2  if n-1 == 1  n 0 - 9  if n-1 == 2 n 0 - 6         dp[0] = 1;        dp[1] = s.charAt(0) == &#39;0&#39; ? 0 : 1;        for (int i = 2; i &lt;= s.length(); i ++) {            if (s.charAt(i - 1) != &#39;0&#39;) {                dp[i] += dp[i-1];            }            if (s.charAt(i-2) == &#39;1&#39; || (s.charAt(i-2) == &#39;2&#39; &amp;&amp;                                          s.charAt(i - 1) &gt;= &#39;0&#39; &amp;&amp; s.charAt(i-1) &lt;= &#39;6&#39; )){                dp[i] += dp[i-2];            }        }        return dp[s.length()];    }}</code></pre><h2 id="Solution-2-Recursive"><a href="#Solution-2-Recursive" class="headerlink" title="Solution 2: Recursive"></a>Solution 2: Recursive</h2><ul><li>每次都可以往前走两步或者一步</li></ul><pre><code>class Solution {    HashMap&lt;Integer, Integer&gt; memo = new HashMap&lt;&gt;();    public int numDecodings(String s) {        if (s == null || s.length() == 0) {            return 0;        }        return helper(s, 0);    }    private int helper(String s, int pos) {        if (pos == s.length()) {            return 1;        }         if (s.charAt(pos) == &#39;0&#39;) {            return 0;        }        if (pos == s.length() - 1) {            return 1;        }        if (memo.containsKey(pos)) {            return memo.get(pos);        }        int ans = helper(s, pos + 1);        if (Integer.parseInt(s.substring(pos, pos+2)) &lt;= 26) {             ans += helper(s, pos + 2);        }        memo.put(pos, ans);        return ans;    }}</code></pre><h1 id="121-Best-Time-to-Buy-and-Sell-Stock"><a href="#121-Best-Time-to-Buy-and-Sell-Stock" class="headerlink" title="121. Best Time to Buy and Sell Stock"></a>121. Best Time to Buy and Sell Stock</h1><h2 id="Solution-1-Brute-Force"><a href="#Solution-1-Brute-Force" class="headerlink" title="Solution 1: Brute Force"></a>Solution 1: Brute Force</h2><pre><code>class Solution {    public int maxProfit(int[] prices) {        int result = 0;        for (int i = 0; i &lt; prices.length; i++) {            for (int j = i + 1; j &lt; prices.length; j++) {                result = Math.max(result, prices[j] - prices[i]);            }        }        return result;    }}</code></pre><h2 id="Solution-2-One-Pass"><a href="#Solution-2-One-Pass" class="headerlink" title="Solution 2: One Pass"></a>Solution 2: One Pass</h2><ul><li>记录当前的最小值，碰到比最小值大的值，就更新最大利润</li></ul><pre><code>class Solution {    public int maxProfit(int prices[]) {        int minprice = Integer.MAX_VALUE;        int maxprofit = 0;        for (int i = 0; i &lt; prices.length; i++) {            if (prices[i] &lt; minprice)                minprice = prices[i];            else if (prices[i] - minprice &gt; maxprofit)                maxprofit = prices[i] - minprice;        }        return maxprofit;    }}</code></pre><h1 id="LC-139-Word-Break"><a href="#LC-139-Word-Break" class="headerlink" title="LC 139 Word Break"></a>LC 139 Word Break</h1><ul><li>字典里的词可以选用</li><li>可以重复使用</li><li>目的是需要拼起string</li><li>DFS 深度优先遍历</li></ul><h2 id="Solution-1-Brute-Force-1"><a href="#Solution-1-Brute-Force-1" class="headerlink" title="Solution 1 Brute Force"></a>Solution 1 Brute Force</h2><ul><li>Kind of DFS</li><li>Go through one way to the very bottom, and we need to go through all possible result if needed</li></ul><pre><code>class Solution {    public boolean wordBreak(String s, List&lt;String&gt; wordDict) {        if (s == null || s.length() == 0 || wordDict == null || wordDict.size() == 0) {            return false;        }        return subString(s, new HashSet(wordDict), 0);    }    private boolean subString(String testStr, Set&lt;String&gt; wordDict, int position) {        if (position == testStr.length()) {            return true;        }        // 注意这里的&lt;=  因为substring的第二个参数是不包括在第二个参数里面的        for (int n = position + 1; n &lt;= testStr.length(); n ++) {            if (wordDict.contains(testStr.substring(position, n)) &amp;&amp; subString(testStr, wordDict, n)) {                return true;            }        }        return false;    }}</code></pre><h2 id="Solution-1-1-Brute-Force-with-array-record"><a href="#Solution-1-1-Brute-Force-with-array-record" class="headerlink" title="Solution 1.1 Brute Force with array record"></a>Solution 1.1 Brute Force with array record</h2><ul><li>解法1 包含太多的重复，通过记录在某个位置以后能不能成功来保证不会重复运行</li></ul><pre><code>class Solution {    public boolean wordBreak(String s, List&lt;String&gt; wordDict) {        if (s == null || s.length() == 0 || wordDict == null || wordDict.size() == 0) {            return false;        }        return subString(s, new HashSet(wordDict), 0, new Boolean[s.length()]);    }    private boolean subString(String testStr, Set&lt;String&gt; wordDict, int position, Boolean[] memo) {        if (position == testStr.length()) {            return true;        }        if (memo[position] != null) {            return memo[position];        }        // 注意这里的&lt;=  因为substring的第二个参数是不包括在第二个参数里面的        for (int n = position + 1; n &lt;= testStr.length(); n ++) {            if (wordDict.contains(testStr.substring(position, n)) &amp;&amp; subString(testStr, wordDict, n, memo)) {                return true;            }        }        memo[position] = false;        return false;    }}</code></pre><h2 id="Solution-2-Dynamic-Programming"><a href="#Solution-2-Dynamic-Programming" class="headerlink" title="Solution 2 Dynamic Programming"></a>Solution 2 Dynamic Programming</h2><pre><code>class Solution {    public boolean wordBreak(String s, List&lt;String&gt; wordDict) {        Set&lt;String&gt; wordDictSet = new HashSet(wordDict);        boolean[] dp = new boolean[s.length() + 1];        dp[0] = true;        for (int i = 1; i &lt;= s.length(); i ++) {            for (int j = 0; j &lt; i; j++) {                if (dp[j] &amp;&amp; wordDictSet.contains(s.substring(j, i))) {                    dp[i] = true;                    break;                }            }        }        return dp[s.length()];    }}</code></pre><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://labuladong.gitbook.io/algo/dong-tai-gui-hua-xi-lie/1.1-dong-tai-gui-hua-ji-ben-ji-qiao/dong-tai-gui-hua-xiang-jie-jin-jie" target="_blank" rel="noopener">https://labuladong.gitbook.io/algo/dong-tai-gui-hua-xi-lie/1.1-dong-tai-gui-hua-ji-ben-ji-qiao/dong-tai-gui-hua-xiang-jie-jin-jie</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;动态规划&quot;&gt;&lt;a href=&quot;#动态规划&quot; class=&quot;headerlink&quot; title=&quot;动态规划&quot;&gt;&lt;/a&gt;动态规划&lt;/h1&gt;&lt;h1 id=&quot;1-动态规划问题的一些思路&quot;&gt;&lt;a href=&quot;#1-动态规划问题的一些思路&quot; class=&quot;headerlink
      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="DP" scheme="https://www.llchen60.com/tags/DP/"/>
    
  </entry>
  
  <entry>
    <title>Instagram Design Scratch</title>
    <link href="https://www.llchen60.com/Instagram-Design-Scratch/"/>
    <id>https://www.llchen60.com/Instagram-Design-Scratch/</id>
    <published>2020-11-15T17:59:24.000Z</published>
    <updated>2020-11-15T18:00:08.339Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Intro-and-Instagram"><a href="#1-Intro-and-Instagram" class="headerlink" title="1. Intro and Instagram"></a>1. Intro and Instagram</h1><ul><li>social networking service </li><li>upload and share photos and videos </li><li>share info either publicly or privately </li><li>share through other social networking platforms <ul><li>facebook</li><li>twitter</li><li>flickr</li><li>tumblr</li></ul></li></ul><ul><li><p>requirement </p><ul><li><p>functional </p><ul><li>users should be able to upload/ download/ view photos </li><li>search based on photo/ video titles </li><li>users can follow other users </li><li>system will generate and display a user’s News Feed consisting of top photos from all the people the user follows </li></ul></li><li><p>non-functional </p><ul><li>services need to be highly available </li><li>latency come to be 200ms for News Feed Generation </li></ul></li></ul></li></ul><h1 id="2-Design"><a href="#2-Design" class="headerlink" title="2. Design"></a>2. Design</h1><ul><li>patterns<ul><li>read heavy system <ul><li>need to effectively manage storage </li><li>100% reliable of data </li><li>low latency </li></ul></li></ul></li></ul><ul><li><p>system design </p><ul><li><p>we need to support two scenarios</p><ul><li>upload photos </li><li>view/ search photos</li></ul></li><li><p>db </p><ul><li>image storage </li><li>image metadata storage </li></ul></li></ul></li></ul><ul><li><p>Separate read and write servers </p><ul><li>web servers have a connection limit </li><li>assume max is 500 connections, that means server cannot have more than 500 concurrent uploads or reads </li></ul></li><li><p>reliability </p><ul><li>run a redundant secondary copy of the service that is not serving any traffic if only one instance of a service is required to run at any point </li></ul></li><li><p>data sharding </p><ul><li><p>partition based on UserID?</p><ul><li>hot user </li><li>non-uniform storage as some users may upload a lot of images </li><li>what if one shard is full? </li><li>unavailability – all users data come to be unavailable all of sudden </li></ul></li><li><p>use photoId as partition key </p></li></ul></li><li><p>news feeds - e.g 100 feeds </p><ul><li><p>real time generation </p><ul><li>need to go to user follow table to get all of its following people </li><li>then query with user id to get their 100 updates </li><li>combine them, go through our ranking algo </li><li>then generate the output </li></ul></li><li><p>should choose pre-generating way as it will save a lot of times </p><ul><li>have dedicated servers that are continuously generating users’ News Feeds and storing them in a UserNewsFeed table</li></ul></li></ul></li><li><p>approaches for seanding news feed contents to users </p><ul><li><p>pull</p><ul><li><p>clients can pull the News Feed contents from the server on a regular basis or manually whenever they need it </p><ul><li><p>problems</p><ul><li>new data might not be shown to the users until clients issue a pull request </li><li>most of time, pull requests response could be empty </li></ul></li><li><p>push </p><ul><li>servers can push new data to the users as soon as it is available </li><li>users then need to maintain a long poll request with the server for receiving the updates </li><li>issue <ul><li>user has follow a lot of people </li><li>celebrity user who has meillions of followers </li><li>server then have to push updates quite frequently </li></ul></li></ul></li><li><p>hybrid </p><ul><li>move all the users who have a high number of follows to a pull based model and only push data to those users who have a few hundred follows </li></ul></li></ul></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Intro-and-Instagram&quot;&gt;&lt;a href=&quot;#1-Intro-and-Instagram&quot; class=&quot;headerlink&quot; title=&quot;1. Intro and Instagram&quot;&gt;&lt;/a&gt;1. Intro and Instagram
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>Designing Pastebin</title>
    <link href="https://www.llchen60.com/Designing-Pastebin/"/>
    <id>https://www.llchen60.com/Designing-Pastebin/</id>
    <published>2020-11-13T04:46:26.000Z</published>
    <updated>2020-11-13T04:47:05.219Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Intro-and-requirements"><a href="#1-Intro-and-requirements" class="headerlink" title="1. Intro and requirements"></a>1. Intro and requirements</h1><ul><li>Pastebin<ul><li>service enable users to store plain text or images over the network and generate unique urls to access the uploaded data </li></ul></li></ul><ul><li><p>requirements</p><ul><li><p>functional </p><ul><li>users could upload/ paste their data and get a unique URL to access it </li><li>users only able to upload text </li><li>data and links will expire after a specific timespan, user should be able to specify expeiration time </li><li>users should optionally be able to pick a custom alias for their paste </li></ul></li><li><p>non functional </p><ul><li>system should be highly reliable, no data loss any time </li><li>system should be highly available</li><li>small latency </li><li>paste links should not be guessable </li></ul></li></ul></li></ul><h1 id="2-thoughts"><a href="#2-thoughts" class="headerlink" title="2. thoughts"></a>2. thoughts</h1><ul><li><p>requirement clarification </p><ul><li>what’s the accept types of the file? </li><li>what’s the maximum size of the file? <ul><li>10MB</li></ul></li></ul></li><li><p>hardware requirement </p><ul><li>read write request number </li><li>average request size </li><li>storage size <ul><li>we probably don’t want to use more than 70% of our total storage capacity at any point </li></ul></li><li>cache size </li></ul></li><li><p>how to achieve highly reliable, and highly available </p><ul><li>duplicate host </li></ul></li><li><p>how you want to store those file </p><ul><li>of course key value<ul><li>then question come to be what’s the overall number of key we need to store </li></ul></li></ul></li><li><p>high level design </p><ul><li>besides normal client, application server, and object storage </li><li>we could have another db to store metadata, like paste id, user </li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Intro-and-requirements&quot;&gt;&lt;a href=&quot;#1-Intro-and-requirements&quot; class=&quot;headerlink&quot; title=&quot;1. Intro and requirements&quot;&gt;&lt;/a&gt;1. Intro and 
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>Designing a URL Shortening service like TinyURL</title>
    <link href="https://www.llchen60.com/Designing-a-URL-Shortening-service-like-TinyURL/"/>
    <id>https://www.llchen60.com/Designing-a-URL-Shortening-service-like-TinyURL/</id>
    <published>2020-11-09T00:03:55.000Z</published>
    <updated>2020-11-09T00:04:36.117Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Thoughts"><a href="#1-Thoughts" class="headerlink" title="1. Thoughts"></a>1. Thoughts</h1><p>Based on System Design Guide, our thoughts could follow the framework described <a href="https://llchen60.com/System-Design-General-Guides-from-Grokking-SDI/" target="_blank" rel="noopener">here</a></p><p>Based on that: </p><ul><li><p>Requirement Clarification </p><ul><li>what feature we need<ul><li>create new tinyUrl </li><li>read existing tinyUrl, parse it to websites url </li></ul></li></ul></li><li><p>Envelope estimation </p><ul><li>what traffic amount it would be? Read/ Write Ratio? </li><li>Compute storage size we need</li><li>Network bandwidth usage </li></ul></li><li><p>System Interface Design </p><ul><li>generateNewTinyUrl<ul><li>original url </li><li>output parsed url </li></ul></li><li>parseTinyUrl()<ul><li>input tiny url </li><li>output - mapped original url </li></ul></li></ul></li><li><p>Data Model </p><ul><li>how data will flow between different components of the system </li></ul></li><li><p>High level design and detailed design </p></li><li><p>bottle neck resolving </p></li></ul><h1 id="2-Detail"><a href="#2-Detail" class="headerlink" title="2. Detail"></a>2. Detail</h1><h2 id="2-1-Requirements"><a href="#2-1-Requirements" class="headerlink" title="2.1 Requirements"></a>2.1 Requirements</h2><ul><li><p>Functional </p><ul><li>shorten given url </li><li>access a short url, user will be redirected to original link </li><li>users should optionally be able to pick a custom short link for their url </li><li>links will expire after a standard timespan, users should be able to specify the expiration time </li></ul></li><li><p>Non-Functional Requirements </p><ul><li>System should be highly available </li><li>Url redirection should happen in real time with minimal latency </li><li>Shortened links should not be guessable </li></ul></li><li><p>Extended Requirements</p><ul><li>analytics - how many times a redirection happen</li><li>should also be accessible through REST APIs by other services </li></ul></li></ul><h2 id="2-2-Capacity-Estimation-and-Constraints"><a href="#2-2-Capacity-Estimation-and-Constraints" class="headerlink" title="2.2 Capacity Estimation and Constraints"></a>2.2 Capacity Estimation and Constraints</h2><ul><li><p>read heavy system </p><ul><li>estimate read write ratio 100:1 </li></ul></li><li><p>traffic estimates </p><ul><li>suppose 500M new URL shortening request </li><li>then based on ratio, could be 50B read request </li><li>QPS<ul><li>for write <ul><li>500MM/ (30days * 24 hours * 3600 seconds) = ~ 200 URL/s</li></ul></li><li>for read <ul><li>200 * 100 = 20K/s</li></ul></li></ul></li></ul></li><li><p>storage estimate </p><ul><li>how long do we want to store them? <ul><li>what’s the total number of objects we want to store? <ul><li>suppose 5 years </li><li>500MM * 5 years * 12 months = 30 billion </li><li>suppose every object is about 500 bytes, then in total <ul><li>30B * 500 bytes = 15 TB</li></ul></li></ul></li></ul></li></ul></li><li><p>bandwidth estimate </p><ul><li><p>for write </p><ul><li>200 * 500 bytes/ s = ~100KB/s</li></ul></li><li><p>for read </p><ul><li>20K * 500 bytes = ~ 10MB/s </li></ul></li></ul></li><li><p>memory estimates </p><ul><li>we may want to cache some hot URLs that are frequently accessed, 20/ 80 rule </li><li>cache 20% hot URLs for a day </li><li>memory usage <ul><li>0.2 * 1.7 B * 500 bytes = ~170GB </li></ul></li></ul></li></ul><h2 id="2-3-System-APIs"><a href="#2-3-System-APIs" class="headerlink" title="2.3 System APIs"></a>2.3 System APIs</h2><ul><li><p>createURL (clientId, originalUrl, userName, customAlias, expireDate)</p><ul><li>clientId could be used for throttling based on allocated quota </li><li>return <ul><li>shortened url </li></ul></li></ul></li><li><p>deleteURL (clientId, urlKey)</p></li></ul><h2 id="2-4-DB-design"><a href="#2-4-DB-design" class="headerlink" title="2.4 DB design"></a>2.4 DB design</h2><ul><li>need two table, one store info for the URL mappings; one for the user’s data about who created the short link </li></ul><h2 id="2-5-System-Design-and-Algorithm"><a href="#2-5-System-Design-and-Algorithm" class="headerlink" title="2.5 System Design and Algorithm"></a>2.5 System Design and Algorithm</h2><ul><li><p>How to generate a short and unique key for a given URL </p><ul><li><p>Encoding actual URL </p><ul><li>compute a unique hash of the given URL </li><li>hash could be encoded for display </li><li>If we use MD5 algorithm as our hash function, it’ll produce a 128 bit hash value</li><li>After base64 encoding, we’ll get a string having more than 21 characters<ul><li>take the first 8 letters for the key <ul><li>for duplication, choose some other characters out of the encoding string or swap some characters </li></ul></li></ul></li></ul></li><li><p>generate keys offline </p><ul><li>have a standalon key generation service that generates random six letter strings beforehand and stores them in a database</li><li>whenever we want to shorten a URL, we will take one of the already generated keys and use it </li></ul></li></ul></li></ul><h2 id="2-6-Deta-Partitioning-and-Replication"><a href="#2-6-Deta-Partitioning-and-Replication" class="headerlink" title="2.6 Deta Partitioning and Replication"></a>2.6 Deta Partitioning and Replication</h2><ul><li>Range based partitioning<ul><li>Store URLs in separate partitions based on the hash key’s first letter</li><li>could lead to unbalanced DB servers  </li></ul></li><li>Hash Based Partitioning <ul><li>use hash, and calculate which partition to use based on that </li></ul></li></ul><h2 id="2-7-Cache"><a href="#2-7-Cache" class="headerlink" title="2.7 Cache"></a>2.7 Cache</h2><ul><li><p>could use memcache or other similar in memory cache solution </p></li><li><p>how muach cache memory should we have?</p><ul><li>20% of daily traffic </li></ul></li><li><p>what cache eviction policy should we use?</p><ul><li>LRU </li></ul></li></ul><h2 id="2-8-Load-Balancer"><a href="#2-8-Load-Balancer" class="headerlink" title="2.8 Load Balancer"></a>2.8 Load Balancer</h2><ul><li>We could choose to add LB at: <ul><li>between clients and application servers</li><li>between application servers and database servers</li><li>between apllication servers and cache servers </li></ul></li></ul><h2 id="2-9-Purging-DB-cleanup"><a href="#2-9-Purging-DB-cleanup" class="headerlink" title="2.9 Purging / DB cleanup"></a>2.9 Purging / DB cleanup</h2><ul><li>We should not actively search for expired links to remove them, as it would put a lot of pressure on our db <ul><li>slowlyremove and do a lazy cleanup</li></ul></li></ul><h2 id="2-10-Metrics-and-Security"><a href="#2-10-Metrics-and-Security" class="headerlink" title="2.10 Metrics and Security"></a>2.10 Metrics and Security</h2><ul><li><p>Record metrics </p><ul><li>country of visitor</li><li>data and time </li><li>web page that referred the click </li><li>browser </li><li>platform</li><li>etc. </li></ul></li><li><p>security and permissions </p><ul><li>store the permission level with each URL in the database </li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Thoughts&quot;&gt;&lt;a href=&quot;#1-Thoughts&quot; class=&quot;headerlink&quot; title=&quot;1. Thoughts&quot;&gt;&lt;/a&gt;1. Thoughts&lt;/h1&gt;&lt;p&gt;Based on System Design Guide, our th
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>LC - Tree</title>
    <link href="https://www.llchen60.com/LC-Tree/"/>
    <id>https://www.llchen60.com/LC-Tree/</id>
    <published>2020-11-01T18:54:24.000Z</published>
    <updated>2020-11-05T04:20:50.681Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Lc-98-Validate-Binary-Search-Tree"><a href="#Lc-98-Validate-Binary-Search-Tree" class="headerlink" title="Lc 98. Validate Binary Search Tree"></a>Lc 98. Validate Binary Search Tree</h1><h2 id="Solution-1-Recursion"><a href="#Solution-1-Recursion" class="headerlink" title="Solution 1: Recursion"></a>Solution 1: Recursion</h2><pre><code>class Solution {    public boolean isValidBST(TreeNode root) {        return isValidSubTree(root, null, null);    }    boolean isValidSubTree(TreeNode node, Integer lower, Integer upper) {        if (node == null) {            return true;        }        int val = node.val;        if (lower != null &amp;&amp; val &lt;= lower) return false;        if (upper != null &amp;&amp; val &gt;= upper) return false;        if (!isValidSubTree(node.right, val, upper)) return false;        if (!isValidSubTree(node.left, lower, val)) return false;        return true;    }}</code></pre><h2 id="Solution-2-Inorder-Traversal"><a href="#Solution-2-Inorder-Traversal" class="headerlink" title="Solution 2: Inorder Traversal"></a>Solution 2: Inorder Traversal</h2><ul><li>这样遍历前一个比后一个小才是BST</li></ul><pre><code>class Solution {  public boolean isValidBST(TreeNode root) {    Stack&lt;TreeNode&gt; stack = new Stack();    double inorder = - Double.MAX_VALUE;    while (!stack.isEmpty() || root != null) {      while (root != null) {        stack.push(root);        root = root.left;      }      root = stack.pop();      // If next element in inorder traversal      // is smaller than the previous one      // that&#39;s not BST.      if (root.val &lt;= inorder) return false;      inorder = root.val;      root = root.right;    }    return true;  }}</code></pre><h1 id="LC-105-Construct-Binary-Tree-from-Preorder-and-Inorder-Traversal"><a href="#LC-105-Construct-Binary-Tree-from-Preorder-and-Inorder-Traversal" class="headerlink" title="LC 105. Construct Binary Tree from Preorder and Inorder Traversal"></a>LC 105. Construct Binary Tree from Preorder and Inorder Traversal</h1><ul><li>前序遍历  根节点，左子节点，右子节点</li><li>中序遍历  左子节点，根节点，右子节点</li><li>没有重复数字</li><li>前序遍历的第一个节点是根节点，其在中序遍历当中出现的位置，其之前是左子树，其之后是右子树，可以用这个逻辑来递归进行判断</li></ul><h1 id="LC-226-Invert-Binary-Tree"><a href="#LC-226-Invert-Binary-Tree" class="headerlink" title="LC 226. Invert Binary Tree"></a>LC 226. Invert Binary Tree</h1><h2 id="Solution-1-Recursion-1"><a href="#Solution-1-Recursion-1" class="headerlink" title="Solution 1: Recursion"></a>Solution 1: Recursion</h2><ul><li>逻辑是每个根节点的左子树是其右子树的翻转，即层层都翻转过了  用递归完成这个逻辑就好</li></ul><pre><code>class Solution {    public TreeNode invertTree(TreeNode root) {        if (root == null) {            return null;        }        TreeNode right = invertTree(root.right);        TreeNode left = invertTree(root.left);        root.left = right;        root.right = left;        return root;    }}</code></pre><h2 id="Solution-2-Iteration"><a href="#Solution-2-Iteration" class="headerlink" title="Solution 2: Iteration"></a>Solution 2: Iteration</h2><pre><code>class Solution {    public TreeNode invertTree(TreeNode root) {        if (root == null) return null;        Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;();        queue.add(root);        while (!queue.isEmpty()) {            TreeNode current = queue.poll();            TreeNode temp = current.left;            current.left = current.right;            current.right = temp;            if (current.left != null) queue.add(current.left);            if (current.right != null) queue.add(current.right);        }        return root;    }}</code></pre><h2 id="Solution-2-Iterative"><a href="#Solution-2-Iterative" class="headerlink" title="Solution 2: Iterative"></a>Solution 2: Iterative</h2><h1 id="LC-543-Diameter-of-Binary-Tree"><a href="#LC-543-Diameter-of-Binary-Tree" class="headerlink" title="LC 543. Diameter of Binary Tree"></a>LC 543. Diameter of Binary Tree</h1><ul><li><p>Diameter – length of the longest path between any two nodes in a tre </p></li><li><p>Binary Tree </p><pre><code>public class Solution {  int max = 0;  public int diameterOfBinaryTree(TreeNode root) {      maxDepth(root);      return max;  }  private int maxDepth(TreeNode root) {      if (root == null) return 0;      int left = maxDepth(root.left);      int right = maxDepth(root.right);      max = Math.max(max, left + right);      return Math.max(left, right) + 1;  }}</code></pre></li></ul><h1 id="LC-938-Range-Sum-of-BST"><a href="#LC-938-Range-Sum-of-BST" class="headerlink" title="LC 938. Range Sum of BST"></a>LC 938. Range Sum of BST</h1><ul><li><p>二叉查找树</p><ul><li>左子树的值会小于根节点的值</li><li>右子树的值都大于根节点的值</li></ul></li><li><p>二叉查找树当中找区间，一定是需要用到二叉查找树本身的性质的</p></li></ul><h2 id="Solution-1-Brute-Force"><a href="#Solution-1-Brute-Force" class="headerlink" title="Solution 1: Brute Force"></a>Solution 1: Brute Force</h2><ul><li>遍历整个树，对于每个节点，对其进行判断，若值在区间内，则累加到一个全局变量上</li></ul><pre><code>class Solution {    int sum = 0;    public int rangeSumBST(TreeNode root, int L, int R) {        addToSum(root, L, R);        return sum;    }    void addToSum(TreeNode node, int L, int R) {        if (node == null) return;        if (node.val &gt;= L &amp;&amp; node.val &lt;= R) {            sum += node.val;        }        addToSum(node.left, L, R);        addToSum(node.right, L, R);    }}</code></pre><h2 id="Solution-2-Leverage-on-BST"><a href="#Solution-2-Leverage-on-BST" class="headerlink" title="Solution 2: Leverage on BST"></a>Solution 2: Leverage on BST</h2><ul><li><p>逻辑是如果node.val &lt; L, 那么只有右子树是有可能获取在区间内的值的</p><pre><code>class Solution {  int ans;  public int rangeSumBST(TreeNode root, int L, int R) {      ans = 0;      dfs(root, L, R);      return ans;  }  public void dfs(TreeNode node, int L, int R) {      if (node != null) {          if (L &lt;= node.val &amp;&amp; node.val &lt;= R)              ans += node.val;          if (L &lt; node.val)              dfs(node.left, L, R);          if (node.val &lt; R)              dfs(node.right, L, R);      }  }}</code></pre></li><li><p>Q: L13 – L16, 为什么不能写成</p><ul><li><code>node.val &gt; R</code></li><li><code>node.val &lt; L</code></li></ul></li><li><p>因为其实第10行的if判断只是为了加进去ans，没有做其他的逻辑运算的</p></li></ul><h1 id="LC-199-Binary-Tree-Right-Side-View"><a href="#LC-199-Binary-Tree-Right-Side-View" class="headerlink" title="LC 199: Binary Tree Right Side View"></a>LC 199: Binary Tree Right Side View</h1><ul><li>对树的分层操作<ul><li>使用BFS算法</li></ul></li></ul><h2 id="Solution-1-使用两个队列"><a href="#Solution-1-使用两个队列" class="headerlink" title="Solution 1: 使用两个队列"></a>Solution 1: 使用两个队列</h2><ul><li>两个队列，一个为当前层，一个为下一层</li></ul><pre><code>class Solution {    public List&lt;Integer&gt; rightSideView(TreeNode root) {        if (root == null) {            return new ArrayList&lt;&gt;();        }        ArrayDeque&lt;TreeNode&gt; currentQueue = new ArrayDeque&lt;&gt;();        ArrayDeque&lt;TreeNode&gt; nextQueue = new ArrayDeque&lt;&gt;();        nextQueue.add(root);        List&lt;Integer&gt; result = new ArrayList&lt;&gt;();        TreeNode node = null;        while(!nextQueue.isEmpty()) {            currentQueue = nextQueue.clone();            nextQueue.clear();            while (!currentQueue.isEmpty()) {                node = currentQueue.poll();                if (node.left != null) {                    nextQueue.add(node.left);                }                if (node.right != null) {                    nextQueue.add(node.right);                }            }            if (currentQueue.isEmpty()) {                result.add(node.val);            }        }        return result;    }}</code></pre><h2 id="Solution-2-一个队列-Sentinel"><a href="#Solution-2-一个队列-Sentinel" class="headerlink" title="Solution 2: 一个队列 + Sentinel"></a>Solution 2: 一个队列 + Sentinel</h2><ul><li>使用null作为卫兵节点来区分不同层</li></ul><pre><code>class Solution {    public List&lt;Integer&gt; rightSideView(TreeNode root) {        if (root == null) {            return new ArrayList&lt;&gt;();        }        Queue&lt;TreeNode&gt; queue = new LinkedList() {            {offer(root);            offer(null);}        };        TreeNode prev, cur = root;        List&lt;Integer&gt; result = new ArrayList();        while (!queue.isEmpty()) {            prev = cur;            cur = queue.poll();            while(cur != null) {                if (cur.left != null) {                    queue.offer(cur.left);                }                if (cur.right != null) {                    queue.offer(cur.right);                }                prev = cur;                cur = queue.poll();            }            result.add(prev.val);            if (!queue.isEmpty()) queue.offer(null);        }        return result;    }}</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Lc-98-Validate-Binary-Search-Tree&quot;&gt;&lt;a href=&quot;#Lc-98-Validate-Binary-Search-Tree&quot; class=&quot;headerlink&quot; title=&quot;Lc 98. Validate Binary Sea
      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="Tree" scheme="https://www.llchen60.com/tags/Tree/"/>
    
  </entry>
  
  <entry>
    <title>LC - Linked List</title>
    <link href="https://www.llchen60.com/LC-Linked-List/"/>
    <id>https://www.llchen60.com/LC-Linked-List/</id>
    <published>2020-10-25T03:33:44.000Z</published>
    <updated>2020-11-20T01:03:08.251Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LC-23-Merge-K-Sorted-Lists"><a href="#LC-23-Merge-K-Sorted-Lists" class="headerlink" title="LC-23 Merge K Sorted Lists"></a>LC-23 Merge K Sorted Lists</h1><ul><li>给定k个链表，每个链表都按照升序排列，合并链表使得最终的链表也按照升序排列</li><li>最直观的想法</li><li>以一个链表为基准，拿出一个链表来做比较，各保留一个指针，然后来做合并操作</li></ul><h2 id="Solution1-Brute-Force"><a href="#Solution1-Brute-Force" class="headerlink" title="Solution1: Brute Force"></a>Solution1: Brute Force</h2><ul><li><p>都放到一个arraylist当中</p></li><li><p>使用Collection的排序算法，对其进行排序</p></li><li><p>然后创建一个新的链表，来返回结果</p><pre><code>class Solution {  public ListNode mergeKLists(ListNode[] lists) {      if (lists == null || lists.length == 0) {          return null;      }      List&lt;Integer&gt; list = new ArrayList();      for (ListNode ln : lists) {          while (ln != null) {              list.add(ln.val);              ln = ln.next;          }      }      Collections.sort(list);      ListNode dummy = new ListNode(0);      ListNode result = dummy;      for (int i = 0; i &lt; list.size(); i++) {          result.next = new ListNode(list.get(i));          result = result.next;      }      return dummy.next;  }}</code></pre></li></ul><h2 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution 2"></a>Solution 2</h2><ul><li>比较每个链表头，将最小的放到result中，一个一个这样比较，相当于只遍历了以便就生成了结果</li><li>注意终止循环的判断，应该是几个链表都到头了才退出</li></ul><pre><code>class Solution {    public ListNode mergeKLists(ListNode[] lists) {        if (lists == null || lists.length == 0) {            return null;        }        ListNode result = new ListNode(0);        ListNode head = result;        while (true) {            int min = Integer.MAX_VALUE;            int min_index = -1;            boolean shouldBreak = true;            for (int i = 0; i &lt; lists.length; i++) {                if (lists[i] != null &amp;&amp; lists[i].val &lt; min) {                    min = lists[i].val;                    min_index = i;                    shouldBreak = false;                }            }            if (shouldBreak) break;            // get one value min             head.next = lists[min_index];            head = head.next;            lists[min_index] = lists[min_index].next;        }        return result.next;    }}</code></pre><h2 id="Solution-3-使用Priority-Queue"><a href="#Solution-3-使用Priority-Queue" class="headerlink" title="Solution 3: 使用Priority Queue"></a>Solution 3: 使用Priority Queue</h2><pre><code>public ListNode mergeKLists(ListNode[] lists) {         PriorityQueue q = new PriorityQueue&lt;&gt;((o1, o2) -&gt; o1.val - o2.val);        for(ListNode l : lists){            if(l!=null){                // 这里加进去的不是value 而是几个ListNode                q.add(l);            }                }        ListNode head = new ListNode(0);        ListNode point = head;        while(!q.isEmpty()){             point.next = q.poll();            point = point.next;             ListNode next = point.next;            if(next!=null){                // 顺次下移以后比较下一个节点                q.add(next);            }        }        return head.next;    }</code></pre><h1 id="LC-24-Swap-Nodes-in-Pairs"><a href="#LC-24-Swap-Nodes-in-Pairs" class="headerlink" title="LC-24 Swap Nodes in Pairs"></a>LC-24 Swap Nodes in Pairs</h1><ul><li><p>交换相邻节点</p></li><li><p>临界条件</p><ul><li>单个节点</li><li>没有节点</li></ul></li><li><p>因为会涉及到首节点的替换变动，所以为了简化问题，需要设置一个 dummy node，使得<code>dummy.next = head</code></p></li><li><p>画一个基本的swap示意图可以发现整个替换会涉及四个节点</p><ul><li>首节点的上个节点原先指向首节点的指针</li><li>首节点本身</li><li>次节点本身</li><li>次节点原先指向次节点下一个节点的指针</li></ul></li><li><p>因为是单向链表，我们没法用prev指针来找到上一个节点，所以需要在过程中做好prev的记录，而对于次节点的下一个节点，完全可以用next指针来表示</p></li><li><p>另外要注意做swap的时候的顺序问题</p></li></ul><pre><code>// Iteration的版本class Solution {    public ListNode swapPairs(ListNode head) {        ListNode dummy = new ListNode(0);        dummy.next = head;        ListNode prev = dummy;        while (head != null &amp;&amp; head.next != null) {            ListNode cur = head;            ListNode next = head.next;            // Swapping             prev.next = next;            cur.next = next.next;            next.next = cur;            prev = cur;            head = cur.next;        }        return dummy.next;    }}</code></pre><h1 id="92-Reverse-Linked-List-II"><a href="#92-Reverse-Linked-List-II" class="headerlink" title="92. Reverse Linked List II"></a>92. Reverse Linked List II</h1><ul><li>reverse a linked list from position m to n </li></ul><pre><code>/** * Definition for singly-linked list. * public class ListNode { *     int val; *     ListNode next; *     ListNode() {} *     ListNode(int val) { this.val = val; } *     ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */class Solution {    public ListNode reverseBetween(ListNode head, int m, int n) {        // how to reverse a         ListNode prev = null;        ListNode cur = head;        while (m &gt; 1) {            prev = cur;            cur = cur.next;            m --;            n --;        }        ListNode next;        // at this point, current prev is the final front we need to keep record        ListNode toConnectStart = prev;        ListNode toConnextEnd = cur;        while (n &gt; 0) {            next = cur.next;            cur.next = prev;            prev = cur;            cur = cur.next;            n --;        }        if (toConnectStart != null) {            toConnectStart.next = prev;        } else {            head = prev;        }        toConnextEnd.next = cur;        return head;    }}</code></pre><h1 id="LC-146-LRU-Cache"><a href="#LC-146-LRU-Cache" class="headerlink" title="LC-146 LRU Cache"></a>LC-146 LRU Cache</h1><ul><li>为了实现O(1)的get put请求，需要使用双向链表 + HashMap</li><li>Hashmap来跟踪在双向链表当中的key 和value</li></ul><pre><code>class LRUCache {    private Map&lt;Integer, LinkedNode&gt; cache = new HashMap&lt;&gt;();    private LinkedNode head, tail;    private int size;    private int capacity;    class LinkedNode {        int key;        int value;        LinkedNode prev;        LinkedNode next;    }    // addNode right after the head    private void addNode (LinkedNode node) {        node.prev = head;        node.next = head.next;        head.next.prev = node;        head.next = node;    }    private void moveToHead(LinkedNode node) {        removeNode(node);        addNode(node);    }    private void removeNode(LinkedNode node) {        LinkedNode prev = node.prev;        LinkedNode next = node.next;        prev.next = next;        next.prev = prev;    }    public LRUCache(int capacity) {        this.size = 0;        this.capacity = capacity;        head = new LinkedNode();        tail = new LinkedNode();        head.next = tail;        tail.prev = head;    }    private LinkedNode popTail() {        /**         * Pop the current tail.         */        LinkedNode res = tail.prev;        removeNode(res);        return res;    }    public int get(int key) {        LinkedNode node = cache.get(key);        if (node == null) return -1;        moveToHead(node);        return node.value;    }    public void put(int key, int value) {        LinkedNode node = cache.get(key);        if (node == null) {            LinkedNode newNode = new LinkedNode();            newNode.key = key;            newNode.value = value;            cache.put(key, newNode);            addNode(newNode);            ++size;          if(size &gt; capacity) {            // pop the tail            LinkedNode tail = popTail();            cache.remove(tail.key);            --size;          }        }  else {            // update the value.            node.value = value;            moveToHead(node);        }      }}/** * Your LRUCache object will be instantiated and called as such: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */</code></pre><h1 id="LC-234-Palindrome-Linked-List"><a href="#LC-234-Palindrome-Linked-List" class="headerlink" title="LC-234 Palindrome Linked List"></a>LC-234 Palindrome Linked List</h1><h2 id="Solution-1-递归"><a href="#Solution-1-递归" class="headerlink" title="Solution 1: 递归"></a>Solution 1: 递归</h2><ul><li><p>反向输出链表的递归逻辑</p><pre><code>function print_values_in_reverse(ListNode head)  if head is NOT null      print_values_in_reverse(head.next)      print head.val</code></pre></li><li><p>题解</p></li></ul><pre><code>class Solution {    private ListNode front;    private boolean recursiveCheck(ListNode cur) {        if (cur != null) {            if (!recursiveCheck(cur.next)) return false;            if (cur.val != front.val) return false;            front = front.next;        }        return true;    }    public boolean isPalindrome(ListNode head) {        front = head;        return recursiveCheck(head);    }}</code></pre><h2 id="Solution-2-反向后半链表来做比较"><a href="#Solution-2-反向后半链表来做比较" class="headerlink" title="Solution 2: 反向后半链表来做比较"></a>Solution 2: 反向后半链表来做比较</h2><pre><code>class Solution {    public boolean isPalindrome(ListNode head) {        if (head == null) return true;        // Find the end of first half and reverse second half.        ListNode firstHalfEnd = endOfFirstHalf(head);        ListNode secondHalfStart = reverseList(firstHalfEnd.next);        // Check whether or not there is a palindrome.        ListNode p1 = head;        ListNode p2 = secondHalfStart;        boolean result = true;        while (result &amp;&amp; p2 != null) {            if (p1.val != p2.val) result = false;            p1 = p1.next;            p2 = p2.next;        }                // Restore the list and return the result.        firstHalfEnd.next = reverseList(secondHalfStart);        return result;    }    // Taken from https://leetcode.com/problems/reverse-linked-list/solution/    private ListNode reverseList(ListNode head) {        ListNode prev = null;        ListNode curr = head;        while (curr != null) {            ListNode nextTemp = curr.next;            curr.next = prev;            prev = curr;            curr = nextTemp;        }        return prev;    }    private ListNode endOfFirstHalf(ListNode head) {        ListNode fast = head;        ListNode slow = head;        while (fast.next != null &amp;&amp; fast.next.next != null) {            fast = fast.next.next;            slow = slow.next;        }        return slow;    }}</code></pre><h1 id="LC-1474-Delete-N-nodes-after-M-nodes-of-a-linked-list"><a href="#LC-1474-Delete-N-nodes-after-M-nodes-of-a-linked-list" class="headerlink" title="LC-1474 Delete N nodes after M nodes of a linked list"></a>LC-1474 Delete N nodes after M nodes of a linked list</h1><ul><li><p>链表热身题</p></li><li><p>注意边界条件的判定，题中已给m和n的限定范围，所以不需要额外做判断了</p></li><li><p>对于n的周期性去除的node，注意可以不用额外空间，通过改变指针的指向即可</p><pre><code>class Solution {  public ListNode deleteNodes(ListNode head, int m, int n) {      ListNode cur = head;      ListNode last = head;      while (cur != null) {          int mCount = m;          int nCount = n;          while (cur != null &amp;&amp; mCount &gt; 0 ) {              last = cur;              cur = cur.next;              mCount --;          }          while (cur != null &amp;&amp; nCount &gt; 0) {              cur = cur.next;              nCount --;          }          last.next = cur;      }      return head;  }}</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;LC-23-Merge-K-Sorted-Lists&quot;&gt;&lt;a href=&quot;#LC-23-Merge-K-Sorted-Lists&quot; class=&quot;headerlink&quot; title=&quot;LC-23 Merge K Sorted Lists&quot;&gt;&lt;/a&gt;LC-23 Me
      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://www.llchen60.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="LinkedList" scheme="https://www.llchen60.com/tags/LinkedList/"/>
    
  </entry>
  
  <entry>
    <title>System Design Basics</title>
    <link href="https://www.llchen60.com/System-Design-Basics/"/>
    <id>https://www.llchen60.com/System-Design-Basics/</id>
    <published>2020-10-12T04:54:08.000Z</published>
    <updated>2020-11-02T17:05:55.381Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Key-Characteristics-of-Distributed-Systems"><a href="#1-Key-Characteristics-of-Distributed-Systems" class="headerlink" title="1. Key Characteristics of Distributed Systems"></a>1. Key Characteristics of Distributed Systems</h1><h2 id="1-1-Scalability"><a href="#1-1-Scalability" class="headerlink" title="1.1 Scalability"></a>1.1 Scalability</h2><ul><li><p>Capability of a system, process, or a network to grow and manage increased demand. Achieve the scaling without performance loss </p></li><li><p>Why need to scale?</p><ul><li>Increased data volume </li><li>Increased amount of work <ul><li>number of transactions </li></ul></li></ul></li><li><p>Performance curve </p><ul><li>Usually performance of a system would decline with the system size due to the management or environment cost <ul><li>network speed come to be slower because machines tend to be far apart from one another </li><li>some tasks may not be distributed, either because of their inherent atomic nature or because of some flaw in the system design </li></ul></li></ul></li></ul><ul><li><p>Horizontal vs Vertical Scaling </p><ul><li><p>Horizontal Scaling </p><ul><li>Scale by adding more servers into your pool of resources </li><li>Easier to scale dynamically by adding more machines into the existing pool </li></ul></li><li><p>Vertical scaling </p><ul><li><p>Scale by adding more power to an existing server </p><ul><li>CPU</li><li>RAM</li><li>Storage </li></ul></li><li><p>limited to the capacity of a single server, and scaling beyond that capacity often <strong>involves downtime</strong> and comes with an upper limit </p></li></ul></li></ul></li></ul><h2 id="1-2-Reliability"><a href="#1-2-Reliability" class="headerlink" title="1.2 Reliability"></a>1.2 Reliability</h2><ul><li>Probability a system will fail in a given period</li><li>A distributed system is considered reliable if it keeps delivering its services even when one or more software or hardware components fail </li><li>A reliable distributed system achieves this through redundancy of both the software components and data <ul><li>Eliminate every single point of failure</li></ul></li></ul><h2 id="1-3-Availability"><a href="#1-3-Availability" class="headerlink" title="1.3 Availability"></a>1.3 Availability</h2><ul><li>The time a system remains operational to perform its required function in s specific period </li><li>Percentage of time that a system, service, or a machine remains operational under normal conditions </li><li>Reliability  is availability over time considering the full range of possible real world conditions that can occur</li><li>If a system is reliable, it is available. However, if it is available, it is not necessarily reliable. In other words, high reliability contributes to high availability, but it is possible to achieve a high availability even with an unreliable product by minimizing repair time and ensuring that spares are always available when they are needed</li></ul><h2 id="1-4-Efficiency"><a href="#1-4-Efficiency" class="headerlink" title="1.4 Efficiency"></a>1.4 Efficiency</h2><ul><li><p>Standard measures of its efficiency </p><ul><li><p>Response time/ latency </p><ul><li>denotes the delay to obtain the first item </li></ul></li><li><p>Throughput / bandwidth</p><ul><li>number of items delivered in a given time unit </li></ul></li></ul></li><li><p>two measures above correspond to the following unit costs </p><ul><li>number of messages globally sent by the nodes of the system regardless of the message size </li><li>size of messages representing the volume of data exchanges </li></ul></li></ul><h2 id="1-5-Serviceability-Manageability"><a href="#1-5-Serviceability-Manageability" class="headerlink" title="1.5 Serviceability / Manageability"></a>1.5 Serviceability / Manageability</h2><ul><li>How easy it is to operate and maintain </li><li>Simplicity and speed with which a system can be repared or maintained </li><li>Ease of diagnosing and understanding problems when they occur, ease of making updates or modifications, </li></ul><h1 id="2-Load-Balancing"><a href="#2-Load-Balancing" class="headerlink" title="2. Load Balancing"></a>2. Load Balancing</h1><h2 id="2-1-What-is-Load-Balancer"><a href="#2-1-What-is-Load-Balancer" class="headerlink" title="2.1 What is Load Balancer?"></a>2.1 What is Load Balancer?</h2><ul><li><p>A critical component of any distributed system</p><ul><li><p>help to spread the traffic across a cluster of servers to improve responsiveness and availability of applications/ websites/ databases </p></li><li><p>also keep track of the status of all the resources while distributing requests </p><ul><li>if one server is not available to take new requests, not responding, has elevated error rate </li><li>LB will stop sending traffic to such server </li></ul></li><li><p>sit between client and the server accepting incoming network and application traffic, distribute traffic across multiple backend servers using various algorithm</p></li><li><p>could prevents any one application server from becoming a single point of failure </p></li></ul></li><li><p>where to add LB?</p><ul><li>between the user and the web server </li><li>between web servers and an internal platform layer, like application servers or cache servers </li><li>between internal platform layer and database <h2 id="2-2-Benefits"><a href="#2-2-Benefits" class="headerlink" title="2.2 Benefits"></a>2.2 Benefits</h2></li></ul></li><li><p>from user side </p><ul><li>faster, uninterrupted service; their requests could be immediately passed on to a more readily available resource </li></ul></li><li><p>from service provider side </p><ul><li>experience less downtime and higher throughput </li></ul></li><li><p>long term benefits</p><ul><li>smart load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen </li></ul></li></ul><h2 id="2-3-Load-Balancing-Algorithms"><a href="#2-3-Load-Balancing-Algorithms" class="headerlink" title="2.3 Load Balancing Algorithms"></a>2.3 Load Balancing Algorithms</h2><ul><li><p>How does the load balancer choose the backend server?</p><ul><li><ol><li>Make sure servers could respond appropriately to requests <ul><li>routinely do health check <ul><li>regularly attempt to connect to backend servers to ensure that servers are listening </li></ul></li></ul></li></ol></li><li><ol start="2"><li><p>Use pre configured algorithm to select one from the set of healthy servers </p><ul><li><p>Least connection Method </p><ul><li>direct traffic to the server with fewest active connections </li><li>useful when there are a large number of persistent client connections which are unevenly distributed between the servers</li></ul></li><li><p>Least Response Time Method </p><ul><li>direct traffic to the server with the fewest active connections and the lowest average response time </li></ul></li><li><p>Least Bandwidth Method </p><ul><li>selects the server that is currently serving the least amount of traffic measured in megabits per second (Mbps)</li></ul></li><li><p>Round Robin Method </p><ul><li>cycles through a list of servers and sends each new request to the next server </li><li>most useful when the servers are of equal specification and there are not many persistent connections</li></ul></li><li><p>weighted round robin </p><ul><li>desifned to better handle servers with different processing capacities </li><li>each server is assigned a weight which indicates the processing capacity </li></ul></li><li><p>IP Hash </p><ul><li>A hash of the IP address of the client is calculated to redirect the request to a server </li></ul></li></ul></li></ol></li></ul></li></ul><h2 id="2-4-Redundant-Load-Balancers"><a href="#2-4-Redundant-Load-Balancers" class="headerlink" title="2.4 Redundant Load Balancers"></a>2.4 Redundant Load Balancers</h2><ul><li>Load Balancer can be a single point of failure <ul><li>thus we need a second load balancer, to form a cluster </li><li>each LB monitors the health of the other</li><li>passive one could be switched to be active anytime since they keep monitoring same </li></ul></li></ul><h1 id="3-Caching"><a href="#3-Caching" class="headerlink" title="3. Caching"></a>3. Caching</h1><p>Caching enable you to make vastly better use of the resources you already have as well as make otherwise unattainable product requirements feasible。 </p><p>It takes advantage of the locality of reference principle: recently requested data is likely to be requested again, could be used in almost every layer of computing </p><h2 id="3-1-Application-Server-Cache"><a href="#3-1-Application-Server-Cache" class="headerlink" title="3.1 Application Server Cache"></a>3.1 Application Server Cache</h2><ul><li><p>place a cache directly on a request layer node </p></li><li><p>cache could be located both in memory and on the node’s local disk </p></li><li><p>One note here </p><ul><li>if expand this to many nodes, depends on your load balancer behavior, if it randomly distributes requests across the nodes, the same request will go to different nodes, thus increasing cache misses. <ul><li>could use either global caches or distributed caches for it </li></ul></li></ul></li></ul><h2 id="3-2-Content-Distribution-Network"><a href="#3-2-Content-Distribution-Network" class="headerlink" title="3.2 Content Distribution Network"></a>3.2 Content Distribution Network</h2><ul><li><p>For sites serving large amounts of static media </p></li><li><p>A typical workflow </p><ul><li>A request first ask the CDN for a piece of static media </li><li>CDN will serve the content if it has it locally available </li><li>If not, CDN will query the back end servers for the file </li><li>Then cache it locally, and serve it to the requesting user <h2 id="3-3-Cache-Invalidation"><a href="#3-3-Cache-Invalidation" class="headerlink" title="3.3 Cache Invalidation"></a>3.3 Cache Invalidation</h2></li></ul></li><li><p>Cache needs maintenance for keeping cache coherent with the source of truth</p><ul><li>if data is modified in db, should be invalidated in the cache </li></ul></li><li><p>Write Through Cache </p><ul><li>Data is written into the cache and the corresponding database at the same time </li><li>It could minimize the risk of data loss, but since every write operation mush be done twice before returning success to the client, latency would be higher </li></ul></li><li><p>Write Around Cache </p><ul><li>Data is written directly to permanent storage, bypassing the cache </li><li>Could reduce the cache being flooded with write operations that will not subsequently be re-read</li><li>But a read request for recently written data will create a cache miss </li></ul></li><li><p>Write Back Cache </p><ul><li>Data is written to cache alone</li><li>Write to permanent storage is done after specified intervals or under certain conditions </li><li>Low latency and high throughput for write intensive applications </li><li>However this speed up could cause issue of data loss in case of a crash or other adverse event because the only copy of the written data is in the cache </li></ul></li></ul><h2 id="3-4-Cache-Eviction-Policies"><a href="#3-4-Cache-Eviction-Policies" class="headerlink" title="3.4 Cache Eviction Policies"></a>3.4 Cache Eviction Policies</h2><ul><li>First In First Out </li><li>Last In First Out </li><li>Least Recently Used </li><li>Most Recently Used </li><li>Least Frequently Used </li><li>Randowm Replacement </li></ul><h1 id="4-Data-Partitioning"><a href="#4-Data-Partitioning" class="headerlink" title="4. Data Partitioning"></a>4. Data Partitioning</h1><p>It aims to break up a big database into many smaller parts. It’s a process of splitting up a DB/ table across multiple machines to improve the manageability, performance, availability, and load balancing of an application. </p><p>The justification for data partitioning is after a certain scale point, it’s cheaper and more feasible to scale horizontally by adding more machines that to grow it vertically by adding beefier servers </p><h2 id="4-1-Partitioning-Methods"><a href="#4-1-Partitioning-Methods" class="headerlink" title="4.1 Partitioning Methods"></a>4.1 Partitioning Methods</h2><h3 id="4-1-1-Horizontal-Partitioning-Sharding"><a href="#4-1-1-Horizontal-Partitioning-Sharding" class="headerlink" title="4.1.1 Horizontal Partitioning/ Sharding"></a>4.1.1 Horizontal Partitioning/ Sharding</h3><ul><li><p>Put different rows into different tables </p></li><li><p>range based partitioning as we store different ranges of data in separate tables </p></li><li><p>Probelm here</p><ul><li>is the range value isn’t chosen carefully, the partitioning scheme will lead to unbalanced servers <h3 id="4-1-2-Vertical-Partitioning"><a href="#4-1-2-Vertical-Partitioning" class="headerlink" title="4.1.2 Vertical Partitioning"></a>4.1.2 Vertical Partitioning</h3></li></ul></li><li><p>store data related to a specific feature in their own server </p><ul><li>like photo in one server, video in another, people they follow in another </li></ul></li><li><p>not quite scalable, if our app experience some high traffic, then the single server will not be enough to handle such traffic </p></li></ul><h3 id="4-1-3-Directory-Based-Partitioning"><a href="#4-1-3-Directory-Based-Partitioning" class="headerlink" title="4.1.3 Directory Based Partitioning"></a>4.1.3 Directory Based Partitioning</h3><ul><li>Create a lookup service which knows your current partitioning scheme and abstracts it away from the DB access code </li><li>To find a particular data entity, query the directory server that holds the mapping between each tuple key to its DB server</li></ul><h2 id="4-2-Partitioning-Criteria"><a href="#4-2-Partitioning-Criteria" class="headerlink" title="4.2 Partitioning Criteria"></a>4.2 Partitioning Criteria</h2><ul><li><p>Key or hash based partitioning </p><ul><li>apply a hash function to some key attributes of the entity we are storing </li><li>need to ensure a uniform allocation of data among servers </li><li>it will change the hash function when every time you add / remove some servers, the workaround is to use consistent hashing </li></ul></li><li><p>List Partitioning </p><ul><li>each partition is assigned a list of values </li></ul></li><li><p>Round robind partitioning </p></li><li><p>Composite Partitioning </p><ul><li>combination of criteria above</li></ul></li></ul><h2 id="4-3-Common-Problems-of-Data-Partitioning"><a href="#4-3-Common-Problems-of-Data-Partitioning" class="headerlink" title="4.3 Common Problems of Data Partitioning"></a>4.3 Common Problems of Data Partitioning</h2><ul><li><p>Joins and Denormalization </p><ul><li>Performing joins on a database that runs on several different servers </li><li>will not be performance efficient </li><li>Workaround is to denormalize database so queries perviously requiring joins can be performed from a single table <ul><li>but need to deal with data inconsistency issue </li></ul></li></ul></li><li><p>Referential Integrity </p><ul><li>nforce data integrity constraints such as foreign keys in a partitioned database can be extremely difficult</li></ul></li><li><p>Rebalancing </p><ul><li><p>need to do that due to </p><ul><li>data distribution is not uniform </li><li>could be a lot of load on one partition </li></ul></li><li><p>In such cases, either we have to create more DB partitions or have to rebalance existing partitions, which means the partitioning scheme changed and all existing data moved to new locations. Doing this without incurring downtime is extremely difficult. Using a scheme like directory based partitioning does make rebalancing a more palatable experience at the cost of increasing the complexity of the system and creating a new single point of failure </p></li></ul></li></ul><h1 id="5-Indexes"><a href="#5-Indexes" class="headerlink" title="5. Indexes"></a>5. Indexes</h1><p>Leverage on indexes when current database performance is no longer satisfactory. Indexing could help make search faster, it could be created using one or more columns of a ddb table, providing the basis for both rapid random lookups and efficient access of ordered records. </p><p>Index can dramatically speed up data retrieval but may itself be large due to the additional keys, which will slow down data insertion and update. </p><p>When adding rows or making updates to existing rows for a table with an active index, we not only have to write the data but also have to update the index. This will decrease the write performance. </p><p>This performance degradation applies to all insert, update, and delete operations for the table. For this reason, adding unnecessary indexes on tables should be avoided and indexes that are no longer used should be removed.</p><p>If the goal of ddb is often written to and rarely read from, in that case, decreasing the performance of the more common operation, which is writing, is probably not worth the increase in performance we get from reading.</p><h1 id="6-Proxies"><a href="#6-Proxies" class="headerlink" title="6. Proxies"></a>6. Proxies</h1><h2 id="6-1-What-is-Proxy-Server"><a href="#6-1-What-is-Proxy-Server" class="headerlink" title="6.1 What is Proxy Server?"></a>6.1 What is Proxy Server?</h2><ul><li><p>Intermediate server between the client and the backend server </p></li><li><p>Clients connect to proxy servers to make a request for a service like</p><ul><li>web page</li><li>file connection </li></ul></li><li><p>Proxy server is a piece of software or hardware that acts as an intermediary for requests from clients seeking resources from other servers </p></li><li><p>Proxy are used to </p><ul><li><p>filter requests </p></li><li><p>transform requests </p><ul><li>add/ remove headers </li><li>encrypt and decrypt</li><li>compress a resource </li></ul></li><li><p>caching </p><ul><li>if multiple clients access a particular resource, the proxy server can cache it and serve it to all clients without going to the remote server </li></ul></li></ul></li></ul><h2 id="6-2-Types"><a href="#6-2-Types" class="headerlink" title="6.2 Types"></a>6.2 Types</h2><ul><li><p>Open Proxy </p><ul><li>A proxy server that is accessible by any internet user </li><li>type<ul><li>anonymous proxy <ul><li>reveals its identity as a server but does not disclose the initial IP address </li></ul></li><li>transparent proxy <ul><li>Identify itself and with the suppot of HTTP headers </li><li>IP address could be viewed </li><li>main benefit of using this sort of server is its ability to cache the websites </li></ul></li></ul></li></ul></li><li><p>reverse proxy </p><ul><li>retrieve resources on behalf of a client from one or more servers </li><li>these resources are then returned to the client, appearing as if they originated from the proxy server itself </li></ul></li></ul><h1 id="7-Redundancy-and-Replication"><a href="#7-Redundancy-and-Replication" class="headerlink" title="7. Redundancy and Replication"></a>7. Redundancy and Replication</h1><ul><li>redundancy <ul><li>duplication of critical components or functions of a system with the intention of increasing the reliability of the system <ul><li>backup</li><li>fail saft </li><li>direct improvement on actual system performance </li></ul></li></ul></li></ul><ul><li><p>replication </p><ul><li><p>shareing information to ensure consistency between redundant resources</p><ul><li>to improve reliability</li><li>fault tolerance</li><li>accessibility </li></ul></li><li><p>primary replica relationship </p><ul><li>primary server gets all the updates </li><li>then ripple through to the replica servers </li><li>each replica outputs a message stating that it has received the update successfully <h1 id="8-SQL-vs-NoSQL"><a href="#8-SQL-vs-NoSQL" class="headerlink" title="8. SQL vs NoSQL"></a>8. SQL vs NoSQL</h1></li></ul></li></ul></li></ul><h2 id="8-1-Concepts"><a href="#8-1-Concepts" class="headerlink" title="8.1 Concepts"></a>8.1 Concepts</h2><ul><li><p>Relational databases </p><ul><li>structured </li><li>predefiend schemas </li></ul></li><li><p>Non-relational database </p><ul><li>unstrutured </li><li>distributed</li><li>dynamic schema </li></ul></li></ul><ul><li><p>SQL</p><ul><li><p>store data in rows and columns </p></li><li><p>each row contains:</p><ul><li>all the info about one entity </li></ul></li><li><p>each column contains:</p><ul><li>all separate data points </li></ul></li></ul></li><li><p>NoSQL</p><ul><li><p>Key-Value Stores </p><ul><li>store in an arry of key value pairs</li><li>key is an attribute name which is linked to a vlue <ul><li>redis</li><li>voldemort</li><li>dynamo</li></ul></li></ul></li><li><p>document database</p><ul><li>data is stored in documents (instead of rows and columns in a table)</li><li>documents are grouped together in collections </li><li>each document can have an entirely different structure</li></ul></li><li><p>wide column databases</p><ul><li>have column families, which are containers for rows </li><li>no need to know all the columns up front and each row doesn’t have to have the same number of columns</li><li>best suited for analyzing large datasets </li><li>type <ul><li>HBase</li><li>Cassandra</li></ul></li></ul></li><li><p>Graph Database</p><ul><li>used to store data whose relations are best represented in a graph </li><li>data is saved in graph structures with:<ul><li>nodes <ul><li>entities</li></ul></li><li>properties<ul><li>information about the entities </li></ul></li><li>lines <ul><li>connections between the entities</li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="8-2-Differences-between-SQL-and-NoSQL"><a href="#8-2-Differences-between-SQL-and-NoSQL" class="headerlink" title="8.2 Differences between SQL and NoSQL"></a>8.2 Differences between SQL and NoSQL</h2><ul><li><p>Storage</p><ul><li><p>SQL </p><ul><li>each row represents an entity </li><li>each column represents a data point about the entity </li></ul></li><li><p>NoSQL</p><ul><li>could be key value</li><li>document</li><li>graph </li></ul></li></ul></li><li><p>Schema</p><ul><li><p>SQL</p><ul><li>each record conforms to a fixed schema <ul><li>columns must be decided and chosen before data entry </li><li>each row must have data for each column </li></ul></li><li>schema modification need to involve modifying the whole database and go offline </li></ul></li><li><p>NoSQL</p><ul><li>schemas are dynamic </li><li>columns can be added on the fly, and each row doesn’t have to contain data for each column</li></ul></li></ul></li><li><p>Query </p><ul><li><p>SQL </p><ul><li>Structured query language for defining and manipulating the data </li></ul></li><li><p>NoSQL </p><ul><li>Query focus on a collection of documents </li><li>UnQL - unstructured query language </li></ul></li></ul></li></ul><ul><li><p>Scalability</p><ul><li><p>SQL</p><ul><li>vetically scalable <ul><li>by increase the horsepower (memory, CPU, etc.) of the hardware </li></ul></li></ul></li><li><p>NoSQL</p><ul><li>horizontally scalable <ul><li>we could add more servers easily in database infrastructure to handle more traffic </li><li>any cheap hardware could host NoSQL database</li></ul></li></ul></li></ul></li><li><p>Reliability or ACID comliancy</p><ul><li><p>ACID</p><ul><li>atomocity</li><li>consistency</li><li>isolation</li><li>durability </li></ul></li><li><p>most NoSQL solutions sacrifice ACID compliance for performance and scalability </p></li></ul></li></ul><h2 id="8-3-Choose-which-one"><a href="#8-3-Choose-which-one" class="headerlink" title="8.3 Choose which one?"></a>8.3 Choose which one?</h2><ul><li>Reasons for use SQL<ul><li>Need ACID compliance <ul><li>ACID reduces anomalies and protects the integrity of your db by prescribing exactly how transactions interact with the database </li></ul></li><li>Data is structured and unchanging </li></ul></li></ul><ul><li><p>Reasons for use NoSQL </p><ul><li><p>Store large volumens of data that often have little to no structure </p><ul><li>NoSQL allows use to add new types </li><li>with document based databases, you can store data in one place without having to define what types of data those are in advance </li></ul></li><li><p>Making the most of cloud computing and storage </p><ul><li>cloud based storage requires data to be easily spread across multiple servers to scale up </li></ul></li><li><p>Rapid development </p></li></ul></li></ul><h1 id="9-CAP-Theorem"><a href="#9-CAP-Theorem" class="headerlink" title="9. CAP Theorem"></a>9. CAP Theorem</h1><p>CAP states it’s impossible for a distributed software system to simultaneously provide more than two out of three of the following gurantees:</p><ul><li>consistency </li><li>availability </li><li>partition tolerance </li></ul><p>CAP say when designing a distributed system we could only pick two of them: </p><ul><li><p>consistency </p><ul><li>all nodes see the same data at the same time </li><li>consistency is achieved by updating several nodes before allowing further reads </li></ul></li><li><p>availability </p><ul><li>every requests get a response on success/ failure </li><li>availability is achieved by replicating the data across different servers </li></ul></li><li><p>partition tolerance </p><ul><li>system continues to work despite msg loss or partial failure </li><li>data is sufficiently replicated across combinations of nodes and networks to keep the system up through intermittent outages </li></ul></li></ul><p>We cannot build a general data store that is continually available, sequentially consistent, and tolerant to any partition failures. We can only build a system that has any two of these three properties. Because, to be consistent, all nodes should see the same set of updates in the same order. But if the network loses a partition, updates in one partition might not make it to the other partitions before a client reads from the out-of-date partition after having read from the up-to-date one. The only thing that can be done to cope with this possibility is to stop serving requests from the out-of-date partition, but then the service is no longer 100% available.</p><h1 id="10-Consistent-Hashing"><a href="#10-Consistent-Hashing" class="headerlink" title="10. Consistent Hashing"></a>10. Consistent Hashing</h1><h2 id="10-1-Existing-Hash-Funciton"><a href="#10-1-Existing-Hash-Funciton" class="headerlink" title="10.1 Existing Hash Funciton"></a>10.1 Existing Hash Funciton</h2><p>Distributed Hash Table is super important in distributed scalable systems. Hash Tables need a key, a value, and a hash function where hash function maps the key to a location where the value is stored. </p><p>A common thought of hash function would be key%n, but it has several drawbacks:</p><ol><li><p>Not horizontally scalable </p><ol><li>whenever a new cache host is added to the system, all existing mappings are broken </li></ol></li><li><p>Not load balanced </p><ol><li>expecially for non-uniformly distributed data </li><li>some caches would come to be hot and saturated while the others idel and are almost empty </li></ol></li></ol><h2 id="10-2-Consistent-Hashing"><a href="#10-2-Consistent-Hashing" class="headerlink" title="10.2 Consistent Hashing"></a>10.2 Consistent Hashing</h2><ul><li>want to minimize reorganization when nodes are added or removed </li><li>when the hash table is resized  only k/n keys need to be remapped where k is the total number of keys and n is the total number of servers </li><li>objects are mapped to the same host if possible </li></ul><ul><li>how it works <ul><li>map a key to an integer</li><li>all integers are placed on a ring such that the values are wrapped around </li><li>each object is assigned to the next server that appears o nthe circle in clockwise order  –&gt; provide an even distribution of objects to servers </li><li>if a server fails and is removed from the circle, only the objects that were mapped to the failed server need to be reassigned to the next server in clockwise order </li></ul></li></ul><h1 id="11-Long-Polling-WebSockets-Server-Sent-Events"><a href="#11-Long-Polling-WebSockets-Server-Sent-Events" class="headerlink" title="11. Long-Polling, WebSockets, Server-Sent Events"></a>11. Long-Polling, WebSockets, Server-Sent Events</h1><p>Long Polling, WebSockets and Server Sent events are popular communication protocols between a client like web browser and a web server </p><h2 id="11-1-Ajax-Polling"><a href="#11-1-Ajax-Polling" class="headerlink" title="11.1 Ajax Polling"></a>11.1 Ajax Polling</h2><ul><li><p>Client repeatedly polls a server for data</p></li><li><p>Client make a request and wait for the server to respond with data. If no data available, an empty response is returned </p></li><li><p>whole workflow</p><ul><li>client opens a connection and requests data from the server using regular HTTP </li><li>the requested webpage sends requests to the server at regular intervals </li><li>the server calculates the response and sends it back, like regular HTTP traffic </li><li>the client repeats the above three steps periodically to get updates from the server </li></ul></li></ul><ul><li>pitfall<ul><li>Polling let client continue to ask server for any new data, as a result, a lot of responses are empty, creating HTTP overhead </li></ul></li></ul><h2 id="11-2-HTTP-Long-Polling"><a href="#11-2-HTTP-Long-Polling" class="headerlink" title="11.2 HTTP Long-Polling"></a>11.2 HTTP Long-Polling</h2><ul><li>Workflow <ul><li>If the server does not have any data available for the client, instead of sending an empty response, the server holds the request and waits until some data becomes available </li><li>once available, a full response is sent to the client. Client then immediately request information from the server so that the server will almost always have an available waiting request that it can use to deliver data in response to an event </li></ul></li></ul><ul><li>The client makes an initial request using regular HTTP and then waits for a response.</li><li>The server delays its response until an update is available or a timeout has occurred.</li><li>When an update is available, the server sends a full response to the client.</li><li>The client typically sends a new long-poll request, either immediately upon receiving a response or after a pause to allow an acceptable latency period.</li><li>Each Long-Poll request has a timeout. The client has to reconnect periodically after the connection is closed due to timeouts.</li></ul><h2 id="11-3-WebSockets"><a href="#11-3-WebSockets" class="headerlink" title="11.3 WebSockets"></a>11.3 WebSockets</h2><ul><li>Provides Full duplex communication channels over a single TCP connection. </li><li>Provides a persistent connection between a client and a server that both parties can use to start sending data at any time. </li><li>lower overhead, real time data transfer <ul><li>it provides a standardized way for the server to send content to the browser without being asked by the client and allowing for messages to be passed back and forth while keeping the connection open </li></ul></li><li>Workflow <ul><li>Client establish a websocket connection through a process known as the WebSocket handshake</li><li>if the process succeeds server and client can exchange data in both directions at any time </li></ul></li></ul><h2 id="11-4-Server-Sent-Events-SSEs"><a href="#11-4-Server-Sent-Events-SSEs" class="headerlink" title="11.4 Server Sent Events - SSEs"></a>11.4 Server Sent Events - SSEs</h2><ul><li>Client establish a persistent and long term connection with the server </li><li>Server use this connection to send data to a client </li><li>But client would need another tech/ protocol to send data to the server </li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://en.wikipedia.org/wiki/Consistent_hashing#:~:text=In%20computer%20science%2C%20consistent%20hashing,is%20the%20number%20of%20slots" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Consistent_hashing#:~:text=In%20computer%20science%2C%20consistent%20hashing,is%20the%20number%20of%20slots</a>. </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Key-Characteristics-of-Distributed-Systems&quot;&gt;&lt;a href=&quot;#1-Key-Characteristics-of-Distributed-Systems&quot; class=&quot;headerlink&quot; title=&quot;1. K
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>System Design General Guides - from Grokking SDI</title>
    <link href="https://www.llchen60.com/System-Design-General-Guides-from-Grokking-SDI/"/>
    <id>https://www.llchen60.com/System-Design-General-Guides-from-Grokking-SDI/</id>
    <published>2020-10-09T00:55:45.000Z</published>
    <updated>2020-10-09T00:56:11.183Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Requirements-Clarifications"><a href="#1-Requirements-Clarifications" class="headerlink" title="1. Requirements Clarifications"></a>1. Requirements Clarifications</h1><ul><li>Ask about the exact scope of the problem <ul><li>For a twitter like system design <ul><li>will users of our service be able to post tweets and follow others? </li><li>Create and display user’s timeline?</li><li>Tweets contain photos and videos?</li><li>Front end design too?</li><li>Searchable tweets?</li><li>Display hot trending topics?</li><li>Push notification for new tweets? </li></ul></li></ul></li></ul><h1 id="2-Back-of-the-envelope-estimation"><a href="#2-Back-of-the-envelope-estimation" class="headerlink" title="2. Back of the envelope estimation"></a>2. Back of the envelope estimation</h1><ul><li><p>estimate the scale of the system we are going to design </p><ul><li><p>could help later when we will be focusing on scaling, partitioning, load balancing and caching</p><ul><li><p>what scale is expected </p><ul><li>number of new tweets</li><li>number of tweet views</li><li>number of timeline generations per sec </li></ul></li><li><p>how much storage will we need</p><ul><li>videos and photos will play important role on our estimation </li></ul></li><li><p>what network bandwidth usage are we expecting</p><ul><li>crucial in deciding how we will manage traffic and balance load between servers </li></ul></li></ul></li></ul></li></ul><h1 id="3-System-Interface-Definition"><a href="#3-System-Interface-Definition" class="headerlink" title="3. System Interface Definition"></a>3. System Interface Definition</h1><p>Establish exact contract expected from the system </p><pre><code>postTweet(user_id, tweet_data, tweet_location, user_location, timestamp, …)  generateTimeline(user_id, current_time, user_location, …)  markTweetFavorite(user_id, tweet_id, timestamp, …)  </code></pre><h1 id="4-Define-Data-Model"><a href="#4-Define-Data-Model" class="headerlink" title="4. Define Data Model"></a>4. Define Data Model</h1><p>Define the data model, to clarify how data will flow between different components of the system. Later, will guide for data partitioning and management. </p><p>User: UserID, Name, Email, DoB, CreationData, LastLogin, etc.<br>Tweet: TweetID, Content, TweetLocation, NumberOfLikes, TimeStamp, etc.<br>UserFollow: UserID1, UserID2<br>FavoriteTweets: UserID, TweetID, TimeStamp</p><h1 id="5-High-level-design"><a href="#5-High-level-design" class="headerlink" title="5. High level design"></a>5. High level design</h1><ul><li>Draw a block diagram with 5 - 6 boxes representing the core components of our system. We need to identify enough components that are needed to solve the actual problem from end2end. </li><li>For twitter <ul><li>Multiple application servers to serve all the read/ write requests with load balancers in front of them for traffic distributions </li><li>we could have separate servers for handling specific traffic (like much more read than write)</li><li>Efficient database that can store all the tweets and can support a huge number of reads </li><li>Also need a distributed file storage system for storing photos and videos </li></ul></li></ul><h1 id="6-Detailed-Design"><a href="#6-Detailed-Design" class="headerlink" title="6. Detailed Design"></a>6. Detailed Design</h1><ul><li>Dig deeper to 2 or 3 components <ul><li>present different approaches</li><li>pros and cons </li><li>explain why we prefer one on the other </li><li>consider tradeoffs between different options while keeping system contraints in mind </li></ul></li></ul><ul><li>Question Examples<ul><li>Since we will be storing a massive amount of data, how should we partition our data to distribute it to multiple databases? Should we try to store all the data of a user on the same database? What issue could it cause?</li><li>How will we handle hot users who tweet a lot or follow lots of people?</li><li>Since users’ timeline will contain the most recent (and relevant) tweets, should we try to store our data in such a way that is optimized for scanning the latest tweets?</li><li>How much and at which layer should we introduce cache to speed things up?</li><li>What components need better load balancing?</li></ul></li></ul><h1 id="7-Identifying-and-resolving-bottlenecks"><a href="#7-Identifying-and-resolving-bottlenecks" class="headerlink" title="7 Identifying and resolving bottlenecks"></a>7 Identifying and resolving bottlenecks</h1><ul><li>Discuss as many bottlenecks as possible and different approaches to mitigate them <ul><li>Any single point of failure in our system? </li><li>Do we have enough replicas of the data so that if we lose a few servers, we can still serve our users? </li><li>Do we have enough copies of different services running such that a fewe failures will not cause a total system shutdown?</li><li>How to monitor performance of our service? </li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Requirements-Clarifications&quot;&gt;&lt;a href=&quot;#1-Requirements-Clarifications&quot; class=&quot;headerlink&quot; title=&quot;1. Requirements Clarifications&quot;&gt;&lt;/
      
    
    </summary>
    
    
      <category term="SystemDesign" scheme="https://www.llchen60.com/categories/SystemDesign/"/>
    
    
  </entry>
  
  <entry>
    <title>缓存的常见问题</title>
    <link href="https://www.llchen60.com/%E7%BC%93%E5%AD%98%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>https://www.llchen60.com/%E7%BC%93%E5%AD%98%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/</id>
    <published>2020-10-06T22:55:09.000Z</published>
    <updated>2020-10-06T22:55:54.435Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-缓存雪崩"><a href="#1-缓存雪崩" class="headerlink" title="1. 缓存雪崩"></a>1. 缓存雪崩</h1><p>缓存系统的IOPS比数据库高很多，因此需要注意短时间内的大量缓存失效的情况。这种情况一旦发生，可能就会在瞬间有大量的数据需要回源到数据库查询，对数据库造成极大的压力，极限情况下甚至导致后端数据库直接崩溃。这就是我们说的缓存雪崩。</p><p>产生雪崩的原因有两种：</p><ul><li>缓存系统本身不可用，导致大量请求直接回源到数据库</li><li>应用设计层面大量的Key在同一时间过期，导致大量的数据回源<ul><li>解决方法<ul><li>差异化缓存过期时间<ul><li>不让大量的Key在同一时间过期</li><li>初始化缓存的时候，设置缓存的过期时间为30秒 + 10秒以内的随机延迟（扰动值）。这样key就不会集中在30秒这个时刻过期，而是会分散在30 - 40秒之间</li></ul></li><li>让缓存不主动过期<ul><li>初始化缓存数据的时候设置缓存永不过期，然后启动一个后台线程30秒一次定时将所有数据更新到缓存，通过适当的休眠，控制从数据库更新数据的频率，降低数据库的压力</li><li>如果无法全量缓存所有的数据，那么就无法使用该种方案</li></ul></li></ul></li></ul></li></ul><pre><code>// 差异化缓存过期时间@PostConstructpublic void rightInit1() {    //这次缓存的过期时间是30秒+10秒内的随机延迟    IntStream.rangeClosed(1, 1000).forEach(i -&gt; stringRedisTemplate.opsForValue().set(&quot;city&quot; + i, getCityFromDb(i), 30 + ThreadLocalRandom.current().nextInt(10), TimeUnit.SECONDS));    log.info(&quot;Cache init finished&quot;);    //同样1秒一次输出数据库QPS：   Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {        log.info(&quot;DB QPS : {}&quot;, atomicInteger.getAndSet(0));    }, 0, 1, TimeUnit.SECONDS);}// 让缓存不主动过期@PostConstructpublic void rightInit2() throws InterruptedException {    CountDownLatch countDownLatch = new CountDownLatch(1);    //每隔30秒全量更新一次缓存     Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {        IntStream.rangeClosed(1, 1000).forEach(i -&gt; {            String data = getCityFromDb(i);            //模拟更新缓存需要一定的时间            try {                TimeUnit.MILLISECONDS.sleep(20);            } catch (InterruptedException e) { }            if (!StringUtils.isEmpty(data)) {                //缓存永不过期，被动更新                stringRedisTemplate.opsForValue().set(&quot;city&quot; + i, data);            }        });        log.info(&quot;Cache update finished&quot;);        //启动程序的时候需要等待首次更新缓存完成        countDownLatch.countDown();    }, 0, 30, TimeUnit.SECONDS);    Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {        log.info(&quot;DB QPS : {}&quot;, atomicInteger.getAndSet(0));    }, 0, 1, TimeUnit.SECONDS);    countDownLatch.await();}</code></pre><h1 id="2-缓存击穿"><a href="#2-缓存击穿" class="headerlink" title="2. 缓存击穿"></a>2. 缓存击穿</h1><p>在某些Key属于极端热点数据并且并发量很大的情况下，如果这个Key过期，可能会在某个瞬间出现大量的并发请求同时回源，相当于大量的并发请求直接打到了数据库。这就是我们所说的缓存击穿或缓存并发问题。</p><p>如果说回源操作比较昂贵的话，那么这种并发就不能忽略不计了。可以考虑使用锁机制来限制回源的并发，或者可以使用类似Semaphore的工具来限制并发数，比如限制为10.这样子既限制了回源并发数不至于太大，又能够使得一定量的线程可以同时回源。</p><h1 id="3-缓存穿透"><a href="#3-缓存穿透" class="headerlink" title="3. 缓存穿透"></a>3. 缓存穿透</h1><p>缓存穿透指的是实际上缓存里有key 和value，但是其value可能为空，如果没做正确处理，那我们的逻辑可能会认为没有对当前的key做好缓存，会对所有的请求都回源到数据库上，这就会给数据库造成压力了。</p><p>针对这种问题，可以用以下方案解决：</p><ul><li><p>对于不存在的数据，同样设置一个特殊的Value到缓存当中，比如NODATA，这样子就不会有缓存穿透的问题</p><ul><li>可能会将大量无效的数据加入到缓存当中</li></ul></li><li><p>使用布隆过滤器</p><ul><li>放在缓存数据读取前先进行过滤操作</li><li>Google Guava BloomFilter </li></ul></li></ul><pre><code>private BloomFilter&lt;Integer&gt; bloomFilter;@PostConstructpublic void init() {    //创建布隆过滤器，元素数量10000，期望误判率1%    bloomFilter = BloomFilter.create(Funnels.integerFunnel(), 10000, 0.01);    //填充布隆过滤器    IntStream.rangeClosed(1, 10000).forEach(bloomFilter::put);}@GetMapping(&quot;right2&quot;)public String right2(@RequestParam(&quot;id&quot;) int id) {    String data = &quot;&quot;;    //通过布隆过滤器先判断    if (bloomFilter.mightContain(id)) {        String key = &quot;user&quot; + id;        //走缓存查询        data = stringRedisTemplate.opsForValue().get(key);        if (StringUtils.isEmpty(data)) {            //走数据库查询            data = getCityFromDb(id);            stringRedisTemplate.opsForValue().set(key, data, 30, TimeUnit.SECONDS);        }    }    return data;}</code></pre><h1 id="4-缓存数据同步策略"><a href="#4-缓存数据同步策略" class="headerlink" title="4. 缓存数据同步策略"></a>4. 缓存数据同步策略</h1><ul><li>当原始数据被修改了以后，我们很可能会采用主动更新缓存的策略<ul><li>可能的策略有<ul><li>先更新缓存，再更新数据库<ul><li>不可行</li><li>数据库操作失败是有可能的，会导致缓存和数据库当中的数据不一致</li></ul></li><li>先更新数据库，再更新缓存<ul><li>不可行</li><li>多线程情况下数据库中更新的顺序和缓存更新的顺序会不同，可能会导致旧数据最后到，导致问题的出现</li></ul></li><li>先删除缓存，再更新数据库，访问的时候按需加载数据到缓存当中<ul><li>很可能删除缓存以后还没来得及更新数据库，就有另外一个线程先读取了旧值到缓存中</li></ul></li><li>先更新数据库，再删除缓存，访问的时候按需加载数据到缓存当中<ul><li>出现缓存不一致的概率很低</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-缓存雪崩&quot;&gt;&lt;a href=&quot;#1-缓存雪崩&quot; class=&quot;headerlink&quot; title=&quot;1. 缓存雪崩&quot;&gt;&lt;/a&gt;1. 缓存雪崩&lt;/h1&gt;&lt;p&gt;缓存系统的IOPS比数据库高很多，因此需要注意短时间内的大量缓存失效的情况。这种情况一旦发生，可能就会在
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
  </entry>
  
  <entry>
    <title>利用注解 + 反射消除重复代码</title>
    <link href="https://www.llchen60.com/%E5%88%A9%E7%94%A8%E6%B3%A8%E8%A7%A3-%E5%8F%8D%E5%B0%84%E6%B6%88%E9%99%A4%E9%87%8D%E5%A4%8D%E4%BB%A3%E7%A0%81/"/>
    <id>https://www.llchen60.com/%E5%88%A9%E7%94%A8%E6%B3%A8%E8%A7%A3-%E5%8F%8D%E5%B0%84%E6%B6%88%E9%99%A4%E9%87%8D%E5%A4%8D%E4%BB%A3%E7%A0%81/</id>
    <published>2020-10-04T00:37:32.000Z</published>
    <updated>2020-10-04T00:38:18.211Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-案例分析"><a href="#1-案例分析" class="headerlink" title="1. 案例分析"></a>1. 案例分析</h1><h2 id="1-1-案例场景"><a href="#1-1-案例场景" class="headerlink" title="1.1 案例场景"></a>1.1 案例场景</h2><p>假设银行提供了一些 API 接口，对参数的序列化有点特殊，不使用 JSON，而是需要我们把参数依次拼在一起构成一个大字符串</p><ul><li>按照银行提供的API文档顺序，将所有的参数构成定长的数据，并且拼接在一起作为一整个字符串</li><li>因为每一种参数都有固定长度，未达到长度需要进行填充处理<ul><li>字符串类型参数不满长度部分要以下划线右填充，即字符串内容靠左</li><li>数字类型的参数不满长度部分以0左填充，即实际数字靠右</li><li>货币类型的表示需要把金额向下舍入2位到分，以分为单位，作为数字类型同样进行左填充</li><li>参数做MD5 操作作为签名</li></ul></li></ul><h2 id="1-2-初步代码实现"><a href="#1-2-初步代码实现" class="headerlink" title="1.2 初步代码实现"></a>1.2 初步代码实现</h2><pre><code>public class BankService {    //创建用户方法    public static String createUser(String name, String identity, String mobile, int age) throws IOException {        StringBuilder stringBuilder = new StringBuilder();        //字符串靠左，多余的地方填充_        stringBuilder.append(String.format(&quot;%-10s&quot;, name).replace(&#39; &#39;, &#39;_&#39;));        //字符串靠左，多余的地方填充_        stringBuilder.append(String.format(&quot;%-18s&quot;, identity).replace(&#39; &#39;, &#39;_&#39;));        //数字靠右，多余的地方用0填充        stringBuilder.append(String.format(&quot;%05d&quot;, age));        //字符串靠左，多余的地方用_填充        stringBuilder.append(String.format(&quot;%-11s&quot;, mobile).replace(&#39; &#39;, &#39;_&#39;));        //最后加上MD5作为签名        stringBuilder.append(DigestUtils.md2Hex(stringBuilder.toString()));        return Request.Post(&quot;http://localhost:45678/reflection/bank/createUser&quot;)                .bodyString(stringBuilder.toString(), ContentType.APPLICATION_JSON)                .execute().returnContent().asString();    }    //支付方法    public static String pay(long userId, BigDecimal amount) throws IOException {        StringBuilder stringBuilder = new StringBuilder();        //数字靠右，多余的地方用0填充        stringBuilder.append(String.format(&quot;%020d&quot;, userId));        //金额向下舍入2位到分，以分为单位，作为数字靠右，多余的地方用0填充        stringBuilder.append(String.format(&quot;%010d&quot;, amount.setScale(2, RoundingMode.DOWN).multiply(new BigDecimal(&quot;100&quot;)).longValue()));        //最后加上MD5作为签名        stringBuilder.append(DigestUtils.md2Hex(stringBuilder.toString()));        return Request.Post(&quot;http://localhost:45678/reflection/bank/pay&quot;)                .bodyString(stringBuilder.toString(), ContentType.APPLICATION_JSON)                .execute().returnContent().asString();    }}</code></pre><ul><li>这样做能够基本满足需求，但是存在一些问题：<ul><li>处理逻辑互相之间有重复，稍有不慎就会出现Bug</li><li>处理流程中字符串拼接、加签和发请求的逻辑，在所有方法重复</li><li>实际方法的入参的参数类型和顺序，不一定和接口要求一致，容易出错</li><li>代码层面参数硬编码，无法清晰进行核对</li></ul></li></ul><h2 id="1-3-使用接口和反射优化代码"><a href="#1-3-使用接口和反射优化代码" class="headerlink" title="1.3 使用接口和反射优化代码"></a>1.3 使用接口和反射优化代码</h2><h3 id="1-3-1-实现定义了所有接口参数的POJO类"><a href="#1-3-1-实现定义了所有接口参数的POJO类" class="headerlink" title="1.3.1 实现定义了所有接口参数的POJO类"></a>1.3.1 实现定义了所有接口参数的POJO类</h3><pre><code>@Datapublic class CreateUserAPI {    private String name;    private String identity;    private String mobile;    private int age;}</code></pre><h3 id="1-3-2-定义注解本身"><a href="#1-3-2-定义注解本身" class="headerlink" title="1.3.2 定义注解本身"></a>1.3.2 定义注解本身</h3><pre><code>@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Documented@Inheritedpublic @interface BankAPI {    String desc() default &quot;&quot;;    String url() default &quot;&quot;;}@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.FIELD)@Documented@Inheritedpublic @interface BankAPIField {    int order() default -1;    int length() default -1;    String type() default &quot;&quot;;}</code></pre><h3 id="1-3-3-反射配合注解实现动态的接口参数组装"><a href="#1-3-3-反射配合注解实现动态的接口参数组装" class="headerlink" title="1.3.3 反射配合注解实现动态的接口参数组装"></a>1.3.3 反射配合注解实现动态的接口参数组装</h3><pre><code>private static String remoteCall(AbstractAPI api) throws IOException {    //从BankAPI注解获取请求地址    BankAPI bankAPI = api.getClass().getAnnotation(BankAPI.class);    bankAPI.url();    StringBuilder stringBuilder = new StringBuilder();    Arrays.stream(api.getClass().getDeclaredFields()) //获得所有字段            .filter(field -&gt; field.isAnnotationPresent(BankAPIField.class)) //查找标记了注解的字段            .sorted(Comparator.comparingInt(a -&gt; a.getAnnotation(BankAPIField.class).order())) //根据注解中的order对字段排序            .peek(field -&gt; field.setAccessible(true)) //设置可以访问私有字段            .forEach(field -&gt; {                //获得注解                BankAPIField bankAPIField = field.getAnnotation(BankAPIField.class);                Object value = &quot;&quot;;                try {                    //反射获取字段值                    value = field.get(api);                } catch (IllegalAccessException e) {                    e.printStackTrace();                }                //根据字段类型以正确的填充方式格式化字符串                switch (bankAPIField.type()) {                    case &quot;S&quot;: {                        stringBuilder.append(String.format(&quot;%-&quot; + bankAPIField.length() + &quot;s&quot;, value.toString()).replace(&#39; &#39;, &#39;_&#39;));                        break;                    }                    case &quot;N&quot;: {                        stringBuilder.append(String.format(&quot;%&quot; + bankAPIField.length() + &quot;s&quot;, value.toString()).replace(&#39; &#39;, &#39;0&#39;));                        break;                    }                    case &quot;M&quot;: {                        if (!(value instanceof BigDecimal))                            throw new RuntimeException(String.format(&quot;{} 的 {} 必须是BigDecimal&quot;, api, field));                        stringBuilder.append(String.format(&quot;%0&quot; + bankAPIField.length() + &quot;d&quot;, ((BigDecimal) value).setScale(2, RoundingMode.DOWN).multiply(new BigDecimal(&quot;100&quot;)).longValue()));                        break;                    }                    default:                        break;                }            });    //签名逻辑   stringBuilder.append(DigestUtils.md2Hex(stringBuilder.toString()));    String param = stringBuilder.toString();    long begin = System.currentTimeMillis();    //发请求    String result = Request.Post(&quot;http://localhost:45678/reflection&quot; + bankAPI.url())            .bodyString(param, ContentType.APPLICATION_JSON)            .execute().returnContent().asString();    log.info(&quot;调用银行API {} url:{} 参数:{} 耗时:{}ms&quot;, bankAPI.desc(), bankAPI.url(), param, System.currentTimeMillis() - begin);    return result;}</code></pre><p>通过反射来动态获得class的信息，并在runtime的时候完成组装过程。这样做的好处是开发的时候会方便直观很多，然后将逻辑与细节隐藏起来，并且集中放到了一个方法当中，减少了重复，以及维护当中bug的出现。</p><h3 id="1-3-4-在代码中的应用"><a href="#1-3-4-在代码中的应用" class="headerlink" title="1.3.4 在代码中的应用"></a>1.3.4 在代码中的应用</h3><pre><code>@BankAPI(url = &quot;/bank/createUser&quot;, desc = &quot;创建用户接口&quot;)@Datapublic class CreateUserAPI extends AbstractAPI {    @BankAPIField(order = 1, type = &quot;S&quot;, length = 10)    private String name;    @BankAPIField(order = 2, type = &quot;S&quot;, length = 18)    private String identity;    @BankAPIField(order = 4, type = &quot;S&quot;, length = 11) //注意这里的order需要按照API表格中的顺序    private String mobile;    @BankAPIField(order = 3, type = &quot;N&quot;, length = 5)    private int age;}@BankAPI(url = &quot;/bank/pay&quot;, desc = &quot;支付接口&quot;)@Datapublic class PayAPI extends AbstractAPI {    @BankAPIField(order = 1, type = &quot;N&quot;, length = 20)    private long userId;    @BankAPIField(order = 2, type = &quot;M&quot;, length = 10)    private BigDecimal amount;}</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-案例分析&quot;&gt;&lt;a href=&quot;#1-案例分析&quot; class=&quot;headerlink&quot; title=&quot;1. 案例分析&quot;&gt;&lt;/a&gt;1. 案例分析&lt;/h1&gt;&lt;h2 id=&quot;1-1-案例场景&quot;&gt;&lt;a href=&quot;#1-1-案例场景&quot; class=&quot;headerlink
      
    
    </summary>
    
    
      <category term="BackEnd" scheme="https://www.llchen60.com/categories/BackEnd/"/>
    
    
      <category term="Java" scheme="https://www.llchen60.com/tags/Java/"/>
    
      <category term="Annotations" scheme="https://www.llchen60.com/tags/Annotations/"/>
    
      <category term="Reflection" scheme="https://www.llchen60.com/tags/Reflection/"/>
    
      <category term="注解" scheme="https://www.llchen60.com/tags/%E6%B3%A8%E8%A7%A3/"/>
    
      <category term="反射" scheme="https://www.llchen60.com/tags/%E5%8F%8D%E5%B0%84/"/>
    
  </entry>
  
</feed>
